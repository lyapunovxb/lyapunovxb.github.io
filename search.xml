<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Git1_gitå‘½ä»¤</title>
    <url>/2025/11/04/Git/Git1_git%20%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[git commmit -vå’Œgit push origin developä¹‹åï¼Œæˆ‘æ€ä¹ˆæ’¤é”€æœ¬æ¬¡commitæäº¤çš„ä»£ç ï¼Œå°†origin developçš„ä»“åº“è¿”å›å€¼æäº¤ä¹‹å‰çš„çŠ¶æ€ï¼Ÿ
åœ¨Gitä¸­ï¼Œå¦‚æœä½ æƒ³è¦æ’¤é”€æœ€è¿‘çš„ä¸€æ¬¡æäº¤å¹¶å°†ä»“åº“çŠ¶æ€å›é€€åˆ°æäº¤ä¹‹å‰ï¼Œä½ å¯ä»¥ä½¿ç”¨git resetå‘½ä»¤ã€‚è¿™é‡Œæœ‰å‡ ç§æ–¹æ³•å¯ä»¥æ’¤é”€æäº¤ï¼š

ä½¿ç”¨git reset --softï¼š

è¿™ä¸ªå‘½ä»¤ä¼šå°†HEADæŒ‡é’ˆç§»åŠ¨åˆ°æŒ‡å®šçš„æäº¤ï¼Œä½†æ˜¯ä¸ä¼šæ”¹å˜å·¥ä½œç›®å½•å’Œç´¢å¼•çš„çŠ¶æ€ã€‚è¿™æ„å‘³ç€ä½ çš„æ›´æ”¹ä»ç„¶åœ¨æš‚å­˜åŒºï¼ˆstagedï¼‰ï¼Œä½ å¯ä»¥é‡æ–°æäº¤ã€‚
å‘½ä»¤ï¼šgit reset --soft HEAD~1ï¼ˆå°†HEADæŒ‡é’ˆå›é€€åˆ°ä¸Šä¸€ä¸ªæäº¤ï¼‰


ä½¿ç”¨git reset --mixedï¼ˆé»˜è®¤é€‰é¡¹ï¼‰ï¼š

è¿™ä¸ªå‘½ä»¤ä¼šå°†HEADæŒ‡é’ˆç§»åŠ¨åˆ°æŒ‡å®šçš„æäº¤ï¼Œå¹¶ä¸”å°†ç´¢å¼•ï¼ˆstaged changesï¼‰é‡ç½®åˆ°HEADçš„çŠ¶æ€ï¼Œä½†æ˜¯ä¸ä¼šæ”¹å˜å·¥ä½œç›®å½•ã€‚
å‘½ä»¤ï¼šgit reset --mixed HEAD~1ï¼ˆå°†HEADå’Œç´¢å¼•å›é€€åˆ°ä¸Šä¸€ä¸ªæäº¤ï¼Œä½†å·¥ä½œç›®å½•ä¸å˜ï¼‰


ä½¿ç”¨git reset --hardï¼š

è¿™ä¸ªå‘½ä»¤ä¼šå°†HEADæŒ‡é’ˆã€ç´¢å¼•å’Œå·¥ä½œç›®å½•éƒ½é‡ç½®åˆ°æŒ‡å®šçš„æäº¤çŠ¶æ€ã€‚è¿™æ„å‘³ç€æ‰€æœ‰æœªæäº¤çš„æ›´æ”¹éƒ½ä¼šä¸¢å¤±ã€‚
å‘½ä»¤ï¼šgit reset --hard HEAD~1ï¼ˆå°†HEADã€ç´¢å¼•å’Œå·¥ä½œç›®å½•éƒ½å›é€€åˆ°ä¸Šä¸€ä¸ªæäº¤ï¼‰



å¦‚æœä½ å·²ç»å°†ä»£ç æ¨é€åˆ°äº†è¿œç¨‹ä»“åº“ï¼ˆæ¯”å¦‚origin developï¼‰ï¼Œå¹¶ä¸”æƒ³è¦æ’¤é”€è¿™æ¬¡æ¨é€ï¼Œä½ å¯ä»¥ä½¿ç”¨git pushå‘½ä»¤é…åˆ--forceé€‰é¡¹æ¥å¼ºåˆ¶æ¨é€æœ¬åœ°çš„æ›´æ”¹åˆ°è¿œç¨‹ä»“åº“ã€‚ä½†æ˜¯è¯·æ³¨æ„ï¼Œè¿™ä¼šè¦†ç›–è¿œç¨‹ä»“åº“çš„å†å²ï¼Œæ‰€ä»¥åªæœ‰åœ¨ä½ ç¡®å®šè¿œç¨‹ä»“åº“æ²¡æœ‰å…¶ä»–äººçš„æäº¤æˆ–è€…ä½ ä¸ä»‹æ„ä¸¢å¤±è¿™äº›æäº¤æ—¶æ‰åº”è¯¥ä½¿ç”¨ã€‚
å‘½ä»¤å¦‚ä¸‹ï¼š
git push origin develop --force
æˆ–è€…ï¼Œå¦‚æœä½ ä½¿ç”¨çš„æ˜¯Git 2.0æˆ–æ›´é«˜ç‰ˆæœ¬ï¼Œå¯ä»¥ä½¿ç”¨æ›´å®‰å…¨çš„--force-with-leaseé€‰é¡¹ï¼Œå®ƒä¼šåœ¨è¿œç¨‹åˆ†æ”¯æ²¡æœ‰æ–°çš„æäº¤æ—¶æ‰å…è®¸å¼ºåˆ¶æ¨é€ã€‚
å‘½ä»¤å¦‚ä¸‹ï¼š
git push origin develop --force-with-lease
è­¦å‘Šï¼šä½¿ç”¨--forceé€‰é¡¹ä¼šè¦†ç›–è¿œç¨‹ä»“åº“çš„å†å²ï¼Œè¿™æ˜¯ä¸€ä¸ªç ´åæ€§çš„æ“ä½œã€‚åœ¨å›¢é˜Ÿåä½œçš„ç¯å¢ƒä¸­ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´å…¶ä»–åä½œè€…çš„å·¥ä½œä¸¢å¤±ï¼Œæ‰€ä»¥è¯·è°¨æ…ä½¿ç”¨ã€‚
]]></content>
      <categories>
        <category>å¼€å‘</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>å¼€å‘</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Git2_gerritä»“åº“ä»£ç æäº¤æµç¨‹</title>
    <url>/2025/11/04/Git/Git2_gerrit%E4%BB%93%E5%BA%93%E4%BB%A3%E7%A0%81%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[
ç”Ÿæˆä¸ªäººå¯†é’¥

ssh-keygen -C wangxiangbo_JTAGI@cmss.chinamobile.com

ä¿®æ”¹configæ–‡ä»¶ï¼Œå¢åŠ gerritä»“åº“é…ç½®ï¼Œå¢åŠ åˆšç”Ÿæˆçš„ä¸ªäººå¯†é’¥id_ed25519

host gerritport 29418hostname gerrit.cmss.comuser wangxiangbo_JTAGIIdentityFile ~/.ssh/id_ed25519

åœ¨gerritä¸­settingè®¾ç½®ä¸­ï¼Œå¢åŠ sshçš„å…¬é’¥ï¼Œå³id_ed25519.pub
è®¾ç½®é‚®ç®±ï¼Œé‚®ç®±åwangxiangbo@cmss.chinamobile.comä¸ºä¸å¸¦JTAGIåç¼€çš„é‚®ç®±ã€‚
ç”¨ssh cloneä»£ç ä»“åº“

git clone &quot;ssh://wangxiangbo_JTAGI@gerrit.cmss.com:29418/AGI/CM_OPTIMUS&quot; &amp;&amp; scp -p -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg &quot;CM_OPTIMUS/.git/hooks/&quot;

æŸ¥çœ‹åˆ†æ”¯

git branch -a

æŸ¥çœ‹è¿œç«¯åˆ†æ”¯

git branch -r

é€šè¿‡è¿œç«¯origin/developä»“åº“ï¼Œåˆ›å»ºä¸€ä¸ªæœ¬åœ°developå¼€å‘åˆ†æ”¯

git checkout -b develop origin/develop

ä¿®æ”¹ä»£ç åï¼Œaddä¹‹åå¹¶commitæäº¤

git commit -v

æ¨é€è¿œç«¯origin/developä»“åº“

git push origin HEAD:refs/for/develop

åœ¨æ¨é€è¿‡ç¨‹ä¸­ä¼šå‡ºç°ç¼ºå¤± Change-Id çš„é”™è¯¯

gitdir=$(git rev-parse --git-dir); scp -p -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg $&#123;gitdir&#125;/hooks/
æ‰§è¡Œå®Œæ¯•åå¦‚æœå‡ºç°subsystem request failed on channel 0ï¼Œåˆ™å°†-pä¿®æ”¹ä¸º-O
gitdir=$(git rev-parse --git-dir); scp -O -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg $&#123;gitdir&#125;/hooks/
å°†æœ¬æ¬¡commitæäº¤çš„æœ«å°¾åŠ ä¸ŠChange-Id
git commit --amend --no-edit
å†æ¬¡push
git push origin HEAD:refs/for/develop

æäº¤å®Œæˆä¹‹åï¼Œæ‰“å¼€gerritï¼Œæ‰¾åˆ°developåˆ†æ”¯çš„gitweb
é€‰æ‹©review
æ‰¾åˆ°åˆšåˆšæäº¤çš„ä»£ç 
ç‚¹å‡»add reviewerï¼Œè¯„å®¡äººè¦æœ€å°‘è¦ä¸¤ä¸ªä»¥ä¸Š
ç¬¬ä¸€æ¬¡ä¹‹åæäº¤ä»£ç æ­¥éª¤

git pull origingit add xxxgit commit -v git push origin HEAD:refs/for/develop
]]></content>
      <categories>
        <category>å¼€å‘</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>å¼€å‘</tag>
        <tag>gerrit</tag>
      </tags>
  </entry>
  <entry>
    <title>Git3_gitlabä»“åº“ä»£ç æäº¤æµç¨‹</title>
    <url>/2025/11/04/Git/Git3_gitlab%E4%BB%93%E5%BA%93%E4%BB%A3%E7%A0%81%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[
gitlabä¸Šforkä¸»ä»“åº“ï¼Œç”Ÿæˆä¸ªäººçš„è¿œç«¯ä»“åº“origin/develop
ä¿®æ”¹gitæäº¤ç”¨æˆ·é…ç½®ä¸ºä¹å¤©è´¦å·

git config --global --listgit config --global user.name wangxiangbo_JTAGIgit config --global user.email wangxiangbo_JTAGI@cmss.chinamobile.com

æ‹‰å–ä¸ªäººçš„è¿œç«¯ä»“åº“

git clone http://gitlab.cmss.com/wangxiangbo/CM_OPTIMUS.git 

æŸ¥çœ‹åˆ†æ”¯

git branch -a

æŸ¥çœ‹è¿œç¨‹ä»“åº“åˆ†æ”¯

git branch -r

é€šè¿‡ä¸ªäººçš„è¿œç«¯origin/developä»“åº“ï¼Œåˆ›å»ºä¸€ä¸ªæœ¬åœ°developå¼€å‘åˆ†æ”¯

git checkout -b develop origin/develop

åˆ—å‡ºæ‰€æœ‰çš„è¿œç¨‹ä»“åº“ä»¥åŠå¯¹åº”çš„ URL

git remote -v

å°†è¿œç¨‹ä»“åº“åœ°å€æ·»åŠ åˆ°æœ¬åœ°Gitä»“åº“çš„è¿œç¨‹ä»“åº“åˆ—è¡¨ä¸­ï¼Œæœ¬åœ°çš„upstream/developä»“åº“ä¼šå’Œgitlabè¿œç¨‹ä»“åº“å…³è”èµ·æ¥

git remote add upstream http://gitlab.cmss.com/AGI/CM_OPTIMUS.git

upstream/developè¿œç«¯ä»“åº“æ‹‰å–æœ€æ–°çš„ä»£ç 

git fetch upstream

å°†upstream/developè¿œç«¯ä»“åº“çš„æœ€æ–°ä»£ç åˆå¹¶åˆ°æœ¬åœ°çš„developåˆ†æ”¯ä¸­

git merge upstream/develop

å°†æœ¬åœ°developæœ€æ–°çš„ä»£ç æ¨åˆ°ä¸ªäººçš„è¿œç«¯ä»“åº“origin/developä¸­

git push origin develop

æäº¤developåˆ°origin/developä»£ç ä¹‹å‰ï¼Œä¿è¯è‡ªå·±çš„æœ¬åœ°ä¸ªäººçš„è¿œç«¯ä»“åº“origin/developå’Œæœ¬åœ°gitlabçš„è¿œç«¯ä»“åº“upstream/developä¿æŒä¸€è‡´å†æäº¤ï¼Œé˜²æ­¢åç»­origin/developåˆå¹¶æ‰“upstream/developä¸Šå‡ºç°å†²çªã€‚

#æœ¬åœ°ä»£ç ä¿®æ”¹åæ‹‰å–æœ€æ–°ä»£ç #1.æ‹‰å–æºä»£ç git fetch upstream #2.æš‚å­˜æœ¬åœ°ä¿®æ”¹çš„ä»£ç git stash#3.åˆå¹¶æºä»£ç åˆ°å½“å‰çš„developä»“åº“git merge upstream/develop#4.å°†æœ¬åœ°ä¿®æ”¹çš„æ¨é€åˆ°è¿œç¨‹çš„originåˆ†æ”¯ï¼Œä½¿origin/developå’Œupstream/developä¿æŒä¸€è‡´git push origin develop#5.å°†æš‚å­˜çš„ä»£ç å¼¹å‡ºæ¥git stash pop

souretreeä¸Šaddéœ€è¦æäº¤çš„ä»£ç 
æäº¤addä¹‹åçš„ä»£ç 

git commit -v
#ä¿®æ”¹ä¸‹æ–¹çš„æäº¤ä¿¡æ¯ï¼Œç¬¬ä¸€è¡Œä¸ºæäº¤æè¿°ä¿¡æ¯deepseek-7Bé¢„è®­ç»ƒCode Source From: Self CodeDescription: deepseek-7Bé¢„è®­ç»ƒJira: #CMOPTIMUS-1089å¸‚åœºé¡¹ç›®ç¼–å·ï¼ˆåç§°ï¼‰ï¼šCM_OPTIMUS

å°†æäº¤å®Œä¹‹åçš„ä»£ç åˆå¹¶åˆ°origin/developä¸Š

git push origin develop

gitlabä¸­æ‰¾çš„åˆ°è‡ªå·±ä»“åº“ä¸‹CM_OPTIMUS
æ‰¾åˆ°Merge requestsï¼Œç„¶åç‚¹å‡»å¼€å§‹ä¸€ä¸ªåˆå¹¶
ç„¶åé€‰æ‹©è‡ªå·±çš„åˆ†æ”¯ä¸ºdevelopï¼Œè¿˜æœ‰æ—è¾¹çš„åˆ†æ”¯developï¼Œç„¶åç‚¹å‡»æœ€ä¸‹æ–¹çš„mergeå³å¯ã€‚è¯¥æ“ä½œæ˜¯å°†è‡ªå·±ä¸ªäººè¿œç«¯çš„orgin/developä»“åº“æäº¤ä¿®æ”¹åçš„ä»£ç åˆå¹¶åˆ°gitlabè¿œç«¯çš„ä¸»ä»“åº“ä¸­ï¼ˆupstream/developå·²å’Œå…¶å…³è”ï¼Œå¹¶é€šè¿‡git fetchä¿æŒæœ€æ–°ï¼‰
é€‰ä¸­è‡ªå·±åˆšåˆšæäº¤åˆ°origin/developä¸Šçš„ä»£ç ï¼Œåˆå¹¶åˆ°AGI/CM_OPTIMUSçš„developåˆ†æ”¯ä¸Š
åœ¨Writeé‡Œæ·»åŠ åˆå¹¶äººä¿¡æ¯

]]></content>
      <categories>
        <category>å¼€å‘</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>å¼€å‘</tag>
        <tag>gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title>å‰ç«¯1_è°ƒç”¨apiæ¥å£çš„å†™æ³•</title>
    <url>/2025/11/06/%E5%89%8D%E7%AB%AF/%E5%89%8D%E7%AB%AF1_%E8%B0%83%E7%94%A8api%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%86%99%E6%B3%95/</url>
    <content><![CDATA[Getè¯·æ±‚å†™æ³•
1. çœ‹Parametersä¸­æ˜¯å¦æœ‰éœ€è¦å¸¦å‚æ•°ï¼Œå…¶ä¸­Authorizationä¸ºæˆæƒè®¤è¯çš„tokenå¯ä»¥ä¸ç”¨è€ƒè™‘ã€‚
1.1 è‹¥æ˜¯æ²¡æœ‰å…¶ä½™çš„å‚æ•°é™å®šï¼Œé‚£ä¹ˆSyncRequestFuncType&lt;è¯·æ±‚å‚æ•°ç±»å‹,å“åº”è¿”å›å‚æ•°ç±»å‹&gt;ï¼Œç¬¬ä¸€ä¸ªå‚æ•°å°±ä¸ºvoidæˆ–è€…undefinedã€‚è¿”å›ç±»å‹å¯ä»¥ä½¿ç”¨anyï¼Œç­‰è·å–åˆ°å“åº”å‚æ•°ä¹‹åï¼Œåœ¨å¯¹å…¶é€šè¿‡Interface/typeè¿›è¡Œæ˜ç¡®ã€‚
export const getApi: SyncRequestFuncType&lt;void,responseType&gt; = () =&gt; &#123;return javaAxios(&#123; method: &quot;get&quot;, url: &quot;xxx/xxxx&quot;,&#125;);&#125;;
1.2 è‹¥æ˜¯æœ‰å…¶ä½™å‚æ•°é™å®šï¼Œè¿™æ—¶éœ€è¦ç®­å¤´å‡½æ•°ä¸­éœ€è¦å¸¦ç€paramsæ¥è¿›è¡Œè¯·æ±‚ï¼Œä¸”reuturnä¸­ä¹Ÿéœ€è¦å°†paramså¸¦ç€ã€‚
1.2.1 `å•ä¸ªå‚æ•°`ï¼Œä¾‹å¦‚idï¼Œè¿™æ—¶åœ¨è¯·æ±‚å‚æ•°ç±»å‹ä¸­ï¼Œéœ€è¦æ˜ç¡®å‡ºè¯·æ±‚å‚æ•°çš„ç±»å‹&lt;&#123;id:string&#125;,responseType&gt;ï¼Œè¿™æ—¶è¯·æ±‚å‚æ•°çš„ç±»å‹ä¸ºå¯¹è±¡&#123;&#125;çš„å½¢å¼ï¼Œå¯ä»¥ç›´æ¥åœ¨å°–æ‹¬å·ä¸­å†™å‡ºæ¥ï¼Œä¹Ÿå¯ä»¥é€šè¿‡å¼•å…¥å®šä¹‰çš„Interface/typeç±»å‹æ¥å†™&lt;IdType,responseType&gt;ã€‚

export const getApi: SyncRequestFuncType&lt; &#123;id:string&#125;, void&gt; = (params) =&gt; &#123;return javaAxios(&#123; method: &quot;get&quot;, url: &quot;xxx/xxxx&quot;, params,&#125;);&#125;;
1.2.2 `å¤šä¸ªå‚æ•°`ï¼Œä¾‹å¦‚emailï¼Œtype...ï¼Œå› ä¸ºå‚æ•°æ¯”è¾ƒå¤šï¼Œæœ€å¥½é€šè¿‡Interface/typeçš„æ–¹å¼SendEmailCaptchaForUserInfoModifyTypeæ¥æ˜ç¡®è¯·æ±‚å‚æ•°ç±»å‹ã€‚

export type SendEmailCaptchaForUserInfoModifyType = &#123;type: ModifyType;email?: string;&#125;;
export const getApi: SyncRequestFuncType&lt;SendEmailCaptchaForUserInfoModifyType,void&gt; = (params) =&gt; &#123;return javaAxios(&#123; method: &quot;get&quot;, url: &quot;xxx/xxx&quot;, params,&#125;);&#125;;
Postè¯·æ±‚å†™æ³•
1. çœ‹çœ‹Parametersä¸­æ˜¯å¦æœ‰éœ€è¦å¸¦å‚æ•°ï¼Œå…¶ä¸­Authorizationä¸ºæˆæƒè®¤è¯çš„tokenå¯ä»¥ä¸ç”¨è€ƒè™‘ã€‚
1.1 è‹¥æ˜¯æ²¡æœ‰å…¶ä½™çš„å‚æ•°é™å®šï¼Œé‚£ä¹ˆSyncRequestFuncType&lt;è¯·æ±‚å‚æ•°ç±»å‹,å“åº”è¿”å›å‚æ•°ç±»å‹&gt;ï¼Œç¬¬ä¸€ä¸ªå‚æ•°å°±ä¸ºvoidæˆ–è€…undefinedã€‚è¿”å›ç±»å‹å¯ä»¥ä½¿ç”¨anyï¼Œç­‰è·å–åˆ°å“åº”å‚æ•°ä¹‹åï¼Œåœ¨å¯¹å…¶é€šè¿‡Interface/typeè¿›è¡Œæ˜ç¡®ã€‚
export const clearVideoRecycle: SyncRequestFuncType&lt;void, void&gt; = () =&gt; &#123;return javaAxios(&#123; method: &quot;post&quot;, url: &quot;xxx/xxx&quot;,&#125;);&#125;;
1.2 è‹¥æ˜¯æœ‰å…¶ä½™å‚æ•°é™å®šï¼Œè¿™æ—¶éœ€è¦ç®­å¤´å‡½æ•°ä¸­éœ€è¦å¸¦ç€paramsæ¥è¿›è¡Œè¯·æ±‚ï¼Œä¸”reuturnä¸­ä¹Ÿéœ€è¦å°†paramså¸¦ç€ã€‚
1.2.1 å•ä¸ªå‚æ•°æ—¶

export const cancelSubscribeVideo: SyncRequestFuncType&lt;&#123; themeId: string &#125;,any&gt; = (params) =&gt; &#123;return javaAxios(&#123; method: &quot;post&quot;, url: &quot;xxx/xxx&quot;, params,&#125;);&#125;;
1.2.2 å¤šä¸ªå‚æ•°æ—¶

export type SendEmailCaptchaForUserInfoModifyType = &#123;type: ModifyType;email?: string;&#125;;
export const sendEmailCaptchaForUserInfoModify: SyncRequestFuncType&lt;SendEmailCaptchaForUserInfoModifyType,void&gt; = (params) =&gt; &#123;return javaAxios(&#123; method: &quot;get&quot;, url: &quot;xxx/xxx&quot;, params,&#125;);&#125;;
2. å½“å‡ºç°è¯·æ±‚ä½“Request bodyæ—¶ï¼Œè¿™æ—¶å°±éœ€è¦åœ¨returnçš„javaAxiosä¸­æ·»åŠ é”®å€¼å¯¹data: paramsï¼Œæ¥å°†å‚æ•°æ·»åŠ åˆ°è¯·æ±‚ä½“ä¸­ä¼ é€’è¿‡å»ã€‚
2.1 Request bodyä¸ºï¼š
[&quot;string&quot;]
export const batchDeleteVideo: SyncRequestFuncType&lt;string[], void&gt; = (params) =&gt; &#123;return javaAxios(&#123; method: &quot;post&quot;, url: &quot;videos/themes/batchDel&quot;, data: params,&#125;);&#125;;
2.2 Request bodyä¸ºï¼šè¿™æ—¶å€™éœ€è¦åœ¨dataä¸­è¿›ä¸€æ­¥åœ¨çº¦æŸä¸€ä¸‹ï¼Œä½¿å…¶å¯¹åº”apiçš„è¯·æ±‚ä½“çš„æ ¼å¼ã€‚
&#123;&quot;themes&quot;: [ &quot;string&quot;]&#125;
export const sortVideos: SyncRequestFuncType&lt;string[], void&gt; = (params) =&gt; &#123;return javaAxios(&#123; method: &quot;post&quot;, url: &quot;videos/themes/changeVideoThemesSort&quot;, data: &#123; themes: params &#125;,&#125;);&#125;;
]]></content>
      <categories>
        <category>å‰ç«¯</category>
      </categories>
      <tags>
        <tag>å‰ç«¯</tag>
        <tag>Javascript</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux1_kimié—®é¢˜</title>
    <url>/2025/11/04/Linux/Linux1_kimi%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[
æƒé‡è½¬æ¢ä»£ç æ¢è¡Œç¬¦
Kimi: å‡ºç°è¿™äº›é”™è¯¯é€šå¸¸æ˜¯å› ä¸ºè„šæœ¬æ–‡ä»¶åœ¨ä¸åŒçš„æ“ä½œç³»ç»Ÿç¯å¢ƒä¸‹è¢«ç¼–è¾‘æˆ–åˆ›å»ºï¼Œå¯¼è‡´æ–‡ä»¶çš„æ¢è¡Œç¬¦ä¸ä¸€è‡´ã€‚Unix/Linuxç³»ç»Ÿä½¿ç”¨LFï¼ˆLine Feedï¼Œ\nï¼‰ä½œä¸ºæ¢è¡Œç¬¦ï¼Œè€ŒWindowsç³»ç»Ÿä½¿ç”¨CR+LFï¼ˆCarriage Return + Line Feedï¼Œ\r\nï¼‰ä½œä¸ºæ¢è¡Œç¬¦ã€‚
é”™è¯¯ä¿¡æ¯ä¸­çš„ $'\r': command not found è¡¨ç¤ºè„šæœ¬å°è¯•æ‰§è¡Œä¸€ä¸ªä»¥ \r å¼€å¤´çš„å‘½ä»¤ï¼Œè¿™é€šå¸¸æ˜¯å› ä¸ºè„šæœ¬æ–‡ä»¶ä¸­çš„æ¢è¡Œç¬¦æ˜¯ \r\n è€Œä¸æ˜¯ \nã€‚
æ­¤å¤–ï¼Œsyntax error near unexpected token 'elif' é”™è¯¯è¡¨æ˜è„šæœ¬è§£æå™¨åœ¨é‡åˆ° elif æ—¶é‡åˆ°äº†è¯­æ³•é”™è¯¯ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸º elif ä¹‹å‰çš„è¡Œï¼ˆå¯èƒ½æ˜¯ç¬¬59è¡Œï¼‰ä»¥ \r\n ç»“å°¾ï¼Œå¯¼è‡´ elif è¡Œå®é™…ä¸Šæ˜¯ä»ä¸€ä¸ªæ–°çš„è¡Œå¼€å§‹ï¼Œè€Œä¸æ˜¯åœ¨é¢„æœŸçš„ä½ç½®ã€‚
è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½ å¯ä»¥ä½¿ç”¨ dos2unix å·¥å…·å°†è„šæœ¬æ–‡ä»¶ä»Windowsæ ¼å¼è½¬æ¢ä¸ºUnix/Linuxæ ¼å¼ã€‚å¦‚æœä½ æ²¡æœ‰ dos2unix å·¥å…·ï¼Œå¯ä»¥ä½¿ç”¨ sed å‘½ä»¤æ¥ç§»é™¤ \r å­—ç¬¦ï¼š

sed -i &#x27;s/\r$//&#x27; hf2mcore_convertor.sh
è¿™æ¡å‘½ä»¤ä¼šæŸ¥æ‰¾æ–‡ä»¶ä¸­çš„æ‰€æœ‰è¡Œï¼Œå¹¶ç§»é™¤æ¯è¡Œæœ«å°¾çš„ \r å­—ç¬¦ã€‚
å¦‚æœä½ æœ‰ dos2unix å·¥å…·ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š
dos2unix hf2mcore_convertor.sh
è¿™å°†æŠŠæ–‡ä»¶ä»Windowsæ ¼å¼è½¬æ¢ä¸ºUnix/Linuxæ ¼å¼ã€‚
åœ¨è¿è¡Œè¿™äº›å‘½ä»¤ä¹‹åï¼Œå†æ¬¡æ‰§è¡Œè„šæœ¬åº”è¯¥å°±ä¸ä¼šå‡ºç°è¿™äº›é”™è¯¯äº†ã€‚
2. ä¿®æ”¹æ–‡ä»¶é‡åˆ°Read-only file system
Kimi: å½“ä½ é‡åˆ° â€œRead-only file systemâ€ é”™è¯¯æ—¶ï¼Œé€šå¸¸æ„å‘³ç€ä½ çš„æ–‡ä»¶ç³»ç»Ÿå‡ºäºæŸç§åŸå› è¢«æŒ‚è½½ä¸ºåªè¯»æ¨¡å¼ã€‚è¿™å¯èƒ½æ˜¯ç”±äºæ–‡ä»¶ç³»ç»ŸæŸåã€ç£ç›˜ç¡¬ä»¶é—®é¢˜ã€æˆ–è€…æŒ‚è½½æ—¶æŒ‡å®šäº†é”™è¯¯çš„å‚æ•°ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›è§£å†³æ­¥éª¤ï¼š

æ£€æŸ¥æ–‡ä»¶ç³»ç»ŸæŒ‚è½½çŠ¶æ€ï¼š
ä½¿ç”¨ mount å‘½ä»¤æŸ¥çœ‹æ‰€æœ‰æŒ‚è½½çš„æ–‡ä»¶ç³»ç»ŸåŠå…¶çŠ¶æ€ã€‚ä½ å¯ä»¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ï¼š

mount | grep /path/to/mount/point
æŸ¥çœ‹æŒ‚è½½ç‚¹æ˜¯å¦ä¸ºåªè¯»ï¼ˆroï¼‰ã€‚
2. é‡æ–°æŒ‚è½½æ–‡ä»¶ç³»ç»Ÿä¸ºè¯»å†™æ¨¡å¼ï¼š
å¦‚æœæ–‡ä»¶ç³»ç»Ÿè¢«æŒ‚è½½ä¸ºåªè¯»ï¼Œä½ å¯ä»¥å°è¯•é‡æ–°æŒ‚è½½å®ƒä¸ºè¯»å†™æ¨¡å¼ã€‚ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š
sudo mount -o remount,rw /path/to/mount/point
è¯·å°† /path/to/mount/point æ›¿æ¢ä¸ºå®é™…çš„æŒ‚è½½ç‚¹è·¯å¾„ã€‚
3. æ£€æŸ¥å’Œä¿®å¤æ–‡ä»¶ç³»ç»Ÿï¼š
å¦‚æœé‡æ–°æŒ‚è½½ä¸èµ·ä½œç”¨ï¼Œå¯èƒ½æ˜¯æ–‡ä»¶ç³»ç»ŸæŸåã€‚ä½ å¯ä»¥ä½¿ç”¨ fsck å‘½ä»¤æ¥æ£€æŸ¥å’Œä¿®å¤æ–‡ä»¶ç³»ç»Ÿã€‚ä¾‹å¦‚ï¼Œå¯¹äº ext4 æ–‡ä»¶ç³»ç»Ÿï¼Œå¯ä»¥ä½¿ç”¨ï¼š
sudo fsck -f /dev/sda1
è¯·ç¡®ä¿æ›¿æ¢ /dev/sda1 ä¸ºä½ çš„å®é™…åˆ†åŒºã€‚
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>è¿ç»´</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>è¿ç»´2_k8så‘½ä»¤</title>
    <url>/2025/11/04/%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%90%E7%BB%B4/%E8%BF%90%E7%BB%B42_k8s%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[
è·å–ç‰¹å®šå‘½åç©ºé—´çš„è¯¦ç»†ä¿¡æ¯

kubectl get ns namespace-name -o wide

kubectlé€šè¿‡nsè·å–æ‰€æœ‰çš„podsçš„è¯¦ç»†ä¿¡æ¯

kubectl get pods -n &lt;ns_name&gt; -o wide

åˆ é™¤å¯åŠ¨å¤±è´¥çš„podï¼Œå¯ä»¥ç›´æ¥åˆ é™¤å¯åŠ¨æ—¶çš„yamlï¼Œå¦åˆ™podä¼šè‡ªåŠ¨é‡å¯

kubectl delete -f qwentest.yaml

æŸ¥çœ‹æ‰€æœ‰çš„nodeçš„lableæ ‡ç­¾ä¿¡æ¯

kubectl get node --show-labelskubectl get node xxx --show-labelskubectl get node --show-labels | grep model

æ‰“æ ‡ç­¾

kubectl label nodes ecs-jhjs-1234-003 key=vaule

åˆ é™¤æŸèŠ‚ç‚¹çš„lableæ ‡ç­¾

kubectl label nodes cce100-64-29-79.cce-stack.com model-

åˆ›å»ºä¸€ä¸ªå‘½åç©ºé—´namespace

kubectl create namespace xxx

æŸ¥çœ‹æ‰€æœ‰èŠ‚ç‚¹çš„æ ‡ç­¾

kubectl get nodes --show-labelskubectl get nodes --show-labels | grep model=llama2-70b
]]></content>
      <categories>
        <category>è¿ç»´</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>è¿ç»´</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>è¿ç»´1_dockerå‘½ä»¤</title>
    <url>/2025/11/04/%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%90%E7%BB%B4/%E8%BF%90%E7%BB%B41_docker%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[
å¯åŠ¨ä¸€ä¸ªå®¹å™¨

sudo docker run -it --name qwen --gpus all nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04
å¯åŠ¨ä¸€ä¸ªå®¹å™¨åï¼Œåˆé€šè¿‡exitå…³é—­ã€‚å‘ç°docker psæ²¡æœ‰äº†ï¼Œè¯¥å¦‚ä½•é‡æ–°å¯åŠ¨è¿™ä¸ªqwenå®¹å™¨ï¼Ÿ
docker ps -adocker start qwendocker exec -it å®¹å™¨id bash

æ‹‰å–ä¸€ä¸ªæ–°é•œåƒåï¼Œé€šè¿‡è¿™ä¸ªé•œåƒåˆ›å»ºä¸€ä¸ªå®¹å™¨ã€‚

docker run -it --name qwen --gpus all é•œåƒå:é•œåƒtags bash

å°†æ­¤æ—¶qwenå®¹å™¨æ‰“æˆé•œåƒ

docker commit -a &quot;wangxiangbo&quot; -m &quot;qwen 7B&quot; 02649afd9710 qwen-7b:v1.0
]]></content>
      <categories>
        <category>è¿ç»´</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>è¿ç»´</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>æµ‹è¯• Hexo æ ‡ç­¾åˆ†ç±»2</title>
    <url>/2025/10/31/%E9%A1%B5%E9%9D%A2%E6%B5%8B%E8%AF%95/test%20copy/</url>
    <content><![CDATA[è¿™æ˜¯æ–‡ç« æ­£æ–‡ã€‚
]]></content>
      <categories>
        <category>æµ‹è¯•</category>
      </categories>
      <tags>
        <tag>æµ‹è¯•2</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux2_Linuxå‘½ä»¤</title>
    <url>/2025/11/04/Linux/Linux2_linux%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[
å½“æ‰§è¡Œè®­ç»ƒä»»åŠ¡æ—¶ï¼Œæ‰‹åŠ¨å…³é—­è®­ç»ƒï¼Œæ­¤æ—¶gupèµ„æºä»ç„¶å ç”¨ï¼Œéœ€è¦æ‰‹åŠ¨killæ‰è¿›ç¨‹
æŸ¥è¯¢æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹å¹¶gerp finetune_lora_single_gpu.sh
ps aux | grep finetune_lora_single_gpu.sh
ps aux | grep finetune_lora_ds.sh
kill -9 372813 æ ¹æ®ä¸Šä¸€æ­¥æŸ¥å‡ºæ¥çš„è¿›ç¨‹å·ï¼Œç”¨kill -9å¼ºåˆ¶åˆ é™¤
nvidia-smi æŸ¥çœ‹æ˜¯å¦gpuæ˜¾å­˜å·²ç»æ¸…ç©º
æŸ¥çœ‹æŸä¸ªæ–‡ä»¶çš„å‰20è¡Œ
head -n 20 train_data_law.json
æŸ¥çœ‹å½“å‰æ–‡ä»¶å¤¹çš„å¤§å°
du -sh .
æŸ¥çœ‹å½“å‰æ–‡ä»¶å¤¹æŒ‚è½½çš„æ˜¯å“ªä¸€ä¸ªç›˜
df -h .
ä»å¤§åˆ°å°æŸ¥çœ‹å½“å‰æ–‡ä»¶å¤¹çš„ä¸‹çš„æ–‡ä»¶å¤§å°
du -sh * | sort -rh
æŸ¥çœ‹å½“å‰æ–‡ä»¶ä¸‹æ‰€æœ‰æ–‡ä»¶çš„å¤§å°
ls -lh
æŠŠä¸€ä¸ªåä¸ºYuan2çš„æ–‡ä»¶å¤¹çš„æ‰€æœ‰å†…å®¹ï¼ˆåŒ…æ‹¬è¯¥æ–‡ä»¶å¤¹åï¼‰ï¼Œcpåˆ°/mnt/users/wangxiangbo/nemo/modelç›®å½•ä¸‹ï¼Œä½¿æœ€åçš„ç›®å½•ä¸º /mnt/users/wangxiangbo/nemo/model/Yuan2
cp -a Yuan2 /mnt/users/wangxiangbo/nemo/model/
é€šè¿‡pidæŸ¥çœ‹æŸä¸ªè¿›ç¨‹çš„è¯¦ç»†ä¿¡æ¯
&lt;font style=&quot;color:rgb(56, 58, 66);background-color:rgb(250, 250, 250);&quot;&gt;ps -fp 167891&lt;/font&gt;
æŸ¥çœ‹å½“å‰ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶çš„ç£ç›˜å ç”¨æƒ…å†µ
du -ah | sort -hr | head -n 20
ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†megatron-coreæ–‡ä»¶å¤¹å‹ç¼©æˆä¸€ä¸ªåä¸ºmegatron-core.zipçš„zipæ–‡ä»¶
zip -r megatron-core.zip megatron-core 
ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†megatron-core.zipè§£å‹ç¼©
unzip megatron-core.zip 
hgä¸Šé¢ä¸‹è½½æ¨¡å‹

pip install -U huggingface_hubhuggingface-cli download bigscience/bloom-560m --local-dir bloom-560mhuggingface-cli download Qwen/Qwen2-7B-Instruct --local-dir Qwen2-7B-Instructhuggingface-cli download Qwen/Qwen2.5-7B-Instruct --local-dir Qwen2.5-7B-Instructhuggingface-cli download BAAI/IndustryCorpus_computer --repo-type dataset --local-dir IndustryCorpus_computerhuggingface-cli download BAAI/IndustryCorpus2_current_affairs_government_administration --repo-type dataset --local-dir government_administrationhuggingface-cli download ShengbinYue/DISC-Law-SFT --repo-type dataset --local-dir DISC-Law-SFThuggingface-cli download TigerResearch/sft_zh

å½’æ¡£å‹ç¼©æ–‡ä»¶

#-cï¼šåˆ›å»ºä¸€ä¸ªæ–°çš„å‹ç¼©æ–‡ä»¶ã€‚#-zï¼šé€šè¿‡ gzip å‹ç¼©æ–‡ä»¶ã€‚#-vï¼šæ˜¾ç¤ºè¯¦ç»†çš„å‹ç¼©è¿‡ç¨‹ã€‚#-fï¼šæŒ‡å®šå‹ç¼©åçš„æ–‡ä»¶åï¼Œè¿™é‡Œæ˜¯ colossalai.tar.gztar -czvf colossalai.tar.gz colossalai/  #-xï¼šè¡¨ç¤ºè§£å‹ã€‚#-zï¼šè¡¨ç¤ºè§£å‹ .gz æ ¼å¼çš„æ–‡ä»¶ã€‚#-vï¼šæ˜¾ç¤ºè§£å‹è¿‡ç¨‹ã€‚#-fï¼šæŒ‡å®šè§£å‹çš„æ–‡ä»¶tar -xzvf file.tar.gz
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>è¿ç»´</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>xinferenceæ¨ç†å®è·µ</title>
    <url>/2026/01/08/LLM/%E6%8E%A8%E7%90%86/xinference%E6%8E%A8%E7%90%86%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[NVç”Ÿæ€
å®¹å™¨åˆ›å»º
docker run -d --restart=always --name=xinference_wxb \ -v /mnt/users/wangxiangbo/xinference:/opt/xinference -e XINFERENCE_HOME=/opt/xinference -e XINFERENCE_MODEL_SRC=modelscope \ -p 9998:9997 --gpus all registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:latest xinference-local -H 0.0.0.0
docker run -d --restart=always --name=xinference_wxb \ -v /mnt/users/wangxiangbo/xinference:/opt/xinference -e XINFERENCE_HOME=/opt/xinference -e XINFERENCE_MODEL_SRC=hf-mirror.com \ -p 9998:9997 --gpus all registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:latest xinference-local -H 0.0.0.0
docker run -d --restart=always --name=xinference_wxb \ -v /mnt/users/wangxiangbo/xinference:/opt/xinference -e XINFERENCE_HOME=/opt/xinference -p 9998:9997 --gpus all registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:latest xinference-local -H 0.0.0.0
è¿›å…¥å®¹å™¨
docker exec -it xinference_wxb bash
æœåŠ¡åˆ›å»º
å¼€å¯xinference-localæœåŠ¡
root@979723eca011:/opt/inference/examples# xinference-local


webç«¯è°ƒç”¨


é€‰æ‹©å¾…æ¨ç†çš„LLMæ¨¡å‹


æ§åˆ¶å°å¼€å§‹ä¸‹è½½å¯¹åº”çš„Qwen1.5-Chat-1.8Bæ¨¡å‹










ç»ˆç«¯APIè°ƒç”¨
from gradio_client import Clientclient = Client(&quot;http://127.0.0.1:54696/qwen1.5-chat/&quot;)result = client.predict(		message=&quot;Hello!!&quot;,		request=512,		param_3=1,		param_4=&quot;Hello!!&quot;,		api_name=&quot;/chat&quot;)print(result)


HWç”Ÿæ€
å®¹å™¨åˆ›å»º
docker run -d --restart=always --name=xinference_wxb \ -v /reason-sharedata/training_inference/wangxiangbo/xinference:/opt/xinference -e XINFERENCE_HOME=/opt/xinference -e XINFERENCE_MODEL_SRC=modelscope \ -p 9998:9997 registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:nightly-main-cpu xinference-local -H 0.0.0.0




ç”±äºXinferenceæ¡†æ¶çš„æœ€æ–°é•œåƒéœ€è¦é…åˆGPUæ¥ä½¿ç”¨ï¼Œæ•…æœ¬æ¬¡åˆ©ç”¨cpuç‰ˆæœ¬è¿›è¡Œæ¨ç†éªŒè¯ã€‚




æœåŠ¡åˆ›å»º
]]></content>
      <categories>
        <category>æ¨¡å‹æ¨ç†</category>
        <category>xinference</category>
      </categories>
      <tags>
        <tag>æ¨¡å‹æ¨ç†</tag>
        <tag>NVIDIA</tag>
        <tag>V100</tag>
        <tag>xinference</tag>
      </tags>
  </entry>
  <entry>
    <title>vllm+mindieæ¨ç†å®è·µ</title>
    <url>/2026/01/08/LLM/%E6%8E%A8%E7%90%86/vllm+mindie%E6%8E%A8%E7%90%86%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[vllm
é•œåƒ
registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:vllmæˆ–registry.paas/library/ray-vllm-inference:v2.0
å®¹å™¨
docker run -it \--name vllm-wxb \--gpus all \--network host -p 8010:8010 \--shm-size 11g \-v /mnt/users/wangxiangbo/:/mnt \-v /usr/local/cuda-12.2:/usr/local/cuda-12.2 \-w /mnt \registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:vllm
DeepSeek-R1-Distill-Qwen-1.5B
å¼€å¯vllmæ¨ç†æœåŠ¡
ä¸ºäº†æ¢ç©¶æ‰€éœ€çš„æœ€ä½GPUæ˜¾å­˜å ç”¨ï¼Œgpu-memory-utilizationä»0.1å¼€å§‹æµ‹è¯•ã€‚
python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.1 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 1


python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.2 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 1


apiæµ‹è¯•
curl &quot;http://127.0.0.1:8010/v1/chat/completions&quot; \-H &quot;Content-Type: application/json&quot; \-d &#x27;&#123;&quot;model&quot;: &quot;/mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B&quot;,&quot;messages&quot;: [    &#123;        &quot;role&quot;: &quot;user&quot;,        &quot;content&quot;: &quot;I like Beijing, because&quot;    &#125;],&quot;max_tokens&quot;:512,&quot;presence_penalty&quot;: 1.2,&quot;frequency_penalty&quot;: 1.0,&quot;temperature&quot;: 0.7,&quot;top_p&quot;: 0.9&#125;&#x27;


evalscopeå‹æµ‹
evalscopeå®‰è£…
#å®‰è£…eval-scopeæµ‹è¯•ç¯å¢ƒï¼Œä½¿ç”¨æºç å®‰è£…git clone https://github.com/modelscope/evalscope.gitpip install -e .
å¯åŠ¨å‹æµ‹æœåŠ¡
:::tips
å‚è€ƒeval-scopeæ–‡æ¡£ï¼šhttps://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/parameters.html
:::
evalscope perf --url &#x27;http://127.0.0.1:8010/v1/chat/completions&#x27; \--parallel 10 \--model &#x27;/mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B&#x27; \--log-every-n-query 10 \--read-timeout=120 \--max-tokens 512 \--dataset-path &#x27;/mnt/script/open_qa.jsonl&#x27; \-n 30 \--max-prompt-length 12800 \--api openai \--stream \--stop &#x27;&lt;ï½œendâ–ofâ–sentenceï½œ&gt;&#x27; \--dataset openqa \--debug
DeepSeek-R1-Distill-Qwen-7B
python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.2 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/deepseek_qwen_hf \--trust-remote-code \--dtype float16 \--tensor-parallel-size 1


python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.5 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/deepseek_qwen_hf \--trust-remote-code \--dtype float16 \--tensor-parallel-size 1


DeepSeek-R1-Distill-Llama-8B
python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.5 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Llama-8B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 1


python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.6 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Llama-8B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 1


DeepSeek-R1-Distill-Qwen-14B
python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.4 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 2


python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.5 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 2


DeepSeek-R1-Distill-Qwen-32B
python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.5 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 4


python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.6 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 4


å‹æµ‹ç»“æœ

mindie
é•œåƒ
aiops-8af5363b.ecis.huhehaote-1.cmecloud.cn/public/mindie-develop-inference-model:1.0.rc3-pytorch2.1.0-py3.10-npu-cann8.0.rc3-ubuntu22.04
å®¹å™¨
sudo docker run --name mindie-wxb -dit --net=host --shm-size=100g \--privileged=true \-w /workspace \--device=/dev/davinci_manager \--device=/dev/hisi_hdc \--device=/dev/devmm_svm \--entrypoint=bash \-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \-v /usr/local/dcmi:/usr/local/dcmi \-v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \-v /usr/local/sbin:/usr/local/sbin \-v /as-sharedata-1/wangxiangbo/mindie:/workspace \-v /mnt/weight-1:/job \-v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime \-e http_proxy=$http_proxy \-e https_proxy=$https_proxy \aiops-8af5363b.ecis.huhehaote-1.cmecloud.cn/public/mindie-develop-inference-model:1.0.rc3-pytorch2.1.0-py3.10-npu-cann8.0.rc3-ubuntu22.04
æ¿€æ´»ç¯å¢ƒ
source /usr/local/Ascend/nnal/atb/set_env.shsource /usr/local/Ascend/mindie/set_env.shsource /usr/local/Ascend/ascend-toolkit/set_env.shsource /usr/local/Ascend/llm_model/set_env.sh
configæ¨ç†é…ç½®ä¿®æ”¹
vim /usr/local/Ascend/mindie/latest/mindie-service/conf/config.json
&#123;  &quot;Version&quot;: &quot;1.0.0&quot;,  &quot;LogConfig&quot;: &#123;    &quot;logLevel&quot;: &quot;Info&quot;,    &quot;logFileSize&quot;: 20,    &quot;logFileNum&quot;: 20,    &quot;logPath&quot;: &quot;logs/mindservice.log&quot;  &#125;,  &quot;ServerConfig&quot;: &#123;    &quot;ipAddress&quot;: &quot;10.208.200.67&quot;,    &quot;managementIpAddress&quot;: &quot;127.0.0.2&quot;,    &quot;port&quot;: 16384,    &quot;managementPort&quot;: 1026,    &quot;metricsPort&quot;: 1027,    &quot;allowAllZeroIpListening&quot;: false,    &quot;maxLinkNum&quot;: 1000,    &quot;httpsEnabled&quot;: false,    &quot;fullTextEnabled&quot;: false,    &quot;tlsCaPath&quot;: &quot;security/ca/&quot;,    &quot;tlsCaFile&quot;: [      &quot;ca.pem&quot;    ],    &quot;tlsCert&quot;: &quot;security/certs/server.pem&quot;,    &quot;tlsPk&quot;: &quot;security/keys/server.key.pem&quot;,    &quot;tlsPkPwd&quot;: &quot;security/pass/key_pwd.txt&quot;,    &quot;tlsCrl&quot;: &quot;security/certs/server_crl.pem&quot;,    &quot;managementTlsCaFile&quot;: [      &quot;management_ca.pem&quot;    ],    &quot;managementTlsCert&quot;: &quot;security/certs/management/server.pem&quot;,    &quot;managementTlsPk&quot;: &quot;security/keys/management/server.key.pem&quot;,    &quot;managementTlsPkPwd&quot;: &quot;security/pass/management/key_pwd.txt&quot;,    &quot;managementTlsCrl&quot;: &quot;security/certs/management/server_crl.pem&quot;,    &quot;kmcKsfMaster&quot;: &quot;tools/pmt/master/ksfa&quot;,    &quot;kmcKsfStandby&quot;: &quot;tools/pmt/standby/ksfb&quot;,    &quot;inferMode&quot;: &quot;standard&quot;,    &quot;interCommTLSEnabled&quot;: false,    &quot;interCommPort&quot;: 1121,    &quot;interCommTlsCaFile&quot;: &quot;security/grpc/ca/ca.pem&quot;,    &quot;interCommTlsCert&quot;: &quot;security/grpc/certs/server.pem&quot;,    &quot;interCommPk&quot;: &quot;security/grpc/keys/server.key.pem&quot;,    &quot;interCommPkPwd&quot;: &quot;security/grpc/pass/key_pwd.txt&quot;,    &quot;interCommTlsCrl&quot;: &quot;security/certs/server_crl.pem&quot;,    &quot;openAiSupport&quot;: &quot;vllm&quot;  &#125;,  &quot;BackendConfig&quot;: &#123;    &quot;backendName&quot;: &quot;mindieservice_llm_engine&quot;,    &quot;modelInstanceNumber&quot;: 1,    &quot;npuDeviceIds&quot;: [      [        0      ]    ],    &quot;tokenizerProcessNumber&quot;: 8,    &quot;multiNodesInferEnabled&quot;: false,    &quot;multiNodesInferPort&quot;: 1120,    &quot;interNodeTLSEnabled&quot;: true,    &quot;interNodeTlsCaFile&quot;: &quot;security/grpc/ca/ca.pem&quot;,    &quot;interNodeTlsCert&quot;: &quot;security/grpc/certs/server.pem&quot;,    &quot;interNodeTlsPk&quot;: &quot;security/grpc/keys/server.key.pem&quot;,    &quot;interNodeTlsPkPwd&quot;: &quot;security/grpc/pass/mindie_server_key_pwd.txt&quot;,    &quot;interNodeTlsCrl&quot;: &quot;security/grpc/certs/server_crl.pem&quot;,    &quot;interNodeKmcKsfMaster&quot;: &quot;tools/pmt/master/ksfa&quot;,    &quot;interNodeKmcKsfStandby&quot;: &quot;tools/pmt/standby/ksfb&quot;,    &quot;ModelDeployConfig&quot;: &#123;      &quot;maxSeqLen&quot;: 2048,      &quot;maxInputTokenLen&quot;: 2048,      &quot;truncation&quot;: false,      &quot;ModelConfig&quot;: [        &#123;          &quot;modelInstanceType&quot;: &quot;Standard&quot;,          &quot;modelName&quot;: &quot;deepseek-r1-distill-qwen-7b&quot;,          &quot;modelWeightPath&quot;: &quot;/job/huggingface/DeepSeek-R1-Distill-qwen-7B&quot;,          &quot;worldSize&quot;: 1,          &quot;cpuMemSize&quot;: 5,          &quot;npuMemSize&quot;: 1,          &quot;backendType&quot;: &quot;atb&quot;        &#125;      ]    &#125;,    &quot;ScheduleConfig&quot;: &#123;      &quot;templateType&quot;: &quot;Standard&quot;,      &quot;templateName&quot;: &quot;Standard_LLM&quot;,      &quot;cacheBlockSize&quot;: 128,      &quot;maxPrefillBatchSize&quot;: 50,      &quot;maxPrefillTokens&quot;: 48000,      &quot;prefillTimeMsPerReq&quot;: 150,      &quot;prefillPolicyType&quot;: 0,      &quot;decodeTimeMsPerReq&quot;: 50,      &quot;decodePolicyType&quot;: 0,      &quot;maxBatchSize&quot;: 200,      &quot;maxIterTimes&quot;: 512,      &quot;maxPreemptCount&quot;: 0,      &quot;supportSelectBatch&quot;: false,      &quot;maxQueueDelayMicroseconds&quot;: 5000    &#125;  &#125;&#125;
å¯åŠ¨æ¨ç†æœåŠ¡
å¢åŠ æƒé™
CONFIG_FILE=/usr/local/Ascend/mindie/latest/mindie-service/conf/config.jsonmodelWeightPath=/job/huggingface/DeepSeek-R1-Distill-qwen-7Bchmod -R 640 $modelWeightPathchmod 640 $modelWeightPathchmod 640 $CONFIG_FILE
å¯åŠ¨æ¨ç†
cd /usr/local/Ascend/mindie/latest/mindie-servicenohup ./bin/mindieservice_daemon &gt; output.log 2&gt;&amp;1 &amp;tail -f output.log










evalscopeå‹æµ‹
evalscopeå®‰è£…




å¯åŠ¨å‹æµ‹æœåŠ¡
]]></content>
      <categories>
        <category>æ¨¡å‹æ¨ç†</category>
        <category>vllm+mindie</category>
      </categories>
      <tags>
        <tag>æ¨¡å‹æ¨ç†</tag>
        <tag>NVIDIA</tag>
        <tag>V100</tag>
        <tag>æ˜‡è…¾</tag>
        <tag>910B</tag>
        <tag>vllm+mindie</tag>
      </tags>
  </entry>
  <entry>
    <title>ColossalAIè®­ç»ƒæ¡†æ¶è§£è¯»</title>
    <url>/2026/01/08/LLM/%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6/ColossalAI/</url>
    <content><![CDATA[æ¡†æ¶ç‰¹ç‚¹
Colossal-AI æ¡†æ¶ä¸­çš„ Gemini æ˜¯ä¸€ç§å†…å­˜ä¼˜åŒ–ç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡æ™ºèƒ½çš„å†…å­˜ç®¡ç†å’Œè®¡ç®—ä¼˜åŒ–æ¥å‡å°‘æ˜¾å­˜å ç”¨ï¼Œä»è€Œæ”¯æŒæ›´å¤§è§„æ¨¡çš„æ¨¡å‹è®­ç»ƒã€‚Gemini çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ åŠ¨æ€å†…å­˜ç®¡ç† å’Œ è®¡ç®—ä¼˜åŒ– æ¥æœ€å¤§åŒ–æ˜¾å­˜åˆ©ç”¨ç‡ï¼ŒåŒæ—¶å‡å°‘å†…å­˜ç¢ç‰‡å’Œå†—ä½™è®¡ç®—ã€‚
ä»¥llama2ä¸ºä¾‹çš„éªŒè¯
å¢åŠ æ¯éƒ¨è¿­ä»£æ‰“å°ä»¥åŠtflopsæ‰“å°
ColossalAIåˆå§‹æ‰“å°æ•ˆæœ






è„šæœ¬
#!/bin/bash# NCCL IB environment variablesexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1export NCCL_IB_DISABLE=0export NCCL_SOCKET_IFNAME=eth0export NCCL_IB_GID_INDEX=3export NCCL_IB_TIMEOUT=23export NCCL_IB_RETRY_CNT=7export OMP_NUM_THREADS=4current_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)LOG_SAVE=&quot;/mnt/colossalai/output/log/$&#123;current_time&#125;-llama2-pretrain.log&quot;PROJECT_NAME=&quot;llama2-7B-pretrain&quot;PARENT_SAVE_DIR=&quot;/mnt/colossalai/output/save/&quot;PARENT_TENSORBOARD_DIR=&quot;/mnt/colossalai/output/tensorboard/&quot;PARENT_CONFIG_FILE=&quot;/mnt/colossalai/output/config/&quot;PRETRAINED_MODEL_PATH=&quot;/mnt/colossalai/weight/Llama-2-7b-hf&quot;declare -a dataset=( &quot;/mnt/colossalai/dataset/llama2_pretraindata/arrow/part-00000&quot;)TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)FULL_PROJECT_NAME=&quot;$&#123;PROJECT_NAME&#125;-$&#123;TIMESTAMP&#125;&quot;SAVE_DIR=&quot;$&#123;PARENT_SAVE_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;TENSORBOARD_DIR=&quot;$&#123;PARENT_TENSORBOARD_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;CONFIG_FILE=&quot;$&#123;PARENT_CONFIG_FILE&#125;$&#123;FULL_PROJECT_NAME&#125;.json&quot;colossalai run --nproc_per_node 4 --master_port 30012 train.py \    --pretrained $PRETRAINED_MODEL_PATH \    --dataset $&#123;dataset[@]&#125; \    --plugin &quot;gemini&quot; \    --microbatch_size 1 \    --batch_size 4 \    --save_interval 400 \    --save_dir $SAVE_DIR \    --tensorboard_dir $TENSORBOARD_DIR \    --config_file $CONFIG_FILE \    --num_epochs 1 \    --accumulation_steps 1 \    --lr 1e-4 \    --mixed_precision &quot;fp16&quot; \    --grad_clip 1.0 \    --weight_decay 0.01 \    --warmup_steps 100 \    --use_grad_checkpoint \    --pad_token &quot;unk&quot; \    --max_length 1024 2&gt;&amp;1 | tee -a &quot;$LOG_SAVE&quot;
è¶…å‚è®¾ç½®/GPUåˆ©ç”¨ç‡/TFlops
mixed_precisionï¼šfp16/pluginï¼šgemini /mbsï¼š1/gbsï¼š4




mixed_precisionï¼šfp16/pluginï¼šgemini_auto /mbsï¼š1/gbsï¼š4




mixed_precisionï¼šbf16/pluginï¼šgemini_auto /mbsï¼š1/gbsï¼š4




mixed_precisionï¼šfp16/pluginï¼šzero2  /mbsï¼š1/gbsï¼š4




Geminiå¹¶è¡Œç­–ç•¥åˆ†æ
ä¼ ç»Ÿå¹¶è¡Œç­–ç•¥ vs Gemini



å¹¶è¡Œç­–ç•¥
ä¸»è¦ç‰¹ç‚¹
ä¸»è¦é—®é¢˜




æ•°æ®å¹¶è¡Œï¼ˆDPï¼‰
æ¯ä¸ª GPU è®¡ç®—æ•´ä¸ªæ¨¡å‹çš„ä¸€éƒ¨åˆ†æ•°æ®ï¼Œæœ€ç»ˆèšåˆæ¢¯åº¦
è®¡ç®—å†—ä½™ï¼Œæ˜¾å­˜æ¶ˆè€—å¤§


å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰
ä¸åŒ GPU å¤„ç†çŸ©é˜µä¹˜æ³•çš„ä¸åŒéƒ¨åˆ†ï¼Œå‡å°‘å•å¡è´Ÿæ‹…
é€šä¿¡å¼€é”€å¤§ï¼Œè´Ÿè½½ä¸å‡è¡¡


æµæ°´çº¿å¹¶è¡Œï¼ˆPPï¼‰
å°†æ¨¡å‹å±‚åˆ’åˆ†åˆ°å¤šä¸ª GPUï¼Œæµæ°´çº¿æ–¹å¼è¿›è¡Œè®¡ç®—
ä¾èµ–ä¸¥æ ¼çš„åŒæ­¥ï¼Œè°ƒåº¦å¤æ‚


ZeROï¼ˆZero Redundancy Optimizerï¼‰
ZeRO-1: ä»…åˆ‡åˆ†ä¼˜åŒ–å™¨çŠ¶æ€   ZeRO-2: åˆ‡åˆ†ä¼˜åŒ–å™¨+æ¢¯åº¦   ZeRO-3: è¿›ä¸€æ­¥åˆ‡åˆ†æ¨¡å‹å‚æ•°
è®¡ç®—å’Œé€šä¿¡å¼€é”€è¾ƒé«˜


Gemini
æ··åˆ ZeRO-2/3 + è®¡ç®—å’Œæ•°æ®è‡ªåŠ¨è°ƒåº¦
é€‚ç”¨äºæ›´å¤§è§„æ¨¡æ¨¡å‹



Geminiçš„å…³é”®ä¼˜åŒ–ç‚¹




ä¼˜åŒ–äº†å‚æ•°åˆ‡åˆ†ç­–ç•¥
åœ¨ GeminiPlugin é‡Œï¼Œshard_param_frac å’Œ offload_optim_frac æ§åˆ¶äº†å‚æ•°åˆ‡åˆ†å’Œä¼˜åŒ–å™¨çŠ¶æ€çš„å­˜å‚¨ï¼š
shard_param_frac: float = 1.0  # 1.0 ç›¸å½“äº ZeRO-3, 0.0 ç›¸å½“äº ZeRO-2offload_optim_frac: float = 0.0  # æ§åˆ¶ä¼˜åŒ–å™¨çŠ¶æ€æ˜¯å¦ä¸‹æ”¾åˆ° CPU

**å¦‚æœ ****shard_param_frac=1.0**ï¼Œé‚£ä¹ˆæ‰€æœ‰å‚æ•°éƒ½ä¼šè¢« shardï¼ˆç±»ä¼¼ ZeRO-3ï¼‰ã€‚
**å¦‚æœ ****shard_param_frac=0.0**ï¼Œåˆ™ä¸ä¼šå¯¹å‚æ•°è¿›è¡Œ shardï¼ˆç±»ä¼¼ ZeRO-2ï¼‰ã€‚
**å¦‚æœ ****offload_optim_frac=1.0**ï¼Œä¼˜åŒ–å™¨çŠ¶æ€å¯ä»¥ä¸‹æ”¾åˆ° CPUï¼Œå‡å°‘ GPU å ç”¨ã€‚

è®¡ç®—å›¾çš„æ™ºèƒ½åˆ†é…
Gemini ç»“åˆ TPï¼ˆå¼ é‡å¹¶è¡Œï¼‰+ DPï¼ˆæ•°æ®å¹¶è¡Œï¼‰+ ZeRO è¿›è¡Œè®¡ç®—ä¼˜åŒ–ï¼š
self.pg_mesh = ProcessGroupMesh(self.zero_size, self.extra_dp_size, self.tp_size)
å…¶ä¸­ï¼š

self.zero_group è´Ÿè´£ ZeRO å‚æ•°ç®¡ç†ï¼ˆç±»ä¼¼ ZeRO-2/3ï¼‰ã€‚
self.extra_dp_group æ§åˆ¶ é¢å¤–çš„æ•°æ®å¹¶è¡Œç»„ã€‚
self.tp_group è´Ÿè´£ å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ã€‚

è¿™æ ·ï¼ŒGemini å…è®¸ åŠ¨æ€åˆ‡æ¢å¹¶è¡Œæ–¹å¼ï¼Œé¿å…é™æ€åˆ†é…å¯¼è‡´çš„èµ„æºæµªè´¹ã€‚
è‡ªé€‚åº”å†…å­˜ç®¡ç†
GPU-CPU æ··åˆå­˜å‚¨
Gemini æä¾›äº† placement_policy å‚æ•°ï¼š
placement_policy: str = &quot;static&quot;  # ä¹Ÿå¯è®¾ä¸º &quot;auto&quot;

&quot;static&quot; æ¨¡å¼ï¼šæ‰‹åŠ¨è®¾å®š shard_param_frac å’Œ offload_optim_frac æ¥æ§åˆ¶ ZeRO çº§åˆ«ã€‚
&quot;auto&quot; æ¨¡å¼ï¼šGemini è‡ªåŠ¨æ£€æµ‹æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼Œæ™ºèƒ½åˆ†é…å‚æ•°åˆ° CPU/GPUï¼Œé˜²æ­¢ OOMã€‚

GPU æ˜¾å­˜åˆ©ç”¨ç‡ä¼˜åŒ–
warmup_non_model_data_ratio: float = 0.8  # è®­ç»ƒå‰é¢„ç•™æ˜¾å­˜steady_cuda_cap_ratio: float = 0.9  # è®­ç»ƒç¨³å®šåï¼Œæ¨¡å‹æœ€å¤§ä½¿ç”¨æ˜¾å­˜æ¯”ä¾‹

è®­ç»ƒåˆæœŸï¼Œé¢„ç•™ 80% æ˜¾å­˜ç»™éæ¨¡å‹æ•°æ®ï¼ˆå¦‚æ¿€æ´»å€¼ç¼“å­˜ï¼‰ã€‚
è®­ç»ƒç¨³å®šåï¼ŒGemini å…è®¸æ¨¡å‹å ç”¨ 90% æ˜¾å­˜ï¼Œæé«˜åˆ©ç”¨ç‡ã€‚

æ··åˆç²¾åº¦è®­ç»ƒ
Gemini é€šè¿‡ precision å‚æ•°æ”¯æŒ fp16/bf16 è®­ç»ƒï¼š
precision: str = &quot;fp16&quot;

**fp16**ï¼šå‡å°‘æ˜¾å­˜å ç”¨ï¼Œæå‡è®¡ç®—é€Ÿåº¦ã€‚
**bf16**ï¼šè®­ç»ƒç¨³å®šæ€§æ›´å¥½ï¼Œé€‚ç”¨äº NVIDIA A100/H100ã€‚

æ­¤å¤–ï¼Œå®ƒè¿˜æ”¯æŒ master_weightsï¼ˆä¼˜åŒ–å™¨ä¿ç•™ FP32 æƒé‡ï¼‰ï¼š
master_weights: bool = True

è¿™æ ·å¯ä»¥åœ¨ fp16 è®¡ç®—æ—¶é¿å…ç²¾åº¦ä¸‹é™ã€‚

æ¢¯åº¦åŒæ­¥ä¼˜åŒ–
Gemini å…è®¸ å¼‚æ­¥æ¢¯åº¦åŒæ­¥ï¼Œå‡å°‘é€šä¿¡ç“¶é¢ˆï¼š
enable_async_reduce: bool = True
å½“ enable_async_reduce=True æ—¶ï¼š

åå‘ä¼ æ’­æ—¶ï¼Œæ¢¯åº¦ä¸ä¼šç«‹å³åŒæ­¥ï¼Œè€Œæ˜¯ ç´¯ç§¯å¤šä¸ª step ä¹‹åå†åŒæ­¥ã€‚
è¿™æ ·å¯ä»¥ å‡å°‘é€šä¿¡æ¬¡æ•°ï¼Œæé«˜ GPU è®¡ç®—æ•ˆç‡ã€‚

ZeRO-3 vs Gemini



ç‰¹æ€§
ZeRO-3
Gemini




å‚æ•°åˆ‡åˆ†
ä»…å‚æ•°
å‚æ•° + è®¡ç®—çŠ¶æ€ï¼ˆæ›´å…¨é¢ï¼‰


ä¼˜åŒ–å™¨çŠ¶æ€å­˜å‚¨
GPU / CPUï¼ˆæ‰‹åŠ¨é…ç½®ï¼‰
åŠ¨æ€è°ƒåº¦ï¼ˆè‡ªåŠ¨è°ƒæ•´ GPU/CPUï¼‰


æ¢¯åº¦åŒæ­¥
æ¯æ­¥æ¢¯åº¦åŒæ­¥
æ”¯æŒå¼‚æ­¥æ¢¯åº¦åŒæ­¥


æ˜¾å­˜åˆ©ç”¨ç‡ä¼˜åŒ–
éœ€è¦æ‰‹åŠ¨è°ƒæ•´ stage
è‡ªåŠ¨åˆ†é…æ˜¾å­˜


è®¡ç®—å›¾ä¼˜åŒ–
ä»…é™ DP/TP/PP
è‡ªåŠ¨é€‚é… DP/TP/PP


é€‚ç”¨åœºæ™¯
é™æ€ä»»åŠ¡
é€‚ç”¨äºåŠ¨æ€ä»»åŠ¡



ä»£ç ç¤ºä¾‹
ï¼ˆ1ï¼‰ä¼ ç»Ÿ ZeRO-3
plugin = LowLevelZeroPlugin(stage=3, precision=&quot;fp16&quot;)
(2) Gemini è‡ªåŠ¨ä¼˜åŒ–
plugin = GeminiPlugin(precision=&quot;fp16&quot;, placement_policy=&quot;auto&quot;, enable_async_reduce=True)

placement_policy=&quot;auto&quot;è‡ªåŠ¨è°ƒæ•´å‚æ•°åœ¨ CPU/GPU ä¹‹é—´çš„å­˜æ”¾ã€‚
enable_async_reduce=Trueå¼€å¯å¼‚æ­¥æ¢¯åº¦åŒæ­¥ï¼Œå‡å°‘é€šä¿¡å¼€é”€ã€‚

Gemini vs Gemini_auto
ä¸»è¦åŒºåˆ«


**gemini**

placement_policy=&quot;static&quot;ï¼šåœ¨è¿™ä¸ªæ¨¡å¼ä¸‹ï¼ŒGemini ä½¿ç”¨å›ºå®šçš„å‚æ•°åˆ‡åˆ†ç­–ç•¥ï¼Œç”¨æˆ·éœ€è¦æ‰‹åŠ¨è®¾ç½® shard_param_fracã€offload_optim_frac ç­‰å‚æ•°æ¥æ§åˆ¶æ¨¡å‹å‚æ•°å’Œä¼˜åŒ–å™¨çŠ¶æ€çš„å­˜æ”¾ä½ç½®ï¼ˆæ¯”å¦‚æ˜¯å¦å­˜æ”¾åœ¨ CPU æˆ– GPU ä¸Šï¼‰ã€‚
åœ¨è¿™ç§æ¨¡å¼ä¸‹ï¼Œæ‰€æœ‰çš„å‚æ•°åˆ‡åˆ†ã€å­˜å‚¨å’Œè®¡ç®—èµ„æºçš„è°ƒåº¦éƒ½ç”±ç”¨æˆ·æŒ‡å®šã€‚è™½ç„¶èƒ½è·å¾—è¾ƒé«˜çš„ç²¾ç»†æ§åˆ¶ï¼Œä½†ä¹Ÿéœ€è¦ç”¨æˆ·è¿›è¡Œæ›´å¤šçš„è°ƒä¼˜å·¥ä½œã€‚

**gemini_auto**:

placement_policy=&quot;auto&quot;ï¼šåœ¨è¿™ç§æ¨¡å¼ä¸‹ï¼ŒGemini ä¼šè‡ªåŠ¨æ ¹æ®ç¡¬ä»¶èµ„æºï¼ˆå¦‚ GPU æ˜¾å­˜ã€CPU æ˜¾å­˜ç­‰ï¼‰çš„æƒ…å†µåŠ¨æ€è°ƒæ•´å‚æ•°åˆ‡åˆ†å’Œå­˜æ”¾ï¼Œè€Œä¸éœ€è¦ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®šã€‚ä¾‹å¦‚ï¼Œå½“ GPU å†…å­˜ä¸è¶³æ—¶ï¼Œå®ƒä¼šè‡ªåŠ¨å°†éƒ¨åˆ†å‚æ•°è½¬ç§»åˆ° CPUã€‚
è‡ªåŠ¨çš„å†…å­˜ç®¡ç†ç­–ç•¥å¯ä»¥å‡å°‘ç”¨æˆ·çš„é…ç½®å·¥ä½œï¼ŒåŒæ—¶æé«˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ˜¾å­˜åˆ©ç”¨ç‡ï¼Œé¿å… OOMï¼ˆå†…å­˜æº¢å‡ºï¼‰é”™è¯¯ã€‚

å†…å­˜å’Œæ˜¾å­˜åˆ†é…ç­–ç•¥
åœ¨ gemini æ¨¡å¼ä¸‹ï¼Œç”¨æˆ·éœ€è¦æ‰‹åŠ¨è®¾ç½®å†…å­˜å’Œè®¡ç®—èµ„æºçš„åˆ†é…ã€‚ä¾‹å¦‚ï¼š
shard_param_frac: float = 1.0  # æ§åˆ¶å‚æ•°æ˜¯å¦è¿›è¡Œåˆ‡åˆ†offload_optim_frac: float = 0.0  # æ§åˆ¶ä¼˜åŒ–å™¨çŠ¶æ€æ˜¯å¦è½¬ç§»åˆ° CPU

shard_param_frac=1.0 ä¼šå°†æ‰€æœ‰å‚æ•°åˆ‡åˆ†åˆ°ä¸åŒçš„ GPU ä¸Šï¼Œç±»ä¼¼äº ZeRO-3ã€‚
offload_optim_frac=0.0 åˆ™æ„å‘³ç€ä¼˜åŒ–å™¨çš„çŠ¶æ€ä»ç„¶ä¿ç•™åœ¨ GPU ä¸Šã€‚

éœ€è¦æ‰‹åŠ¨è°ƒèŠ‚æ˜¾å­˜åˆ©ç”¨ç‡ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡è®¾ç½® shard_param_frac å’Œ offload_optim_frac æ¥æ§åˆ¶å‚æ•°å’Œä¼˜åŒ–å™¨çš„å­˜æ”¾ä½ç½®ï¼‰ã€‚é€‚åˆé‚£äº› å¯¹å†…å­˜ä½¿ç”¨æœ‰è¾ƒé«˜æ§åˆ¶éœ€æ±‚ çš„åœºæ™¯ï¼Œä½†éœ€è¦æ›´å¤šçš„äººå·¥è°ƒè¯•å’Œä¼˜åŒ–ã€‚
åœ¨ gemini_auto æ¨¡å¼ä¸‹ï¼ŒGemini ä¼šè‡ªåŠ¨ä¼˜åŒ–å†…å­˜çš„ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼š
placement_policy=&quot;auto&quot;

è‡ªåŠ¨æ ¹æ®ç¡¬ä»¶çŠ¶æ€è°ƒæ•´å†…å­˜å’Œå‚æ•°çš„å­˜æ”¾ä½ç½®ã€‚
é€‚åˆåœ¨ æ²¡æœ‰è¶³å¤Ÿç¡¬ä»¶ä¿¡æ¯æˆ–ä¸å¸Œæœ›è¿›è¡Œæ‰‹åŠ¨è°ƒä¼˜ çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤š GPU æˆ–å¤šä¸ªæœºå™¨çš„è®­ç»ƒåœºæ™¯ä¸­ï¼Œè‡ªåŠ¨çš„å†…å­˜ç®¡ç†å¯ä»¥æœ‰æ•ˆé¿å… OOM å’Œæå‡æ˜¾å­˜åˆ©ç”¨ç‡ã€‚

è¿™ç§æ¨¡å¼ä¼šæ ¹æ®å½“å‰ GPU çš„è´Ÿè½½ã€æ˜¾å­˜çš„å‰©ä½™æƒ…å†µç­‰å› ç´ ï¼ŒåŠ¨æ€å†³å®šæ˜¯å¦å°†éƒ¨åˆ†æ¨¡å‹å‚æ•°æˆ–ä¼˜åŒ–å™¨çŠ¶æ€ä» GPU è½¬ç§»åˆ° CPU ä¸Šï¼Œå‡å°‘ GPU å†…å­˜çš„å ç”¨ï¼Œé¿å…æ˜¾å­˜æº¢å‡ºã€‚è¿™ç§æ¨¡å¼é€šå¸¸ä¼šåœ¨è®­ç»ƒå¼€å§‹æ—¶è¿›è¡Œé¢„çƒ­ï¼Œå³æ ¹æ®æ¨¡å‹å’Œæ•°æ®çš„ç‰¹ç‚¹è‡ªåŠ¨è°ƒæ•´å‚æ•°çš„åˆ†é…æ–¹å¼ï¼Œä»¥ä¿è¯è®­ç»ƒè¿‡ç¨‹ä¸­å†…å­˜çš„é«˜æ•ˆåˆ©ç”¨ã€‚
é€‚ç”¨åœºæ™¯

**gemini**ï¼šé€‚åˆ **ç²¾ç¡®æ§åˆ¶ **å†…å­˜ä½¿ç”¨å’Œè®­ç»ƒè¿‡ç¨‹çš„åœºæ™¯ï¼Œå°¤å…¶æ˜¯å½“ç”¨æˆ·å¯¹ç¡¬ä»¶èµ„æºæœ‰æ·±å…¥äº†è§£å¹¶èƒ½è¿›è¡Œæ‰‹åŠ¨è°ƒä¼˜æ—¶ã€‚é€‚ç”¨äºè¶…å¤§è§„æ¨¡è®­ç»ƒï¼Œç‰¹åˆ«æ˜¯éœ€è¦æ˜¾å¼æ§åˆ¶æ¨¡å‹åˆ‡åˆ†å’Œå­˜å‚¨ä½ç½®çš„æƒ…å†µã€‚
**gemini_auto**ï¼šé€‚åˆé‚£äº› **å¸Œæœ›å‡å°‘äººå·¥è°ƒä¼˜ **çš„ç”¨æˆ·ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´èµ„æºå’Œå†…å­˜æ—¶ã€‚å®ƒä½¿å¾—è®­ç»ƒè¿‡ç¨‹æ›´åŠ çµæ´»ï¼Œç‰¹åˆ«æ˜¯åœ¨é¢å¯¹ GPU å†…å­˜ç“¶é¢ˆ å’Œ å¤šå¡è®­ç»ƒ æ—¶ï¼Œè‡ªåŠ¨è°ƒæ•´å†…å­˜å’Œè®¡ç®—èµ„æºåˆ†é…èƒ½å¤Ÿæé«˜è®­ç»ƒæ•ˆç‡ã€‚

æ€»ç»“
Gemini çš„å…³é”®æ”¹è¿›ç‚¹ï¼š

å‚æ•°åˆ‡åˆ†æ›´åŠ çµæ´»ï¼šæ”¯æŒ ZeRO-2/3 ä¹‹é—´çš„åŠ¨æ€è°ƒæ•´ã€‚
è®¡ç®—å›¾æ™ºèƒ½ä¼˜åŒ–ï¼šç»“åˆ TP + DP + ZeRO è¿›è¡Œè´Ÿè½½å‡è¡¡ã€‚
æ˜¾å­˜ç®¡ç†æ›´é«˜æ•ˆï¼šè‡ªåŠ¨è°ƒæ•´å‚æ•°åœ¨ CPU/GPU ä¹‹é—´çš„å­˜æ”¾ï¼Œé˜²æ­¢ OOMã€‚
æ··åˆç²¾åº¦æ”¯æŒï¼šFP16/BF16 è®¡ç®—ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚
å¼‚æ­¥æ¢¯åº¦åŒæ­¥ï¼šå‡å°‘é€šä¿¡å¼€é”€ï¼Œæé«˜ GPU åˆ©ç”¨ç‡ã€‚

]]></content>
      <categories>
        <category>æ¨¡å‹è®­ç»ƒæ¡†æ¶</category>
        <category>ColossalAI</category>
      </categories>
      <tags>
        <tag>NVIDIA</tag>
        <tag>V100</tag>
        <tag>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</tag>
        <tag>ColossalAI</tag>
      </tags>
  </entry>
  <entry>
    <title>å¾®è°ƒæ•°æ®é›†å¤„ç†å…¨æµç¨‹è§£æ</title>
    <url>/2026/01/08/LLM/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/%E5%BE%AE%E8%B0%83%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%84%E7%90%86%E5%85%A8%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[è¡Œä¸šæ¨¡å‹å¾®è°ƒæµç¨‹å›¾

å¤§æ¨¡å‹å¾®è°ƒå…¨æµç¨‹è§£å†³æ–¹æ¡ˆ


pdfæ•°æ®æå–å¤„ç†
æ”¿ä¼ç±»æ•°æ®é›†.pdf


pdfæ•°æ®æå–
é‡‡ç”¨mineruæ–¹å¼
mineruç¯å¢ƒå®‰è£…
https://github.com/opendatalab/MinerU
conda create -n mineru python=3.10conda activate minerupip install -U &quot;magic-pdf[full]&quot; --extra-index-url https://wheels.myhloli.com -i https://mirrors.aliyun.com/pypi/simple


magic-pdf.jsoné…ç½®æ–‡ä»¶ä¿®æ”¹
&#123;    &quot;bucket_info&quot;: &#123;        &quot;bucket-name-1&quot;: [            &quot;ak&quot;,            &quot;sk&quot;,            &quot;endpoint&quot;        ],        &quot;bucket-name-2&quot;: [            &quot;ak&quot;,            &quot;sk&quot;,            &quot;endpoint&quot;        ]    &#125;,    &quot;models-dir&quot;: &quot;/mnthp/users/wangxiangbo/model/opendatalab/PDF-Extract-Kit-1___0/models&quot;,    &quot;layoutreader-model-dir&quot;: &quot;/mnthp/users/wangxiangbo/model/ppaanngggg/layoutreader&quot;,    &quot;device-mode&quot;: &quot;cpu&quot;,    &quot;layout-config&quot;: &#123;        &quot;model&quot;: &quot;doclayout_yolo&quot;    &#125;,    &quot;formula-config&quot;: &#123;        &quot;mfd_model&quot;: &quot;yolo_v8_mfd&quot;,        &quot;mfr_model&quot;: &quot;unimernet_small&quot;,        &quot;enable&quot;: true    &#125;,    &quot;table-config&quot;: &#123;        &quot;model&quot;: &quot;rapid_table&quot;,        &quot;sub_model&quot;: &quot;slanet_plus&quot;,        &quot;enable&quot;: false,        &quot;max_time&quot;: 400    &#125;,    &quot;llm-aided-config&quot;: &#123;        &quot;formula_aided&quot;: &#123;            &quot;api_key&quot;: &quot;your_api_key&quot;,            &quot;base_url&quot;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,            &quot;model&quot;: &quot;qwen2.5-7b-instruct&quot;,            &quot;enable&quot;: false        &#125;,        &quot;text_aided&quot;: &#123;            &quot;api_key&quot;: &quot;your_api_key&quot;,            &quot;base_url&quot;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,            &quot;model&quot;: &quot;qwen2.5-7b-instruct&quot;,            &quot;enable&quot;: false        &#125;,        &quot;title_aided&quot;: &#123;            &quot;api_key&quot;: &quot;your_api_key&quot;,            &quot;base_url&quot;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,            &quot;model&quot;: &quot;qwen2.5-32b-instruct&quot;,            &quot;enable&quot;: false        &#125;    &#125;,    &quot;config_version&quot;: &quot;1.1.1&quot;&#125;
pyè„šæœ¬æ–¹å¼å¤„ç†pdfæ–‡ä»¶
import osfrom magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReaderfrom magic_pdf.data.dataset import PymuDocDatasetfrom magic_pdf.model.doc_analyze_by_custom_model import doc_analyzefrom magic_pdf.config.enums import SupportedPdfParseMethod# argspdf_file_name = &quot;/mnthp/users/wangxiangbo/data/pdf2.pdf&quot;  # replace with the real pdf path# name_without_suff = pdf_file_name.split(&quot;.&quot;)[0]name_without_suff = os.path.basename(pdf_file_name).split(&quot;.&quot;)[0]# prepare envlocal_image_dir, local_md_dir = &quot;/mnthp/users/wangxiangbo/mineru/images&quot;, &quot;/mnthp/users/wangxiangbo/mineru/data&quot;image_dir = str(os.path.basename(local_image_dir))os.makedirs(local_image_dir, exist_ok=True)image_writer, md_writer = FileBasedDataWriter(local_image_dir), FileBasedDataWriter(    local_md_dir)# read bytesreader1 = FileBasedDataReader(&quot;&quot;)pdf_bytes = reader1.read(pdf_file_name)  # read the pdf content# proc## Create Dataset Instanceds = PymuDocDataset(pdf_bytes)## inferenceif ds.classify() == SupportedPdfParseMethod.OCR:    infer_result = ds.apply(doc_analyze, ocr=True)    ## pipeline    pipe_result = infer_result.pipe_ocr_mode(image_writer)else:    infer_result = ds.apply(doc_analyze, ocr=False)    ## pipeline    pipe_result = infer_result.pipe_txt_mode(image_writer)### draw model result on each pageinfer_result.draw_model(os.path.join(local_md_dir, f&quot;&#123;name_without_suff&#125;_model.pdf&quot;))### get model inference resultmodel_inference_result = infer_result.get_infer_res()### draw layout result on each pagepipe_result.draw_layout(os.path.join(local_md_dir, f&quot;&#123;name_without_suff&#125;_layout.pdf&quot;))### draw spans result on each pagepipe_result.draw_span(os.path.join(local_md_dir, f&quot;&#123;name_without_suff&#125;_spans.pdf&quot;))### get markdown contentmd_content = pipe_result.get_markdown(image_dir)### dump markdownpipe_result.dump_md(md_writer, f&quot;&#123;name_without_suff&#125;.md&quot;, image_dir)### get content list contentcontent_list_content = pipe_result.get_content_list(image_dir)### dump content listpipe_result.dump_content_list(md_writer, f&quot;&#123;name_without_suff&#125;_content_list.json&quot;, image_dir)### get middle jsonmiddle_json_content = pipe_result.get_middle_json()### dump middle jsonpipe_result.dump_middle_json(md_writer, f&#x27;&#123;name_without_suff&#125;_middle.json&#x27;)


å¤„ç†ç»“æœ


åˆ©ç”¨Langchainåˆ‡åˆ†mdæ•°æ®ï¼Œå¹¶è½¬ä¸ºjsonlæ ¼å¼
Langchainç¯å¢ƒå®‰è£…
pip install langchain


pyè„šæœ¬åˆ‡åˆ†è½¬å¥½çš„mdæ•°æ®
LangChain çš„ **RecursiveCharacterTextSplitter** ä¼š æŒ‰ä¼˜å…ˆçº§ ä¾æ¬¡å°è¯• åˆ†å‰²æ–‡æœ¬ï¼š

**\n**ï¼ˆæ¢è¡Œç¬¦ï¼‰ï¼šå¦‚æœèƒ½æŒ‰æ®µè½åˆ†å‰²ï¼Œå°±ä¸ç»§ç»­å¾€ä¸‹ã€‚
**&quot;. &quot;**** / ****&quot;ã€‚&quot;**ï¼ˆå¥å·ï¼‰ï¼šå¦‚æœæ²¡æœ‰æ¢è¡Œç¬¦ï¼Œå°±æŒ‰å¥å·åˆ†å‰²ã€‚
**&quot;!&quot;**** / **&quot;?&quot;** / **&quot;ï¼›&quot;** / ****&quot;;&quot;**ï¼ˆå…¶ä»–æ ‡ç‚¹ï¼‰ï¼šå¦‚æœå¥å·ä¹Ÿæ²¡æœ‰ï¼Œå°±æŒ‰è¿™äº›ç¬¦å·åˆ‡åˆ†ã€‚

ğŸš€** è¿™æ ·èƒ½ä¿è¯**ï¼š

å…ˆæŒ‰æ®µè½åˆ†å‰²ï¼Œå¦‚æœæ®µè½å¤ªé•¿ï¼Œå†æŒ‰ å¥å· ç»†åˆ†ï¼›
å¦‚æœå¥å·ä¹Ÿä¸å¤Ÿç”¨ï¼Œå°±ç»§ç»­ç”¨å…¶ä»–æ ‡ç‚¹åˆ†å‰²ï¼Œç¡®ä¿åˆ‡ç‰‡åˆç†ã€‚

from langchain.text_splitter import RecursiveCharacterTextSplitterimport json # è¯»å– Markdown æ–‡ä»¶with open(&quot;/mnthp/users/wangxiangbo/mineru/data/pdf2.md&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:    markdown_text = f.read()# ä½¿ç”¨ RecursiveCharacterTextSplitterï¼ŒæŒ‰å¥å·åˆ†å‰²splitter = RecursiveCharacterTextSplitter(    chunk_size=300,   # æ¯ä¸ªå—æœ€å¤§é•¿åº¦    chunk_overlap=0, # å…è®¸çš„é‡å å­—ç¬¦    separators=[&quot;\n&quot;, &quot;. &quot;, &quot;ã€‚&quot;, &quot;!&quot;, &quot;?&quot;, &quot;ï¼›&quot;, &quot;;&quot;],  # å…ˆæŒ‰æ¢è¡Œç¬¦åˆ†å‰²ï¼Œå†æŒ‰å¥å·ã€æ„Ÿå¹å·ç­‰)chunks = splitter.split_text(markdown_text)# ä¿å­˜ä¸º JSONL æ–‡ä»¶output_path = &quot;/mnthp/users/wangxiangbo/mineru/data/output6.jsonl&quot;with open(output_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:    for chunk in chunks:        f.write(json.dumps(&#123;&quot;text&quot;: chunk&#125;, ensure_ascii=False) + &quot;\n&quot;)print(f&quot;æ–‡æœ¬å·²æŒ‰å¥å·åˆ†å‰²å¹¶ä¿å­˜åˆ° &#123;output_path&#125;&quot;)
åˆ‡åˆ†ç»“æœ


Data-juicerè¿›è¡Œæ•°æ®æ¸…æ´—ä¸è´¨é‡åˆ†æ
data-juicerç¯å¢ƒå®‰è£…
https://github.com/modelscope/data-juicer
ä¾èµ–ç¯å¢ƒæ­¥éª¤ç•¥å»ï¼Œå·²æ¨è‡³é•œåƒ
registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:data-juicer
æ•°æ®æ¸…æ´—
æ¸…æ´—é…ç½®
# Process config example for dataset# global parametersproject_name: &#x27;test-process&#x27;dataset_path: &#x27;/mnt/mineru/data/output6.jsonl&#x27;  # path to your dataset directory or filenp: 4  # number of subprocess to process your datasetexport_path: &#x27;./outputs/demo-process/test-processed4.jsonl&#x27;# process schedule# a list of several process operators with their argumentsprocess:  - language_id_score_filter:      lang: &#x27;zh&#x27;      min_score: 0.8  - clean_email_mapper:                                     # remove emails from text.  - clean_html_mapper:                                      # remove html formats form text.  - clean_ip_mapper:                                        # remove ip addresses from text.  - clean_links_mapper:                                     # remove web links from text.  - average_line_length_filter:                             # filter text with the average length of lines out of specific range.      min_len: 50                                             # the min length of filter range      max_len: 10000                                          # the max length of filter range#   - perplexity_filter:#       lang: &#x27;zh&#x27;#       max_ppl: 5000                                             #è¿‡æ»¤å›°æƒ‘åº¦å¤§äº5000çš„æ–‡æœ¬  #   - alphanumeric_filter:                                    # filter text with alphabet/numeric ratio out of specific range.#       tokenization: false                                     # whether to count the ratio of alphanumeric to the total number of tokens.#       min_ratio: 0.0                                          # ç¡®ä¿æ–‡æœ¬åŒ…å«è¶³å¤Ÿå¤šçš„å­—æ¯æ•°å­—å†…å®¹ï¼Œå­—æ¯æ•°å­—/æ€»å­—ç¬¦æ•° &gt;0.7#       max_ratio: 0.9                                         # å…è®¸é«˜è¾¾90%çš„å­—æ¯æ•°å­—ï¼Œè¿‡æ»¤æ‰æ¥è¿‘çº¯å­—æ¯æ•°å­—çš„å¼‚å¸¸æ–‡æœ¬ã€‚
python tools/process_data.py --config configs/demo/process.yaml




æ•°æ®åˆ†æï¼ˆè´¨é‡ï¼‰
åˆ†æé…ç½®
# Process config example for dataset# global parametersproject_name: &#x27;demo-analyzer&#x27;dataset_path: &#x27;./outputs/demo-process/test-processed4.jsonl&#x27;  # path to your dataset directory or filenp: 4  # number of subprocess to process your datasetexport_path: &#x27;./outputs/demo-analyzer/demo-analyzer-result4.jsonl&#x27;# process schedule# a list of several process operators with their argumentsprocess:  - language_id_score_filter:      lang: &#x27;zh&#x27;  - perplexity_filter:      lang: &#x27;zh&#x27;    #   max_ppl: 5000                                             #è¿‡æ»¤å›°æƒ‘åº¦å¤§äº5000çš„æ–‡æœ¬                                        #   - llm_difficulty_score_filter:                            # filter to keep sample with high difficulty score estimated by LLM.#       api_or_hf_model: &#x27;gpt-4o&#x27;                               # API or huggingface model name.#       min_score: 0.5                                          # The lowest difficulty score threshold to keep the sample.#       api_endpoint: null                                      # URL endpoint for the API.#       response_path: null                                     # Path to extract content from the API response. Defaults to &#x27;choices.0.message.content&#x27;.#       input_keys: [&#x27;text&#x27;]                                    # Sub set of keys in the sample. Support data with multi fields such as &#x27;query&#x27;, &#x27;analysis&#x27; and &#x27;answer&#x27; in RFT data.#       field_names: [&#x27;text&#x27;]                                   # Corresponding field names for input keys.#       system_prompt: null                                     # System prompt for the task.#       input_template: null                                    # The input template.#       field_template: null                                    # Template for each field in the prompt.#       try_num: 3                                              # The number of retry attempts when there is an API call error or output parsing error.#       enable_vllm: false                                      # If true, use VLLM for loading hugging face or local llm. Otherwise, use API for reference.#       model_params: &#123;&#125;                                        # Parameters for initializing the API model.#       sampling_params: &#123;&#125;                                     # Extra parameters passed to the API call. e.g &#123;&#x27;temperature&#x27;: 0.9, &#x27;top_p&#x27;: 0.95&#125;#   - llm_quality_score_filter:                               # filter to keep sample with high quality score estimated by LLM.#       api_or_hf_model: &#x27;gpt-4o&#x27;                               # API or huggingface model name.#       min_score: 0.5                                          # The lowest quality score threshold to keep the sample.#       api_endpoint: https://dashscope.aliyuncs.com/compatible-mode/v1                                      # URL endpoint for the API.#       response_path: null                                     # Path to extract content from the API response. Defaults to &#x27;choices.0.message.content&#x27;.#       input_keys: [&#x27;text&#x27;]                                    # Sub set of keys in the sample. Support data with multi fields such as &#x27;query&#x27;, &#x27;analysis&#x27; and &#x27;answer&#x27; in RFT data.#       field_names: [&#x27;text&#x27;]                                   # Corresponding field names for input keys.#       system_prompt: null                                     # System prompt for the task.#       input_template: null                                    # The input template.#       field_template: null                                    # Template for each field in the prompt.#       try_num: 3                                              # The number of retry attempts when there is an API call error or output parsing error.#       enable_vllm: false                                      # If true, use VLLM for loading hugging face or local llm. Otherwise, use API for reference.#       model_params: &#123;#         &#x27;api_key&#x27;: &quot;sk-312f14f4e65144f1b9f9240a31b584f9&quot;,  # ä½ çš„ OpenAI API Key#         &#x27;base_url&#x27;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;  # OpenAI API Base URL#       &#125;                                        # Parameters for initializing the API model.#       sampling_params: &#123;#         &#x27;temperature&#x27;: 0.9,#         &#x27;top_p&#x27;: 0.95#       &#125;                                     # Extra parameters passed to the API call. e.g &#123;&#x27;temperature&#x27;: 0.9, &#x27;top_p&#x27;: 0.95&#125;
python tools/analyze_data.py --config configs/demo/analyzer.yaml




1. è¯­è¨€æ£€æµ‹ï¼ˆlangï¼‰

count: 19.0ï¼ˆ19 ä¸ªæ ·æœ¬éƒ½æœ‰è¯­è¨€æ ‡è®°ï¼‰ã€‚
unique: 1.0ï¼ˆåªæœ‰ä¸€ç§è¯­è¨€ï¼‰ã€‚
top: zhï¼ˆæœ€å¸¸è§çš„è¯­è¨€æ˜¯ä¸­æ–‡ï¼‰ã€‚
freq: 19.0ï¼ˆæ‰€æœ‰ 19 ä¸ªæ ·æœ¬éƒ½æ˜¯ä¸­æ–‡ï¼‰ã€‚
ç»“è®ºï¼šè¿™ä¸ªæ•°æ®é›†æ˜¯çº¯ä¸­æ–‡æ•°æ®é›†ï¼Œæ²¡æœ‰å…¶ä»–è¯­è¨€æ··å…¥ã€‚

2. è¯­è¨€å¾—åˆ†ï¼ˆlang_scoreï¼‰
å®šä¹‰ï¼šè¯­è¨€å¾—åˆ†é€šå¸¸è¡¨ç¤ºæ–‡æœ¬è¢«è¯†åˆ«ä¸ºæŸç§è¯­è¨€çš„å¯ä¿¡åº¦ï¼Œåˆ†æ•°èŒƒå›´ä¸€èˆ¬æ˜¯ 0 åˆ° 1ï¼Œè¶Šé«˜è¡¨ç¤ºè¶Šå¯èƒ½æ˜¯è¯¥è¯­è¨€ã€‚
ç»Ÿè®¡ç»“æœï¼š

mean: 0.949412ï¼ˆå¹³å‡å¾—åˆ†çº¦ä¸º 0.95ï¼Œè¡¨æ˜æ–‡æœ¬è¢«è¯†åˆ«ä¸ºä¸­æ–‡çš„ç½®ä¿¡åº¦å¾ˆé«˜ï¼‰ã€‚
std: 0.02133ï¼ˆæ ‡å‡†å·®è¾ƒå°ï¼Œè¯´æ˜å¾—åˆ†åˆ†å¸ƒè¾ƒä¸ºé›†ä¸­ï¼Œæ ·æœ¬é—´å·®å¼‚ä¸å¤§ï¼‰ã€‚
min: 0.909303ï¼ˆæœ€ä½å¾—åˆ†ä»é«˜äº 0.9ï¼Œè¯´æ˜å³ä½¿æ˜¯æœ€å·®çš„æ ·æœ¬ï¼Œä¸­æ–‡è¯†åˆ«ç½®ä¿¡åº¦ä¹Ÿå¾ˆé«˜ï¼‰ã€‚
25%: 0.937739ï¼ˆ25% åˆ†ä½æ•°ï¼Œè¯´æ˜å¤§éƒ¨åˆ†æ ·æœ¬å¾—åˆ†é«˜äº 0.94ï¼‰ã€‚
50%: 0.95398ï¼ˆä¸­ä½æ•°ï¼Œæ¥è¿‘å‡å€¼ï¼Œåˆ†å¸ƒè¾ƒä¸ºå¯¹ç§°ï¼‰ã€‚
75%: 0.958543ï¼ˆ75% åˆ†ä½æ•°ï¼Œå¤§éƒ¨åˆ†æ ·æœ¬å¾—åˆ†é›†ä¸­åœ¨ 0.96 ä»¥ä¸‹ï¼‰ã€‚
max: 0.98278ï¼ˆæœ€é«˜å¾—åˆ†æ¥è¿‘ 1ï¼Œè¡¨æ˜æœ‰äº›æ ·æœ¬å‡ ä¹è¢«å®Œç¾è¯†åˆ«ä¸ºä¸­æ–‡ï¼‰ã€‚

ç»“è®ºï¼šè¯­è¨€å¾—åˆ†æ•´ä½“è¾ƒé«˜ä¸”åˆ†å¸ƒç¨³å®šï¼Œè¯´æ˜æ•°æ®é›†çš„æ–‡æœ¬è¯­è¨€ä¸€è‡´æ€§éå¸¸å¥½ï¼Œå‡ ä¹å¯ä»¥ç¡®å®šä¸ºé«˜è´¨é‡çš„ä¸­æ–‡æ–‡æœ¬ã€‚
3. å›°æƒ‘åº¦ï¼ˆperplexityï¼‰
å®šä¹‰ï¼šå›°æƒ‘åº¦æ˜¯è¡¡é‡è¯­è¨€æ¨¡å‹å¯¹æ–‡æœ¬é¢„æµ‹éš¾åº¦çš„æŒ‡æ ‡ï¼Œé€šå¸¸ä¸æ–‡æœ¬çš„å¤æ‚æ€§ã€å¯é¢„æµ‹æ€§ç›¸å…³ã€‚å€¼è¶Šä½ï¼Œæ–‡æœ¬è¶Šå®¹æ˜“è¢«æ¨¡å‹ç†è§£ï¼›å€¼è¶Šé«˜ï¼Œæ–‡æœ¬å¯èƒ½æ›´å¤æ‚æˆ–å™ªå£°æ›´å¤§ã€‚
ç»Ÿè®¡ç»“æœï¼š

mean: 4351.347368ï¼ˆå¹³å‡å›°æƒ‘åº¦çº¦ä¸º 4351ï¼Œåé«˜ï¼‰ã€‚
std: 1360.413693ï¼ˆæ ‡å‡†å·®è¾ƒå¤§ï¼Œè¯´æ˜æ ·æœ¬é—´å›°æƒ‘åº¦å·®å¼‚æ˜æ˜¾ï¼‰ã€‚
min: 1270.3ï¼ˆæœ€ä½å›°æƒ‘åº¦ï¼Œè¡¨æ˜æœ‰äº›æ ·æœ¬è¾ƒä¸ºç®€å•æˆ–è§„å¾‹æ€§å¼ºï¼‰ã€‚
25%: 3974.1ï¼ˆ25% åˆ†ä½æ•°ï¼Œå¤§å¤šæ•°æ ·æœ¬å›°æƒ‘åº¦é«˜äº 3974ï¼‰ã€‚
50%: 4415.3ï¼ˆä¸­ä½æ•°ï¼Œä¸å‡å€¼æ¥è¿‘ï¼Œåˆ†å¸ƒç›¸å¯¹å‡åŒ€ï¼‰ã€‚
75%: 5062.0ï¼ˆ75% åˆ†ä½æ•°ï¼Œè¡¨æ˜ 25% çš„æ ·æœ¬å›°æƒ‘åº¦é«˜äº 5062ï¼‰ã€‚
max: 7357.8ï¼ˆæœ€é«˜å›°æƒ‘åº¦ï¼Œè¯´æ˜æŸäº›æ ·æœ¬å¯èƒ½éå¸¸å¤æ‚æˆ–åŒ…å«å™ªå£°ï¼‰ã€‚

ç»“è®ºï¼šå›°æƒ‘åº¦å‡å€¼å’ŒèŒƒå›´éƒ½è¾ƒé«˜ï¼ˆ1270.3 åˆ° 7357.8ï¼‰ï¼Œä¸”æ ‡å‡†å·®è¾ƒå¤§ï¼Œè¡¨æ˜æ•°æ®é›†æ–‡æœ¬å¤æ‚åº¦å·®å¼‚æ˜¾è‘—ã€‚éƒ¨åˆ†æ ·æœ¬å¯èƒ½åŒ…å«ä¸è§„åˆ™å†…å®¹ã€å™ªå£°æˆ–è¿‡äºå¤æ‚çš„è¡¨è¾¾ï¼Œè¿™å¯èƒ½ä¼šå½±å“æ¨¡å‹è®­ç»ƒæ•ˆæœã€‚
jsonlæ ¼å¼æ•°æ®-&gt;Q&amp;Aé—®ç­”å¯¹æ ¼å¼
jsonlæ ¼å¼æ•°æ®é›†
é‡‡ç”¨huggingfaceå¼€æºçš„BAAI/IndustryCorpuså’ŒBAAI/IndustryCorpus2
è®¡ç®—æœºæ•°æ®é›†ï¼šhttps://huggingface.co/datasets/BAAI/IndustryCorpus_computer
å›½å®¶æ”¿æ²»ç±»æ•°æ®é›†ï¼šhttps://huggingface.co/datasets/BAAI/IndustryCorpus2_current_affairs_government_administration
æ¸…æ´—å¤„ç†åçš„æ”¿ä¼ç±»æ•°æ®é›†ï¼š/mnthp/users/wangxiangbo/data-juicer-main/outputs/demo-process/test-processed4.jsonl
è®¡ç®—æœºç±»æ•°æ®é›†jsonlæŠ½å–
ä¸ºäº†æµ‹è¯•ä»£ç å’ŒèŠ‚çº¦apiï¼Œä»…æµ‹è¯•å…¶ä¸­10æ¡æ•°æ®
&#123;&quot;text&quot;:&quot;æ¨ç‰¹æ€æ ·æŸ¥çœ‹è¯„è®º(æ¨ç‰¹æ€ä¹ˆçœ‹è¯„è®ºåŒº)\nå»å¹´22æœˆå¼€å§‹,æ¨ç‰¹å¼€å§‹æµ‹è¯•ç±»ä¼¼äºRedditçš„å¯¹è¯æ ‘ç”¨æˆ·ç•Œé¢,ä»¥å¸®åŠ©ç”¨æˆ·æ›´è½»æ¾åœ°è¿½è¸ªè½¬å‘å’Œåç»­æ¨æ–‡.åœ¨ç»è¿‡5ä¸ªå¤šæœˆçš„æµ‹è¯•ä¹‹å,è¯¥åŠŸèƒ½ç»ˆäºé¢å‘æ‰€æœ‰ç”¨æˆ·å¼€æ”¾.\nç±»ä¼¼äºRedditçš„å¯¹è¯æ ‘ç°åœ¨å¯ä»¥åœ¨Windows10çš„Twitter PWAå’Œç½‘é¡µç«¯ä¸­ä½¿ç”¨.å¦‚æœä½ ä½¿ç”¨çš„æ˜¯Androidæ‰‹æœº,å¹¶ä¸”æ‰‹æœºä¸Šå®‰è£…äº†Twitter Lite,ä½ ä¹Ÿä¼šçœ‹åˆ°è¿™ä¸ªå˜åŒ–.å€¼å¾—æ³¨æ„çš„æ˜¯,ç”±äºè¿™æ˜¯ä¸€ä¸ªæœåŠ¡å™¨ç«¯çš„æ›´æ–°,æ‰€ä»¥ä¸éœ€è¦ä½ è®¿é—®Google Play Storeæˆ–Microsoft Store,ä¹Ÿä¸éœ€è¦ä½ å»æ›´æ–°Twitter webåº”ç”¨.&quot;,&quot;alnum_ratio&quot;:0.9172661871,&quot;avg_line_length&quot;:92.6666666667,&quot;char_rep_ratio&quot;:0.029739777,&quot;flagged_words_ratio&quot;:0.0,&quot;industry_type&quot;:&quot;è®¡ç®—æœº&quot;,&quot;lang&quot;:&quot;zh&quot;,&quot;lang_score&quot;:0.8875751495,&quot;max_line_length&quot;:180,&quot;num_words&quot;:73,&quot;perplexity&quot;:1061.4,&quot;special_char_ratio&quot;:0.1079136691,&quot;word_rep_ratio&quot;:0.0,&quot;id&quot;:3822520893459&#125;


å›½å®¶æ”¿æ²»ç±»æ•°æ®é›†parquetæ ¼å¼è½¬ä¸ºjsonlæ ¼å¼
import pandas as pd# è¯»å– parquet æ–‡ä»¶df = pd.read_parquet(&#x27;/mnt/data/government_administration/chinese/middle/rank_00410.parquet&#x27;)# è½¬æ¢ä¸º JSON Lines æ ¼å¼å¹¶ä¿å­˜df.to_json(&quot;data.json&quot;, orient=&quot;records&quot;, lines=True, force_ascii=False)
&#123;&quot;text&quot;:&quot;ä¸­å›½å¤–äº¤éƒ¨å‘è¨€äººè€¿çˆ½27æ—¥åœ¨åŒ—äº¬è¡¨ç¤º,ä¸­æ–¹å‚ä¸æ‰“å‡»ç´¢é©¬é‡Œæµ·ç›—å›½é™…åˆä½œçš„è¡ŒåŠ¨,å¾—åˆ°äº†æœ‰å…³å›½å®¶çš„ç§¯æè¯„ä»·,ä¸ºç»´æŠ¤å›½é™…å’Œåœ°åŒºçš„å’Œå¹³ä¸å®‰å…¨ä½œå‡ºé‡è¦è´¡çŒ®,ä¹Ÿå±•ç°äº†ä¸­å›½è´Ÿè´£ä»»,æœ‰æ‹…å½“çš„å¤§å›½å½¢è±¡.\nä¸­æ–°ç¤¾è®°è€… ä¾¯å®‡ æ‘„\n  åœ¨å½“å¤©ä¸¾è¡Œçš„å¤–äº¤éƒ¨ä¾‹è¡Œè®°è€…ä¼šä¸Š,æœ‰è®°è€…æé—®:æ—¥å‰,è”åˆå›½ç§˜ä¹¦é•¿å‘å®‰ç†ä¼šæäº¤ç´¢é©¬é‡Œæµ·ç›—é—®é¢˜æŠ¥å‘Š.æŠ¥å‘Šå¤šæ¬¡æåŠä¸­å›½åœ¨äºšä¸æ¹¾å’Œç´¢é©¬é‡Œæµ·åŸŸæ‰§è¡ŒæŠ¤èˆªä»»åŠ¡,ä¸ºåº”å¯¹æµ·ç›—å¨èƒå‘æŒ¥äº†é‡è¦ä½œç”¨.ä¸­æ–¹å¯¹æ­¤æœ‰ä½•è¯„è®º?\n  è€¿çˆ½å›åº”è¯´,2008å¹´12æœˆ,ç»è”åˆå›½å®‰ç†ä¼šæˆæƒ,ä¸­å›½æµ·å†›æ´¾å‡ºé¦–æ‰¹èˆ°è‰‡ç¼–é˜Ÿ,èµ´äºšä¸æ¹¾,ç´¢é©¬é‡Œæµ·åŸŸæ‰§è¡ŒæŠ¤èˆªä»»åŠ¡.æˆªè‡³2017å¹´7æœˆ,ä¸­å›½æµ·å†›å…ˆåæ´¾å‡ºäº†26æ‰¹æŠ¤èˆªç¼–é˜Ÿ,ä¸ºä¸­å¤–6400ä½™è‰˜èˆ¹åªæä¾›æŠ¤èˆª,é©±ç¦»ç–‘ä¼¼æµ·ç›—èˆ¹åª3000ä½™è‰˜æ¬¡,æœ‰åŠ›ä¿éšœäº†å›½é™…é‡è¦è´¸æ˜“èˆªé“å®‰å…¨.ä¸­æ–¹å‚ä¸æ‰“å‡»ç´¢é©¬é‡Œæµ·ç›—å›½é™…åˆä½œçš„è¡ŒåŠ¨,å¾—åˆ°äº†æœ‰å…³å›½å®¶çš„ç§¯æè¯„ä»·,ä¸ºç»´æŠ¤å›½é™…å’Œåœ°åŒºçš„å’Œå¹³ä¸å®‰å…¨ä½œå‡ºé‡è¦è´¡çŒ®,ä¹Ÿå±•ç°äº†ä¸­å›½è´Ÿè´£ä»»,æœ‰æ‹…å½“çš„å¤§å›½å½¢è±¡.\n  \&quot;è”åˆå›½ç§˜ä¹¦é•¿çš„æœ‰å…³æŠ¥å‘Šå¯¹ä¸­æ–¹æ‰€ä½œå·¥ä½œå’Œè´¡çŒ®ç»™äºˆäº†ç§¯æè¯„ä»·,æˆ‘ä»¬å¯¹æ­¤è¡¨ç¤ºèµèµ.\&quot;è€¿çˆ½è¯´,æ­£å¦‚ä¹ è¿‘å¹³æ€»ä¹¦è®°åœ¨åä¹å¤§æŠ¥å‘Šä¸­æ‰€æŒ‡å‡ºçš„,ä¸­å›½å°†æªå®ˆç»´æŠ¤ä¸–ç•Œå’Œå¹³,ä¿ƒè¿›å…±åŒå‘å±•çš„å¤–äº¤æ”¿ç­–å®—æ—¨,å§‹ç»ˆåšä¸–ç•Œå’Œå¹³çš„å»ºè®¾è€…,å…¨çƒå‘å±•çš„è´¡çŒ®è€…,å›½é™…ç§©åºçš„ç»´æŠ¤è€….ä¸­æ–¹å°†ç»§ç»­ç§¯æå‚ä¸äºšä¸æ¹¾å’Œç´¢é©¬é‡Œæµ·åŸŸæŠ¤èˆªè¡ŒåŠ¨,æ‹“å±•å›½é™…æŠ¤èˆªåˆä½œ,å±¥è¡Œå›½é™…è´£ä»»å’Œä¹‰åŠ¡,ä¸ºç»´æŠ¤å›½é™…å’Œåœ°åŒºå’Œå¹³ä¸ç¨³å®š,ä¿éšœå›½é™…é‡è¦é€šé“å®‰å…¨å‘æŒ¥æ›´å¤§ä½œç”¨.(å®Œ)&quot;,&quot;alnum_ratio&quot;:0.9024793388,&quot;avg_line_length&quot;:121.0,&quot;char_rep_ratio&quot;:0.0771812081,&quot;flagged_words_ratio&quot;:0.0,&quot;max_line_length&quot;:204,&quot;num_words&quot;:213,&quot;perplexity&quot;:701.0,&quot;quality_score&quot;:3.79296875,&quot;special_char_ratio&quot;:0.1371900826,&quot;word_rep_ratio&quot;:0.1848184818,&quot;_id&quot;:1005000038194,&quot;industry_type&quot;:&quot;æ—¶æ”¿_æ”¿åŠ¡_è¡Œæ”¿&quot;&#125;


é€šè¿‡é˜¿é‡Œapi_keyåˆ†æåŸå§‹jsonlå¹¶ç”ŸæˆQAå¾®è°ƒç±»æ•°æ®é›†
pyè„šæœ¬
import osimport jsonfrom openai import OpenAI# åˆå§‹åŒ–å®¢æˆ·ç«¯try:    client = OpenAI(        api_key=&quot;sk-312f14f4e65144f1b9f9240a31b584f9&quot;,  # è¯·æ›¿æ¢ä¸ºä½ çš„å®é™…API Key        base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,    )    # è¾“å…¥æ–‡ä»¶è·¯å¾„    input_file = &quot;/mnt/data/IndustryCorpus_computer/10.jsonl&quot;        # è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆåªä¿å­˜Q&amp;Aé—®ç­”å¯¹ï¼‰    output_file = &quot;/mnt/data/IndustryCorpus_computer/10_qa_pairs.jsonl&quot;    # è®¡æ•°å™¨    processed_count = 0    # æ‰“å¼€è¾“å…¥æ–‡ä»¶å¹¶é€è¡Œå¤„ç†    with open(input_file, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f:        for line in f:            try:                # è§£ææ¯ä¸€è¡Œçš„JSONæ•°æ®                data = json.loads(line.strip())                                # æ„é€ è¯·æ±‚æ¶ˆæ¯ï¼Œè¦æ±‚è¿”å›ç‰¹å®šJSONæ ¼å¼                completion = client.chat.completions.create(                    model=&quot;qwen-plus&quot;,                    messages=[                        &#123;                            &#x27;role&#x27;: &#x27;system&#x27;,                             &#x27;content&#x27;: &#x27;&#x27;&#x27;ä½ æ˜¯ä¸€ä¸ªæ“…é•¿ä¿¡æ¯æå–å’Œé—®ç­”å¯¹è½¬æ¢çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æ–‡æœ¬ï¼Œåˆ†æå…¶å†…å®¹å¹¶è½¬æ¢ä¸ºé€‚åˆè®­ç»ƒçš„é—®ç­”å¯¹æ ¼å¼ï¼Œä¸€å¯¹é—®ç­”å¯¹å³å¯ã€‚è¾“å‡ºå¿…é¡»æ˜¯ä»¥ä¸‹JSONæ ¼å¼ï¼š                            &#123;                                &quot;instruction&quot;: &quot;é—®é¢˜&quot;,                                &quot;output&quot;: &quot;ç­”æ¡ˆ&quot;                            &#125;                            ç¡®ä¿é—®é¢˜å’Œç­”æ¡ˆå†…å®¹å‡†ç¡®åæ˜ è¾“å…¥æ–‡æœ¬ä¿¡æ¯ã€‚&#x27;&#x27;&#x27;                        &#125;,                        &#123;                            &#x27;role&#x27;: &#x27;user&#x27;,                             &#x27;content&#x27;: json.dumps(data, ensure_ascii=False)                        &#125;                    ]                )                                # è·å–å¤§æ¨¡å‹çš„è¾“å‡ºï¼ˆJSONæ ¼å¼çš„é—®ç­”å¯¹ï¼‰                qa_pair = completion.choices[0].message.content                                # æ‰“å°ç»“æœ                print(f&quot;å¤„ç†ç¬¬ &#123;processed_count + 1&#125; è¡Œ:&quot;)                print(qa_pair)                print(&quot;-&quot; * 50)                                # ä¿å­˜JSONæ ¼å¼çš„é—®ç­”å¯¹åˆ°æ–‡ä»¶                with open(output_file, &#x27;a&#x27;, encoding=&#x27;utf-8&#x27;) as out_f:                    out_f.write(qa_pair + &quot;\n&quot;)                                processed_count += 1                            except Exception as line_error:                print(f&quot;å¤„ç†ç¬¬ &#123;processed_count + 1&#125; è¡Œæ—¶å‡ºé”™ï¼š&#123;line_error&#125;&quot;)                continue        print(f&quot;å¤„ç†å®Œæˆï¼Œæ€»å…±å¤„ç† &#123;processed_count&#125; è¡Œæ•°æ®&quot;)except Exception as e:    print(f&quot;åˆå§‹åŒ–æˆ–ä¸»ç¨‹åºé”™è¯¯ï¼š&#123;e&#125;&quot;)    print(&quot;è¯·å‚è€ƒæ–‡æ¡£ï¼šhttps://help.aliyun.com/zh/model-studio/developer-reference/error-code&quot;)
å¤„ç†ç»“æœ
è®¡ç®—æœºç±»æ•°æ®é›†


é€šè¿‡ç§»åŠ¨äº‘api_keyåˆ†æåŸå§‹jsonlå¹¶ç”ŸæˆQAå¾®è°ƒç±»æ•°æ®é›†
pyè„šæœ¬
from openai import OpenAIimport jsonimport osimport reimport logging# è®¾ç½®æ—¥å¿—logging.basicConfig(    level=logging.INFO,    format=&#x27;%(asctime)s - %(levelname)s - %(message)s&#x27;,    handlers=[        logging.FileHandler(&quot;/mnt/api/log/government.log&quot;, encoding=&#x27;utf-8&#x27;),        logging.StreamHandler()    ])logger = logging.getLogger(__name__)# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯client = OpenAI(    api_key=&quot;jn8P1B1vl7AbKZanw-rDQ9ahJmUH3eML_hAaKGP9wkA&quot;,    base_url=&quot;https://zhenze-huhehaote.cmecloud.cn/inference-api/exp-api/inf-1336781912337387520/v1&quot;)# æ–‡ä»¶è·¯å¾„input_file_path = &quot;/mnt/data-juicer-main/outputs/demo-process/test-processed2.jsonl&quot;output_file_path = &quot;/mnt/api/res/government_qa.jsonl&quot;# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨output_dir = os.path.dirname(output_file_path)if not os.path.exists(output_dir):    os.makedirs(output_dir)# ç³»ç»Ÿæç¤ºsystem_prompt = &#123;    &quot;role&quot;: &quot;system&quot;,    &quot;content&quot;: &quot;&quot;&quot;ä½ æ˜¯ä¸€ä¸ªæ“…é•¿ä¿¡æ¯æå–å’Œé—®ç­”å¯¹è½¬æ¢çš„åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æ–‡æœ¬ï¼Œåˆ†æå…¶å†…å®¹å¹¶è½¬æ¢ä¸ºä¸€ä¸ªé—®ç­”å¯¹ï¼Œæ ¼å¼å¿…é¡»ä¸ºï¼š    &#123;&quot;instruction&quot;: &quot;é—®é¢˜&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;ç­”æ¡ˆ&quot;&#125;    åªç”Ÿæˆä¸€å¯¹é—®ç­”å¯¹ï¼Œè¿”å›JSONå­—ç¬¦ä¸²ã€‚    &quot;&quot;&quot;&#125;# è®¡æ•°å™¨processed_count = 0error_count = 0# ä»å“åº”ä¸­æå–JSONçš„å‡½æ•°def extract_json_from_response(response):    # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”ä¸ºJSON    try:        qa_json = json.loads(response)        if all(key in qa_json for key in [&quot;instruction&quot;, &quot;input&quot;, &quot;output&quot;]):            if qa_json[&quot;input&quot;] != &quot;&quot;:                qa_json[&quot;input&quot;] = &quot;&quot;  # å¼ºåˆ¶è®¾ä¸ºç©ºå­—ç¬¦ä¸²            return qa_json    except json.JSONDecodeError:        pass        # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…JSON    json_match = re.search(r&#x27;```json\s*(&#123;.*?&#125;)\s*```&#x27;, response, re.DOTALL)    if json_match:        json_str = json_match.group(1)        try:            qa_json = json.loads(json_str)            if all(key in qa_json for key in [&quot;instruction&quot;, &quot;input&quot;, &quot;output&quot;]):                if qa_json[&quot;input&quot;] != &quot;&quot;:                    qa_json[&quot;input&quot;] = &quot;&quot;  # å¼ºåˆ¶è®¾ä¸ºç©ºå­—ç¬¦ä¸²                return qa_json        except json.JSONDecodeError:            pass        # å¦‚æœä»ç„¶å¤±è´¥ï¼Œè¿”å›None    return None# è¯»å–è¾“å…¥æ–‡ä»¶å¹¶å¤„ç†with open(input_file_path, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as input_file, \     open(output_file_path, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as output_file:        for line_num, line in enumerate(input_file, 1):        try:            # è§£ææ¯ä¸€è¡Œçš„JSONæ•°æ®            data = json.loads(line.strip())            user_input = json.dumps(data, ensure_ascii=False)                        # æ„é€ æ¶ˆæ¯            messages = [                system_prompt,                &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input&#125;            ]                        # è°ƒç”¨API            response = client.chat.completions.create(                model=&quot;default&quot;,                messages=messages,                max_tokens=1024,                temperature=0.6,                stream=False            )                        # è·å–ç»“æœ            qa_result = response.choices[0].message.content.strip()            logger.info(f&quot;Line &#123;line_num&#125; - Raw response: &#123;qa_result&#125;&quot;)                        # ä»å“åº”ä¸­æå–JSON            qa_json = extract_json_from_response(qa_result)                        # å¦‚æœæå–å¤±è´¥ï¼Œè®°å½•åŸå§‹å“åº”            if qa_json is None:                qa_json = &#123;                    &quot;instruction&quot;: &quot;é—®é¢˜æå–å¤±è´¥&quot;,                    &quot;input&quot;: &quot;&quot;,                    &quot;output&quot;: f&quot;åŸå§‹å“åº”: &#123;qa_result&#125;&quot;                &#125;                logger.warning(f&quot;Line &#123;line_num&#125; - Failed to extract JSON, using fallback&quot;)            else:                logger.info(f&quot;Line &#123;line_num&#125; - Successfully extracted JSON: &#123;json.dumps(qa_json, ensure_ascii=False)&#125;&quot;)                        # å†™å…¥æ–‡ä»¶            output_file.write(json.dumps(qa_json, ensure_ascii=False) + &#x27;\n&#x27;)            processed_count += 1                        # æ§åˆ¶å°æ˜¾ç¤ºè¿›åº¦            print(f&quot;å·²å¤„ç†: &#123;processed_count&#125; è¡Œ&quot;)                    except json.JSONDecodeError as e:            error_count += 1            logger.error(f&quot;Line &#123;line_num&#125; - JSONè§£æé”™è¯¯: &#123;e&#125;, Input: &#123;line.strip()&#125;&quot;)            print(f&quot;Line &#123;line_num&#125; - JSONè§£æé”™è¯¯: &#123;e&#125;&quot;)            print(&quot;-&quot; * 50)        except Exception as e:            error_count += 1            logger.error(f&quot;Line &#123;line_num&#125; - å¤„ç†å‡ºé”™: &#123;e&#125;, Input: &#123;line.strip()&#125;&quot;)            print(f&quot;Line &#123;line_num&#125; - å¤„ç†å‡ºé”™: &#123;e&#125;&quot;)            print(&quot;-&quot; * 50)# æ‰“å°æ€»ç»“print(&quot;=&quot; * 50)print(f&quot;å¤„ç†å®Œæˆï¼&quot;)print(f&quot;æ€»è®¡å¤„ç†è¡Œæ•°: &#123;processed_count + error_count&#125;&quot;)print(f&quot;æˆåŠŸç”Ÿæˆé—®ç­”å¯¹: &#123;processed_count&#125;&quot;)print(f&quot;é”™è¯¯è¡Œæ•°: &#123;error_count&#125;&quot;)print(f&quot;ç»“æœå·²ä¿å­˜è‡³: &#123;output_file_path&#125;&quot;)print(f&quot;æ—¥å¿—å·²ä¿å­˜è‡³: /mnt/api/log/government.log&quot;)
å¤„ç†ç»“æœ
å›½å®¶æ”¿æ²»ç±»æ•°æ®é›†


&#123;&quot;instruction&quot;: &quot;ä¸­å›½æµ·å†›é¦–æ‰¹èµ´äºšä¸æ¹¾ã€ç´¢é©¬é‡Œæµ·åŸŸæ‰§è¡ŒæŠ¤èˆªä»»åŠ¡çš„èˆ°è‰‡ç¼–é˜Ÿæ˜¯åœ¨ä½•æ—¶æ´¾å‡ºçš„ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;2008å¹´12æœˆï¼Œç»è”åˆå›½å®‰ç†ä¼šæˆæƒï¼Œä¸­å›½æµ·å†›æ´¾å‡ºé¦–æ‰¹èˆ°è‰‡ç¼–é˜Ÿèµ´äºšä¸æ¹¾ã€ç´¢é©¬é‡Œæµ·åŸŸæ‰§è¡ŒæŠ¤èˆªä»»åŠ¡ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;é—µè¡ŒåŒºä¸ƒå®é•‡å…šå§”åœ¨æå‡å…šç¾¤æœåŠ¡é˜µåœ°èƒ½çº§æ–¹é¢é‡‡å–äº†å“ªäº›æªæ–½ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;ä¸ƒå®é•‡å…šå§”é€šè¿‡ç»‡å¥½&#x27;ç†æƒ³ä¿¡å¿µç½‘&#x27;å¼ºåŒ–æ”¿æ²»å»ºè®¾ï¼Œé‡‡ç”¨&#x27;çº¿ä¸Š+çº¿ä¸‹&#x27;ç›¸ç»“åˆçš„ç«‹ä½“åŒ–å­¦ä¹ æ•™è‚²æ–¹å¼ï¼Œæ¨å‡ºåŒ…å«ä¸ƒå·æ¡¥ç¢‰å ¡ã€å¼ å……ä»çºªå¿µé¦†ç­‰18ä¸ªçº¢è‰²ç‚¹ä½ç»„æˆçš„è¡ŒçŸ¥è·¯çº¿ï¼Œå¹¶åˆ›æ–°å¼€å±•æƒ…æ™¯å…šè¯¾ï¼Œå…¨é¢æå‡å…šç¾¤æœåŠ¡é˜µåœ°èƒ½çº§ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;æ±ªè¾œä¼šè°ˆåœ¨ä¸¤å²¸å…³ç³»å‘å±•ä¸­å…·æœ‰æ€æ ·çš„å†å²æ„ä¹‰ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;æ±ªè¾œä¼šè°ˆæ˜¯æµ·å³¡ä¸¤å²¸æˆæƒçš„æ°‘é—´æœºæ„æœ€é«˜è´Ÿè´£äººä¹‹é—´çš„é¦–æ¬¡ä¼šæ™¤ï¼Œä¹Ÿæ˜¯ä¸¤å²¸é«˜å±‚äººå£«åœ¨é•¿æœŸéš”æ–­åçš„é¦–æ¬¡æ­£å¼æ¥è§¦ï¼Œæ ‡å¿—ç€ä¸¤å²¸èµ°å‘å’Œè§£çš„å†å²æ€§çªç ´ï¼Œæˆä¸ºä¸¤å²¸å…³ç³»å‘å±•è¿›ç¨‹ä¸­çš„\&quot;é‡è¦é‡Œç¨‹ç¢‘\&quot;ã€‚å°½ç®¡ä¼šè°ˆèŒƒå›´é™äºæ°‘é—´æ€§ã€ç»æµæ€§å’Œäº‹åŠ¡æ€§é¢†åŸŸï¼Œä½†æˆåŠŸå¯åŠ¨äº†ä¸¤å²¸å¯¹è¯è°ˆåˆ¤æœºåˆ¶ï¼Œæ¨åŠ¨ä¸¤å²¸å…³ç³»å‘æ›´æ·±å±‚æ¬¡å‘å±•ï¼Œä¸ºåç»­ä¸¤å²¸å…³ç³»å’Œå¹³å‘å±•è¿›å…¥å·©å›ºæ·±åŒ–æœŸå¥ å®šäº†åŸºç¡€ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;æ³°å·å¸‚äº”å±Šäººå¤§äºŒæ¬¡ä¼šè®®é€‰ä¸¾äº§ç”Ÿçš„å¸‚äººå¤§å¸¸å§”ä¼šä¸»ä»»å’Œå¸‚ç›‘å¯Ÿå§”å‘˜ä¼šä¸»ä»»åˆ†åˆ«æ˜¯è°ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;æ›²ç¦ç”°å½“é€‰ä¸ºå¸‚äººå¤§å¸¸å§”ä¼šä¸»ä»»ï¼Œæ±¤æˆå¿«å½“é€‰ä¸ºå¸‚ç›‘å¯Ÿå§”å‘˜ä¼šä¸»ä»»ã€‚&quot;&#125;
æ¸…æ´—å¤„ç†åçš„æ”¿ä¼ç±»æ•°æ®é›†






&#123;&quot;instruction&quot;: &quot;ä¼˜åŒ–æ”¿åŠ¡æœåŠ¡ã€æå‡è¡Œæ”¿æ•ˆèƒ½çš„é‡è¦æŠ“æ‰‹æ˜¯ä»€ä¹ˆï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;æŠŠ\&quot;é«˜æ•ˆåŠæˆä¸€ä»¶äº‹\&quot;ä½œä¸ºä¼˜åŒ–æ”¿åŠ¡æœåŠ¡ã€æå‡è¡Œæ”¿æ•ˆèƒ½çš„é‡è¦æŠ“æ‰‹ï¼Œé€šè¿‡åŠ å¼ºæ•´ä½“è®¾è®¡ã€æ¨åŠ¨æ¨¡å¼åˆ›æ–°ã€æ³¨é‡æ”¹é©å¼•é¢†å’Œæ•°å­—èµ‹èƒ½åŒè½®é©±åŠ¨ï¼Œå®ç°åŠäº‹æ–¹å¼å¤šå…ƒåŒ–ã€æµç¨‹æœ€ä¼˜åŒ–ã€ææ–™æœ€ç®€åŒ–å’Œæˆæœ¬æœ€å°åŒ–ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;åˆ°2027å¹´æˆ‘å›½é«˜æ•ˆæ”¿åŠ¡æœåŠ¡ä½“ç³»å»ºè®¾è¦å®ç°ä»€ä¹ˆç›®æ ‡ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;åŸºæœ¬å½¢æˆæ³›åœ¨å¯åŠã€æ™ºæ…§ä¾¿æ·ã€å…¬å¹³æ™®æƒ çš„é«˜æ•ˆæ”¿åŠ¡æœåŠ¡ä½“ç³»ï¼Œå®ç°ä¼ä¸šå’Œä¸ªäººä¸¤ä¸ªå…¨ç”Ÿå‘½å‘¨æœŸé‡è¦é˜¶æ®µ&#x27;é«˜æ•ˆåŠæˆä¸€ä»¶äº‹&#x27;é‡ç‚¹äº‹é¡¹è½åœ°è§æ•ˆï¼Œå¤§å¹…æå‡ä¼ä¸šå’Œç¾¤ä¼—åŠäº‹æ»¡æ„åº¦ã€è·å¾—æ„Ÿã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;å¦‚ä½•æ¨è¿›çº¿ä¸‹åŠäº‹â€œåªè¿›ä¸€é—¨â€ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;é€šè¿‡å®Œå–„é›†çº¦é«˜æ•ˆçš„çº¿ä¸‹æ”¿åŠ¡æœåŠ¡ä½“ç³»ï¼Œå¿çº§ä»¥ä¸Šæ”¿åŠ¡æœåŠ¡ä¸­å¿ƒåº”ç»Ÿä¸€è®¾ç½®ç»¼åˆ/ä¸“ä¸šæœåŠ¡çª—å£é›†ä¸­åŠç†æ”¿åŠ¡æœåŠ¡äº‹é¡¹ï¼›æ•´åˆéƒ¨é—¨å•è®¾çª—å£è‡³æœ¬çº§ä¸­å¿ƒæˆ–å®è¡Œä¸€ä½“åŒ–ç®¡ç†ï¼›ç»Ÿç­¹å»ºè®¾ä¹¡é•‡(è¡—é“)ä¾¿æ°‘æœåŠ¡ä¸­å¿ƒå’Œæ‘(ç¤¾åŒº)ä¾¿æ°‘æœåŠ¡ç«™ï¼›é¼“åŠ±åœ¨é“¶è¡Œç½‘ç‚¹ã€é‚®æ”¿ç½‘ç‚¹ã€å›­åŒºè®¾ç½®ä¾¿æ°‘æœåŠ¡ç‚¹ï¼Œå¹¶åˆ©ç”¨é›†æˆå¼è‡ªåŠ©ç»ˆç«¯æä¾›24å°æ—¶æœåŠ¡ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;å¦‚ä½•æ¨è¿›çº¿ä¸ŠåŠäº‹â€œä¸€ç½‘é€šåŠâ€ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;åŠ å¼ºå›½å®¶æ”¿åŠ¡æœåŠ¡å¹³å°ä¸ºæ€»æ¢çº½çš„å…¨å›½æ”¿åŠ¡æœåŠ¡â€œä¸€å¼ ç½‘â€å»ºè®¾ï¼Œæ¨åŠ¨é«˜é¢‘äº‹é¡¹ç½‘ä¸ŠåŠã€æŒä¸ŠåŠã€ä¸€æ¬¡åŠï¼›å»ºè®¾çœçº§æ”¿åŠ¡æœåŠ¡å¹³å°ç»Ÿä¸€å—ç†ç«¯å®ç°åŠä»¶ä¿¡æ¯å®æ—¶å…±äº«ï¼›æ•´åˆå„çº§å¹³å°èµ„æºæ¨åŠ¨æ”¿åŠ¡æœåŠ¡äº‹é¡¹å’Œåº”ç”¨â€œåº”æ¥å°½æ¥ã€åº”ä¸Šå°½ä¸Šâ€ï¼Œé™¤æ³•å®šæˆ–æ¶‰å¯†æƒ…å†µå¤–æ‰€æœ‰äº‹é¡¹å‡çº³å…¥åŒçº§æ”¿åŠ¡æœåŠ¡å¹³å°åŠç†ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;å¦‚ä½•æ¨è¿›ä¼ä¸šå’Œç¾¤ä¼—è¯‰æ±‚çš„â€œä¸€çº¿åº”ç­”â€ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;é€šè¿‡ä¾æ‰˜12345æ”¿åŠ¡æœåŠ¡ä¾¿æ°‘çƒ­çº¿åŠ å¼ºçƒ­çº¿å½’å¹¶ï¼Œè®¾ç½®é‡ç‚¹é¢†åŸŸä¸“å¸­æå‡æ¥åŠæ•ˆç‡ï¼Œå»ºç«‹â€œæ¥è¯‰å³åŠâ€æœºåˆ¶åŠæ—¶å“åº”è¯‰æ±‚ï¼Œæ¨åŠ¨12345ä¸110é«˜æ•ˆå¯¹æ¥è”åŠ¨ï¼Œå¹¶åŠ å¼ºçƒ­çº¿ä¸æ”¿åŠ¡æœåŠ¡å¹³å°æŠ•è¯‰å»ºè®®ä½“ç³»ã€â€œå¥½å·®è¯„â€ä½“ç³»çš„ä¸šåŠ¡ååŒï¼Œæ‰“é€ æ™ºæ…§æ”¿åŠ¡æœåŠ¡â€œæ€»å®¢æœâ€ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;æ¨è¿›å…³è”äº‹é¡¹é›†æˆåŠéœ€è¦é‡‡å–å“ªäº›å…·ä½“æªæ–½ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;1. å»ºç«‹&#x27;ä¸€ä»¶äº‹&#x27;ç‰µå¤´éƒ¨é—¨ä¸é…åˆéƒ¨é—¨çš„èŒè´£åˆ†å·¥ä½“ç³»ï¼›2. é‡æ„è·¨éƒ¨é—¨ä¸šåŠ¡æµç¨‹ï¼Œå®ç°å¤šè¡¨åˆä¸€ã€ä¸€ç½‘é€šåŠå’Œææ–™ä¸€æ¬¡æäº¤ï¼›3. å¼€å±•çº¿ä¸Šçº¿ä¸‹å¹¶è”å®¡æ‰¹ã€è”åˆè¯„å®¡å’Œè”åˆéªŒæ”¶ï¼›4. é€šè¿‡ç³»ç»ŸååŒå’Œæ•°æ®å…±äº«å‹ç¼©åŠç†æ—¶é—´ï¼Œé™ä½åŠäº‹æˆæœ¬ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;æ¨è¿›å®¹ç¼ºäº‹é¡¹æ‰¿è¯ºåŠä¸­æåˆ°çš„â€œå‘ŠçŸ¥æ‰¿è¯º+å®¹ç¼ºåŠç†â€å®¡æ‰¹æœåŠ¡æ¨¡å¼çš„ä¸»è¦ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;åœ¨é£é™©å¯æ§ã€çº é”™æˆæœ¬ä½ä¸”èƒ½å¤Ÿé€šè¿‡äº‹ä¸­äº‹åç›‘ç®¡æœ‰æ•ˆé˜²èŒƒé£é™©çš„å‰æä¸‹ï¼Œæ¨è¿›æ”¿åŠ¡æœåŠ¡ä¾¿åˆ©åŒ–ï¼Œé€šè¿‡ç”³è¯·ææ–™åè¡¥/å…äº¤ã€å®è´¨å®¡æŸ¥åç½®/è±å…ç­‰æ–¹å¼å®ç°æ”¿åŠ¡æœåŠ¡äº‹é¡¹æ‰¿è¯ºåŠç†ï¼Œå¹¶å»ºç«‹å·®å¼‚åŒ–çš„å‘ŠçŸ¥æ‰¿è¯ºäº‹åæ ¸æŸ¥å’Œé£é™©é˜²èŒƒæœºåˆ¶ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;å¦‚ä½•æ¨è¿›æ”¿åŠ¡æœåŠ¡å¼‚åœ°äº‹é¡¹è·¨åŸŸåŠç†ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;é€šè¿‡æ¨åŠ¨æ”¿åŠ¡æœåŠ¡äº‹é¡¹çœå†…é€šåŠã€è·¨çœé€šåŠï¼Œä¾æ‰˜å…¨å›½ä¸€ä½“åŒ–æ”¿åŠ¡æœåŠ¡å¹³å°å®ç°æ•°æ®å…±äº«å’Œç½‘ä¸ŠåŠç†ï¼›ä¼˜åŒ–çº¿ä¸‹ä»£æ”¶ä»£åŠæœåŠ¡æ¨¡å¼ï¼Œå»ºç«‹è·¨åŸŸçª—å£ååŒæœºåˆ¶ï¼›è¿ç”¨è¿œç¨‹è™šæ‹Ÿçª—å£æŠ€æœ¯æä¾›å¸®åŠæœåŠ¡ï¼Œå¹¶æ”¯æŒäº¬æ´¥å†€ç­‰é‡ç‚¹åŒºåŸŸæ¢ç´¢æ›´å¤šè·¨åŒºåŸŸåŠç†äº‹é¡¹ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;å¦‚ä½•æ¨è¿›æ”¿ç­–æœåŠ¡å…ç”³åŠï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;é€šè¿‡å…¨é¢æ¢³ç†æ”¿ç­–æ¡ä»¶ä¸è§„åˆ™ï¼Œä¾æ‰˜æ•°æ®å½’é›†å…±äº«ã€æ¨¡å‹ç®—æ³•å’Œå¤§æ•°æ®åˆ†æï¼Œç²¾å‡†åŒ¹é…ç¬¦åˆæ¡ä»¶çš„ä¼ä¸šä¸ç¾¤ä¼—ï¼Œæ¨åŠ¨æ”¿ç­–â€œå…ç”³å³äº«â€ï¼›å¯¹éœ€ç”³è¯·çš„æ”¿ç­–è‡ªåŠ¨ç”Ÿæˆç”³è¯·è¡¨å¹¶ç²¾å‡†æ¨é€ç”³è¯·ææ–™ï¼ŒåŒæ—¶ä¼˜åŒ–æ”¿åŠ¡æœåŠ¡å¹³å°ä¸“å±æœåŠ¡ç©ºé—´ï¼Œå®ç°æ”¿ç­–ç²¾å‡†ç›´è¾¾ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;æ ¹æ®æ–‡æœ¬å†…å®¹ï¼Œæ¨åŠ¨æ”¿åŠ¡æœåŠ¡å¹³å°çš„æ•´åˆä¸­ï¼Œå…³äºåœ°å¸‚çº§ä»¥ä¸‹æ”¿åŠ¡æœåŠ¡å¹³å°çš„å»ºè®¾æœ‰ä»€ä¹ˆè§„å®šï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;åŸåˆ™ä¸Šä¸å†å•ç‹¬å»ºè®¾åœ°å¸‚çº§ä»¥ä¸‹æ”¿åŠ¡æœåŠ¡å¹³å°ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;å¦‚ä½•æå‡æ”¿åŠ¡æ•°æ®å…±äº«å®æ•ˆï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;é€šè¿‡å®Œå–„æ”¿åŠ¡æ•°æ®å…±äº«è´£ä»»æ¸…å•æœºåˆ¶ï¼Œä¾æ‰˜å…¨å›½ä¸€ä½“åŒ–æ”¿åŠ¡æœåŠ¡å¹³å°æ•°æ®å…±äº«æ¢çº½æ¨åŠ¨éƒ¨é—¨æ•°æ®å›æµå’Œç›´è¾¾åŸºå±‚ï¼Œç¼–åˆ¶&#x27;ä¸€ä¼ä¸€æ¡£ã€ä¸€äººä¸€æ¡£&#x27;æ•°æ®è§„èŒƒå®ç°&#x27;ä¸€æ•°ä¸€æºä¸€æ ‡å‡†&#x27;ï¼Œæ·±åŒ–ç”µå­è¯ç…§è·¨åœ°åŒºè·¨éƒ¨é—¨äº’é€šäº’è®¤ï¼Œæ¨åŠ¨æ”¿åºœéƒ¨é—¨æ ¸å‘ææ–™å…æäº¤ã€å…±äº«æ ¸éªŒäº‹é¡¹å…è¯æ˜ã€ç”µå­è¯ç…§å…å®ä½“è¯ç…§ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;å¦‚ä½•æ¨åŠ¨æ”¿åŠ¡æœåŠ¡ç”±äººåŠ›æœåŠ¡å‹å‘äººæœºäº¤äº’å‹å’Œæ•°æ®åˆ†æå‹è½¬å˜ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;é€šè¿‡åˆ›æ–°åº”ç”¨å¤§æ•°æ®ã€åŒºå—é“¾ã€äººå·¥æ™ºèƒ½ç­‰æ–°æŠ€æœ¯ï¼Œä¼˜åŒ–é‡æ„æœåŠ¡æµç¨‹å¹¶å®Œå–„æ™ºèƒ½é¢„å¡«ã€é¢„å®¡åŠŸèƒ½ï¼ŒåŒæ—¶åœ¨å…·ä½“åŠäº‹åœºæ™¯ä¸­åº”ç”¨è‡ªç„¶è¯­è¨€å¤§æ¨¡å‹æŠ€æœ¯æå‡æ™ºèƒ½å®¢æœèƒ½åŠ›ï¼Œæ¨åŠ¨æ”¿åŠ¡æœåŠ¡å‘äººæœºäº¤äº’å‹å’Œæ•°æ®åˆ†æå‹è½¬å˜ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;å¦‚ä½•å¢å¼ºåœ¨é«˜æ–°åŒºã€äº§ä¸šå›­åŒºçš„é¡¹ç›®å¸®åŠä»£åŠèƒ½åŠ›ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;é€šè¿‡åŠ å¼ºé¡¹ç›®å…¨æµç¨‹å¸®åŠä»£åŠï¼Œç»“åˆå®é™…å»ºç«‹ä¸“ç­æœåŠ¡ã€ä¸“å‘˜è·Ÿè¿›ç­‰æœºåˆ¶ï¼ŒåŠæ—¶è§£å†³é¡¹ç›®æ¨è¿›ä¸­çš„éš¾ç‚¹é—®é¢˜ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;åœ¨ä¸°å¯Œå…¬å…±æœåŠ¡ä¾›ç»™æ–¹é¢ï¼Œæ–°å¢äº†å“ªäº›ä¸ä¼ä¸šå’Œç¾¤ä¼—ç”Ÿäº§ç”Ÿæ´»å¯†åˆ‡ç›¸å…³çš„æœåŠ¡é¢†åŸŸï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;æ–°å¢äº†æ°´ç”µæ°”çƒ­ã€ç½‘ç»œé€šä¿¡ç­‰å…¬ç”¨äº‹ä¸šé¢†åŸŸçš„é«˜é¢‘åŠç†æœåŠ¡äº‹é¡¹ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;å¦‚ä½•æ‹“å±•æ¶‰ä¼å¢å€¼æœåŠ¡å†…å®¹ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;é€šè¿‡ç»Ÿç­¹è¡Œä¸šåä¼šã€å¸‚åœºåŒ–ä¸“ä¸šæœåŠ¡æœºæ„ç­‰èµ„æºï¼Œä¸€ç«™å¼æä¾›æ”¿ç­–æœåŠ¡ã€æ³•å¾‹æœåŠ¡ã€é‡‘èæœåŠ¡ã€äººæ‰æœåŠ¡ã€ç§‘åˆ›æœåŠ¡å’Œå›½é™…è´¸æ˜“æœåŠ¡ï¼Œå…·ä½“åŒ…æ‹¬æ”¿ç­–æ¨è/ç”³æŠ¥ã€å…¬è¯/åˆè§„æŒ‡å¯¼ã€èèµ„æ‹…ä¿/äº§ä¸šåŸºé‡‘å¯¹æ¥ã€äººæ‰è®¤å®š/ä½æˆ¿å®‰å±…ã€ç§‘æŠ€ä¼ä¸šåŸ¹è‚²ã€æŠ¥å…³é€€ç¨å’¨è¯¢ç­‰æœåŠ¡ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;å¦‚ä½•å¥å…¨æ”¿åŠ¡æœåŠ¡æ ‡å‡†ä½“ç³»ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;å¥å…¨æ”¿åŠ¡æœåŠ¡æ ‡å‡†ä½“ç³»éœ€åŠ å¼ºæ€»ä½“è®¾è®¡ï¼Œåˆ¶å®šå…¨å›½ä¸€ä½“åŒ–æ”¿åŠ¡æœåŠ¡å¹³å°ã€æ”¿åŠ¡æœåŠ¡ä¸­å¿ƒå»ºè®¾åŠ12345çƒ­çº¿è¿è¡Œæ ‡å‡†ï¼Œç»Ÿä¸€æ”¿åŠ¡æœåŠ¡äº‹é¡¹åŸºç¡€æ ‡å‡†ï¼Œç¼–åˆ¶é›†æˆåŠç†\&quot;ä¸€ä»¶äº‹\&quot;æŒ‡å—ï¼Œå¹¶å®Œå–„è·¨åŒºåŸŸåŠäº‹ä¸šåŠ¡æ ‡å‡†ä¸æ“ä½œè§„ç¨‹ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;å¦‚ä½•å¼ºåŒ–æ”¿åŠ¡æœåŠ¡åˆ¶åº¦ä¾›ç»™ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;å¼ºåŒ–æ”¿åŠ¡æœåŠ¡åˆ¶åº¦ä¾›ç»™éœ€é€šè¿‡ä¸‰æ–¹é¢æªæ–½ï¼š1.åŠ å¼ºå›½å®¶å±‚é¢ç«‹æ³•ç ”ç©¶å¹¶æ¸…ç†ä¿®è®¢ä¸é€‚åº”æ€§æ³•è§„ï¼›2.ç ´é™¤é›†æˆåŠ/æ‰¿è¯ºåŠ/è·¨åŸŸåŠ/å…ç”³åŠçš„åˆ¶åº¦éšœç¢ï¼›3.å®Œå–„æ•°å­—åŒ–åº”ç”¨é…å¥—æ”¿ç­–ä¿éšœç”µå­è¯ç…§å…±äº«å’Œç”µå­æ¡£æ¡ˆæ•ˆåŠ›ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;å¦‚ä½•å¥å…¨æ”¿åŠ¡æœåŠ¡å·¥ä½œä½“ç³»ï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;æ¨è¿›å›½å®¶ã€çœã€å¸‚ã€å¿ã€ä¹¡äº”çº§æ”¿åŠ¡æœåŠ¡ä½“ç³»å»ºè®¾ï¼Œå¥å…¨ä¸€ä½“è”åŠ¨ã€é«˜æ•ˆä¾¿æ·ã€æƒè´£æ¸…æ™°çš„å·¥ä½œæœºåˆ¶ï¼›åŠ å¼ºå„çº§æ”¿åŠ¡æœåŠ¡çª—å£ä»ä¸šäººå‘˜é…å¤‡ã€ç®¡ç†ã€åŸ¹è®­å’Œè€ƒæ ¸ï¼Œæ¨è¿›ç»¼åˆæœåŠ¡çª—å£äººå‘˜ç»Ÿä¸€é…å¤‡å’ŒèŒä¸šåŒ–å‘å±•ï¼›åˆ›æ–°æ”¿åŠ¡æœåŠ¡äººæ‰å¼•è¿›ã€åŸ¹å…»ã€é€‰æ‹”å’Œè¯„ä»·æœºåˆ¶ï¼Œæå‡å¹²éƒ¨é˜Ÿä¼æ³•æ²»æ€ç»´ã€æœåŠ¡æ„è¯†å’Œæ•°å­—ç´ å…»ï¼Œå¼ºåŒ–ä¸“ä¸šåŒ–é˜Ÿä¼å»ºè®¾ã€‚&quot;&#125;&#123;&quot;instruction&quot;: &quot;å¦‚ä½•ç¡®ä¿â€œé«˜æ•ˆåŠæˆä¸€ä»¶äº‹â€å–å¾—å®æ•ˆï¼Ÿ&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;1.å„åœ°åŒºè¦å¼ºåŒ–çœçº§ç»Ÿç­¹ï¼Œé€é¡¹åˆ¶å®šå·¥ä½œæ–¹æ¡ˆå¹¶æ˜ç¡®ç›®æ ‡ä»»åŠ¡ã€æ”¹é©æªæ–½å’ŒèŒè´£åˆ†å·¥ï¼›\n2.å›½åŠ¡é™¢å„éƒ¨é—¨éœ€åŠ å¼ºæ¡å—è”åŠ¨ï¼Œæä¾›ä¸šåŠ¡æŒ‡å¯¼å’Œæ•°æ®å…±äº«æ”¯æ’‘ï¼›\n3.å›½åŠ¡é™¢åŠå…¬å…è¦ç»Ÿç­¹åè°ƒï¼Œå¼€å±•è¯•ç‚¹ç¤ºèŒƒå¹¶é€å¹´æ¨å‡ºé‡ç‚¹äº‹é¡¹æ¸…å•ï¼Œæ¨åŠ¨æ”¿åŠ¡æœåŠ¡æè´¨å¢æ•ˆã€‚&quot;&#125;


å¾®è°ƒæµ‹è¯•
æ•°æ®é›†
æŒ‰ä¸Šè¿°æ–¹æ³•ï¼Œé‡æ–°å¢åŠ äº†æ”¿ä¼ç±»æ•°æ®é›†ã€‚





æå–å¹¶åˆ‡åˆ†åçš„æ•°æ®




æ¸…æ´—ç»“æœå¦‚ä¸‹




æ¸…æ´—ç»“æœå¯¹æ¯”








é—®ç­”å¯¹ç”Ÿæˆ






ms-swift æ¡†æ¶å¾®è°ƒ
ç¯å¢ƒæ­å»º
ms-swift è®­ç»ƒç¯å¢ƒå·²ä¼ è‡³é•œåƒ
registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:swift
è®­ç»ƒè„šæœ¬
åŸºäºæ”¿ä¼ç±»æ•°æ®é›†çš„ qwen2.5-7B-Ins-lora å¾®è°ƒ
# 22GBCUDA_VISIBLE_DEVICES=0,1,2,3 \swift sft \    --model /mnt/model/Qwen/Qwen2.5-7B-Instruct \    --train_type lora \    --dataset /mnt/ms-swift/data/train.jsonl \    --torch_dtype float16 \    --num_train_epochs 6 \    --per_device_train_batch_size 1 \    --per_device_eval_batch_size 1 \    --learning_rate 1e-4 \    --lora_rank 8 \    --lora_alpha 32 \    --target_modules all-linear \    --gradient_accumulation_steps 1 \    --eval_steps 50 \    --save_steps 50 \    --save_total_limit 5 \    --logging_steps 1 \    --max_length 2048 \    --output_dir /mnt/ms-swift/output \    --system &#x27;ä½ æ˜¯ä¸€ä¸ªè‹å·æ”¿ä¼é—®ç­”å°åŠ©æ‰‹&#x27; \    --warmup_ratio 0.05 \    --dataloader_num_workers 4 \    --model_author swift \    --model_name swift-qwen2.5-7b
åŸºäºæ”¿ä¼ç±»æ•°æ®é›†çš„ qwen2.5-0.5B-Ins-å…¨å‚å¾®è°ƒ
# 76GiBCUDA_VISIBLE_DEVICES=0,1,2,3 \swift sft \    --model /mnt/model/Qwen2.5-0.5B-Instruct \    --train_type full \    --dataset /mnt/ms-swift/data/train.jsonl \    --torch_dtype float32 \    --num_train_epochs 6 \    --per_device_train_batch_size 1 \    --per_device_eval_batch_size 1 \    --learning_rate 1e-5 \    --gradient_accumulation_steps 16 \    --eval_steps 100 \    --save_steps 100 \    --save_total_limit 5 \    --logging_steps 1 \    --max_length 2048 \    --output_dir /mnt/ms-swift/output \    --system &#x27;You are a helpful assistant.&#x27; \    --warmup_ratio 0.05 \    --dataloader_num_workers 4 \    --model_author swift \    --model_name swift-qwen2.5-0.5b
è®­ç»ƒ
æ—¥å¿—




æ˜¾å­˜å ç”¨æƒ…å†µ


loss æ›²çº¿


æ¨ç†æµ‹è¯•
å¼€å¯ swift web ç•Œé¢æœåŠ¡
swift web-ui --lang zh --server_port 16686







]]></content>
      <categories>
        <category>æ¨¡å‹å¾®è°ƒ</category>
        <category>æ•°æ®é›†å¤„ç†</category>
      </categories>
      <tags>
        <tag>NVIDIA</tag>
        <tag>V100</tag>
        <tag>æ¨¡å‹å¾®è°ƒ</tag>
        <tag>æ•°æ®é›†å¤„ç†</tag>
      </tags>
  </entry>
  <entry>
    <title>ms-swiftæ¡†æ¶å¾®è°ƒæµ‹è¯•</title>
    <url>/2026/01/08/LLM/%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6/ms-swift%E6%A1%86%E6%9E%B6/</url>
    <content><![CDATA[ms-swift æ¡†æ¶å¾®è°ƒ
ç¯å¢ƒæ­å»º
è®­ç»ƒè„šæœ¬
åŸºäºæ”¿ä¼ç±»æ•°æ®é›†çš„ qwen2.5-7B-Ins-lora å¾®è°ƒ
# 22GBCUDA_VISIBLE_DEVICES=0,1,2,3 \swift sft \    --model /mnt/model/Qwen/Qwen2.5-7B-Instruct \    --train_type lora \    --dataset /mnt/ms-swift/data/train.jsonl \    --torch_dtype float16 \    --num_train_epochs 6 \    --per_device_train_batch_size 1 \    --per_device_eval_batch_size 1 \    --learning_rate 1e-4 \    --lora_rank 8 \    --lora_alpha 32 \    --target_modules all-linear \    --gradient_accumulation_steps 1 \    --eval_steps 50 \    --save_steps 50 \    --save_total_limit 5 \    --logging_steps 1 \    --max_length 2048 \    --output_dir /mnt/ms-swift/output \    --system &#x27;ä½ æ˜¯ä¸€ä¸ªè‹å·æ”¿ä¼é—®ç­”å°åŠ©æ‰‹&#x27; \    --warmup_ratio 0.05 \    --dataloader_num_workers 4 \    --model_author swift \    --model_name swift-qwen2.5-7b
åŸºäºæ”¿ä¼ç±»æ•°æ®é›†çš„ qwen2.5-0.5B-Ins-å…¨å‚å¾®è°ƒ
# 76GiBCUDA_VISIBLE_DEVICES=0,1,2,3 \swift sft \    --model /mnt/model/Qwen2.5-0.5B-Instruct \    --train_type full \    --dataset /mnt/ms-swift/data/train.jsonl \    --torch_dtype float32 \    --num_train_epochs 6 \    --per_device_train_batch_size 1 \    --per_device_eval_batch_size 1 \    --learning_rate 1e-5 \    --gradient_accumulation_steps 16 \    --eval_steps 100 \    --save_steps 100 \    --save_total_limit 5 \    --logging_steps 1 \    --max_length 2048 \    --output_dir /mnt/ms-swift/output \    --system &#x27;You are a helpful assistant.&#x27; \    --warmup_ratio 0.05 \    --dataloader_num_workers 4 \    --model_author swift \    --model_name swift-qwen2.5-0.5b
è®­ç»ƒ
æ—¥å¿—


æ˜¾å­˜å ç”¨æƒ…å†µ


loss æ›²çº¿


æ¨ç†æµ‹è¯•
å¼€å¯ swift web ç•Œé¢æœåŠ¡
swift web-ui --lang zh --server_port 16686







]]></content>
      <categories>
        <category>æ¨¡å‹å¾®è°ƒ</category>
        <category>ms-swift</category>
      </categories>
      <tags>
        <tag>NVIDIA</tag>
        <tag>V100</tag>
        <tag>æ¨¡å‹å¾®è°ƒ</tag>
        <tag>ms-swift</tag>
      </tags>
  </entry>
  <entry>
    <title>tulu3è®ºæ–‡è§£è¯»</title>
    <url>/2026/01/08/LLM/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/tulu3%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[è¯­è¨€æ¨¡å‹çš„åè®­ç»ƒå¯ä»¥æ”¹è¿›å…¶æ€§èƒ½å¹¶è§£é”æ–°æŠ€èƒ½ï¼Œä¸ºäº†ç¼©å°å¼€æºæ–¹æ³•åœ¨è¿™æ–¹é¢é€šå¸¸è½åäºä¸“æœ‰é—­æºæŠ€æœ¯è¿™ä¸€å·®è·ï¼Œç ”ç©¶è€…æ¨å‡ºäº†TÃœLU3ï¼Œä¸€ä¸ªå®Œå…¨å¼€æºçš„é«˜çº§åè®­ç»ƒæ¨¡å‹ç³»åˆ—ï¼ŒåŒ…æ‹¬å…¶æ•°æ®ã€ä»£ç å’Œè®­ç»ƒæ–¹æ³•ã€‚åŸºäºLlama 3.1åŸºç¡€æ¨¡å‹ï¼ŒTÃœLU3çš„è¡¨ç°ä¼˜äºå¤šä¸ªçŸ¥åæ¨¡å‹ï¼Œå¦‚Llama 3.1ã€Qwen 2.5ã€Mistralç­‰ï¼Œå¹¶ä¸”åœ¨æŸäº›æ–¹é¢ç”šè‡³è¶…è¿‡äº†GPT-4o-miniå’ŒClaude 3.5-Haikuè¿™æ ·çš„é—­æºæ¨¡å‹ã€‚TÃœLU3é‡‡ç”¨**ç›‘ç£å¾®è°ƒ(SFT)ã€ç›´æ¥åå¥½ä¼˜åŒ–(DPO)ä»¥åŠä¸€ç§æ–°çš„å¯éªŒè¯å¥–åŠ±å¼ºåŒ–å­¦ä¹ (RLVR)**æ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚æ­¤å¤–ï¼Œè¿˜å»ºç«‹äº†ä¸€ä¸ªå…¨é¢çš„å¤šä»»åŠ¡è¯„ä¼°æ¡†æ¶ï¼Œæ¶µç›–äº†å¼€å‘ä¸æœªè§æµ‹è¯•ã€æ ‡å‡†åŸºå‡†åŠå¯¹ç°æœ‰å¼€æºæ•°æ®é›†çš„å¤§è§„æ¨¡æ¸…ç†ã€‚æœ€åï¼Œé¡¹ç›®æä¾›äº†æ¨¡å‹æƒé‡ã€ç¤ºä¾‹ã€æ•°æ®é›†ã€å·¥å…·åŒ…ã€è®­ç»ƒä»£ç å’Œè¯¦ç»†æŠ¥å‘Šï¼Œä»¥ä¾¿ä»–äººå¤åˆ¶ç»“æœå¹¶å°†è¿™äº›æŠ€æœ¯åº”ç”¨äºæ›´å¹¿æ³›çš„é¢†åŸŸã€‚
TÃœLU3 70B
TÃœLU3 8B
TÃœLU3 Data
TÃœLU3 Code
TÃœLU3 Eval
Introduction
â€œæ­£å¦‚éª†é©¼ä¸å•†é˜Ÿä¸­çš„å…¶ä»–éª†é©¼åˆ†æ‹…è´Ÿæ‹…ä¸€æ ·ï¼Œæ™ºè€…ä¹Ÿä¼šåˆ†äº«ä»–ä»¬çš„è§è§£ï¼Œä»¥å‡è½»æ— çŸ¥çš„è´Ÿæ‹…ã€‚â€ â€“ TÃœLU3äº§ç”Ÿçš„è°šè¯­ã€‚


ä¸ºäº†ç¼©å°å¼€æºå¼å’Œé—­æºå¼åè®­ç»ƒä¹‹é—´çš„å·®è·ï¼Œä½œè€…ä»¬æ¨å‡ºäº†è¿™æ˜¯ä¸€ç³»åˆ—å¼€æºçš„å…ˆè¿›åè®­ç»ƒæ¨¡å‹ï¼Œä»¥åŠæ‰€æœ‰æ•°æ®ã€è®­ç»ƒæ–¹æ³•ã€ä»£ç ã€åŸºç¡€è®¾æ–½å’Œè¯„ä¼°æ¡†æ¶ã€‚
ä¸ºäº†æ„å»º TÃœLU3ï¼Œä½œè€…ä»¬ç¡®å®šäº†ä¸€ç»„åè®­ç»ƒéœ€è¦æ”¹è¿›çš„æ ¸å¿ƒæŠ€èƒ½ï¼ˆä¾‹å¦‚æ¨ç†ã€æ•°å­¦ã€ç¼–ç ã€å®‰å…¨ã€ç²¾ç¡®éµå¾ªæŒ‡ä»¤ã€çŸ¥è¯†å›å¿†ç­‰ï¼‰ï¼Œå¹¶å»ºç«‹äº†ä¸€ä¸ªè¯„ä¼°æ¡†æ¶ï¼Œä»¥å»ºç«‹æ˜ç¡®çš„ç»©æ•ˆç›®æ ‡å¹¶æŒ‡å¯¼æ¨¡å‹åœ¨ä¸€ç³»åˆ—å¼€å‘å’Œæœªè§è¿‡çš„ä»»åŠ¡ä¸Šçš„æ”¹è¿›ã€‚TÃœLU3å—ç›ŠåŒªæµ…ï¼Œå®ƒå……åˆ†åˆ©ç”¨äº†å…¬å¼€å¯ç”¨çš„å¼€æºæ•°æ®ï¼Œåœ¨å„ä¸ªè®­ç»ƒé˜¶æ®µç”Ÿæˆå¤šæ ·åŒ–ã€ç‰¹å®šæŠ€èƒ½çš„åˆæˆæ•°æ®ï¼Œå¹¶æ ¹æ®ä½œè€…ä»¬çš„è¯„ä¼°å¥—ä»¶ç§¯æåœ°å‡€åŒ–è¿™äº›æ•°æ®ã€‚
TÃœLU3è®­ç»ƒRecipeæ¶‰åŠå¤šä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½ä»¥ä¹‹å‰çš„æ¨¡å‹ä¸ºåŸºç¡€ï¼Œå¹¶ä¾§é‡äºä¸åŒç±»å‹çš„æ•°æ®â€”â€”å³ç›‘ç£å¾®è°ƒçš„å¿«é€Ÿå®Œæˆå®ä¾‹ã€åå¥½è°ƒæ•´ä»¥åŠå¼ºåŒ–å­¦ä¹ çš„å¯éªŒè¯å¥–åŠ±ã€‚ä½œè€…ä»¬çš„æ–¹æ³•æœ‰åŠ©äºè¯†åˆ«æŠ€èƒ½ç¼ºé™·å¹¶æ”¹è¿›æ•°æ®ç»„åˆã€æ–¹æ³•å’Œå‚æ•°ï¼Œç¡®ä¿åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­æ ¸å¿ƒæŠ€èƒ½çš„å¹³è¡¡è¡¨ç°ã€‚é€šè¿‡ä¸¥æ ¼ã€æœ‰åŸåˆ™çš„å®éªŒï¼Œä½œè€…ä»¬ç¡®å®šäº†ç›‘ç£å¾®è°ƒçš„æœ€ä½³æ•°æ®ç»„åˆï¼Œä»è€Œå¾—å‡º TÃœLU3SFT æ£€æŸ¥ç‚¹ã€‚åˆ©ç”¨åå¥½è°ƒæ•´æ–¹é¢çš„æœ€æ–°è¿›å±•ï¼Œä½œè€…ä»¬éšåé€šè¿‡æ¯”è¾ƒ TÃœLU3SFT å®Œæˆæƒ…å†µä¸å…¶ä»–è¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼Œä½¿ç”¨ç²¾å¿ƒç­–åˆ’çš„ç­–ç•¥åå¥½æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚æ­¤å¤–ï¼Œä½œè€…ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„æœ€ç»ˆå¾®è°ƒé˜¶æ®µâ€”â€”å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹  (RLVR)â€”â€”å®ƒé‡‡ç”¨äº†ä¸€ç§æ–°é¢–çš„ RL ç›®æ ‡ï¼Œæ—¨åœ¨å¢å¼ºç‰¹å®šæŠ€èƒ½ï¼Œå…·æœ‰å¯éªŒè¯çš„ç­”æ¡ˆï¼Œä¾‹å¦‚æ•°å­¦å’Œç²¾ç¡®çš„æŒ‡ä»¤ã€‚
ä½œè€…ä»¬è¡¨ç°æœ€ä½³çš„æ–¹æ¡ˆäº§ç”Ÿçš„ TÃœLU3æ¨¡å‹çš„è¡¨ç°ä¼˜äºåŒç­‰è§„æ¨¡çš„æœ€å…ˆè¿›çš„åè®­ç»ƒå¼€æºæƒé‡æ¨¡å‹ï¼Œä¾‹å¦‚ Llama 3.1 Instructã€Qwen2.5 Instruct æˆ– Mistral-Instructï¼Œå¹¶ä¸”åœ¨ 70B è§„æ¨¡ä¸‹ï¼ŒTÃœLUå¯ä¸ Claude 3.5 Haiku å’Œ GPT-4o mini ç­‰é—­æºæä¾›å•†çš„äº§å“ç›¸åª²ç¾ã€‚
é—­æºæ¨¡å‹ç‰ˆæœ¬ï¼šGPT-3.5-Turbo-0125ã€GPT-4o-mini-2024-07-18ã€Claude 3.5 Haiku 20241022
æ€»è€Œè¨€ä¹‹ï¼ŒTÃœLU3ä»£è¡¨äº†ä¸€ç³»åˆ—æœ€å…ˆè¿›çš„å¼€æºè¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰ç°ä»£çš„åè®­ç»ƒæ¡†æ¶ã€å®Œå…¨å¼€æºçš„æ•°æ® TÃœLU3D ATA ã€è¯„ä¼° TÃœLU3E VAL ã€è®­ç»ƒä»£ç  TÃœLU3C ODE å’Œå¼€å‘Recipe TÃœLU3R ECIPE ã€‚ä»¥ä¸‹æ˜¯ TÃœLUå¼€å‘çš„ä¸€äº›å…³é”®è´¡çŒ®ï¼š

ä¸ºè¯„ä¼°ã€å‡€åŒ–å’ŒRecipeè®¾è®¡æä¾›å¹¿æ³›çš„æŒ‡å¯¼å’Œå·¥å…·ï¼Œ
æ‰©å±•çš„ã€æ–°çš„åˆæˆæ•™å­¦æ•°æ®é›†ï¼Œ
ä½¿ç”¨åŸºäºç­–ç•¥çš„ç”Ÿæˆæ¥æ‰©å±•åå¥½æ•°æ®ï¼Œ
å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼Œè¿™æ˜¯ä¸€ç§åŸºäº RL çš„æ–¹æ³•ï¼Œåªæœ‰åœ¨æ¨¡å‹çš„å®Œæˆè¢«éªŒè¯ä¸ºæ­£ç¡®æ—¶æ‰ä¼šè·å¾—å¥–åŠ±ï¼Œ
å…ˆè¿›çš„åŸºç¡€è®¾æ–½ã€ç»†èŠ‚å’Œä»£ç ï¼Œä»¥ä¿ƒè¿›å¤§å‹æ¨¡å‹çš„æˆåŠŸå®æ–½

ä½œè€…ä»¬çš„å·¥ä½œæˆæœæ˜¯å®Œå…¨å¼€æºçš„è¯­è¨€æ¨¡å‹å¾®è°ƒpipelineã€‚ä½œè€…ä»¬å‘å¸ƒäº†åœ¨ Llama 3.1 åŸºç¡€ç‰ˆæœ¬åŸºç¡€ä¸Šè®­ç»ƒçš„æœ€ç»ˆæ¨¡å‹ï¼Œå…¶ä¸­åŒ…å«ä¸­é—´æ£€æŸ¥ç‚¹ã€è®­ç»ƒæ•°æ®ã€è®­ç»ƒä»£ç å’Œè¯„ä¼°ä»£ç ï¼ˆå‘å¸ƒçš„å®Œæ•´å·¥ä»¶åˆ—è¡¨è§è¡¨ 1ï¼‰ã€‚å€ŸåŠ©æ‰€æœ‰å‘å¸ƒçš„èµ„æºï¼Œå…¶ä»–äººå¯ä»¥é‡‡ç”¨å¼€æºçš„åŸºç¡€æ¨¡å‹å¹¶å¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œä»¥åœ¨ä»»ä½•æ„Ÿå…´è¶£çš„ä»»åŠ¡ä¸Šå®ç°é«˜æ€§èƒ½ï¼Œä¸ºå¤æ‚ã€å¤šç›®æ ‡å’Œå¤šé˜¶æ®µè®­ç»ƒæ–¹æ¡ˆä¸­çš„åè®­ç»ƒç ”ç©¶å¥ å®šåŸºç¡€ã€‚


TÃœLU3  Overview
è¡¨ 2ï¼šTÃœLU3 EVALç”±å¼€å‘å’Œéšè—çš„åˆ†å‰²ç»„æˆï¼Œç”¨äºè¯„ä¼°æ ¸å¿ƒæŠ€èƒ½ã€‚å€ŸåŠ© TÃœLU3 EVAL ï¼Œä½œè€…ä»¬å‘å¸ƒäº†ç»Ÿä¸€çš„æ ‡å‡†åŒ–è¯„ä¼°å¥—ä»¶å’Œå·¥å…·åŒ…ï¼Œç”¨äºæ ¹æ®åŸºå‡†å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œå‡€åŒ–ã€‚ä¸‹æ ‡æ˜¾ç¤ºäº†ä½œè€…ä»¬ç”¨äºè¯„ä¼°çš„æŒ‡æ ‡ã€‚TÃœLU3 Safety æ˜¯å®‰å…¨è¯„ä¼°çš„é›†åˆï¼Œè¯¦æƒ…è¯·å‚é˜…ç¬¬ 7.2.1 èŠ‚ã€‚


TÃœLU3çªç ´äº†åè®­ç»ƒç ”ç©¶çš„ç•Œé™ï¼Œç¼©å°äº†å¼€æºå¼å’Œé—­æºå¼å¾®è°ƒæ–¹æ³•ä¹‹é—´çš„å·®è·ã€‚å€ŸåŠ© TÃœLU3ï¼Œä½œè€…ä»¬å¸Œæœ›å‘ç°å¼€æºç¤¾åŒºçš„å“ªäº›è·¯å¾„å°†å–å¾—æˆåŠŸï¼Œå“ªäº›è·¯å¾„ä¸ä¼šï¼ˆé€šè¿‡æŠ¥å‘Šè´Ÿé¢ç»“æœï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„è®­ç»ƒè¿‡ç¨‹ï¼Œå°†ä¸“æœ‰æ–¹æ³•çš„éƒ¨åˆ†ç»†èŠ‚ä¸æ–°æŠ€æœ¯å’Œæˆç†Ÿçš„å­¦æœ¯ç ”ç©¶ç›¸ç»“åˆã€‚mdelname 3 æˆåŠŸçš„å…³é”®å› ç´ åœ¨äºä»”ç»†çš„æ•°æ®ç®¡ç†ã€ä¸¥æ ¼çš„å®éªŒå’Œè¯„ä¼°ã€åˆ›æ–°çš„æ–¹æ³•å’Œæ”¹è¿›çš„è®­ç»ƒåŸºç¡€è®¾æ–½ã€‚ä½œè€…ä»¬éµå¾ªç³»ç»Ÿçš„æŒ‡å¯¼æ–¹é’ˆï¼Œé€šè¿‡åˆ›å»ºå¼€å‘å’Œæµ‹è¯•é›†è¿›è¡Œè¯„ä¼°ï¼Œå¯¹è¿™ä¸€è¿‡ç¨‹è¿›è¡Œç§‘å­¦è¯„ä¼°ï¼Œå¹¶å¯¹å…¬å¼€å¯ç”¨çš„æ•°æ®é›†è¿›è¡Œä»”ç»†çš„å‡€åŒ–ã€‚å¼€æºåè®­ç»ƒçš„TÃœLU3 ä¸ä»…ä»…æ˜¯ä¸€ä¸ªå·¥ä»¶ï¼Œè€Œæ˜¯ä¸€å¥—å…¨é¢çš„æ•°æ®å’Œå·¥å…·ï¼Œæ—¨åœ¨æ¨åŠ¨å‰æ²¿æŠ€æœ¯çš„å‘å±•ã€‚é€šè¿‡å…¬å¼€åˆ†äº«ä½œè€…ä»¬çš„æ•°æ®ã€Recipeå’Œå‘ç°ï¼Œä½œè€…ä»¬æ—¨åœ¨ä½¿ç¤¾åŒºèƒ½å¤Ÿæ¢ç´¢æ–°çš„å’Œåˆ›æ–°çš„åè®­ç»ƒæ–¹æ³•ã€‚
TÃœLU3 Data
æ ¸å¿ƒæŠ€èƒ½TÃœLU3 çš„å·¥ä½œé¦–å…ˆç¡®å®šäº†å¼€æºçš„åè®­ç»ƒæ–¹æ³•ç»å¸¸è½åçš„å…³é”®é¢†åŸŸï¼Œè¿™äº›é¢†åŸŸæ˜¯é€šç”¨è¯­è¨€æ¨¡å‹æ‰€éœ€çš„èƒ½åŠ›ã€‚è¡¨ 3 æ¦‚è¿°äº†ä½œè€…ä»¬æ—¨åœ¨å¢å¼ºçš„æ ¸å¿ƒèƒ½åŠ›ä»¥åŠä¸ºæ¶µç›–è¿™äº›æŠ€èƒ½è€Œé€‰æ‹©çš„è¯„ä¼°åŸºå‡†ã€‚åœ¨ TÃœLU3 ä¸­ï¼Œä½œè€…ä»¬ä¸“æ³¨äºçŸ¥è¯†å›å¿†ã€æ¨ç†ã€æ•°å­¦ã€ç¼–ç ã€æŒ‡ä»¤éµå¾ªã€ä¸€èˆ¬èŠå¤©å’Œå®‰å…¨ç­‰æ ¸å¿ƒæŠ€èƒ½ã€‚
ä½œè€…ä»¬é€šè¿‡ä»å…¬å…±æ•°æ®ä¸­è·å–æ•°æ®å¹¶ç»¼åˆæ•´ç†æ•°æ®æ¥æ•´ç†å’Œæ”¶é›† TÃœLU3D ATAä»¥é’ˆå¯¹è¿™äº›æ ¸å¿ƒæŠ€èƒ½ã€‚ä½œè€…ä»¬åœ¨è®­ç»ƒçš„ä¸åŒé˜¶æ®µä½¿ç”¨å„ç§æ•°æ®æ ¼å¼ã€‚è¡¨ 6 æ¦‚è¿°äº†ç”¨äºè®­ç»ƒä½œè€…ä»¬æ¨¡å‹çš„æ•°æ®é›†é›†åˆï¼Œæ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è§ç¬¬ 3 èŠ‚ã€‚
TÃœLU3  Evaluation
ä½œè€…ä»¬çš„åè®­ç»ƒæ–¹æ³•å–å¾—æˆåŠŸçš„ä¸€ä¸ªå…³é”®å› ç´ æ˜¯å»ºç«‹æ˜ç¡®çš„ç»©æ•ˆç›®æ ‡å’Œè¯„ä¼°å·¥å…·ï¼Œä»¥æŒ‡å¯¼è¿™äº›é˜¶æ®µçš„æ”¹è¿›ã€‚å€ŸåŠ© TÃœLU3E VAL ï¼Œä½œè€…ä»¬å‘å¸ƒäº†ç»Ÿä¸€çš„æ ‡å‡†åŒ–è¯„ä¼°å¥—ä»¶å’Œå·¥å…·åŒ…ï¼Œä»¥æŒ‡å¯¼å¼€å‘å’Œè¯„ä¼°æœ€ç»ˆæ¨¡å‹å¹¶æ ¹æ®è¯„ä¼°åŸºå‡†å‡€åŒ–è®­ç»ƒæ•°æ®ã€‚
ä½œè€…ä»¬çš„æ¡†æ¶åŒ…æ‹¬ä¸€ä¸ªç”¨äºå¯é‡å¤è¯„ä¼°çš„å¼€æºè¯„ä¼°å·¥å…·åŒ…ï¼ˆç¬¬ 7.1 èŠ‚ï¼‰ã€ä¸€å¥—ç”¨äºè¯„ä¼°æŒ‡ä»¤è°ƒæ•´æ¨¡å‹ä¸­æ ¸å¿ƒæŠ€èƒ½çš„å¥—ä»¶ï¼ˆå…·æœ‰å•ç‹¬å¼€å‘ï¼ˆç¬¬ 7.2 èŠ‚ï¼‰å’Œä¿ç•™è¯„ä¼°ï¼ˆç¬¬ 7.3 èŠ‚ï¼‰ï¼Œä»¥åŠä¸€ç»„åŸºäºä½œè€…ä»¬å¯¹å„ç§æ¨¡å‹çš„å®éªŒåœ¨ä½œè€…ä»¬çš„è¯„ä¼°å¥—ä»¶ä¸Šè¿›è¡Œè¯„ä¼°çš„æ¨èè®¾ç½®ã€‚è¿™ä¸¤ç§åˆ’åˆ†éƒ½æ¶µç›–äº†æ‰€æœ‰å·²è¯†åˆ«çš„æŠ€èƒ½ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œä½œè€…ä»¬æ²¡æœ‰æ£€æŸ¥ä½œè€…ä»¬æœªè§é›†ä¸Šçš„åˆ†æ•°
TÃœLU3  Recipe
åœ¨æœ¬èŠ‚ä¸­ï¼Œä½œè€…ä»¬æ¦‚è¿°äº† TÃœLU3 Recipe  ï¼Œä»¥è·å¾—æœ€å…ˆè¿›çš„åè®­ç»ƒæ¨¡å‹ã€‚ä½œè€…ä»¬åœ¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„åŸºç¡€ä¸Šé€šè¿‡å››é˜¶æ®µåè®­ç»ƒRecipeç”Ÿæˆ TÃœLU3æ¨¡å‹ï¼ˆè§å›¾ 1ï¼‰ã€‚TÃœLU3RECIPEæ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¤šé˜¶æ®µè®­ç»ƒpipelineï¼Œç»“åˆäº†å¼ºåŒ–å­¦ä¹ ä¸­çš„æ–°ç®—æ³•è¿›æ­¥ã€å°–ç«¯åŸºç¡€è®¾æ–½å’Œä¸¥æ ¼çš„å®éªŒï¼Œä»¥æ•´ç†æ•°æ®å¹¶ä¼˜åŒ–å„ä¸ªè®­ç»ƒé˜¶æ®µçš„æ•°æ®ç»„åˆã€æ–¹æ³•å’Œå‚æ•°ã€‚åœ¨æ‰€æœ‰é˜¶æ®µï¼Œä½œè€…ä»¬ä½¿ç”¨ç²¾å¿ƒé€‰æ‹©çš„è¯„ä¼°å¥—ä»¶æ¥è¡¡é‡æ¨¡å‹æ€§èƒ½ã€‚é˜¶æ®µå¦‚ä¸‹ï¼š

**ç¬¬ 1 é˜¶æ®µï¼šæ•°æ®ç®¡ç† (Â§3)ï¼Œ**ä½œè€…ä»¬ç®¡ç†å„ç§Promptsï¼Œä»¥åˆ†é…åˆ°å¤šä¸ªä¼˜åŒ–é˜¶æ®µã€‚ä½œè€…ä»¬åˆ›å»ºæ–°çš„åˆæˆPromptsï¼Œæˆ–è€…åœ¨å¯ç”¨æ—¶ä»ç°æœ‰æ•°æ®é›†ä¸­è·å–æºPromptsï¼Œä»¥é’ˆå¯¹ç‰¹å®šåŠŸèƒ½ã€‚ä½œè€…ä»¬ç¡®ä¿Promptsä¸ä¼šå—åˆ°ä½œè€…ä»¬çš„è¯„ä¼°å¥—ä»¶ TÃœLU3 EVAL çš„æ±¡æŸ“ã€‚
**ç¬¬ 2 é˜¶æ®µï¼šç›‘ç£å¾®è°ƒ (Â§4)ï¼Œ**ä½œè€…ä»¬å¯¹ç²¾å¿ƒæŒ‘é€‰çš„Promptså’Œå®Œæˆå†…å®¹è¿›è¡Œç›‘ç£å¾®è°ƒ (SFT)ã€‚é€šè¿‡å½»åº•çš„å®éªŒï¼Œåœ¨ä½œè€…ä»¬çš„è¯„ä¼°æ¡†æ¶çš„æŒ‡å¯¼ä¸‹ï¼Œç¡®å®šæœ€ç»ˆçš„ SFT æ•°æ®å’Œè®­ç»ƒè¶…å‚æ•°ï¼Œä»¥å¢å¼ºç›®æ ‡æ ¸å¿ƒæŠ€èƒ½ï¼Œè€Œä¸ä¼šæ˜¾è‘—å½±å“å…¶ä»–æŠ€èƒ½çš„è¡¨ç°ã€‚
**ç¬¬ 3 é˜¶æ®µï¼šåå¥½è°ƒæ•´ (Â§5)ï¼Œ**ä½œè€…ä»¬å°†åå¥½è°ƒæ•´ï¼ˆç‰¹åˆ«æ˜¯ DPOï¼‰åº”ç”¨äºæ–°æ•´ç†çš„åŸºäºç­–ç•¥çš„åˆæˆåå¥½æ•°æ®ï¼ˆä»é€‰å®šçš„Promptsä»¥åŠéç­–ç•¥æ•°æ®ä¸­ç”Ÿæˆï¼‰ã€‚ä¸ SFT é˜¶æ®µä¸€æ ·ï¼Œä½œè€…ä»¬é€šè¿‡å½»åº•çš„å®éªŒç¡®å®šæœ€ä½³åå¥½æ•°æ®ç»„åˆï¼Œå‘ç°å“ªäº›æ ¼å¼çš„æ•°æ®ã€æ–¹æ³•æˆ–è¶…å‚æ•°å¯ä»¥å¸¦æ¥æ”¹è¿›ã€‚
ç¬¬ 4 é˜¶æ®µï¼šå…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹ ï¼ˆÂ§6ï¼‰ï¼Œä½œè€…ä»¬å¼•å…¥äº†ä¸€ä¸ªæ–°çš„åŸºäº RL çš„åè®­ç»ƒé˜¶æ®µï¼Œè¯¥é˜¶æ®µä½¿ç”¨å¯éªŒè¯å¥–åŠ±è€Œä¸æ˜¯å¥–åŠ±æ¨¡å‹æ¥è®­ç»ƒæ¨¡å‹ï¼Œè¿™åœ¨ä¼ ç»Ÿ RLHF PPO è®­ç»ƒä¸­å¾ˆå¸¸è§ã€‚ä½œè€…ä»¬é€‰æ‹©å…·æœ‰å¯éªŒè¯ç»“æœçš„ä»»åŠ¡ï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜è§£å†³ï¼Œå¹¶ä¸”ä»…åœ¨æ¨¡å‹çš„ç”Ÿæˆè¢«éªŒè¯ä¸ºæ­£ç¡®æ—¶æ‰æä¾›å¥–åŠ±ã€‚ç„¶åä½œè€…ä»¬ä½¿ç”¨ RL æ¥è®­ç»ƒè¿™äº›å¥–åŠ±ã€‚

ä½œè€…ä»¬çš„ TÃœLU3æµç¨‹çš„ä¸»è¦è´¡çŒ®åœ¨äºæ”¹è¿›äº†æ•°æ®ã€æ–¹æ³•ã€åŸºç¡€è®¾æ–½å’Œä¸¥æ ¼çš„è¯„ä¼°ã€‚ä½œè€…ä»¬æµç¨‹çš„å…³é”®è¦ç´ åŒ…æ‹¬ï¼š

**æ•°æ®è´¨é‡ã€æ¥æºå’Œè§„æ¨¡ï¼ˆÂ§3ï¼‰**ä½œè€…ä»¬é€šè¿‡ä»”ç»†è°ƒæŸ¥å¯ç”¨çš„å¼€æºæ•°æ®é›†ã€åˆ†æå…¶æ¥æºå¹¶å¯¹å…¶è¿›è¡Œå‡€åŒ–ï¼Œä»¥åŠç­–åˆ’é’ˆå¯¹æ ¸å¿ƒæŠ€èƒ½çš„åˆæˆPromptsæ¥è·å¾—Promptsã€‚ä¸ºäº†ç¡®ä¿æœ‰æ•ˆæ€§ï¼Œä½œè€…ä»¬è¿›è¡Œäº†å½»åº•çš„å®éªŒæ¥ç ”ç©¶å®ƒä»¬å¯¹ä½œè€…ä»¬çš„å¼€å‘è¯„ä¼°å¥—ä»¶çš„å½±å“ã€‚
**åˆ›å»ºå¤šæŠ€èƒ½ SFT æ•°æ®é›†(Â§4.1) **é€šè¿‡å¯¹å„ç§æ•°æ®mixturesè¿›è¡Œå¤šè½®ç›‘ç£å¾®è°ƒï¼Œæ”¹è¿›äº†â€œä¸€èˆ¬â€å’Œâ€œç‰¹å®šæŠ€èƒ½â€ç±»åˆ«ä¸­Promptsçš„åˆ†å¸ƒã€‚ä¾‹å¦‚ï¼Œä¸ºäº†æé«˜æ•°å­¦æ¨ç†èƒ½åŠ›ï¼Œä½œè€…ä»¬é¦–å…ˆé€šè¿‡åˆ›å»ºæ•°å­¦ä¸“ä¸šæ¨¡å‹åœ¨è¯„ä¼°å¥—ä»¶ä¸­å»ºç«‹ä¸Šé™ï¼Œç„¶åæ··åˆæ•°æ®ä»¥ä½¿ä¸€èˆ¬æ¨¡å‹æ›´æ¥è¿‘è¿™ä¸ªä¸Šé™ã€‚
æ•´ç†åŸºäºç­–ç•¥çš„åå¥½æ•°æ®é›†(Â§5.2) ä½œè€…ä»¬å¼€å‘äº†åŸºäºç­–ç•¥çš„æ•°æ®æ•´ç†æµç¨‹æ¥æ‰©å±•ä½œè€…ä»¬çš„åå¥½æ•°æ®é›†ç”Ÿæˆã€‚å…·ä½“æ¥è¯´ï¼Œä½œè€…ä»¬ä» TÃœLU3-SFT å’Œå…¶ä»–æ¨¡å‹ä¸­ä¸ºç»™å®šçš„Promptsç”Ÿæˆå›ç­”ï¼Œå¹¶é€šè¿‡å®ƒä»¬çš„æˆå¯¹æ¯”è¾ƒè·å¾—åå¥½æ ‡ç­¾ã€‚ä½œè€…ä»¬çš„æ–¹æ³•æ‰©å±•å¹¶æ”¹è¿›äº† Cui ç­‰äººæå‡ºçš„åŸºäºç­–ç•¥çš„åå¥½æ•°æ®ç”Ÿæˆæ–¹æ³• [2023]ã€‚é€šè¿‡ç²¾å¿ƒé€‰æ‹©åå¥½æ•°æ®ï¼Œä½œè€…ä»¬è·å¾—äº† 354,192 ä¸ªç”¨äºåå¥½è°ƒæ•´çš„å®ä¾‹ï¼Œåœ¨ä¸€ç³»åˆ—ä»»åŠ¡ä¸­è¡¨ç°å‡ºæ˜¾è‘—çš„æ”¹è¿›ã€‚
**åå¥½è°ƒæ•´ç®—æ³•è®¾è®¡(Â§5.4) **ä½œè€…ä»¬è¯•éªŒäº†å‡ ç§åå¥½è°ƒæ•´ç®—æ³•ï¼Œå¹¶è§‚å¯Ÿåˆ°ä½¿ç”¨é•¿åº¦å½’ä¸€åŒ–çš„ç›´æ¥åå¥½ä¼˜åŒ–å¯ä»¥æé«˜æ€§èƒ½ã€‚ä½œè€…ä»¬åœ¨å®éªŒä¸­ä¼˜å…ˆè€ƒè™‘ç®€å•æ€§å’Œæ•ˆç‡ï¼Œå¹¶åœ¨æ•´ä¸ªå¼€å‘è¿‡ç¨‹å’Œè®­ç»ƒæœ€ç»ˆæ¨¡å‹æ—¶ä½¿ç”¨é•¿åº¦å½’ä¸€åŒ–çš„ DPOï¼Œè€Œä¸æ˜¯å¯¹åŸºäº PPO çš„æ–¹æ³•è¿›è¡Œæˆæœ¬æ›´é«˜çš„ç ”ç©¶ã€‚
**å…·æœ‰å¯éªŒè¯å¥–åŠ±çš„æŠ€èƒ½ç‰¹å®šå¼ºåŒ–å­¦ä¹ (Â§6) **ä½œè€…ä»¬é‡‡ç”¨ä¸€ç§æ–°æ–¹æ³•ï¼Œåˆ©ç”¨æ ‡å‡†å¼ºåŒ–å­¦ä¹ èŒƒå¼æ¥é’ˆå¯¹å¯ä»¥æ ¹æ®çœŸå®ç»“æœï¼ˆä¾‹å¦‚æ•°å­¦ï¼‰è¿›è¡Œè¯„ä¼°çš„æŠ€èƒ½ã€‚ä½œè€…ä»¬å°†æ­¤ç®—æ³•ç§°ä¸ºå…·æœ‰å¯éªŒè¯å¥–åŠ±çš„å¼ºåŒ–å­¦ä¹  (RLVR)ï¼›å¦‚æœå®ŒæˆæˆåŠŸï¼Œå®ƒå°†è·å¾—æ’å®šçš„å¥–åŠ±å€¼ã€‚ä½œè€…ä»¬çš„ç»“æœè¡¨æ˜ï¼ŒRLVR å¯ä»¥æé«˜ GSM8Kã€MATH å’Œ IFEval æ€§èƒ½ã€‚
**å¼ºåŒ–å­¦ä¹ çš„è®­ç»ƒåŸºç¡€è®¾æ–½ï¼ˆÂ§6.3ï¼‰**ä½œè€…ä»¬å®æ–½äº†å¼‚æ­¥ RL è®¾ç½®ï¼šä½œè€…ä»¬é€šè¿‡ vLLM é«˜æ•ˆè¿è¡Œ LLM æ¨ç†ï¼ŒåŒæ—¶å­¦ä¹ è€…åŒæ—¶æ‰§è¡Œæ¢¯åº¦æ›´æ–°ã€‚ä½œè€…ä»¬çš„ RL ä»£ç åº“ä¹Ÿå…·æœ‰é«˜åº¦å¯æ‰©å±•æ€§ï¼Œå¯ä»¥è®­ç»ƒ 70B RLVR ç­–ç•¥æ¨¡å‹ã€‚
**è¯„ä¼°æ¡†æ¶ï¼šTÃœLU3EVALï¼ˆÂ§7ï¼‰**é™¤äº†è¯„ä¼°æœ€ç»ˆæ¨¡å‹ä¹‹å¤–ï¼Œä½œè€…ä»¬çš„è¯„ä¼°æ¡†æ¶æ˜¯ä¸€ä¸ªå¼€æºçš„è¯„ä¼°å·¥å…·åŒ…ï¼Œæ—¨åœ¨é€šè¿‡ç²¾å¿ƒæŒ‘é€‰çš„è¯„ä¼°å¥—ä»¶å’Œå‡€åŒ–å·¥å…·æ¥æŒ‡å¯¼å¼€å‘è¿›åº¦ã€‚

Evaluation and Results
åœ¨æ•´ä¸ªå·¥ä½œè¿‡ç¨‹ä¸­æŠ¥å‘Šåˆ†æ•°æ—¶ï¼Œä½œè€…ä»¬ä½¿ç”¨è¡¨ 3 ä¸­ç¡®å®šçš„æŒ‡æ ‡ï¼›åˆ†æ•°è¶Šé«˜è¶Šå¥½ã€‚åœ¨è®¡ç®—æ•´ä½“æ€§èƒ½æ—¶ï¼Œä½œè€…ä»¬åªæ˜¯å¯¹æ‰€æœ‰è¯„ä¼°å–å¹³å‡åˆ†æ•°ï¼Œå¯¹æ¯ä¸ªè¯„ä¼°ä¸€è§†åŒä»ã€‚åœ¨ä½œè€…ä»¬çš„å¼€å‘è¯„ä¼°å¥—ä»¶ä¸­ï¼Œåœ¨ Llama 3 åŸºç¡€æ¨¡å‹ä¸Šè®­ç»ƒçš„ TÃœLU3çš„è¡¨ç°ä¼˜äºå…¶å¤§å°ç±»åˆ«ä¸­çš„æ‰€æœ‰å…¶ä»–å¼€æºæƒé‡æ¨¡å‹ã€‚ä¸é—­æºæ¨¡å‹ç›¸æ¯”ï¼ŒTÃœLU370B ç”šè‡³è¶…è¶Šäº† GPT-3.5-Turbo-0125 æˆ– GPT-4o-mini-2024-07-18 ç­‰é—­æºæ¨¡å‹ï¼ŒåŒæ—¶æ¥è¿‘ Claude 3.5 Haiku 20241022 çš„æ€§èƒ½ã€‚è¡¨ 3æ˜¾ç¤ºäº†åœ¨ 80 äº¿å’Œ 700 äº¿ä¸ªå‚æ•°çš„ Llama 3 ä¸Šè®­ç»ƒçš„ TÃœLU3ä¸å®ƒä»¬å¤§å°ç±»åˆ«ä¸­çš„é¢†å…ˆæ¨¡å‹çš„æ€»ç»“ã€‚è¡¨ 4 æ˜¾ç¤ºäº† 8B ç‰ˆæœ¬æŒ‰è®­ç»ƒé˜¶æ®µçš„æ€§èƒ½ç»†åˆ†ï¼Œè¡¨ 5 æ˜¾ç¤ºäº† 70B ç‰ˆæœ¬æŒ‰è®­ç»ƒé˜¶æ®µçš„æ€§èƒ½ç»†åˆ†ã€‚






TÃœLU3 Data
Promptsä»£è¡¨ç”¨æˆ·ä¸æ¨¡å‹äº¤äº’çš„å„ç§æ–¹å¼ï¼Œæ˜¯æ‰€æœ‰åè®­ç»ƒé˜¶æ®µçš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚ä½œè€…ä»¬ç²¾å¿ƒæŒ‘é€‰äº†æ•°ç™¾ä¸‡ä¸ªPromptsä½œä¸º TÃœLU3åè®­ç»ƒRecipeçš„èµ·ç‚¹ã€‚ä»è¿™äº›Promptsä¸­é€‰æ‹©äº†ç”¨äºä¸‹ä¸€é˜¶æ®µè®­ç»ƒçš„æ•°æ®ã€‚è¡¨ 6 æ€»ç»“äº†è¿™äº›Promptsçš„å…³é”®ä¿¡æ¯ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œä½œè€…ä»¬æè¿°äº†ä½œè€…ä»¬çš„Promptsç­–åˆ’è¿‡ç¨‹å’Œå‡€åŒ–å·¥ä½œï¼Œä»¥ç¡®ä¿ä½œè€…ä»¬çš„è¯„ä¼°ä¸ä¼šæ³„éœ²åœ¨è¿™äº›Promptsä¸­ã€‚åœ¨ä»¥ä¸‹éƒ¨åˆ†ä¸­ï¼Œä½œè€…ä»¬å°†æè¿°å¦‚ä½•ä½¿ç”¨Promptsè¿›è¡Œç›‘ç£å¾®è°ƒ Â§4 å’Œåå¥½è°ƒæ•´ Â§5ã€‚
Prompt æ‹“å±•
ä¸ºäº†ç„å‡†æ‰€éœ€çš„æ ¸å¿ƒæŠ€èƒ½ï¼Œä½œè€…ä»¬ä»å…·æœ‰æ˜ç¡®æ¥æºçš„å…¬å¼€æ•°æ®é›†ä¸­æŒ‘é€‰å‡ºä¸€ç»„å¤šæ ·åŒ–ã€é«˜è´¨é‡çš„Promptsï¼Œå¹¶ç»¼åˆç”ŸæˆPromptsä»¥å¡«è¡¥ä»»ä½•ç©ºç™½ã€‚
ä»å…¬å…±æ•°æ®é›†è·å–æ•°æ®
è‡ªä½œè€…ä»¬å‘å¸ƒ TÃœLU2 ä»¥æ¥ï¼Œç¤¾åŒºè§è¯äº†å¤§é‡å·¥ä½œä¸ºåæœŸè®­ç»ƒåˆ›å»ºæ•°æ®é›†ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒå’Œåå¥½è°ƒæ•´ã€‚TÃœLU3æ—¨åœ¨æ•´åˆå’Œæ‰©å±•è¿™äº›èµ„æºä»¥æ„å»ºæ›´å¼ºå¤§çš„æ¨¡å‹ã€‚ä½œè€…ä»¬ä»å¯¹å…¬å…±æ•°æ®é›†çš„å¹¿æ³›è°ƒæŸ¥å¼€å§‹è¿™ä¸€è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ç”±ä¸“èŒäººå‘˜æ³¨é‡Šçš„æ•°æ®é›†ã€æ¥è‡ªçœŸå®ç”¨æˆ·çš„æ•°æ®é›†ä»¥åŠä¸æ¨¡å‹åˆæˆçš„æ•°æ®é›†ã€‚ç„¶åï¼Œä½œè€…ä»¬æ‰‹åŠ¨å®¡æŸ¥æ¯ä¸ªæ•°æ®é›†ï¼Œå¹¶æ ¹æ®ä»¥ä¸‹è€ƒè™‘å› ç´ è¿›è¡Œé€‰æ‹©ã€‚


é’ˆå¯¹ç›®æ ‡æŠ€èƒ½è¿›è¡Œåˆæˆ
ä¸ºäº†æ»¡è¶³å¯¹å¤šæ ·åŒ–å’Œç‰¹å®šæŠ€èƒ½æ•°æ®é›†æ—¥ç›Šå¢é•¿çš„éœ€æ±‚ï¼Œä½œè€…ä»¬é‡‡ç”¨äº†åˆæˆæ•°æ®ç”Ÿæˆä½œä¸ºè¡¥å……æ–¹æ³•ã€‚åˆæˆæ•°æ®ç”Ÿæˆå·²æˆä¸ºäººå·¥ç¼–å†™æ•°æ®çš„ä¸€ç§æœ‰å‰é€”çš„æ›¿ä»£æ–¹æ¡ˆï¼Œå› ä¸ºå®ƒæ›´å®¹æ˜“è·å–ã€å¯é’ˆå¯¹ä¸åŒç›®çš„è¿›è¡Œå®šåˆ¶ï¼Œå¹¶ä¸”åæ˜ äº†åº•å±‚æ¨¡å‹çš„ä¸°å¯ŒçŸ¥è¯† [Dubey et al., 2024]ã€‚ç„¶è€Œï¼Œå¤§è§„æ¨¡ç”Ÿæˆå¤šæ ·åŒ–å’Œé«˜è´¨é‡çš„æ•°æ®å¹¶éæ˜“äº‹ï¼Œå› ä¸º LM å¾ˆå®¹æ˜“é™·å…¥é‡å¤çš„æ¨¡å¼æˆ–æ¨¡å¼ï¼Œç§°ä¸ºâ€œæ¨¡å¼å´©æºƒâ€ã€‚ä¸ºäº†ç¡®ä¿ç”Ÿæˆçš„å¤šæ ·æ€§ï¼Œä½œè€…ä»¬éµå¾ª Chan ç­‰äºº [2024] ä¸­æœ€è¿‘é‡‡ç”¨çš„è§’è‰²é©±åŠ¨æ–¹æ³•æ¥ç”Ÿæˆåˆæˆæ•°æ®ã€‚å…³é”®æ€æƒ³æ˜¯ä½¿ç”¨ä¸åŒçš„è§’è‰²ï¼ˆä¾‹å¦‚ï¼Œâ€œä¸“æ³¨äºç¥ç»ç½‘ç»œçš„æœºå™¨å­¦ä¹ ç ”ç©¶äººå‘˜â€ï¼‰å’Œæ•°æ®åˆæˆPromptsï¼ˆä¾‹å¦‚ï¼Œâ€œåˆ›å»ºç¼–ç é—®é¢˜â€ï¼‰æ¥å¼•å¯¼ LLM åˆæˆå…·æœ‰ç›¸åº”è§†è§’çš„æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œä½œè€…ä»¬ä»¥ Persona Hub [Chan et al., 2024] ä¸­çš„ âˆ¼250K ä¸ªè§’è‰²ä¸ºæ¡ä»¶ï¼Œç”Ÿæˆé’ˆå¯¹ç‰¹å®šæŠ€èƒ½ï¼ˆä¾‹å¦‚ç²¾ç¡®éµå¾ªæŒ‡ä»¤ã€æ•°å­¦å’Œç¼–ç ï¼‰çš„Promptsã€‚

**ç²¾ç¡®çš„æŒ‡ä»¤éµå¾ªï¼Œ**ç²¾ç¡®çš„æŒ‡ä»¤éµå¾ªæ˜¯æŒ‡éµå¾ªè‡ªç„¶è¯­è¨€ä¸­å¯éªŒè¯çš„æŒ‡ä»¤ï¼ˆä¾‹å¦‚â€œå†™ 3 æ®µè¯â€ï¼‰çš„èƒ½åŠ›ï¼Œè¿™äº›æŒ‡ä»¤å¯ä»¥é€šè¿‡å¯å‘å¼æ–¹æ³•è‡ªåŠ¨éªŒè¯ã€‚ä½œè€…ä»¬ä½¿ç”¨è§’è‰²é©±åŠ¨çš„æ–¹æ³•ç»¼åˆç”Ÿæˆå¯éªŒè¯æŒ‡ä»¤ï¼Œæ¶µç›– IFEval åŸºå‡†ä¸­å®šä¹‰çš„ 25 ç§ä¸åŒçº¦æŸç±»å‹ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œä½œè€…ä»¬é¦–å…ˆä¸ºæ¯ä¸ªçº¦æŸæ‰‹åŠ¨ç¼–å†™ 1-2 ä¸ªç¤ºä¾‹æŒ‡ä»¤ï¼ˆç”¨ä½œseedPromptsã€‚ç„¶åï¼Œä½œè€…ä»¬ä½¿ç”¨ä¾‹å¦‚å•è¯æ•°æ¥ç”Ÿæˆæ–°æŒ‡ä»¤ï¼‰ï¼Œä»è€Œå¾—åˆ°æ€»å…±33 æ¡å¯éªŒè¯æŒ‡ä»¤ï¼Œä½œè€…ä»¬ç»™å‡ºäº†æ•°æ®åˆæˆPromptsã€è§’è‰²å’Œå•ä¸ªå¯éªŒè¯æŒ‡ä»¤ä½œä¸ºç¤ºä¾‹ã€‚
**æ•°å­¦å’Œç¼–ç ï¼Œ**ä½œè€…ä»¬é‡‡ç”¨ç±»ä¼¼çš„è§’è‰²é©±åŠ¨æ–¹æ³•æ¥ç»¼åˆç”Ÿæˆå„ç§æ•°å­¦å•è¯å’Œç¼–ç é—®é¢˜ã€‚æ•°å­¦é—®é¢˜åŒ…æ‹¬éœ€è¦é«˜ç­‰æ•°å­¦æŠ€èƒ½çš„é—®é¢˜ä»¥åŠå°å­¦é—®é¢˜ã€‚å¯¹äºç¼–ç ï¼Œä½œè€…ä»¬ç”Ÿæˆå…¥é—¨åˆ°ä¸­çº§ç¨‹åºå‘˜éƒ½å¯ä»¥è§£å†³çš„ Python ç¼–ç¨‹é—®é¢˜ã€‚ä¸ç²¾ç¡®çš„æŒ‡ä»¤éµå¾ªä¸åŒï¼Œä½œè€…ä»¬é›¶æ ·æœ¬Prompts GPT-4o ç”Ÿæˆç‰¹å®šäºç»™å®šè§’è‰²è¾“å…¥çš„ç‹¬ç‰¹é—®é¢˜ã€‚ç”Ÿæˆé—®é¢˜åï¼Œä½œè€…ä»¬ç”Ÿæˆå¤šæ­¥éª¤ä½¿ç”¨ GPT-4o ç¼–å†™æ•°å­¦è§£å†³æ–¹æ¡ˆï¼Œä½¿ç”¨ claude-3-5-sonnet ç¼–å†™ Python ç¨‹åºã€‚
ä¸åˆè§„å’Œå®‰å…¨ï¼Œéšç€ä½œè€…ä»¬å¢å¼ºæ¨¡å‹æœ‰æ•ˆååŠ©ç”¨æˆ·çš„èƒ½åŠ›ï¼Œç¡®ä¿å®ƒä»¬èƒ½å¤Ÿå¯é åœ°æ‹’ç»ä¸å®‰å…¨æŸ¥è¯¢å¹¶é€‚å½“å¤„ç†ç»†å¾®å’Œè¶…å‡ºèŒƒå›´çš„æŸ¥è¯¢è‡³å…³é‡è¦ã€‚

Prompt  æ¸…æ´—
åœ¨æ‹“å±•ä½œè€…ä»¬çš„è®­ç»ƒç»„åˆæ—¶ï¼Œä¸€ä¸ªé‡è¦çš„è€ƒè™‘å› ç´ æ˜¯è®­ç»ƒé›†å’Œè¯„ä¼°é›†ä¹‹é—´å¯èƒ½å­˜åœ¨é‡å ã€‚ä½œè€…ä»¬æŒ‰å¦‚ä¸‹æ–¹å¼é‡åŒ–è¿™ç§é‡å ï¼Œå¹¶æ ¹æ®éœ€è¦ä»ä½œè€…ä»¬çš„è®­ç»ƒç»„åˆä¸­åˆ é™¤å®ä¾‹ï¼Œä»¥é˜²æ­¢æµ‹è¯•é›†æ±¡æŸ“ã€‚

**åŒ¹é…æ–¹æ³•ã€‚**ä½œè€…ä»¬å°è¯•äº†å…¨å­—ç¬¦ä¸²ã€n-gram å’ŒåŸºäºåµŒå…¥çš„åŒ¹é…ï¼Œå‘ç° n-gram åŒ¹é…äº§ç”Ÿäº†æœ€æœ‰ç”¨çš„ç»“æœâ€”â€”è™½ç„¶åŸºäºåµŒå…¥çš„æ–¹æ³•åŸåˆ™ä¸Šå¯ä»¥è¯†åˆ«ç”±äºé‡Šä¹‰è€Œäº§ç”Ÿçš„éå¹³å‡¡æ±¡æŸ“ [Yang et al., 2023]ï¼Œä½†ä½œè€…ä»¬å‘ç°å¾ˆéš¾åŒºåˆ†å•çº¯çš„åˆ†å¸ƒç›¸ä¼¼æ€§å’Œå®é™…çš„é‡Šä¹‰ã€‚æ­¤å¤–ï¼Œä½¿ç”¨ n-gram åŒ¹é…çš„éƒ¨åˆ†è¡¨é¢é‡å æˆåŠŸåœ°è¯†åˆ«äº†æ±¡æŸ“çš„æƒ…å†µï¼Œå…¶ä¸­å®ä¾‹ç•¥æœ‰ä¸åŒï¼Œä¾‹å¦‚æ•°å­¦é—®é¢˜ä»…æ•°å­—ä¸åŒã€‚
**è¯†åˆ«åŒ¹é…å®ä¾‹ã€‚**ç”±äºè®­ç»ƒæ•°æ®é›†ä¸­çš„è¡¥å…¨é€šå¸¸ä½¿ç”¨è¯­è¨€æ¨¡å‹é‡æ–°ç”Ÿæˆï¼Œå› æ­¤ä½œè€…ä»¬é€‰æ‹©ä»…è®¡ç®—Promptsä¸­çš„é‡å ï¼ˆæˆ–æ›´ä¸€èˆ¬åœ°è®¡ç®—å¤šè½®å¯¹è¯ä¸­çš„ç”¨æˆ·è½®æ¬¡ï¼‰ã€‚
å»æ±¡ã€‚ å¦‚æœè®­ç»ƒé›†ä¸­çš„ä»»ä½•å®ä¾‹æ•°é‡ä¸ä½œè€…ä»¬å¼€å‘å’Œæœªè§å¥—ä»¶ä¸­çš„ä»»ä½•è¯„ä¼°ä¸­è¶…è¿‡ 2% çš„å®ä¾‹é‡å ï¼Œåˆ™ä½œè€…ä»¬è®¤ä¸ºè¯¥è®­ç»ƒé›†å—åˆ°æ±¡æŸ“ã€‚ä½œè€…ä»¬ä¼šåˆ é™¤æ‰€æœ‰å—åˆ°æœªè§è¯„ä¼°æ±¡æŸ“çš„è®­ç»ƒé›†ã€‚å¯¹äºå—åˆ°å¼€å‘è¯„ä¼°æ±¡æŸ“çš„è®­ç»ƒé›†ï¼Œå¦‚æœè¿™æ ·åšä¸ä¼šæ˜¾è‘—å½±å“æœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½ï¼Œä½œè€…ä»¬ä¼šåˆ é™¤æ•´ä¸ªæ•°æ®é›†ï¼›å¦åˆ™ï¼Œä½œè€…ä»¬ä¼šåˆ é™¤ä¸ä»»ä½•æµ‹è¯•å®ä¾‹åŒ¹é…çš„ç‰¹å®šå®ä¾‹ã€‚

SFT
å°†é¢„è®­ç»ƒçš„åŸºç¡€æ¨¡å‹è°ƒæ•´ä¸ºå„ç§ä»»åŠ¡å’Œç”¨æˆ·è¯·æ±‚é€šå¸¸ä¾èµ–äºç›‘ç£å¾®è°ƒ (SFT)ï¼Œä¹Ÿç§°ä¸ºæŒ‡ä»¤å¾®è°ƒã€‚æ­¤è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜æ˜¯å¹³è¡¡ä»£è¡¨ä¸åŒæŠ€èƒ½çš„æ··åˆè®­ç»ƒæ•°æ®é›†çš„æ¯”ä¾‹ã€‚å¯¹äº TÃœLU3ï¼Œä½œè€…ä»¬è¿›è¡Œäº†æ•°æ®æ··åˆæ¶ˆèå¹¶æ¢ç´¢äº†æ¨¡å‹åˆå¹¶æ•°æ®æœ€åˆç”¨äºæ„å»º Vicuna æ¨¡å‹ï¼Œä½†ç¡®åˆ‡çš„æ•°æ®é›†å°šæœªå‘å¸ƒã€‚


SFT æ•°æ®
ä»Promptsåˆ° SFT æ•°æ®
ä¸ºäº†åˆ›å»ºä½œè€…ä»¬çš„ SFT ç»„åˆï¼Œä½œè€…ä»¬é€šè¿‡ä¸¤ç§æ–¹å¼æ”¶é›†æˆ–åˆ›å»ºç¬¬ 3 èŠ‚ä¸­æè¿°çš„Promptsçš„å›ç­”ï¼šè¿‡æ»¤ç°æœ‰å›ç­”å’Œåˆ›å»ºæ–°å›ç­”ã€‚
å¯¹äºæœ‰ç°æœ‰å›ç­”çš„Promptsï¼Œå¦‚æœåŸå§‹å›ç­”æ˜¯ç”±äººç±»æˆ–å‰æ²¿æ¨¡å‹ï¼ˆå¦‚ GPT-4oï¼‰ç¼–å†™çš„ï¼Œä½œè€…ä»¬é€šå¸¸ä¼šä¿ç•™åŸå§‹å›ç­”ã€‚å¯¹äºåŒ…å«æ¥è‡ªå‰æ²¿æ¨¡å‹å­é›†çš„å¤§å‹æ•°æ®é›†ï¼ˆä¾‹å¦‚ WildChatï¼‰ï¼Œä½œè€…ä»¬ä½¿ç”¨æ¥è‡ªæœ€ä½³æ¨¡å‹çš„å­é›†ã€‚ä½œè€…ä»¬è¿˜ä¼šè¿‡æ»¤ç©ºå›ç­”å’ŒåŒ…å«æœ‰å…³æ¨¡å‹æˆ–å…¶å¼€å‘äººå‘˜çš„ä¿¡æ¯çš„å›ç­”ã€‚å¦‚æœä¸€ç»„Promptsæ²¡æœ‰å›ç­”ï¼ˆä¾‹å¦‚ä½œè€…ä»¬çš„ Persona Promptsï¼‰ï¼Œæˆ–è€…åŸå§‹å›ç­”æ¥è‡ªè¾ƒå¼±çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ WildGuardMixï¼‰ï¼Œä½œè€…ä»¬ä¼šä½¿ç”¨ GPT-4o ç”Ÿæˆæ–°å›ç­”ã€‚ä½œè€…ä»¬è¿˜æ‰‹å†™äº†å¯¹ç¡¬ç¼–ç Promptsçš„å›ç­”ã€‚
TÃœLU3 SFT Mix
ä¸ºäº†å¼€å‘ä½œè€…ä»¬çš„ SFT ç»„åˆï¼Œä½œè€…ä»¬é¦–å…ˆä½¿ç”¨åœ¨ TÃœLU2 ä¸Šè®­ç»ƒçš„ Llama3.1 ä½œä¸ºåŸºçº¿ï¼Œç¡®å®šäº†è½åäºæœ€å…ˆè¿›æ¨¡å‹çš„æŠ€èƒ½ã€‚é’ˆå¯¹æ¯ä¸€é¡¹æŠ€èƒ½ï¼Œä½œè€…ä»¬æ”¶é›†äº†é«˜è´¨é‡çš„å…¬å¼€æ•°æ®é›†å¹¶åˆ›å»ºäº†åˆæˆæ•°æ®é›†ï¼ˆå¦‚ç¬¬ 3.1.2 èŠ‚æ‰€è¿°ï¼‰ï¼ŒåŒæ—¶åˆ é™¤äº†ä¸€äº›ä½œè€…ä»¬çš„å‘ç°ä¸å…¶ä»–è¾ƒæ–°çš„æ•°æ®é›†ç›¸æ¯”ï¼Œå…¶è´¨é‡ç›¸å¯¹è¾ƒä½çš„æ•°æ®é›†ã€‚
ä¸ºäº†è®¾è®¡æœ€ç»ˆçš„ SFT ç»„åˆï¼Œä½œè€…ä»¬é¦–å…ˆæ„å»ºäº†ç‰¹å®šæŠ€èƒ½çš„æ•°æ®ç»„åˆå’Œæ¨¡å‹ï¼Œä¿ç•™äº†åœ¨å•ä¸ªæŠ€èƒ½ä¸Šè¡¨ç°æœ€ä½³çš„æ··åˆæ•°æ®ï¼Œå¿½ç•¥äº†å…¶ä»–è¯„ä¼°ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†æ ¹æ®ä½œè€…ä»¬çš„è®¾ç½®ä¼°ç®—å‡ºæ¯ä¸ªè¯„ä¼°çš„ä¸Šé™ã€‚
ç„¶åï¼Œä½œè€…ä»¬å°†è¿™äº›mixturesç»„åˆèµ·æ¥ï¼Œåˆ›å»ºäº†ä½œè€…ä»¬çš„åˆå§‹ TÃœLU3é¢„è§ˆç»„åˆã€‚ç„¶åï¼Œä½œè€…ä»¬ç»§ç»­é€šè¿‡æ·»åŠ æˆ–åˆ é™¤æ•°æ®é›†æ¥è¿­ä»£mixturesï¼Œä»¥æé«˜æ»åæŠ€èƒ½ï¼Œå¹¶æ ¹æ®ä½œè€…ä»¬çš„è¯„ä¼°è¿›è¡Œå‡€åŒ–æ··åˆï¼Œä»¥åŠä½œè€…ä»¬çš„ä¸­é—´å’Œæœ€ç»ˆ TÃœLU3SFT æ··åˆã€‚




å…³é”®æ•°æ®å®éªŒ
åœ¨å¼€å‘æœ€ç»ˆçš„ SFT mixturesä¹‹åï¼Œä½œè€…ä»¬è¿˜è¿›è¡Œäº†ä¸€ç³»åˆ—æ§åˆ¶å®éªŒï¼Œä»¥æ¢ç´¢åœ¨æ•°æ®æ··åˆå’Œè®­ç»ƒæœŸé—´åšå‡ºçš„ä¸åŒå†³ç­–çš„é‡è¦æ€§ã€‚

**å¤šæ ·åŒ–çš„èŠå¤©æ•°æ®ã€‚**åœ¨ä½œè€…ä»¬çš„ç»„åˆä¸­ï¼Œä½œè€…ä»¬è¿˜å¼ºè°ƒæ·»åŠ å¤šæ ·åŒ–çš„èŠå¤©æ•°æ®ï¼Œä¸»è¦æ¥è‡ª WildChatã€‚ä½œè€…ä»¬åœ¨è¡¨9ä¸­å±•ç¤ºäº†åˆ é™¤ WildChat çš„å½±å“ï¼Œä½œè€…ä»¬å‘ç°å¤§å¤šæ•°æŠ€èƒ½éƒ½å‡ºç°äº†è½»å¾®ä½†æ˜æ˜¾çš„é€€åŒ–ï¼Œæœ€æ˜æ˜¾çš„æ˜¯ Alpaca Evalï¼Œè¿™å‡¸æ˜¾äº†å¤šæ ·åŒ–ç°å®ä¸–ç•Œæ•°æ®çš„é‡è¦æ€§ã€‚
å®‰å…¨æ€§æ˜¯æ­£äº¤çš„ã€‚ä½œè€…ä»¬å‘ç°ï¼Œä½œè€…ä»¬çš„å®‰å…¨æ€§ SFT æ•°æ®é€šå¸¸ä¸ä½œè€…ä»¬çš„å…¶ä»–æ•°æ®é›†æ­£äº¤ã€‚ä½œè€…ä»¬åœ¨è¡¨9ä¸­æŠ¥å‘Šäº†åˆ é™¤ç‰¹å®šäºå®‰å…¨æ€§çš„æ•°æ®é›†çš„æ•ˆæœï¼Œå¹¶ä¸”ä½œè€…ä»¬çœ‹åˆ°ï¼Œé™¤äº†å®‰å…¨æ€§å¹³å‡å€¼ä¹‹å¤–ï¼Œå¤§å¤šæ•°æŠ€èƒ½å¤§è‡´ä¿æŒä¸å˜ã€‚ä½œè€…ä»¬è¿˜å‘ç°ï¼Œæ·»åŠ è¯¸å¦‚ CoCoNot ä¸­çš„é‚£äº›æ„é€ æ€§Promptsæœ‰åŠ©äºé˜²æ­¢ä½œè€…ä»¬çš„æ¨¡å‹è¿‡åº¦æ‹’ç»å®‰å…¨Promptsã€‚
**æ–°äººç‰©è§’è‰²æ•°æ®ã€‚**ä½œè€…ä»¬æ–°çš„äººç‰©è§’è‰²æ•°æ®é›†æ˜¯é’ˆå¯¹ç‰¹å®šæŠ€èƒ½è€Œæ„å»ºçš„ï¼šæ•°å­¦ã€ç¼–ç å’Œéµå¾ªæŒ‡ä»¤ã€‚åœ¨è¡¨9ä¸­ï¼Œä½œè€…ä»¬æ˜¾ç¤ºåˆ é™¤äººç‰©è§’è‰²æ•°æ®é›†åï¼ŒHumanEval(+)ã€GSM8Kã€MATH å’Œ IFEval ä¸Šçš„æ€§èƒ½ä¸‹é™ï¼Œæ˜¾ç¤ºäº†åˆ›å»ºå¤šæ ·åŒ–ã€ç‰¹å®šæŠ€èƒ½çš„ SFT æ•°æ®é›†çš„ä»·å€¼ã€‚é’ˆå¯¹ç‰¹å®šæŠ€èƒ½ã€‚ä½œè€…ä»¬è¿˜åœ¨è¡¨9 ä¸­å±•ç¤ºäº†é’ˆå¯¹ç‰¹å®šæŠ€èƒ½çš„å½±å“ï¼Œå…¶ä¸­ä½œè€…ä»¬å±•ç¤ºäº†åˆ é™¤æ‰€æœ‰ç‰¹å®šæ•°å­¦æ•°æ®çš„æ•ˆæœã€‚ä¸ä½œè€…ä»¬çš„æœ€ç»ˆç»„åˆç›¸æ¯”ï¼ŒGSM8K å’Œ MATH éƒ½æ˜¾ç€ä¸‹é™ï¼Œå¼ºè°ƒäº†åœ¨æ•°æ®æ”¶é›†æœŸé—´å…³æ³¨ç‰¹å®šæŠ€èƒ½çš„ç§¯æå½±å“ã€‚
**é’ˆå¯¹ç‰¹å®šæŠ€èƒ½ã€‚**ä½œè€…ä»¬å¾ˆå¤§ä¸€éƒ¨åˆ†ç²¾åŠ›éƒ½é›†ä¸­åœ¨æ”¶é›†æˆ–åˆ›å»ºé’ˆå¯¹ç‰¹å®šèƒ½åŠ›çš„æ•°æ®é›†ä¸Šã€‚ä»¥æ•°å­¦æ¨ç†ä¸ºä¾‹ï¼Œä½œè€…ä»¬åœ¨è¡¨8ä¸­å±•ç¤ºäº†æ•°å­¦ç‰¹å®šæ•°æ®å¯¹ GSM8K å’Œ MATH çš„å½±å“ã€‚ä½œè€…ä»¬å‘ç°æ•°å­¦ç‰¹å®šçš„ SFT æ•°æ®æ˜¾è‘—æé«˜äº† GSM8K å’Œ MATHï¼Œæ˜¾ç¤ºäº†æœ€ç»ˆç»„åˆä¸­åŒ…å«çš„æ•°æ®çš„ä»·å€¼ã€‚
**SFT æ•°æ®é‡ã€‚**åœ¨å›¾ 4 ä¸­ï¼Œä½œè€…ä»¬å±•ç¤ºäº†å¯¹ SFT ç»„åˆè¿›è¡Œåˆ†å±‚å­æŠ½æ ·çš„æ•ˆæœã€‚ä½œè€…ä»¬å‘ç°ï¼Œéšç€æ›´å¤š SFT æ•°æ®è¢«çº³å…¥ï¼Œä½œè€…ä»¬çš„æ¨¡å‹å¹³å‡æ°´å¹³ç»§ç»­æé«˜ï¼Œå¹¶ä¸”éšç€ä½œè€…ä»¬å°†æ•°æ®é‡å¢åŠ åˆ°å®Œæ•´ç»„åˆï¼Œä½œè€…ä»¬çœ‹åˆ° GSM8K ç­‰æŒ‡æ ‡çš„å·¨å¤§æ”¹è¿›ã€‚æœ‰è¶£çš„æ˜¯ï¼Œéšç€ç»„åˆä¸­çš„æ•°æ®é‡å¢åŠ ï¼ŒTruthfulQA æ€§èƒ½å®é™…ä¸Šä¼šä¸‹é™ã€‚ä½œè€…ä»¬ä¸ä¼šå°† SFT æ•°æ®å¤§å°å¢åŠ åˆ°å½“å‰ç»„åˆä¹‹å¤–ï¼Œå› ä¸ºä½œè€…ä»¬åˆ†é…äº†å…¶ä»–Promptsä»¥è¿›è¡Œåå¥½ä¼˜åŒ–ã€‚



SFT  Recipe and Analyses
**è®­ç»ƒè®¾ç½® **ä¸ºäº†è®­ç»ƒä½œè€…ä»¬çš„ TÃœLU 3 æ¨¡å‹ï¼Œä½œè€…ä»¬ä½¿ç”¨äº† 4 åˆ° 16 ä¸ª 8xH100 èŠ‚ç‚¹ï¼Œå¹¶è¿›è¡Œäº†é«˜é€Ÿäº’è¿ã€‚ä½œè€…ä»¬ä½¿ç”¨äº† 128 çš„batch size  å’Œ 4,096 ä¸ª token çš„æœ€å¤§åºåˆ—é•¿åº¦ã€‚ä½œè€…ä»¬è®­ç»ƒäº†ä¸¤ä¸ªæ—¶æœŸï¼Œå¯¹äº 8B æ¨¡å‹ï¼Œä½œè€…ä»¬ä½¿ç”¨ 5e-6 çš„å­¦ä¹ ç‡ï¼Œå¯¹äº 70B æ¨¡å‹ï¼Œä½œè€…ä»¬ä½¿ç”¨ 2e-6 çš„å­¦ä¹ ç‡ï¼Œè¿™æ˜¯ä½œè€…ä»¬åœ¨è¶…å‚æ•°æœç´¢åå‘ç°çš„ã€‚ä½œè€…ä»¬çš„è¶…å‚æ•°è®¾ç½®ä¹Ÿæ€»ç»“åœ¨è¡¨ 10 ä¸­ã€‚


å…³é”®è®­ç»ƒå®éªŒ
**åŸºç¡€æ¨¡å‹çš„é€‰æ‹©ã€‚**ä½œè€…ä»¬è¿˜ä½¿ç”¨å®Œæ•´çš„ SFT ç»„åˆæµ‹è¯•äº†è®­ç»ƒä¸åŒçš„åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹å¯¹æ•°å­¦æ€§èƒ½çš„å½±å“ã€‚åœ¨è¡¨ 11 ä¸­ï¼Œä½œè€…ä»¬å±•ç¤ºäº†é€šè¿‡åœ¨ Llama 3.1 8B å’Œ 70B ä¸Šè¿›è¡Œè®­ç»ƒæ¥æ”¹å˜æ¨¡å‹å¤§å°çš„å½±å“ï¼Œä»¥åŠé€šè¿‡åœ¨ Qwen 2.5 7Bå’Œ Qwen 2.5 Math 7B ä¸Šè¿›è¡Œè®­ç»ƒæ¥æ·»åŠ ç‰¹å®šé¢†åŸŸçš„é¢„è®­ç»ƒæ•°æ®çš„å½±å“ã€‚åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œä½œè€…ä»¬éƒ½çœ‹åˆ° GSM8K å’Œ MATH éƒ½æœ‰äº†æ˜¾è‘—çš„æå‡ï¼Œå‡¸æ˜¾äº†æ¨¡å‹å¤§å°å’Œé¢„è®­ç»ƒæ•°æ®å¯¹äºä¸‹æ¸¸æŠ€èƒ½çš„é‡è¦æ€§ã€‚


**èŠå¤©æ¨¡æ¿å˜åŒ–ã€‚**åœ¨åˆ›å»º TÃœLU3 æœŸé—´ï¼Œä½œè€…ä»¬æ¢ç´¢äº†æ›´æ”¹ç”¨äºæŒ‡å¯¼ç”Ÿæˆå¾®è°ƒæ¨¡å‹çš„èŠå¤©æ¨¡æ¿ã€‚ä½œè€…ä»¬å¯¹ä»¥å‰ TÃœLUç‰ˆæœ¬ä¸­ä½¿ç”¨çš„èŠå¤©æ¨¡æ¿åšäº†ä¸€äº›å°æ”¹åŠ¨ï¼Œå…·ä½“æ¥è¯´æ˜¯åˆ é™¤äº†æ¨¡æ¿æœ«å°¾çš„æ–°è¡Œï¼ˆåœ¨æ¨¡å‹å›ç­”ä¹‹å‰ï¼‰ã€‚è¡¨ 12 æ˜¾ç¤ºäº†ä½œè€…ä»¬ SFT è®¾ç½®çš„æ—©æœŸç‰ˆæœ¬ä¸­å¯¹èŠå¤©æ¨¡æ¿çš„ä¸åŒæ›´æ”¹ä¹‹é—´çš„æ€§èƒ½ã€‚ä½œè€…ä»¬å‘ç°ç”¨ eos tokenæ›¿æ¢åŠ©æ‰‹æ¶ˆæ¯æœ«å°¾çš„æ¢è¡Œç¬¦å¯ä»¥è·å¾—æœ€ä½³æ€§èƒ½ï¼Œä½†ä½œè€…ä»¬é€‰æ‹©ä¸ä½¿ç”¨å®ƒï¼Œä»¥é¿å…ä¸åè®­ç»ƒçš„pipelineä¸­çš„åç»­æ­¥éª¤ç”Ÿæˆä¸ä¸€è‡´ã€‚


**éšæœºseedå’Œæ¨¡å‹soupsã€‚**ä½œè€…ä»¬è¿˜æ¢ç´¢äº†åœ¨ SFT æœŸé—´æ›´æ”¹éšæœºseedï¼Œç„¶åä½¿ç”¨è¿™äº›æ¨¡å‹åˆ›å»ºæ¨¡å‹soupsã€‚åœ¨è¡¨ 13 ä¸­ï¼Œä½œè€…ä»¬å°†ä½¿ç”¨å¤šä¸ªä¸åŒseedè®­ç»ƒçš„ 8B å’Œ 70B æ¨¡å‹ä¸æœ€ä½³æ¨¡å‹soupsè¿›è¡Œäº†æ¯”è¾ƒã€‚ä½œè€…ä»¬å‘ç° SFT æ€§èƒ½å› seedçš„ä¸åŒè€Œæ˜æ˜¾ä¸åŒï¼Œè¿™å‡¸æ˜¾äº†å¤šæ¬¡è®­ç»ƒè¿è¡Œçš„é‡è¦æ€§ï¼Œå¹¶ä¸”æœ€ä½³æ¨¡å‹soupså¹¶ä¸æ€»æ˜¯ä¼˜äºæœ€ä½³å•æ¬¡è®­ç»ƒè¿è¡Œã€‚å› æ­¤ï¼Œä½œè€…ä»¬å°†æ¯ç§æ¨¡å‹å¤§å°ï¼ˆ8Bå’Œ70Bï¼‰çš„æœ€ä½³å•æ¬¡ SFT è®­ç»ƒè¿è¡Œç”¨ä½œæœ€ç»ˆçš„ SFT æ¨¡å‹ã€‚


æ‰¹é‡èšåˆ
åœ¨è®­ç»ƒ Tframework å’Œåœ¨å…¶ä»–è®¾ç½®ï¼ˆä¾‹å¦‚ TPUï¼‰ä¸Šè®­ç»ƒçš„æ¨¡å‹çš„æ—©æœŸé˜¶æ®µã€‚TÃœLU 3 ä¸­ï¼Œä½œè€…ä»¬æ³¨æ„åˆ°åœ¨ä½œè€…ä»¬çš„ Open-Instruct 8ä¸Šè®­ç»ƒçš„ SFT æ¨¡å‹ä¹‹é—´å­˜åœ¨æ€§èƒ½å·®è·ã€‚ä½œè€…ä»¬å‘ç°è¿™ä¸ªé—®é¢˜ä¸»è¦æ˜¯ç”±äºTransformers å†…éƒ¨æŸå¤±èšåˆé—®é¢˜é€ æˆçš„ï¼šåœ¨ä¸è€ƒè™‘æ¢¯åº¦ç´¯ç§¯æˆ–åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®çš„æƒ…å†µä¸‹å¯¹å¡«å……æ ‡è®°ä¹‹é—´çš„æŸå¤±è¿›è¡Œå¹³å‡ã€‚
è¿™é‡Œä½œè€…ä»¬ç”¨ä¸€ä¸ªä¾‹å­æ¥è¯´æ˜è¿™ä¸ªé—®é¢˜ã€‚å‡è®¾ä½œè€…ä»¬åœ¨ä¸€ä¸ªæ‰¹æ¬¡ä¸­æœ‰ä¸¤ä¸ªæ ·æœ¬,å…¶ä¸­æœ‰ n1n_1n1â€‹ã€n2n_2n2â€‹ ä¸ªéå¡«å……æ ‡è®°å’Œ m1m_1m1â€‹ã€m2m_2m2â€‹ ä¸ªå¡«å……æ ‡è®°ã€‚å¦‚æœä½œè€…ä»¬åŒæ—¶å°†ä¸¤ä¸ªæ ·æœ¬ä¼ é€’åˆ°é»˜è®¤çš„ Transformers å‰å‘ä¼ é€’ä¸­,ä½œè€…ä»¬ä¼šå¾—åˆ°:
L=ln1+ln2n1+n2L = \frac{l_{n1} + l_{n2}}{n_1 + n_2}
L=n1â€‹+n2â€‹ln1â€‹+ln2â€‹â€‹
å½“ä¸¤ä¸ªæ ·æœ¬åŒæ—¶é€šè¿‡Transformeræ¨¡å‹çš„å‰å‘ä¼ æ’­æ—¶,æŸå¤±å‡½æ•° L æ˜¯ä¸¤ä¸ªæ ·æœ¬çš„æŸå¤±å€¼ä¹‹å’Œé™¤ä»¥ä¸¤ä¸ªæ ·æœ¬ä¸­éå¡«å……tokensçš„æ€»æ•°ã€‚è¿™é‡Œçš„ ln1l_{n1}ln1â€‹ å’Œ ln2l_{n2}ln2â€‹ åˆ†åˆ«æ˜¯ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªæ ·æœ¬çš„æŸå¤±å€¼,å®ƒä»¬æ˜¯æ ¹æ®å„è‡ªæ ·æœ¬ä¸­çš„éå¡«å……tokensè®¡ç®—å¾—å‡ºçš„ã€‚
ç„¶è€Œ,å¦‚æœä½œè€…ä»¬åº”ç”¨æ¢¯åº¦ç§¯ç´¯,åˆ†åˆ«è¾“å…¥ä¸¤ä¸ªæ ·æœ¬,è®¡ç®—æŸå¤±,ç„¶åé™¤ä»¥æŸå¤±å°±ä¼šè¿™æ ·è®¡ç®—:
L=ln1n1+ln2n2L = \frac{l_{n1}}{n_1} + \frac{l_{n2}}{n_2}
L=n1â€‹ln1â€‹â€‹+n2â€‹ln2â€‹â€‹
å½“ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ—¶,æ¯ä¸ªæ ·æœ¬å•ç‹¬é€šè¿‡æ¨¡å‹,åˆ†åˆ«è®¡ç®—æŸå¤±,ç„¶åå†å°†è¿™ä¸¤ä¸ªæŸå¤±å€¼å¹³å‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹,æ¯ä¸ªæ ·æœ¬çš„æŸå¤±å€¼æ˜¯æŒ‰ç…§å…¶éå¡«å……tokensçš„æ•°é‡æ¥åŠ æƒçš„ã€‚
ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨ç¬¬äºŒç§æƒ…å†µä¸‹ï¼Œä½œè€…ä»¬å¯¹æ¯ä¸ªç¤ºä¾‹èµ‹äºˆç›¸åŒçš„æƒé‡ï¼Œè€Œåœ¨ç¬¬ä¸€ç§æƒ…å†µä¸‹ï¼Œä½œè€…ä»¬å¯¹æ¯ä¸ªæ ‡è®°èµ‹äºˆç›¸åŒçš„æƒé‡ã€‚å› æ­¤ï¼Œæ”¹å˜æ¢¯åº¦ç´¯ç§¯ä¼šå¯¹æ€§èƒ½äº§ç”Ÿå¾ˆå¤§å½±å“ï¼Œå› ä¸ºå®é™…ä¸Šä¼šæ”¹å˜æ ·æœ¬æƒé‡ã€‚ç”±äºè·¨è®¾å¤‡å¹³å‡ï¼Œåˆ†å¸ƒå¼è®­ç»ƒä¸­ä¹Ÿä¼šå‡ºç°ç±»ä¼¼çš„é—®é¢˜ã€‚
ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œä½œè€…ä»¬é€šå¸¸é€‰æ‹©åœ¨è®­ç»ƒæ—¶ä½¿ç”¨æ€»æŸå¤±è€Œä¸æ˜¯å¹³å‡æŸå¤±ã€‚è¿™åªéœ€ä»ä¸Šè¿°æ–¹ç¨‹ä¸­åˆ é™¤åˆ†æ¯å³å¯æ¶ˆé™¤é—®é¢˜ï¼Œå¹¶éœ€è¦è°ƒæ•´å­¦ä¹ ç‡ã€‚è¿™å®é™…ä¸Šå¯¹æ‰€æœ‰æ ‡è®°èµ‹äºˆäº†ç›¸åŒçš„æƒé‡ï¼ˆä½œè€…ä»¬å‘ç°è¿™é€šå¸¸ä¼šä½¿åˆå§‹æ··åˆçš„æ€§èƒ½æ›´å¥½ï¼‰ã€‚ä½œè€…ä»¬é€šè¿‡åœ¨ TÃœLU2 SFT æ··åˆä¸Šä½¿ç”¨å„ç§å­¦ä¹ ç‡ã€æ—¶æœŸå’ŒæŸå¤±ç±»å‹å¯¹ Llama 3.0 è¿›è¡Œå¾®è°ƒæ¥éªŒè¯ä½œè€…ä»¬è®¾ç½®çš„æ€§èƒ½ï¼Œå¦‚å›¾ 5 å’Œå›¾ 6 æ‰€ç¤ºã€‚æœ€ç»ˆï¼Œä½œè€…ä»¬å‘ç°ä½¿ç”¨å­¦ä¹ ç‡ä¸º 5.00E-06 çš„æ€»æŸå¤±æ•ˆæœæœ€å¥½ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œä½œè€…ä»¬è¿˜å‘ç°æ›´é•¿æ—¶é—´çš„è®­ç»ƒå¹¶æ²¡æœ‰å¸¦æ¥è¿›ä¸€æ­¥çš„æ”¹è¿›ï¼Œå› æ­¤ä½¿ç”¨äº† 2 ä¸ªæ—¶æœŸè¿›è¡Œè®­ç»ƒã€‚




Preference Finetuning
å¯¹äºTÃœLU3ï¼Œä½œè€…ä»¬æ¢ç´¢äº†è®¸å¤šåå¥½å¾®è°ƒæ–¹æ³•ï¼Œç›®çš„æ˜¯æ”¹è¿›æ•´ä¸ªè¯„ä¼°å¥—ä»¶ã€‚ä½œè€…ä»¬æ¢ç´¢äº†å¤šç§è®­ç»ƒç®—æ³•ï¼Œä»ç›´æ¥åå¥½ä¼˜åŒ– (DPO) åŠå…¶è¡ç”Ÿç®—æ³•åˆ°å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œä¾‹å¦‚è¿‘ç«¯ç­–ç•¥ä¼˜åŒ– (PPO)ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œä½œè€…ä»¬è¯¦ç»†ä»‹ç»äº†ä»äººç±»åå¥½å’Œä½œè€…ä»¬çš„ä¼˜åŒ–å™¨ä¸­å­¦ä¹ çš„é—®é¢˜è¡¨è¿°ã€‚æ¥ä¸‹æ¥ï¼Œä½œè€…ä»¬å°†è§£é‡Šå¦‚ä½•å°†ä»–ä»¬çš„Promptsè½¬æ¢ä¸ºæ¥è‡ªåœ¨çº¿ç­–ç•¥ï¼ˆTÃœLU3å¥—ä»¶ï¼‰å’Œç¦»çº¿ç­–ç•¥æ¨¡å‹ï¼ˆå…¶ä»–æŒ‡å¯¼æ¨¡å‹ï¼‰çš„åˆæˆåå¥½æ•°æ®ã€‚ä½œè€…ä»¬å±•ç¤ºäº†å¦‚ä½•ä¸ºæ„Ÿå…´è¶£çš„ç‰¹å®šæŠ€èƒ½åˆ›å»ºåå¥½æ•°æ®ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ DPO ç¨³å¥åœ°æ”¹è¿›ä»–ä»¬çš„æ¨¡å‹ã€‚
èƒŒæ™¯
å…ˆå‰çš„ç ”ç©¶å·²å°†åå¥½æ•°æ®è®­ç»ƒç¡®ç«‹ä¸ºæé«˜æ¨¡æ‹Ÿäººç±»æˆ–åˆæˆåå¥½çš„åŸºå‡†æ¨¡å‹æ€§èƒ½çš„å…³é”®æ­¥éª¤ ã€‚å…¸å‹çš„ç¨‹åºæ˜¯ä»äººç±»æˆ–åˆæˆåé¦ˆä¸­è¿›è¡Œå¼ºåŒ–å­¦ä¹ ã€‚
è®¾ç½®
åå¥½æ•°æ®


å›¾ 7å±•ç¤ºäº†åå¥½æ•°æ®é›†åˆ¶ä½œçš„æµç¨‹ï¼Œä¸»è¦åˆ†ä¸ºä¸‰ä¸ªé˜¶æ®µï¼šPrompt Selectionï¼ˆæç¤ºé€‰æ‹©ï¼‰ã€Response Generationï¼ˆå›ç­”ç”Ÿæˆï¼‰ å’Œ **Preference Annotationï¼ˆåå¥½æ ‡æ³¨ï¼‰**åœ¨è¿™ä¸ªé˜¶æ®µï¼Œä¼šä»ä»¥ä¸‹ä¸‰ç±»æ¥æºä¸­é€‰æ‹©æç¤ºï¼ˆPromptsï¼‰ï¼š

Prompts used in SFTï¼ˆç›‘ç£å¾®è°ƒä¸­ä½¿ç”¨çš„æç¤ºï¼‰ï¼šç”¨äºæ¨¡å‹çš„ç›‘ç£å¾®è°ƒè®­ç»ƒçš„æ•°æ®é›†ä¸­çš„æç¤ºã€‚
Prompts from datasets subsampled for SFTï¼ˆä»ç”¨äºSFTçš„æ•°æ®é›†ä¸­é‡‡æ ·çš„æç¤ºï¼‰ï¼šä»æ›´å¤§çš„è®­ç»ƒæ•°æ®é›†ä¸­æŒ‘é€‰å‡ºä¸€éƒ¨åˆ†æç¤ºã€‚
New OOD promptsï¼ˆè¶…å‡ºåˆ†å¸ƒçš„æç¤ºï¼Œä¾‹å¦‚Ultrafeedbackå’ŒPersonaï¼‰ï¼šè®¾è®¡æ–°çš„ã€è¶…å‡ºæ¨¡å‹è®­ç»ƒåˆ†å¸ƒçš„æç¤ºï¼Œä»¥æµ‹è¯•æ¨¡å‹åœ¨æ–°é¢†åŸŸä¸Šçš„è¡¨ç°ã€‚

**Response Generationï¼ˆå›ç­”ç”Ÿæˆï¼‰**è¿™é‡Œæåˆ°äº†ä¸€ä¸ªæ¨¡å‹æ± ï¼ŒåŒ…å«22ä¸ªä¸åŒçš„æ¨¡å‹ï¼Œå…¶ä¸­å›ç­”ç”Ÿæˆçš„è¿‡ç¨‹åŒ…æ‹¬ï¼š

Off-policy dataï¼ˆç¦»ç­–ç•¥æ•°æ®ï¼‰ï¼šç”±å…¶ä»–æ¨¡å‹ç”Ÿæˆçš„å›ç­”ï¼ˆå¦‚Metaã€Gemmaã€æŠ€æœ¯åˆ›æ–°ç ”ç©¶æ‰€ç­‰ï¼‰ã€‚è¿™äº›æ¨¡å‹ç”Ÿæˆçš„å›ç­”å¯èƒ½ä¼šè¢«æ ‡æ³¨ä¸ºâ€œè¾ƒå·®â€æˆ–â€œè¢«æ‹’ç»â€ã€‚
On-policy dataï¼ˆç­–ç•¥å†…æ•°æ®ï¼‰ï¼šç”±TÃ¼lu 3ï¼ˆSFT 8Bæˆ–SFT 70Bï¼‰ç”Ÿæˆçš„å›ç­”ï¼Œè¿™äº›å›ç­”å¯èƒ½æ˜¯é«˜è´¨é‡ã€ç¬¦åˆåå¥½çš„ç­”æ¡ˆã€‚

Preference Annotationï¼ˆåå¥½æ ‡æ³¨ï¼‰  åœ¨è¿™ä¸ªé˜¶æ®µï¼Œä½¿ç”¨GPT-4oååŠ©æ ‡æ³¨å›ç­”è´¨é‡ï¼Œå…·ä½“æ“ä½œåŒ…æ‹¬ï¼š

æ ‡æ³¨ç»´åº¦ï¼šå¯¹æ¯ä¸ªç”Ÿæˆçš„å›ç­”ï¼Œä»ä»¥ä¸‹å››ä¸ªæ–¹é¢è¿›è¡Œè¯„åˆ†ï¼ˆ1åˆ°5åˆ†ï¼‰ï¼š

Helpfulnessï¼ˆæœ‰ç”¨æ€§ï¼‰ï¼šå›ç­”æ˜¯å¦å¯¹ç”¨æˆ·æœ‰å¸®åŠ©ã€‚
Instruction Followingï¼ˆæŒ‡ä»¤éµå¾ªæ€§ï¼‰ï¼šæ˜¯å¦ä¸¥æ ¼æŒ‰ç…§æç¤ºæŒ‡ä»¤è¿›è¡Œå›ç­”ã€‚
Truthfulnessï¼ˆçœŸå®æ€§ï¼‰ï¼šå›ç­”å†…å®¹æ˜¯å¦çœŸå®ã€å‡†ç¡®ã€‚
Honestyï¼ˆè¯šå®æ€§ï¼‰ï¼šå›ç­”æ˜¯å¦æœ‰è¯¯å¯¼æ€§æˆ–éšç’ä¿¡æ¯ã€‚


Binarizeï¼ˆäºŒå€¼åŒ–ï¼‰ï¼šå°†è¯„åˆ†ç»“æœè½¬åŒ–ä¸ºâ€œChosenï¼ˆé€‰æ‹©ï¼‰â€æˆ–â€œRejectedï¼ˆæ‹’ç»ï¼‰â€ï¼š

Chosenï¼šå¾—åˆ†è¾ƒé«˜çš„å›ç­”ã€‚
Rejectedï¼šå¾—åˆ†è¾ƒä½çš„å›ç­”ã€‚



**GPT-4o ä¸ç›´æ¥ç”Ÿæˆå›ç­”ï¼Œè€Œæ˜¯ä½œä¸ºæ ‡æ³¨åŠ©æ‰‹ï¼Œä¸ºæ¯ä¸ªå›ç­”åœ¨ä¸åŒç»´åº¦ä¸Šç»™å‡ºå…·ä½“è¯„åˆ†ã€‚**é™¤äº†åˆ¤æ–­â€œå¥½â€æˆ–â€œåâ€ï¼Œè¿˜å¯ä»¥æä¾›æ›´ç²¾ç»†çš„è´¨é‡ç»´åº¦ï¼ˆå¦‚æœ‰ç”¨æ€§ã€æŒ‡ä»¤éµå¾ªæ€§ç­‰ï¼‰ï¼Œä»¥ä¾¿åç»­æ›´ç»†ç²’åº¦åœ°ä¼˜åŒ–æ¨¡å‹ã€‚


å¥–åŠ±æ¨¡å‹
ç»™å®šåå¥½æ•°æ®é›†,è®­ç»ƒå¥–åŠ±æ¨¡å‹ (RM) rÏ•r_{\phi}rÏ•â€‹,ç›®æ ‡å¦‚ä¸‹:
maxâ¡Ï•E(x,yc,yr)âˆ¼D[logâ¡Ïƒ(rÏ•(x,yc)âˆ’rÏ•(x,yr))]\max_{\phi} \mathbb{E}_{(x,y_c,y_r)\sim D} \left[ \log \sigma(r_{\phi}(x,y_c) - r_{\phi}(x,y_r)) \right]
Ï•maxâ€‹E(x,ycâ€‹,yrâ€‹)âˆ¼Dâ€‹[logÏƒ(rÏ•â€‹(x,ycâ€‹)âˆ’rÏ•â€‹(x,yrâ€‹))]

æ•°æ®é›† D\mathcal{D}D åŒ…å«äº†ä¸€äº›æç¤ºï¼ˆpromptsï¼‰ xxx å’Œä¸¤ä¸ªå€™é€‰å›ç­” yyy å’Œ yâ€²y&#x27;yâ€²ã€‚
å¯¹äºæ¯ä¸ªæç¤º xxxï¼Œä¸€äº›è¯„å®¡è€…ä¼šä» yyy å’Œ yâ€²y&#x27;yâ€² ä¸­é€‰æ‹©ä¸€ä¸ªä½œä¸ºæ›´å¥½çš„å›ç­” ycy_cycâ€‹ï¼Œå¦ä¸€ä¸ªä¼šè¢«æ ‡è®°ä¸ºè¢«æ‹’ç»çš„å›ç­” yry_ryrâ€‹ã€‚
rÏ•(x,y)r_{\phi}(x, y)rÏ•â€‹(x,y)ï¼šå¥–åŠ±æ¨¡å‹å¯¹äºè¾“å…¥ xxx å’Œå›ç­” yyy çš„è¯„åˆ†ã€‚
Ïƒ(z)\sigma(z)Ïƒ(z)ï¼šæ¿€æ´»å‡½æ•° Sigmoidï¼ŒÏƒ(z)=11+eâˆ’z\sigma(z)=\frac{1}{1+e^{-z}}Ïƒ(z)=1+eâˆ’z1â€‹ï¼Œå°†åˆ†æ•°å·®è½¬åŒ–ä¸ºä¸€ä¸ªæ¦‚ç‡å€¼ã€‚
(x,yc,yr)âˆ¼D(x,y_c,y_r)\sim \mathcal{D}(x,ycâ€‹,yrâ€‹)âˆ¼Dï¼šä»åå¥½æ•°æ®é›†ä¸­é‡‡æ ·çš„ä¸€ä¸ªä¸‰å…ƒç»„ï¼Œå…¶ä¸­ ycy_cycâ€‹ æ˜¯é€‰æ‹©çš„å›ç­”ï¼Œyry_ryrâ€‹ æ˜¯è¢«æ‹’ç»çš„å›ç­”ã€‚
rÏ•(x,yc)âˆ’rÏ•(x,yr)r_{\phi}(x,y_c)-r_{\phi}(x,y_r)rÏ•â€‹(x,ycâ€‹)âˆ’rÏ•â€‹(x,yrâ€‹)ï¼šè¡¨ç¤ºå¥–åŠ±æ¨¡å‹è®¤ä¸ºé€‰æ‹©å›ç­” ycy_cycâ€‹ ä¸è¢«æ‹’ç»å›ç­” yry_ryrâ€‹ çš„åˆ†æ•°å·®ã€‚
logâ¡Ïƒ(rÏ•(x,yc)âˆ’rÏ•(x,yr))\log \sigma(r_{\phi}(x,y_c)-r_{\phi}(x,y_r))logÏƒ(rÏ•â€‹(x,ycâ€‹)âˆ’rÏ•â€‹(x,yrâ€‹))ï¼šè¡¨ç¤ºæ ¹æ®åˆ†æ•°å·®çš„å¯¹æ•°ä¼¼ç„¶ï¼Œæè¿°äº†æ¨¡å‹å°† ycy_cycâ€‹ è§†ä¸ºé€‰æ‹©å›ç­”çš„æ¦‚ç‡ï¼Œå®ƒè¶Šæ¥è¿‘ 0ï¼Œæ„å‘³ç€å¯¹åº”çš„åŸå§‹æ¦‚ç‡ Ïƒ(z)\sigma(z)Ïƒ(z) è¶Šé«˜ã€‚

ä¼˜åŒ–ç›®æ ‡ï¼šæœ€å¤§åŒ–è¿™ä¸€å¯¹æ•°ä¼¼ç„¶ï¼Œå¼ºåŒ–å¥–åŠ±æ¨¡å‹å¯¹é€‰æ‹©å›ç­”ä¸è¢«æ‹’ç»å›ç­”çš„åŒºåˆ†èƒ½åŠ›ã€‚




5.1.2	ç­–ç•¥ä¼˜åŒ–
PPO
ç”¨ KL æ•£åº¦ä½œä¸ºæ­£åˆ™åŒ–é¡¹,é™åˆ¶æ¨¡å‹æ›´æ–°çš„æ­¥é•¿ã€‚æ¨¡å‹å¸Œæœ›æœ€å¤§åŒ–å¥–åŠ±(ç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½çš„å†…å®¹),ä½†æ¨¡å‹ä¹Ÿéœ€è¦å—é™äºâ€œä¸è¦åç¦»é¢„è®­ç»ƒæ¨¡å‹å¤ªè¿œâ€,ä»¥é¿å…ç”Ÿæˆå¥‡æ€ªã€ä¸ç¨³å®šçš„å†…å®¹ã€‚KL æ•£åº¦æä¾›äº†ä¸€ä¸ªåº¦é‡å·¥å…·,Î²\betaÎ² åˆ™å†³å®šäº†åç¦»çš„å…è®¸ç¨‹åº¦ã€‚
R(x,y)=rÏ•(x,y)âˆ’Î²KL[Ï€Î¸(yâˆ£x)âˆ¥Ï€ref(yâˆ£x)]R(x,y) = r_{\phi}(x,y) - \beta \text{KL}[\pi_{\theta}(y|x) \| \pi_{\text{ref}}(y|x)]
R(x,y)=rÏ•â€‹(x,y)âˆ’Î²KL[Ï€Î¸â€‹(yâˆ£x)âˆ¥Ï€refâ€‹(yâˆ£x)]

Ï€Î¸(yâˆ£x)\pi_{\theta}(y \mid x)Ï€Î¸â€‹(yâˆ£x)ï¼šå½“å‰è®­ç»ƒçš„è¯­è¨€æ¨¡å‹çš„ç­–ç•¥ï¼ˆç”Ÿæˆ yyy çš„æ¦‚ç‡åˆ†å¸ƒï¼‰ã€‚
Ï€ref(yâˆ£x)\pi_{\text{ref}}(y \mid x)Ï€refâ€‹(yâˆ£x)ï¼šåˆå§‹å‚è€ƒç­–ç•¥ï¼ˆé¢„è®­ç»ƒæ¨¡å‹çš„ç­–ç•¥ï¼Œç”¨äºä¿æŒç”Ÿæˆç»“æœçš„ç¨³å®šæ€§ï¼‰ã€‚
KL[Ï€Î¸(yâˆ£x)â€‰âˆ¥â€‰Ï€ref(yâˆ£x)]\mathrm{KL}\big[\pi_{\theta}(y \mid x)\,\|\,\pi_{\text{ref}}(y \mid x)\big]KL[Ï€Î¸â€‹(yâˆ£x)âˆ¥Ï€refâ€‹(yâˆ£x)]ï¼šKullbackâ€“Leiblerï¼ˆKLï¼‰æ•£åº¦ï¼Œç”¨æ¥è¡¡é‡å½“å‰ç­–ç•¥ Ï€Î¸\pi_\thetaÏ€Î¸â€‹ åç¦»å‚è€ƒç­–ç•¥ Ï€ref\pi_\text{ref}Ï€refâ€‹ çš„ç¨‹åº¦ã€‚
rÏ•(x,y)r_\phi(x, y)rÏ•â€‹(x,y): å¥–åŠ±æ¨¡å‹çš„è¾“å‡ºï¼Œè¡¨ç¤ºå¯¹è¾“å…¥ xxx å’Œç”Ÿæˆç»“æœ yyy çš„è´¨é‡è¯„åˆ†ã€‚
Î²\betaÎ²: æ§åˆ¶ KL æ•£åº¦çš„æƒé‡ï¼Œå¹³è¡¡æ¨¡å‹å¯¹å¥–åŠ±æœ€å¤§åŒ–å’Œä¸å‚è€ƒç­–ç•¥ä¿æŒæ¥è¿‘ä¹‹é—´çš„å–èˆã€‚

ç›®æ ‡ï¼š

ç›®æ ‡æ˜¯ä¼˜åŒ–ç­–ç•¥ Ï€Î¸\pi_\thetaÏ€Î¸â€‹ï¼Œä½¿å¾—ç”Ÿæˆçš„ç»“æœ yyy åœ¨ç»™å®š xxx æ—¶èƒ½è·å¾—æœ€é«˜çš„å¥–åŠ± R(x,y)R(x,y)R(x,y)ã€‚
å¥–åŠ± R(x,y) è¢«åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š

rÏ•(x,y)r_\phi(x, y)rÏ•â€‹(x,y): å¥–åŠ±æ¨¡å‹ç»™å‡ºçš„åå¥½è¯„åˆ†ã€‚
Î²KL[Ï€Î¸âˆ¥Ï€ref]\beta \text{KL}[\pi_\theta \| \pi_\text{ref}]Î²KL[Ï€Î¸â€‹âˆ¥Ï€refâ€‹]: æƒ©ç½šé¡¹ï¼Œé™åˆ¶ Ï€Î¸\pi_\thetaÏ€Î¸â€‹åç¦»Ï€ref\pi_\text{ref}Ï€refâ€‹ å¤ªè¿œã€‚



ä½œç”¨ï¼š

å¥–åŠ±æœ€å¤§åŒ–ï¼šé€šè¿‡ rÏ•(x,y)r_\phi(x, y)rÏ•â€‹(x,y) è®©ç­–ç•¥ç”Ÿæˆæ›´ç¬¦åˆäººç±»åå¥½çš„å†…å®¹ã€‚
ç¨³å®šæ€§æ§åˆ¶ï¼šé€šè¿‡ KL æ•£åº¦ï¼Œç¡®ä¿æ¨¡å‹ä¸ä¼šåç¦»åˆå§‹ç­–ç•¥å¤ªè¿œï¼Œé¿å…ç”Ÿæˆè´¨é‡å´©å¡Œæˆ–å‡ºç°æç«¯è¡Œä¸ºã€‚
**æƒé‡ **Î²\betaÎ²ï¼š
Î²\betaÎ² æ§åˆ¶ä¸¤éƒ¨åˆ†çš„æƒè¡¡ï¼š

å¦‚æœ Î²\betaÎ² è¾ƒå¤§ï¼Œæ¨¡å‹æ›´å€¾å‘äºä¿æŒå’Œå‚è€ƒç­–ç•¥æ¥è¿‘ã€‚
å¦‚æœ Î²\betaÎ² è¾ƒå°ï¼Œæ¨¡å‹æ›´å€¾å‘äºè¿½æ±‚å¥–åŠ±æœ€å¤§åŒ–ï¼Œä½†å¯èƒ½ä¼šå¯¼è‡´ç”Ÿæˆä¸ç¨³å®šã€‚



DPO
ç›´æ¥ç”¨åå¥½æ•°æ®ä¼˜åŒ–ç­–ç•¥,é¿å…å¤§å¹…åç¦»åˆå§‹å‚è€ƒç­–ç•¥,å…¶ç­‰æ•ˆç›®æ ‡å¦‚ä¸‹:
maxâ¡Î¸E(x,yc,yr)âˆ¼D[logâ¡Ïƒ(Î²logâ¡Ï€Î¸(ycâˆ£x)Ï€ref(ycâˆ£x)âˆ’Î²logâ¡Ï€Î¸(yrâˆ£x)Ï€ref(yrâˆ£x))]\max_{\theta} \mathbb{E}_{(x,y_c,y_r)\sim D} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)} - \beta \log \frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)} \right) \right]
Î¸maxâ€‹E(x,ycâ€‹,yrâ€‹)âˆ¼Dâ€‹[logÏƒ(Î²logÏ€refâ€‹(ycâ€‹âˆ£x)Ï€Î¸â€‹(ycâ€‹âˆ£x)â€‹âˆ’Î²logÏ€refâ€‹(yrâ€‹âˆ£x)Ï€Î¸â€‹(yrâ€‹âˆ£x)â€‹)]
è¾“å…¥æ•°æ®ï¼š

æ•°æ®é›†D\mathcal{D}DåŒ…å«ä¸€å¯¹åå¥½æ ‡æ³¨ $ (y_c, y_r) $ï¼š

ycy_{c}ycâ€‹ï¼šä¼˜é€‰é¡¹ï¼ˆchosenï¼‰ã€‚
yry_{r}yrâ€‹ï¼šéä¼˜é€‰é¡¹ï¼ˆrejectedï¼‰ã€‚


xxxæ˜¯è¾“å…¥ä¸Šä¸‹æ–‡ã€‚

ç›®æ ‡å‡½æ•°çš„æ ¸å¿ƒï¼š

åˆ†æ•°å·®ï¼šåˆ†æ•°çš„è®¡ç®—åŸºäºæ¨¡å‹ Ï€Î¸\pi_\thetaÏ€Î¸â€‹ å’Œå‚è€ƒç­–ç•¥ Ï€ref\pi_\text{ref}Ï€refâ€‹ çš„å¯¹æ•°æ¯”å€¼ï¼ˆå³å¯¹æ•°æ¦‚ç‡æ¯”ï¼‰ã€‚ä¼˜é€‰é¡¹ ycy_cycâ€‹ å’Œéä¼˜é€‰é¡¹ yry_ryrâ€‹ çš„åˆ†æ•°å·®å®šä¹‰ä¸ºï¼š

Î”=Î²logâ¡Ï€Î¸(ycâˆ£x)Ï€ref(ycâˆ£x)âˆ’Î²logâ¡Ï€Î¸(yrâˆ£x)Ï€ref(yrâˆ£x).\Delta = \beta \log \frac{\pi_\theta(y_c\mid x)}{\pi_\text{ref}(y_c\mid x)} - \beta \log \frac{\pi_\theta(y_r\mid x)}{\pi_\text{ref}(y_r\mid x)}.Î”=Î²logÏ€refâ€‹(ycâ€‹âˆ£x)Ï€Î¸â€‹(ycâ€‹âˆ£x)â€‹âˆ’Î²logÏ€refâ€‹(yrâ€‹âˆ£x)Ï€Î¸â€‹(yrâ€‹âˆ£x)â€‹.
Î²\betaÎ² æ˜¯ä¸€ä¸ªæƒé‡å‚æ•°ï¼Œç”¨äºè°ƒæ•´ç­–ç•¥å’Œå‚è€ƒç­–ç•¥ä¹‹é—´çš„åç¦»ç¨‹åº¦ã€‚



ä¸Šé¢çš„ Î”\DeltaÎ” å¯è¿›ä¸€æ­¥ç®€åŒ–,æœ€ç»ˆä¸º:
logâ¡Ï€Î¸(ycâˆ£x)=âˆ‘t=1Tclogâ¡Ï€Î¸(yc,tâˆ£x,yc,1:tâˆ’1)\log \pi_{\theta}(y_c|x) = \sum_{t=1}^{T_c} \log \pi_{\theta}(y_{c,t}|x, y_{c,1:t-1})
logÏ€Î¸â€‹(ycâ€‹âˆ£x)=t=1âˆ‘Tcâ€‹â€‹logÏ€Î¸â€‹(yc,tâ€‹âˆ£x,yc,1:tâˆ’1â€‹)
å…¶ä¸­,å‡è®¾ ycy_cycâ€‹ æ˜¯ç”±å¤šä¸ª token ç»„æˆçš„åºåˆ—: y=(y1,y2,â€¦,yT)y = (y_1, y_2, \ldots, y_T)y=(y1â€‹,y2â€‹,â€¦,yTâ€‹),é‚£ä¹ˆæ¡ä»¶æ¦‚ç‡å¯ä»¥è¢«åˆ†è§£ä¸º:
Ï€Î¸(yâˆ£x)=âˆt=1TÏ€Î¸(ytâˆ£x,y1:tâˆ’1)\pi_{\theta}(y|x) = \prod_{t=1}^{T} \pi_{\theta}(y_t|x, y_{1:t-1})
Ï€Î¸â€‹(yâˆ£x)=t=1âˆTâ€‹Ï€Î¸â€‹(ytâ€‹âˆ£x,y1:tâˆ’1â€‹)
åœ¨ç”Ÿæˆç¬¬ tä¸ª token yty_tytâ€‹ æ—¶,æ¨¡å‹ä¸ä»…å‚è€ƒè¾“å…¥ x,è¿˜å‚è€ƒä¹‹å‰å·²ç»ç”Ÿæˆçš„ token y1,â€¦,ytâˆ’1y_{1},\ldots,y_{t-1}y1â€‹,â€¦,ytâˆ’1â€‹ã€‚
åœ¨ DPO çš„è¿‡ç¨‹ä¸­ï¼Œå¸Œæœ›æ¨¡å‹å­¦ä¼šå¯¹å¥½çš„å›ç­” ycy_cycâ€‹ èµ‹äºˆ æ›´é«˜çš„æ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯ Ï€Î¸(ycâˆ£x)\pi_\theta(y_c\mid x)Ï€Î¸â€‹(ycâ€‹âˆ£x) æ›´å¤§ã€‚åŒæ—¶ï¼Œå¯¹å·®çš„å›ç­” yry_ryrâ€‹ èµ‹äºˆ æ›´ä½çš„æ¦‚ç‡ï¼Œä¹Ÿå°±æ˜¯ Ï€Î¸(yrâˆ£x)\pi_\theta(y_r\mid x)Ï€Î¸â€‹(yrâ€‹âˆ£x) æ›´å°ã€‚åœ¨ä¼˜åŒ–æ¨¡å‹å‚æ•° Î¸\thetaÎ¸ çš„è¿‡ç¨‹ä¸­ï¼Œé€æ­¥å­¦ä¹ ä»è¾“å…¥ xxx åˆ°ä¼˜è´¨è¾“å‡º yyy çš„æ˜ å°„å…³ç³»ï¼Œå³ä¼˜åŒ– Ï€Î¸(yâˆ£x)\pi_\theta(y\mid x)Ï€Î¸â€‹(yâˆ£x) çš„åˆ†å¸ƒï¼Œä½¿å…¶æ›´è´´åˆåå¥½æ•°æ®ï¼Œé€æ­¥è®© ä¼˜è´¨å›ç­”çš„ç”Ÿæˆæ¦‚ç‡ Ï€Î¸(ycâˆ£x)\pi_\theta(y_c\mid x)Ï€Î¸â€‹(ycâ€‹âˆ£x) è¶…è¿‡åŠ£è´¨å›ç­”çš„ç”Ÿæˆæ¦‚ç‡ Ï€Î¸(yrâˆ£x)\pi_\theta(y_r\mid x)Ï€Î¸â€‹(yrâ€‹âˆ£x)ã€‚
Length-Normalized DPO
ä½œè€…åœ¨ä¸Šè¿°DPOçš„åŸºç¡€ä¸Š,è¿›ä¸€æ­¥åŠ å…¥äº†å½’ä¸€åŒ–æ¥æ¶ˆé™¤,å› å›ç­”çš„åºåˆ—é•¿åº¦(å¯¹æ¯”æ¥è¯´å¾ˆé•¿çš„è¯)å¸¦æ¥çš„æ¦‚ç‡åå¥½å½±å“ã€‚
maxâ¡Î¸E(x,yc,yr)âˆ¼D[logâ¡Ïƒ(Î²1âˆ£ycâˆ£logâ¡Ï€Î¸(ycâˆ£x)Ï€ref(ycâˆ£x)âˆ’Î²1âˆ£yrâˆ£logâ¡Ï€Î¸(yrâˆ£x)Ï€ref(yrâˆ£x))]\max_{\theta} \mathbb{E}_{(x,y_c,y_r)\sim D} \left[ \log \sigma \left( \beta \frac{1}{|y_c|} \log \frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)} - \beta \frac{1}{|y_r|} \log \frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)} \right) \right]
Î¸maxâ€‹E(x,ycâ€‹,yrâ€‹)âˆ¼Dâ€‹[logÏƒ(Î²âˆ£ycâ€‹âˆ£1â€‹logÏ€refâ€‹(ycâ€‹âˆ£x)Ï€Î¸â€‹(ycâ€‹âˆ£x)â€‹âˆ’Î²âˆ£yrâ€‹âˆ£1â€‹logÏ€refâ€‹(yrâ€‹âˆ£x)Ï€Î¸â€‹(yrâ€‹âˆ£x)â€‹)]
å…¶ä¸­,å‡è®¾ ycy_cycâ€‹ æ˜¯ç”±å¤šä¸ª token ç»„æˆçš„åºåˆ—: y=(y1,y2,â€¦,yT)y = (y_1, y_2, \ldots, y_T)y=(y1â€‹,y2â€‹,â€¦,yTâ€‹),é‚£ä¹ˆæ¡ä»¶æ¦‚ç‡å¯ä»¥è¢«åˆ†è§£ä¸º:
Ï€Î¸(yâˆ£x)=âˆt=1TÏ€Î¸(ytâˆ£x,y1:tâˆ’1)\pi_{\theta}(y|x) = \prod_{t=1}^{T} \pi_{\theta}(y_t|x, y_{1:t-1})
Ï€Î¸â€‹(yâˆ£x)=t=1âˆTâ€‹Ï€Î¸â€‹(ytâ€‹âˆ£x,y1:tâˆ’1â€‹)
å…¶å¯¹æ•°æ¦‚ç‡ä¸º:
logâ¡Ï€Î¸(yâˆ£x)=âˆ‘t=1Tlogâ¡Ï€Î¸(ytâˆ£x,y1:tâˆ’1)\log \pi_{\theta}(y|x) = \sum_{t=1}^{T} \log \pi_{\theta}(y_t|x, y_{1:t-1})
logÏ€Î¸â€‹(yâˆ£x)=t=1âˆ‘Tâ€‹logÏ€Î¸â€‹(ytâ€‹âˆ£x,y1:tâˆ’1â€‹)

è¾ƒé•¿çš„åºåˆ— y ä¼šç´¯ç§¯æ›´å¤šçš„æ¦‚ç‡é¡¹ï¼Œå³ç´¯ç§¯æ›´å¤šçš„å¯¹æ•°å€¼ï¼Œå³ä½¿å•ä¸ªè¯çš„æ¦‚ç‡è¾ƒä½ï¼Œå…¶æ•´ä½“å¯¹æ•°æ¦‚ç‡å¯èƒ½ä»ç„¶è¾ƒé«˜ã€‚å› æ­¤ï¼Œè¾ƒé•¿çš„åºåˆ—å¯èƒ½ä¼šåœ¨ä¼˜åŒ–ç›®æ ‡ä¸­è·å¾—æ›´å¤§çš„å¾—åˆ†ï¼Œè¿›è€Œè¢«æ¨¡å‹ä¼˜å…ˆé€‰æ‹©ã€‚

DOPå’ŒLength-Normalized DPO çš„åŒºåˆ«
é•¿åº¦å½’ä¸€åŒ–ï¼š

åœ¨åˆ†æ•°è®¡ç®—ä¸­,åŠ å…¥äº†é•¿åº¦å½’ä¸€åŒ–é¡¹ 1âˆ£yâˆ£\frac{1}{|y|}âˆ£yâˆ£1â€‹,åˆ†åˆ«å¯¹ ycy_cycâ€‹ å’Œ yry_ryrâ€‹ çš„åˆ†æ•°è¿›è¡Œé•¿åº¦è°ƒæ•´ã€‚
ç›´è§‚ç†è§£ï¼šè¾ƒé•¿çš„è¾“å‡ºåºåˆ—é€šå¸¸ä¼šç´¯ç§¯æ›´å¤§çš„å¯¹æ•°æ¦‚ç‡ï¼Œä»è€Œå¯èƒ½äº§ç”Ÿé•¿åº¦åå·®ã€‚é•¿åº¦å½’ä¸€åŒ–å¯ä»¥å‡è½»è¿™ç§åå·®ã€‚

ä¼˜åŒ–ç›®æ ‡ï¼š

åœ¨ DPO çš„åŸºç¡€ä¸Šï¼Œå¼•å…¥é•¿åº¦å½’ä¸€åŒ–ï¼Œé€‚é…æ›´å¹¿æ³›çš„äººç±»åå¥½æ ‡æ³¨æ•°æ®ï¼Œç‰¹åˆ«æ˜¯åœ¨è¾“å‡ºé•¿åº¦å·®å¼‚è¾ƒå¤§çš„æƒ…å†µä¸‹ã€‚

DPO å’Œ Length-Normalized DPO çš„ç‰¹ç‚¹
DPOï¼š

ä¸éœ€è¦æ˜¾å¼åˆ©ç”¨è®­ç»ƒå¥–åŠ±æ¨¡å‹è¯„åˆ†ï¼Œç›´æ¥ä¼˜åŒ–ç­–ç•¥ $ \pi_\theta $ã€‚
é€šè¿‡åˆ†æ•°å·® Î”\DeltaÎ”è¡¨è¾¾åå¥½å…³ç³»ã€‚
ç®€æ´é«˜æ•ˆï¼Œä½†å¯èƒ½å—è¾“å‡ºé•¿åº¦åå·®å½±å“ã€‚

Length-Normalized DPOï¼š

åœ¨ DPO åŸºç¡€ä¸ŠåŠ å…¥é•¿åº¦å½’ä¸€åŒ–ï¼Œå‡è½»æ¨¡å‹ç”Ÿæˆåå¥½è¾ƒé•¿è¾“å‡ºçš„å€¾å‘ã€‚
æ›´åŠ é€‚é…åŒ…å«é•¿åº¦åå·®çš„æ ‡æ³¨æ•°æ®ã€‚

DPOå’ŒPPOå¯¹æ¯”



å¯¹æ¯”ç»´åº¦
DPO
PPO




å¦éœ€è¦å¥–åŠ±æ¨¡å‹
ä¸éœ€è¦ï¼ˆç›´æ¥ç”¨åå¥½å¯¹ï¼‰
éœ€è¦è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹


è®­ç»ƒå¤æ‚åº¦
è¾ƒä½ï¼ˆåªä¼˜åŒ–ç­–ç•¥æ¨¡å‹ï¼‰
è¾ƒé«˜ï¼ˆéœ€è¦ä¼˜åŒ–ç­–ç•¥ã€å¥–åŠ±ã€ä»·å€¼ç­‰å¤šä¸ªç»„ä»¶ï¼‰


ä¾èµ–å¼ºåŒ–å­¦ä¹ 
ä¸éœ€è¦
éœ€è¦


å¯¹äººç±»åå¥½çš„åˆ©ç”¨
æ›´ç›´æ¥ï¼ˆé€šè¿‡æ¦‚ç‡å¯¹æ¯”ï¼‰
é—´æ¥ï¼ˆé€šè¿‡å¥–åŠ±æ¨¡å‹è¯„ä¼°ï¼‰


å®ç°éš¾åº¦
å®ç°ç®€å•
å®ç°å¤æ‚


KL æ§åˆ¶
å†…ç½®åœ¨åå¥½å¯¹æ¯”ä¸­ï¼ˆæ›´ç›´è§‚ï¼‰
éœ€è¦æ˜¾å¼æ§åˆ¶ KL æƒ©ç½šé¡¹



TÃœLU3åå¥½æ•°æ®
ä»Promptsåˆ°åå¥½æ•°æ®
æ ¹æ® Â§3 ä¸­çš„Promptsï¼Œä½œè€…ä»¬é€šè¿‡è°ƒæ•´å’Œæ”¹è¿›åå¥½æ•°æ®çš„pipelineåˆ›å»ºç¬¦åˆç­–ç•¥çš„åå¥½æ•°æ® ( ğ‘¥,ğ‘¦,ğ‘¦ â€²,ğ‘™ğ‘ğ‘ğ‘’ğ‘™ ) ã€‚ä½œè€…ä»¬çš„æ—©æœŸå®éªŒè¡¨æ˜ï¼Œè¯¥æµç¨‹åœ¨åˆ›å»ºåå¥½æ•°æ®æ–¹é¢å…·æœ‰ä¼˜åŠ¿ï¼Œå¯ä»¥ç”Ÿæˆé«˜è´¨é‡çš„åˆæˆåå¥½æ•°æ®é›†ã€‚ä½œè€…ä»¬çš„æ•°æ®åˆ›å»ºæµç¨‹ï¼ˆå¦‚å›¾ 7 æ‰€ç¤ºï¼‰åŒ…æ‹¬ä¸‰ä¸ªé˜¶æ®µï¼šPromptsé€‰æ‹©ã€ä»æ¨¡å‹æ± ç”Ÿæˆå›ç­”ä»¥åŠä½¿ç”¨ LLM-as-a-judge è¿›è¡Œåå¥½æ³¨é‡Šä»¥åˆ›å»ºï¼ˆåå¥½ã€æ‹’ç»ï¼‰å¯¹ã€‚

ç¬¬ 1 é˜¶æ®µï¼šPromptsé€‰æ‹©ã€‚å‡†å¤‡æ•°æ®é›†ä»¥è¿›è¡Œåå¥½å¾®è°ƒçš„ç¬¬ä¸€æ­¥æ˜¯é€‰æ‹©Promptsæˆ–ç”¨æˆ·æŒ‡ä»¤ä»¥ç”Ÿæˆå›ç­”å¹¶è·å–åå¥½ã€‚é‰´äºè¡¨ 6 ä¸­çš„Promptsé›†ï¼Œä½œè€…ä»¬ç²¾å¿ƒæŒ‘é€‰äº†ä½œè€…ä»¬çš„é€‰æ‹©ï¼Œä»¥åŒ…æ‹¬ SFT æœŸé—´ä½¿ç”¨çš„Promptsï¼Œä»¥åŠä»ç›¸åŒæ¥æºæŠ½æ ·ä½†æœªç”¨äº SFT çš„Promptsã€‚ä½œè€…ä»¬è¿˜åŒ…æ‹¬æ¥è‡ªå…¶ä»–æ¥æºçš„Promptsï¼Œä¾‹å¦‚æ²¡æœ‰ TruthfulQA å®ä¾‹çš„ Ultrafeedback ç‰ˆæœ¬ï¼Œæˆ–è€…é€šè¿‡å‘Promptsæ·»åŠ æ–°çš„ IF çº¦æŸã€‚
**ç¬¬ 2 é˜¶æ®µï¼šå›ç­”ç”Ÿæˆã€‚**å¯¹äºç»™å®šçš„Promptsï¼Œä½œè€…ä»¬ä»æ¨¡å‹æ± ä¸­éšæœºæŠ½å–å››ä¸ªæ¨¡å‹æ¥ç”Ÿæˆå›ç­”ã€‚ä½œè€…ä»¬çš„æ¨¡å‹é€‰æ‹©å—åˆ° Ultrafeedback pipelineçš„å¯å‘ï¼Œè¯¥pipelineç”±å¼€æºå’Œä¸“æœ‰æ¨¡å‹ç»„æˆï¼Œè¿™äº›æ¨¡å‹åœ¨å‚æ•°å¤§å°å’Œæ¨¡å‹ç³»åˆ—ä¸Šæœ‰æ‰€ä¸åŒã€‚ä½œè€…ä»¬é€šè¿‡ä½¿ç”¨æŸäº›æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬ï¼ˆLlama 2 â†’ Llama 3.1ï¼‰æ¥æ›´æ–° Ultrafeedback çš„æ¨¡å‹æ± ï¼Œæ·»åŠ æ€§èƒ½æœ€ä½³çš„æ¨¡å‹ä»¥å¢åŠ æ± å¤§å°ï¼Œå¹¶ç”¨å¼€æºæ›¿ä»£æ–¹æ¡ˆæ›¿æ¢å½“å‰æ— æ³•è®¿é—®çš„æ¨¡å‹ï¼ˆä¾‹å¦‚ WizardLMï¼‰ã€‚æœ€åï¼Œä½œè€…ä»¬è¿˜é€šè¿‡ä» TÃœLU SFT æ¨¡å‹ä¸­æŠ½æ ·å®Œæˆæ¥åŒ…å«åœ¨çº¿ç­–ç•¥æ•°æ®ã€‚ä½œè€…ä»¬é€šè¿‡æ·»åŠ ä¸€ç³»åˆ—Promptsæ¥å®ç°è¿™ä¸€ç‚¹ï¼Œå…¶ä¸­ä¸€ä¸ªå›ç­”æ¥è‡ªåœ¨çº¿ç­–ç•¥æ¨¡å‹ï¼Œå¦ä¸€ä¸ªå›ç­”æ¥è‡ªéç­–ç•¥æ¨¡å‹ã€‚
ç¬¬ 3 é˜¶æ®µï¼šåå¥½æ³¨é‡Šã€‚åœ¨ä¸ºæ¯ä¸ªPromptsç”Ÿæˆå››ä¸ªå›ç­”åï¼Œä½œè€…ä»¬ä½¿ç”¨ LLM-as-a-judgeï¼Œç‰¹åˆ«æ˜¯ GPT-4o-2024-0806ï¼Œä»å››ä¸ªä¸åŒæ–¹é¢å¯¹æ¯ä¸ªå›ç­”ä» 1 åˆ° 5 è¿›è¡Œè¯„åˆ†ï¼šæœ‰å¸®åŠ©æ€§ã€éµå¾ªæŒ‡ç¤ºã€è¯šå®å’ŒçœŸå®æ€§ã€‚

TÃœLU3åå¥½ç»„åˆ




ä½œè€…ä»¬ä¸º 8b å’Œ 70b æ¨¡å‹é€‰æ‹©äº†æœ€ç»ˆçš„åå¥½ç»„åˆï¼Œè¿™å¯ä»¥æœ€å¤§é™åº¦åœ°æé«˜å¼€å‘è¯„ä¼°çš„å¹³å‡è¡¨ç°ï¼ŒåŒæ—¶åœ¨ç›®æ ‡æŠ€èƒ½æ–¹é¢ä¹Ÿè¡¨ç°å‡ºè‰²ã€‚å¤§å¤šæ•°åå¥½æ•°æ®ç»„åˆæ¶ˆèéƒ½æ˜¯é’ˆå¯¹ 8b æ¨¡å‹è¿è¡Œçš„ï¼Œä½œè€…ä»¬ä»ç”¨äº SFT çš„Promptså¼€å§‹ï¼Œç”Ÿæˆåœ¨ç­–ç•¥å’Œç¦»ç­–ç•¥åå¥½æ•°æ®ï¼Œä»è€Œäº§ç”Ÿ 96911 ä¸ªï¼ˆæ‹’ç»ç­–ç•¥ï¼‰å’Œ 19444 ä¸ªï¼ˆæ¥å—ç­–ç•¥ï¼‰åå¥½å®ä¾‹ã€‚æ ¹æ®è¿™ä¸ªåå¥½åŸºç¡€ï¼Œä½œè€…ä»¬å‘ç»„åˆä¸­æ·»åŠ å…¶ä»–Promptsæºï¼Œä»¥åŠè¿™äº›æ·»åŠ å¦‚ä½•å½±å“ä¸‹æ¸¸è¯„ä¼°æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹è¯¸å¦‚ç²¾ç¡®æŒ‡ä»¤éµå¾ªã€æ•°å­¦å’Œ alpacaeval ä¸Šçš„ä¸€èˆ¬èŠå¤©è¡¨ç°ç­‰æŠ€èƒ½ã€‚æ€»ä¹‹ï¼Œä½œè€…ä»¬çš„åå¥½ç»„åˆæ¥è‡ªä¸åŒçš„Promptsæºï¼Œä¾‹å¦‚ SFT æ•°æ®ã€WildChat å’Œ Persona IFã€‚å®ƒåŒ…æ‹¬ SFT è®­ç»ƒæœŸé—´çœ‹åˆ°çš„Promptsï¼Œä¹ŸåŒ…æ‹¬æ–°çš„ã€æœªè§è¿‡çš„Promptsã€‚æœ€åï¼Œå®ƒåŒ…å«åœ¨çº¿å’Œç¦»çº¿ç­–ç•¥å®Œæˆçš„æ··åˆã€‚
æ•°æ®æ¶ˆèçš„ä¸»è¦å‘ç°
]]></content>
      <categories>
        <category>è®ºæ–‡è§£è¯»</category>
        <category>tulu3</category>
      </categories>
      <tags>
        <tag>NVIDIA</tag>
        <tag>V100</tag>
        <tag>è®ºæ–‡è§£è¯»</tag>
        <tag>tulu3</tag>
      </tags>
  </entry>
  <entry>
    <title>åŸºäºMindformersçš„llama3.1æ¨¡å‹å¾®è°ƒ</title>
    <url>/2025/11/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/%E6%98%87%E8%85%BE/Mindformers/llama3.1%E5%BE%AE%E8%B0%83/</url>
    <content><![CDATA[å¯åŠ¨å®¹å™¨
docker run -dit --ipc=host --net=host \--name=wxb_mindformers \--device=/dev/davinci0 \--device=/dev/davinci1 \--device=/dev/davinci2 \--device=/dev/davinci3 \--device=/dev/davinci4 \--device=/dev/davinci5 \--device=/dev/davinci6 \--device=/dev/davinci7 \--device=/dev/davinci_manager \--device=/dev/devmm_svm \--device=/dev/hisi_hdc \-v /etc/ascend_install.info:/etc/ascend_install.info \-v /etc/hccn.conf:/etc/hccn.conf \-v /etc/localtime:/etc/localtime \-v /var/log/npu/:/usr/slog \-v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \-v /reason-sharedata/training_inference/wangxiangbo/mindformers/code:/job/code \-v /reason-sharedata/training_inference/wangxiangbo/mindformers/data:/job/data \-v /reason-sharedata/training_inference/wangxiangbo/mindformers/output:/job/output \-v /mnt/weight-1:/job/mnt \swr.cn-central-221.ovaijisuan.com/mindformers/mindformers1.3_mindspore2.4:20241114 \/bin/bash

ä»£ç å‡†å¤‡
git clone https://gitee.com/mindspore/mindformers.git
æ•°æ®é›†
python research/llama3/llama_preprocess.py \--dataset_type qa \--input_glob /job/data/alpaca/alpaca-data-conversation.json \--model_file /job/mnt/huggingface/llama-3-8b-hf \--seq_length 8192 \--output_file /job/data/alpaca
æƒé‡è½¬æ¢
python convert_weight.py --model llama --input_path /job/mnt/huggingface/llama-3-8b-hf --output_path /job/data/mindformers/llama3_8b_ckpt
è®­ç»ƒè„šæœ¬
seed: 0output_dir: &#x27;/job/output&#x27; # path to save checkpoint/strategyload_checkpoint: &#x27;/job/data/mindformers/llama3_8b_ckpt/&#x27;src_strategy_path_or_dir: &#x27;&#x27;auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed modelonly_save_strategy: Falseresume_training: Falserun_mode: &#x27;finetune&#x27; # trainer configtrainer:  type: CausalLanguageModelingTrainer  model_name: &#x27;llama3_8b&#x27; # runner configrunner_config:  epochs: 2  batch_size: 1  sink_mode: True  sink_size: 2 # optimizeroptimizer:  type: FP32StateAdamWeightDecay  beta1: 0.9  beta2: 0.95  eps: 1.e-8 # lr sechdulelr_schedule:  type: CosineWithWarmUpLR  learning_rate: 1.e-5  lr_end: 0.0  warmup_ratio: 0.03  total_steps: -1 # -1 means it will load the total steps of the dataset # datasettrain_dataset: &amp;train_dataset  data_loader:    type: MindDataset    dataset_dir: &quot;/job/data/alpaca/alpaca_llama3_8192/alpaca-fastchat8192.mindrecord&quot;    shuffle: True  input_columns: [&quot;input_ids&quot;,&quot;labels&quot;]  # &quot;input_ids&quot;, &quot;labels&quot; , labels are used in instruction finetune.  num_parallel_workers: 8  python_multiprocessing: False  drop_remainder: True  batch_size: 6  repeat: 1  numa_enable: False  prefetch_size: 1train_dataset_task:  type: CausalLanguageModelDataset  dataset_config: *train_dataset# if True, do evaluate during the training process. if false, do nothing.# note that the task trainer should support _evaluate_in_training function.do_eval: False # eval dataseteval_dataset: &amp;eval_dataset  data_loader:    type: MindDataset    dataset_dir: &quot;&quot;    shuffle: False  input_columns: [&quot;input_ids&quot;]  num_parallel_workers: 8  python_multiprocessing: False  drop_remainder: False  repeat: 1  numa_enable: False  prefetch_size: 1eval_dataset_task:  type: CausalLanguageModelDataset  dataset_config: *eval_dataset use_parallel: True# parallel context configparallel:  parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel  gradients_mean: False  enable_alltoall: False  full_batch: True  search_mode: &quot;sharding_propagation&quot;  enable_parallel_optimizer: True  strategy_ckpt_save_file: &quot;./ckpt_strategy.ckpt&quot;  parallel_optimizer_config:    gradient_accumulation_shard: False    parallel_optimizer_threshold: 64# default parallel of device num = 8 for Atlas 800T A2parallel_config:  data_parallel: 1  model_parallel: 4  pipeline_stage: 2  use_seq_parallel: False  micro_batch_num: 8  vocab_emb_dp: True  gradient_aggregation_group: 4# when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.micro_batch_interleave_num: 1 # recompute configrecompute_config:  recompute: True  select_recompute: False  parallel_optimizer_comm_recompute: True  mp_comm_recompute: True  recompute_slice_activation: True # callbackscallbacks:  - type: MFLossMonitor  - type: CheckpointMointor    prefix: &quot;llama3_8b&quot;    save_checkpoint_steps: 10000    integrated_save: False    async_save: False  - type: ObsMonitor # mindspore context init configcontext:  mode: 0 #0--Graph Mode; 1--Pynative Mode  device_target: &quot;Ascend&quot;  enable_graph_kernel: False  graph_kernel_flags: &quot;--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true&quot;  max_call_depth: 10000  max_device_memory: &quot;26GB&quot;  save_graphs: False  save_graphs_path: &quot;./graph&quot;  device_id: 0  runtime_num_threads: 1 # model configmodel:  model_config:    type: LlamaConfig    batch_size: 1 # add for increase predict    seq_length: 8192    hidden_size: 4096    num_layers: 32    num_heads: 32    n_kv_heads: 8    vocab_size: 128256    intermediate_size: 14336    rms_norm_eps: 1.0e-5    bos_token_id: 128000    eos_token_id: 128001    pad_token_id: 128002    ignore_token_id: -100    compute_dtype: &quot;bfloat16&quot;    layernorm_compute_type: &quot;float32&quot;    softmax_compute_type: &quot;float32&quot;    rotary_dtype: &quot;float32&quot;    param_init_type: &quot;bfloat16&quot;    use_past: False    scaling_factor: 1.0    theta: 500000    extend_method: &quot;None&quot; # support &quot;None&quot;, &quot;PI&quot;, &quot;NTK&quot;    use_flash_attention: True # FA can accelerate training or finetune    offset: 0    fine_grain_interleave: 1    checkpoint_name_or_path: &quot;&quot;    repetition_penalty: 1    max_decode_length: 512    top_k: 3    top_p: 1    do_sample: False  arch:    type: LlamaForCausalLM # metricmetric:  type: PerplexityMetric # wrapper cell configrunner_wrapper:  type: MFTrainOneStepCell  scale_sense: 1.0  use_clip_grad: True eval_callbacks:  - type: ObsMonitor auto_tune: Falsefilepath_prefix: &#x27;./autotune&#x27;autotune_per_step: 10 profile: Falseprofile_start_step: 5profile_stop_step: 7init_start_profile: Trueprofile_communication: Trueprofile_memory: Truelayer_scale: Falselayer_decay: 0.65lr_scale_factor: 256 # aiccremote_save_url: &quot;Please input obs url on AICC platform.&quot;
è®­ç»ƒæ‹‰èµ·

git
 clone https://gitee.com/mindspore/mindformers.gitgit
 clone https://gitee.com/mindspore/mindformers.git
]]></content>
      <categories>
        <category>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</category>
        <category>æ˜‡è…¾</category>
      </categories>
      <tags>
        <tag>æ˜‡è…¾</tag>
        <tag>910B</tag>
        <tag>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</tag>
        <tag>Mindformers</tag>
      </tags>
  </entry>
  <entry>
    <title>llama-factoryç¯å¢ƒæ­å»º-æ˜‡è…¾</title>
    <url>/2026/01/08/LLM/%E5%AE%A2%E6%88%B7%E6%94%AF%E6%92%91/%E6%9F%90%E5%BE%8B%E6%89%80%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/llama-factory%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA-%E6%98%87%E8%85%BE/</url>
    <content><![CDATA[è®­ç»ƒ
conda create --name llamafactory python=3.10conda activate llamafactorypip install -e &quot;.[torch-npu,metrics]&quot;pip install --upgrade torch==2.6.0 -i https://pypi.tuna.tsinghua.edu.cn/simplepip install --upgrade torch-npu==2.6.0rc1 -i https://pypi.tuna.tsinghua.edu.cn/simplepip uninstall scikit-learnsource /usr/local/Ascend/ascend-toolkit/set_env.shllamafactory-cli train examples/train_lora/qwen3_lora_sft.yaml
æ¨ç†
llamafactory-cli chat examples/inference/qwen3_lora_sft.yamlvllmæ¨ç†sudo apt updatesudo apt install libnuma-devpip install vllm==0.8.5 -i https://pypi.tuna.tsinghua.edu.cn/simplepip install vllm-ascend==ASCEND_RT_VISIBLE_DEVICES=2,4 llamafactory-cli chat examples/inference/qwen3_lora_sft.yamlbash Ascend-cann-nnal_8.0.0_linux-aarch64.run --installsource /usr/local/Ascend/nnal/atb/set_env.sh
]]></content>
      <categories>
        <category>è®­ç»ƒç¯å¢ƒæ­å»º</category>
        <category>llama-factory</category>
      </categories>
      <tags>
        <tag>æ˜‡è…¾</tag>
        <tag>910B</tag>
        <tag>è®­ç»ƒç¯å¢ƒæ­å»º</tag>
        <tag>llama-factory</tag>
      </tags>
  </entry>
  <entry>
    <title>åŸºäºllama-factoryçš„æ³•å¾‹æ¨¡å‹å¾®è°ƒ</title>
    <url>/2026/01/08/LLM/%E5%AE%A2%E6%88%B7%E6%94%AF%E6%92%91/%E6%9F%90%E5%BE%8B%E6%89%80%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/%E5%9F%BA%E4%BA%8Ellama-factory%E7%9A%84%E5%BE%8B%E6%89%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/</url>
    <content><![CDATA[æ•°æ®é›†




å¾®è°ƒ
lora
deepseek-r1-distill-qwen-7b
è®­ç»ƒ log




loss æ›²çº¿


lora ä¼˜åŒ–å™¨ä¸æƒé‡åˆå¹¶




qwen3-8b
è®­ç»ƒ log




loss æ›²çº¿


lora ä¼˜åŒ–å™¨ä¸æƒé‡åˆå¹¶




æ¨ç†
]]></content>
      <categories>
        <category>æ¨¡å‹å¾®è°ƒ</category>
        <category>llama-factory</category>
      </categories>
      <tags>
        <tag>æ˜‡è…¾</tag>
        <tag>910B</tag>
        <tag>æ¨¡å‹å¾®è°ƒ</tag>
        <tag>llama-factory</tag>
      </tags>
  </entry>
  <entry>
    <title>åŸºäºEasy-Datasetçš„å¾‹æ‰€æ•°æ®é›†å¤„ç†</title>
    <url>/2026/01/08/LLM/%E5%AE%A2%E6%88%B7%E6%94%AF%E6%92%91/%E6%9F%90%E5%BE%8B%E6%89%80%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/%E5%9F%BA%E4%BA%8EEasy-Dataset%E7%9A%84%E5%BE%8B%E6%89%80%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[ç¯å¢ƒ
git clone https://github.com/ConardLi/easy-dataset.gitcd easy-datasetnpm installnpm run buildnpm run start


æ¨¡å‹æµ‹è¯•


]]></content>
      <categories>
        <category>æ•°æ®é›†å¤„ç†</category>
        <category>Easy-Dataset</category>
      </categories>
      <tags>
        <tag>æ˜‡è…¾</tag>
        <tag>910B</tag>
        <tag>æ•°æ®é›†å¤„ç†</tag>
        <tag>Easy-Dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>åŸºäºModellinkçš„llama2-7bå’ŒMistral-7bæ¨¡å‹å¾®è°ƒ</title>
    <url>/2025/11/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/%E6%98%87%E8%85%BE/Modellink/llama2-7B+Mistral-7B/</url>
    <content><![CDATA[åŸºäºdockerçš„mistral-7Bå¾®è°ƒ
å®¹å™¨æŒ‚è½½
sudo docker run -dit --ipc=host --net=host \--name=modellink_wxb \--device=/dev/davinci0 \--device=/dev/davinci1 \--device=/dev/davinci2 \--device=/dev/davinci3 \--device=/dev/davinci4 \--device=/dev/davinci5 \--device=/dev/davinci6 \--device=/dev/davinci7 \--device=/dev/davinci_manager \--device=/dev/devmm_svm \--device=/dev/hisi_hdc \-v /etc/ascend_install.info:/etc/ascend_install.info \-v /etc/hccn.conf:/etc/hccn.conf \-v /etc/localtime:/etc/localtime \-v /var/log/npu/:/usr/slog \-v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \-v /reason-sharedata/training_inference/wangxiangbo/modellink/code/Modellink:/job/code \-v /reason-sharedata/training_inference/wangxiangbo/modellink/data:/job/data \-v /reason-sharedata/training_inference/wangxiangbo/modellink/output:/job/output \-v /mnt/weight-1:/job/mnt \registry.paas/cmss/modellink-cann8.0-torch2.1-mindspeed-0.7:v1.0 \/bin/bash
æƒé‡è½¬æ¢
# ä¿®æ”¹ ascend-toolkit è·¯å¾„source /usr/local/Ascend/ascend-toolkit/set_env.sh
python  tools/checkpoint/convert_ckpt.py \    --model-type GPT \    --loader llama2_hf \    --saver megatron \    --load-dir /job/mnt/huggingface/mistral-7b-hf/ \    --save-dir /job/data/megatron/mistral-7b-tp4-pp2 \    --tokenizer-model /job/mnt/huggingface/mistral-7b-hf/tokenizer.model \    --target-tensor-parallel-size 4 \    --target-pipeline-parallel-size 2

æ•°æ®é›†
ä¸‹è½½
sudo wget https://hf-mirror.com/datasets/silk-road/alpaca-data-gpt4-chinese/resolve/main/Alpaca_data_gpt4_zh.jsonl
å¤„ç†
python tools/preprocess_data.py \    --input /job/data/dataset/mistral/Alpaca_data_gpt4_zh.jsonl \    --output-prefix /job/data/dataset/mistral/Alpaca_finetune/ \    --tokenizer-type PretrainedFromHF \    --tokenizer-name-or-path /job/mnt/huggingface/mistral-7b-hf/ \    --append-eod \    --tokenizer-not-use-fast \    --handler-name GeneralInstructionHandler \    --workers 4

å¤„ç†å®Œæˆ

å¾®è°ƒ
å¾®è°ƒè„šæœ¬
#!/bin/bashexport CUDA_DEVICE_MAX_CONNECTIONS=1export PYTORCH_NPU_ALLOC_CONF=expandable_segments:TrueGPUS_PER_NODE=8MASTER_ADDR=localhostMASTER_PORT=6006NNODES=1NODE_RANK=0WORLD_SIZE=$(($GPUS_PER_NODE * $NNODES))DISTRIBUTED_ARGS=&quot;    --nproc_per_node $GPUS_PER_NODE \    --nnodes $NNODES \    --node_rank $NODE_RANK \    --master_addr $MASTER_ADDR \    --master_port $MASTER_PORT&quot;echo &quot;NODE_RANK $&#123;NODE_RANK&#125;&quot;DATA_PATH=&quot;/job/data/dataset/mistral/Alpaca_finetune/&quot;TOKENIZER_MODEL=&quot;/job/mnt/huggingface/mistral-7b-hf&quot;CKPT_SAVE_DIR=&quot;/job/output/mistral_ckpt/&quot;CKPT_LOAD_DIR=&quot;/job/data/megatron/mistral-7b-tp4-pp2/&quot;TP=4PP=2NUM_LAYERS=32GPT_ARGS=&quot;    --tensor-model-parallel-size $&#123;TP&#125; \    --pipeline-model-parallel-size $&#123;PP&#125; \    --sequence-parallel \    --sliding-window 4096 \    --num-layers $&#123;NUM_LAYERS&#125; \    --hidden-size 4096 \    --ffn-hidden-size 14336 \    --num-attention-heads 32 \    --group-query-attention \    --num-query-groups 8 \    --tokenizer-type PretrainedFromHF \    --tokenizer-name-or-path $&#123;TOKENIZER_MODEL&#125; \    --seq-length 32768 \    --max-position-embeddings 32768 \    --micro-batch-size 1 \    --global-batch-size 32 \    --make-vocab-size-divisible-by 1 \    --lr 1.25e-6 \    --train-iters 1000 \    --lr-decay-style cosine \    --untie-embeddings-and-output-weights \    --disable-bias-linear \    --attention-dropout 0.0 \    --init-method-std 0.01 \    --hidden-dropout 0.0 \    --position-embedding-type rope \    --normalization RMSNorm \    --use-fused-rmsnorm \    --use-fused-swiglu \    --use-rotary-position-embeddings \    --use-fused-rotary-pos-emb \    --use-mc2 \    --swiglu \    --use-flash-attn \    --no-masked-softmax-fusion \    --attention-softmax-in-fp32 \    --min-lr 1.25e-7 \    --weight-decay 1e-1 \    --lr-warmup-fraction 0.01 \    --clip-grad 1.0 \    --adam-beta1 0.9 \    --initial-loss-scale 65536 \    --adam-beta2 0.95 \    --no-gradient-accumulation-fusion \    --no-load-optim \    --no-load-rng \    --use-distributed-optimizer \    --overlap-grad-reduce \    --load $&#123;CKPT_LOAD_DIR&#125; \    --save $&#123;CKPT_SAVE_DIR&#125; \    --bf16 \    --finetune \    --is-instruction-dataset \    --log-throughput \    --recompute-granularity full \    --recompute-method block \    --recompute-num-layers 32&quot;DATA_ARGS=&quot;    --data-path $DATA_PATH  \    --split 100,0,0 \&quot;OUTPUT_ARGS=&quot;    --log-interval 1 \    --save-interval 1000 \    --eval-interval 1000 \    --eval-iters 0 \&quot;torchrun $DISTRIBUTED_ARGS ../pretrain_gpt.py \  $GPT_ARGS \  $DATA_ARGS \  $OUTPUT_ARGS \  --distributed-backend nccl \  | tee /job/output/logs/train_mistral_7B.log
æ‹‰èµ·è®­ç»ƒ

åŸºäºdockerçš„llama2-7Bå¾®è°ƒ
å®¹å™¨æŒ‚è½½
sudo docker run -dit --ipc=host --net=host \--name=modellink_wxb \--device=/dev/davinci0 \--device=/dev/davinci1 \--device=/dev/davinci2 \--device=/dev/davinci3 \--device=/dev/davinci4 \--device=/dev/davinci5 \--device=/dev/davinci6 \--device=/dev/davinci7 \--device=/dev/davinci_manager \--device=/dev/devmm_svm \--device=/dev/hisi_hdc \-v /etc/ascend_install.info:/etc/ascend_install.info \-v /etc/hccn.conf:/etc/hccn.conf \-v /etc/localtime:/etc/localtime \-v /var/log/npu/:/usr/slog \-v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \-v /reason-sharedata/training_inference/wangxiangbo/modellink/code/Modellink:/job/code \-v /reason-sharedata/training_inference/wangxiangbo/modellink/data:/job/data \-v /reason-sharedata/training_inference/wangxiangbo/modellink/output:/job/output \-v /mnt/weight-1:/job/mnt \registry.paas/cmss/modellink-cann8.0-torch2.1-mindspeed-0.7:v1.0 \/bin/bash
æƒé‡è½¬æ¢
# ä¿®æ”¹ ascend-toolkit è·¯å¾„source /usr/local/Ascend/ascend-toolkit/set_env.sh
python  tools/checkpoint/convert_ckpt.py \       --model-type GPT \       --loader llama2_hf \       --saver megatron \       --target-tensor-parallel-size 8 \       --target-pipeline-parallel-size 1 \       --load-dir /job/mnt/huggingface/Llama-2-7b-hf// \       --save-dir /job/data/megatron/llama-2-7b-hf-v0.1-tp8-pp1/ \       --tokenizer-model /job/mnt/huggingface/Llama-2-7b-hf/tokenizer.json

æ•°æ®é›†
python tools/preprocess_data.py \    --input /job/data/dataset/mistral/Alpaca_data_gpt4_zh.jsonl \    --output-prefix /job/data/dataset/llama2/Alpaca_finetune/ \    --tokenizer-type PretrainedFromHF \    --tokenizer-name-or-path /job/mnt/huggingface/Llama-2-7b-hf/ \    --append-eod \    --tokenizer-not-use-fast \    --handler-name GeneralInstructionHandler \    --workers 4


å¾®è°ƒ
å¾®è°ƒè„šæœ¬
#!/bin/bashexport CUDA_DEVICE_MAX_CONNECTIONS=1export PYTORCH_NPU_ALLOC_CONF=expandable_segments:TrueGPUS_PER_NODE=8MASTER_ADDR=localhostMASTER_PORT=6006NNODES=1NODE_RANK=0WORLD_SIZE=$(($GPUS_PER_NODE * $NNODES))DISTRIBUTED_ARGS=&quot;    --nproc_per_node $GPUS_PER_NODE \    --nnodes $NNODES \    --node_rank $NODE_RANK \    --master_addr $MASTER_ADDR \    --master_port $MASTER_PORT&quot;echo &quot;NODE_RANK $&#123;NODE_RANK&#125;&quot;DATA_PATH=&quot;/job/data/dataset/llama2/Alpaca_finetune&quot;TOKENIZER_MODEL=&quot;/job/mnt/huggingface/Llama-2-7b-hf/&quot;CKPT_SAVE_DIR=&quot;/job/output/llama2_ckpt/&quot;CKPT_LOAD_DIR=&quot;/job/data/megatron/llama-2-7b-hf-v0.1-tp8-pp1&quot;TP=8PP=1NUM_LAYERS=32GPT_ARGS=&quot;    --tensor-model-parallel-size $&#123;TP&#125; \    --pipeline-model-parallel-size $&#123;PP&#125; \    --sequence-parallel \    --num-layers 32 \    --hidden-size 4096 \    --ffn-hidden-size 11008 \    --num-attention-heads 32 \    --tokenizer-type Llama2Tokenizer \    --tokenizer-model $&#123;TOKENIZER_MODEL&#125; \    --seq-length 4096 \    --max-position-embeddings 4096 \    --micro-batch-size 1 \    --global-batch-size 256 \    --make-vocab-size-divisible-by 1 \    --lr 1.25e-6 \    --train-iters 5000 \    --lr-decay-style cosine \    --untie-embeddings-and-output-weights \    --disable-bias-linear \    --attention-dropout 0.0 \    --init-method-std 0.01 \    --hidden-dropout 0.0 \    --position-embedding-type rope \    --normalization RMSNorm \    --use-fused-rmsnorm \    --swiglu \    --use-flash-attn \    --no-masked-softmax-fusion \    --attention-softmax-in-fp32 \    --min-lr 1.25e-7 \    --weight-decay 1e-1 \    --lr-warmup-fraction 0.01 \    --clip-grad 1.0 \    --adam-beta1 0.9 \    --initial-loss-scale 65536 \    --adam-beta2 0.95 \    --no-gradient-accumulation-fusion \    --no-load-optim \    --no-load-rng \    --use-distributed-optimizer \    --use-fused-swiglu \    --use-fused-rotary-pos-emb \    --overlap-grad-reduce \    --bf16&quot;DATA_ARGS=&quot;    --data-path $DATA_PATH  \    --split 100,0,0 \&quot;OUTPUT_ARGS=&quot;    --log-interval 1 \    --save-interval 1000 \    --eval-interval 1000 \    --eval-iters 0 \&quot;torchrun $DISTRIBUTED_ARGS ../pretrain_gpt.py \  $GPT_ARGS \  $DATA_ARGS \  $OUTPUT_ARGS \  --distributed-backend nccl \  | tee /job/output/logs/train_llama2_7B.log
æ‹‰èµ·è®­ç»ƒ

]]></content>
      <categories>
        <category>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</category>
        <category>æ˜‡è…¾</category>
      </categories>
      <tags>
        <tag>æ˜‡è…¾</tag>
        <tag>910B</tag>
        <tag>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</tag>
        <tag>Mindformers</tag>
      </tags>
  </entry>
  <entry>
    <title>2.2 Deepseek-7Bé¢„è®­ç»ƒé€‚é…</title>
    <url>/2025/11/13/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.2%20Deepseek-7B%E9%A2%84%E8%AE%AD%E7%BB%83%E9%80%82%E9%85%8D/</url>
    <content><![CDATA[é¢„è®­ç»ƒç‰©æ–™åŠä»£ç å‡†å¤‡
Deepseek-7Bçš„Tokenizerä¸‹è½½
ä½ç½®ä½äº/data/code/temp/deepseek/deepseek-ckpt

æ•°æ®é›†å‡†å¤‡
ä½ç½®ä½äº/data/code/temp/deepseek/deepseek-datasets

pretrain_mcore_deepseek.shé¢„è®­ç»ƒè„šæœ¬
export NCCL_P2P_DISABLE=1export NCCL_DEBUG=INFOexport NCCL_SOCKET_IFNAME=eth0export GLOO_SOCKET_IFNAME=eth0export CUDA_DEVICE_MAX_CONNECTIONS=1export CUDA_VISIBLE_DEVICES=0,1,2,3MEGATRON_PATCH_PATH=/data/code MEGATRON_PATH=$&#123;MEGATRON_PATCH_PATH&#125;/Megatron-LMexport PYTHONPATH=$&#123;MEGATRON_PATH&#125;:$&#123;MEGATRON_PATCH_PATH&#125;:$PYTHONPATHNNODES=2NODE_RANK=0GPUS_PER_NODE=4MASTER_ADDR=192.168.0.22MASTER_PORT=29500DISTRIBUTED_ARGS=&quot;--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT&quot;MODEL_SIZE=7BBATCH_SIZE=1GLOBAL_BATCH_SIZE=8LR=1e-5MIN_LR=1e-6SEQ_LEN=1024PAD_LEN=1024EXTRA_VOCAB_SIZE=0PR=fp16TP=4PP=1AC=selDO=trueFL=falseSP=trueTE=trueMOE=falseSAVE_INTERVAL=1000DATASET_PATH=/data/code/temp/deepseek/deepseek-datasets/mmap_deepseek_datasets_text_document#PRETRAIN_CHECKPOINT_PATH=noneTOKENIZER_PATH=/data/code/temp/deepseek/deepseek-ckptTRAIN_TOKENS=10000000WARMUP_TOKENS=0OUTPUT_BASEPATH=/data/code/temp/output/deepseek/pretrain_outputif [ $MODEL_SIZE = 7B ]; thenNUM_LAYERS=32HIDDEN_SIZE=4096NUM_ATTN_HEADS=32INTERMEDIATE_SIZE=11008MAX_POSITION_EMBEDDINGS=4096gqa_options=&quot;&quot;fiif [ $AC = full ]; then    activation_checkpoint_options=&quot; \		    --recompute-method uniform \		    --recompute-granularity full&quot;elif [ $AC = sel ]; then    activation_checkpoint_options=&quot; \        --recompute-activations&quot;elif [ $AC = none ]; then    activation_checkpoint_options=&quot; \    &quot;fiif [ $PR = fp16 ]; then    pr_options=&quot; \		    --fp16 \            --apply-query-key-layer-scaling&quot;    export NVTE_APPLY_QK_LAYER_SCALING=1elif [ $PR = bf16 ]; then    pr_options=&quot; \        --bf16&quot;elif [ $PR = fp8 ]; then    pr_options=&quot; \        --bf16 \        --fp8-hybrid \        --fp8-amax-compute-algo max \        --fp8-amax-history-len 1024 \        --transformer-impl transformer_engine&quot;fiif [ $DO = true ]; then    do_options=&quot; \		    --use-distributed-optimizer&quot;elif [ $DO = false ]; then    do_options=&quot; \                    &quot;fiif [ $FL = true ]; then    flash_options=&quot; \		    --use-flash-attn&quot;elif [ $FL = false ]; then    flash_options=&quot; \                    &quot;fiif [ $TE = true ]; then    te_options=&quot; \		    --transformer-impl transformer_engine&quot;elif [ $TE = false ]; then    te_options=&quot; \                    &quot;fiif [ $MOE = true ]; then    moe_options=&quot; \		    --moe-router-topk 1 \		    --num-experts 4 \		    --expert-model-parallel-size 2&quot;elif [ $MOE = false ]; then    moe_options=&quot; \                    &quot;fiif [ $SP = true ] &amp;&amp; [ $TP -gt 1 ]; then    sp_options=&quot; \		    --sequence-parallel&quot;elif [ $SP = false ]; then    sp_options=&quot; \                    &quot;fiif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then    load_options=&quot; \            --load $PRETRAIN_CHECKPOINT_PATH&quot;fiTRAIN_ITERS=$(( $&#123;TRAIN_TOKENS&#125; / $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))LR_WARMUP_ITERS=$(( $&#123;WARMUP_TOKENS&#125;  / $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))LR_DECAY_ITERS=$(( $&#123;TRAIN_TOKENS&#125; /  $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))NAME=&quot;$&#123;ENV&#125;-pretrain-mcore-deepseek-$&#123;MODEL_SIZE&#125;-lr-$&#123;LR&#125;-bs-$&#123;BATCH_SIZE&#125;-seqlen-$&#123;SEQ_LEN&#125;-pr-$&#123;PR&#125;-tp-$&#123;TP&#125;-pp-$&#123;PP&#125;-ac-$&#123;AC&#125;-do-$&#123;DO&#125;-sp-$&#123;SP&#125;-moe-$&#123;MOE&#125;-tt-$&#123;TRAIN_TOKENS&#125;-wt-$&#123;WARMUP_TOKENS&#125;&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/log/&quot;current_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)TENSORBOARD_DIR=&quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/$&#123;NAME&#125;_$&#123;current_time&#125;&quot;mkdir -p $&#123;TENSORBOARD_DIR&#125;SAVED_PRETRAIN_CHECKPOINT_PATH=&quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/$&#123;NAME&#125;&quot;megatron_options=&quot;  \        --tokenizer-model $TOKENIZER_PATH \        --save $&#123;SAVED_PRETRAIN_CHECKPOINT_PATH&#125; \        --data-path $&#123;DATASET_PATH&#125; \        --lr $&#123;LR&#125; \        --min-lr $&#123;MIN_LR&#125; \        --lr-decay-style linear \        --adam-beta1 0.9 \        --adam-beta2 0.95 \        --weight-decay 0.1 \        --clip-grad 1.0 \        --init-method-std 0.006 \        --attention-dropout 0.0 \        --hidden-dropout 0.0  \        --lr-decay-iters $&#123;LR_DECAY_ITERS&#125; \        --lr-warmup-iters $&#123;LR_WARMUP_ITERS&#125; \        --train-iters $&#123;TRAIN_ITERS&#125; \        --split 99,1,0 \        --micro-batch-size $&#123;BATCH_SIZE&#125; \        --global-batch-size $&#123;GLOBAL_BATCH_SIZE&#125; \        --num-layers $&#123;NUM_LAYERS&#125; \        --hidden-size $&#123;HIDDEN_SIZE&#125; \        --num-attention-heads $&#123;NUM_ATTN_HEADS&#125; \        --ffn-hidden-size $&#123;INTERMEDIATE_SIZE&#125; \        --seq-length $&#123;SEQ_LEN&#125; \        --max-position-embeddings $&#123;MAX_POSITION_EMBEDDINGS&#125; \        --max-padding-length $&#123;PAD_LEN&#125; \        --log-throughput \        --log-interval 1 \        --eval-interval 10000 \        --eval-iters 10 \        --save-interval $&#123;SAVE_INTERVAL&#125; \        --tensorboard-queue-size 1 \        --tensorboard-dir $&#123;TENSORBOARD_DIR&#125; \        --log-timers-to-tensorboard \        --log-batch-size-to-tensorboard \        --log-validation-ppl-to-tensorboard \        --tensor-model-parallel-size $&#123;TP&#125; \        --pipeline-model-parallel-size $&#123;PP&#125; \        --no-load-optim \        --no-load-rng \        --num-workers 8 \        --seed 1234 \        --extra-vocab-size $&#123;EXTRA_VOCAB_SIZE&#125; \        --patch-tokenizer-type DeepseekTokenizer \        --dataset LLama-Pretrain-Idxmap \        --swiglu \        --normalization RMSNorm \        --norm-epsilon 1e-05 \        --use-rotary-position-embeddings \        --no-rope-fusion \        --position-embedding-type rope \        --untie-embeddings-and-output-weights \        --disable-bias-linear \        --rotary-base 10000 \        --use-mcore-models         &quot;LOG_SAVE=&quot;/data/code/temp/output/deepseek/pretrain_output/log/&quot;$&#123;current_time&#125;-deepseek-$&#123;NODE_RANK&#125;.log&quot;&quot; run_cmd=&quot;torchrun $DISTRIBUTED_ARGS pretrain_mcore_deepseek.py $&#123;megatron_options&#125; $&#123;pr_options&#125; $&#123;load_options&#125; $&#123;te_options&#125; $&#123;activation_checkpoint_options&#125; $&#123;do_options&#125; $&#123;flash_options&#125; $&#123;sp_options&#125; $&#123;gqa_options&#125; $&#123;moe_options&#125; &gt; $&#123;LOG_SAVE&#125; 2&gt;&amp;1&quot;echo $&#123;run_cmd&#125;eval $&#123;run_cmd&#125;set +x
pretrain_mcore_deepseek.pyè®­ç»ƒä»£ç 
# Copyright (c) 2023 Alibaba PAI and Nvidia Megatron-LM Team.## Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);# you may not use this file except in compliance with the License.# You may obtain a copy of the License at##     http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.import osimport torchfrom torch import Tensorfrom functools import partialfrom typing import Unionfrom megatron import get_argsfrom megatron import get_timersfrom megatron.core import mpu, tensor_parallelfrom megatron.core.enums import ModelTypeimport megatron.modelfrom megatron.utils import (    get_batch_on_this_tp_rank,    get_batch_on_this_cp_rank,    average_losses_across_data_parallel_group)from megatron.core.datasets.blended_megatron_dataset_builder import BlendedMegatronDatasetBuilderfrom megatron.training import pretrainfrom megatron.core.datasets.gpt_dataset import GPTDatasetConfigfrom megatron.core.datasets.gpt_dataset import GPTDataset# from megatron.core.models.gpt import GPTModelfrom megatron.core.transformer.spec_utils import import_modulefrom megatron.arguments import core_transformer_config_from_argsfrom megatron_patch.model.deepseek_core.model import GPTModelfrom megatron_patch.data import build_pretrain_dataset_from_originalfrom megatron_patch.data.utils import get_batch_on_this_tp_rank_originalfrom megatron_patch.tokenizer import get_tokenizer, build_tokenizerfrom megatron_patch.arguments import get_patch_argsfrom megatron_patch.model.deepseek_core.transformer_config import DeepseekTransformerConfigfrom megatron_patch.model.deepseek_core.layer_specs import get_gpt_layer_with_transformer_engine_specimport torch._dynamotorch._dynamo.config.suppress_errors = Truedef model_provider(    pre_process=True, post_process=True) -&gt; Union[GPTModel, megatron.model.GPTModel]:    &quot;&quot;&quot;Builds the model.    If you set the use_mcore_models to True, it will return the mcore GPT model and if not the legacy GPT model.    Args:        pre_process (bool, optional): Set to true if you need to compute embedings. Defaults to True.        post_process (bool, optional): Set to true if you need to want to compute output logits/loss. Defaults to True.    Returns:        Union[GPTModel, megatron.model.GPTModel]: The returned model    &quot;&quot;&quot;    args = get_args()    build_tokenizer(args)    config = core_transformer_config_from_args(args, DeepseekTransformerConfig)    #config = core_transformer_config_from_args(get_args())    if args.use_mcore_models:        if args.spec is not None:            transformer_layer_spec = import_module(args.spec)        else:            transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.num_experts, args.moe_grouped_gemm)        model = GPTModel(            config=config,            transformer_layer_spec=transformer_layer_spec,            vocab_size=args.padded_vocab_size,            max_sequence_length=args.max_position_embeddings,            pre_process=pre_process,            post_process=post_process,            fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,            parallel_output=True,            share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights,            position_embedding_type=args.position_embedding_type,            rotary_percent=args.rotary_percent,            rotary_base=args.rotary_base,        )    return modeldef get_batch(data_iterator):    &quot;&quot;&quot;Generate a batch.&quot;&quot;&quot;    # TODO: this is pretty hacky, find a better way    if (not mpu.is_pipeline_first_stage()) and (not mpu.is_pipeline_last_stage()):        return None, None, None, None, None    args = get_args()    if &quot;-Raw&quot; in args.dataset:        # get batches based on the TP rank you are on        batch = get_batch_on_this_tp_rank_original(data_iterator)        # slice batch along sequence dimension for context parallelism        batch = get_batch_on_this_cp_rank(batch)    elif &quot;-Idxmap&quot; in args.dataset:        # get batches based on the TP rank you are on        batch = get_batch_on_this_tp_rank(data_iterator)        # slice batch along sequence dimension for context parallelism        batch = get_batch_on_this_cp_rank(batch)    else:        raise ValueError(&quot;please set correct --dataset &quot;)    return batch.values()def loss_func(loss_mask: Tensor, output_tensor: Tensor):    &quot;&quot;&quot;Loss function.    Args:        loss_mask (Tensor): Used to mask out some portions of the loss        output_tensor (Tensor): The tensor with the losses    &quot;&quot;&quot;    args = get_args()    losses = output_tensor.float()    loss_mask = loss_mask.view(-1).float()    if args.context_parallel_size &gt; 1:        loss = torch.cat([torch.sum(losses.view(-1) * loss_mask).view(1), loss_mask.sum().view(1)])        torch.distributed.all_reduce(loss, group=mpu.get_context_parallel_group())        loss = loss[0] / loss[1]    else:        loss = torch.sum(losses.view(-1) * loss_mask) / loss_mask.sum()    # Check individual rank losses are not NaN prior to DP all-reduce.    if args.check_for_nan_in_loss_and_grad:        global_rank = torch.distributed.get_rank()        assert not loss.isnan(), (            f&#x27;Rank &#123;global_rank&#125;: found NaN in local forward loss calculation. &#x27;            f&#x27;Device: &#123;torch.cuda.current_device()&#125;, node: &#123;os.uname()[1]&#125;&#x27;        )    # Reduce loss for logging.    averaged_loss = average_losses_across_data_parallel_group([loss])    return loss * args.context_parallel_size, &#123;&#x27;lm loss&#x27;: averaged_loss[0]&#125;def forward_step(data_iterator, model):    &quot;&quot;&quot;Forward step.&quot;&quot;&quot;    timers = get_timers()    # Get the batch.    timers(&#x27;batch-generator&#x27;, log_level=2).start()    tokens, labels, loss_mask, attention_mask, position_ids = get_batch(        data_iterator)    timers(&#x27;batch-generator&#x27;).stop()    output_tensor = model(tokens, position_ids, attention_mask,                          labels=labels)    return output_tensor, partial(loss_func, loss_mask)def is_dataset_built_on_rank():    return (mpu.is_pipeline_first_stage() or mpu.is_pipeline_last_stage()) and mpu.get_tensor_model_parallel_rank() == 0def core_gpt_dataset_config_from_args(args):    tokenizer = get_tokenizer()    return GPTDatasetConfig(        is_built_on_rank=is_dataset_built_on_rank,        random_seed=args.seed,        sequence_length=args.seq_length,        blend=args.data_path,        split=args.split,        path_to_cache=args.data_cache_path,        reset_attention_mask=args.reset_attention_mask,        eod_mask_loss=args.eod_mask_loss,        eod_id=tokenizer.eod    )def train_valid_test_datasets_provider(train_val_test_num_samples):    &quot;&quot;&quot;Build train, valid, and test datasets.&quot;&quot;&quot;    args = get_args()    if &quot;-Raw&quot; in args.dataset:                train_ds, valid_ds, test_ds = \                                    build_pretrain_dataset_from_original(args.dataset)    else:        train_ds, valid_ds, test_ds = BlendedMegatronDatasetBuilder(            GPTDataset,            train_val_test_num_samples,            core_gpt_dataset_config_from_args(args)        ).build()    return train_ds, valid_ds, test_dsif __name__ == &quot;__main__&quot;:    train_valid_test_datasets_provider.is_distributed = True    pretrain(        train_valid_test_datasets_provider,        model_provider,        ModelType.encoder_or_decoder,        forward_step,        extra_args_provider=get_patch_args,    )
åœ¨/data/code/megatron_patch/modelåœ¨åŠ å…¥deepseek_coreçš„ç›¸å…³é…ç½®

DeepseekTokenizeré…ç½®
åœ¨/data/code/megatron_patch/tokenizer/init.pyä¸‹åŠ å…¥DeepseekTokenizeré…ç½®
elif args.patch_tokenizer_type == &#x27;DeepseekTokenizer&#x27;:        from megatron.core.datasets.megatron_tokenizer import MegatronTokenizer        class _DeepseekTokenizer(MegatronTokenizer):            def __init__(self, tokenizer_path, extra_vocab_size):                super().__init__(tokenizer_path)                self.tokenizer = AutoTokenizer.from_pretrained(                    tokenizer_path,                    padding_side=&quot;right&quot;,                    use_fast=False,                    trust_remote_code=True,                )                self.extra_vocab_size = extra_vocab_size            def __call__(self, text, return_tensors=None,                         padding=None, max_length=None, truncation=None, add_special_tokens=True):                return self.tokenizer(text, return_tensors=return_tensors, padding=padding,                        max_length=max_length, truncation=truncation, add_special_tokens=add_special_tokens)            @property            def vocab_size(self):                return self.tokenizer.vocab_size + self.extra_vocab_size            @property            def vocab(self):                return self.tokenizer.encoder            @property            def inv_vocab(self):                return self.tokenizer.decoder            def tokenize(self, text):                return self.tokenizer.encode(text)            def detokenize(self, token_ids):                return self.tokenizer.decode(token_ids)            @property            def eod(self):                return self.tokenizer.eos_token_id            @property            def eos_token(self):                return self.tokenizer.eos_token            @property            def pad_token_id(self):                return self.tokenizer.pad_token_id            @property            def eos_token_id(self):                return self.tokenizer.eos_token_id        tokenizer = _DeepseekTokenizer(args.tokenizer_model, args.extra_vocab_size)        args.padded_vocab_size = tokenizer.vocab_size
å¯åŠ¨é¢„è®­ç»ƒk8sçš„yaml
apiVersion: apps/v1kind: DaemonSetmetadata:  name: deepseek-core  namespace: deepseekspec:  selector:    matchLabels:      app: deepseek-core  template:    metadata:      labels:        app: deepseek-core    spec:      hostNetwork: true      nodeSelector:        deepseek: deepseek-7B        containers:      - name: deepseek        image: registry.paas/cmss/nemo:24.05_v1.1        imagePullPolicy: IfNotPresent        resources:         limits:           nvidia.com/gpu: 4         requests:           nvidia.com/gpu: 4        command:              - &quot;/bin/bash&quot;              - &quot;-c&quot;              - |                cd /data/code/temp/conf/deepseek_core/pretrain &amp;&amp;                bash setRank.sh &amp;&amp;                cd /data/ &amp;&amp;                bash pretrain_mcore_deepseek.sh        securityContext:          privileged: true        volumeMounts:        - mountPath: /data/code          name: code           - mountPath: /etc/localtime          name: localtime        - mountPath: /dev/shm          name: dshm      volumes:      - name: code        hostPath:          path: /mnt/users/lihai/distribute/core/megatron-core      - name: localtime        hostPath:          path: /etc/localtime      - name: dshm        emptyDir:          medium: Memory          sizeLimit: 20G
setRank.sh
#!/bin/bashorigin_shell=&quot;/data/code/examples/deepseek_core/pretrain_mcore_deepseek.sh&quot;conf_dir=&quot;/data/code/temp/conf/&quot;local_dir=&quot;/data/&quot;shell_name=pretrain_mcore_deepseek.sh## å¤åˆ¶è„šæœ¬åˆ°/workspaceä¸‹cp $origin_shell $local_dir## è¯»å–hostfilereadarray -t ips &lt; &lt;(grep -vE &#x27;^[[:space:]]*$&#x27; &quot;$conf_dir&quot;hostfile-67)## è·å–rank0 IPrank0_ip=$(echo &quot;$&#123;ips[0]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)nodes=$&#123;#ips[@]&#125;## è·å–hostfileä¸­é…ç½®çš„IPå‰ç¼€## ä½¿ç”¨cutæå–IPåœ°å€çš„å‰ä¸‰ä¸ªæ•°å­—éƒ¨åˆ†ip_prefix=$(echo &quot;$&#123;ips[0]&#125;&quot; | cut -d &#x27;.&#x27; -f 1-3)## è·å–æœ¬æœºIPip=$(hostname -I | grep -oE &quot;$ip_prefix\.[0-9]+&quot;)# åˆå§‹åŒ–ranknode_rank=-1# éå†æ•°ç»„for i in &quot;$&#123;!ips[@]&#125;&quot;; do    # ä½¿ç”¨trå‘½ä»¤å»é™¤ç©ºç™½å­—ç¬¦    clean_string=$(echo &quot;$&#123;ips[$i]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)    if [[ &quot;$clean_string&quot; == &quot;$ip&quot; ]]; then        node_rank=$i        break    fidoneif [ $node_rank -ne -1 ]; then    ## ä¿®æ”¹è„šæœ¬ä¸­MASTER_ADDR    sed -i &quot;s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/&quot; $local_dir$shell_name    ## ä¿®æ”¹NNODES    sed -i &quot;s/^NNODES=.*/NNODES=$nodes/&quot; $local_dir$shell_name    ## ä¿®æ”¹NODE_RANK    sed -i &quot;s/^NODE_RANK=.*/NODE_RANK=$node_rank/&quot; $local_dir$shell_namefi
å¤šæœºå¤šå¡æ‹‰èµ·æµ‹è¯•




æ–­ç‚¹ç»­è®­æµ‹è¯•
lossæ›²çº¿å¯¹æ¯”å®éªŒ
patchæ¡†æ¶è®­ç»ƒä»£ç å‡†å¤‡
patchçš„é¢„è®­ç»ƒyaml
apiVersion: apps/v1kind: DaemonSetmetadata:  name: deepseek-core  namespace: deepseekspec:  selector:    matchLabels:      app: deepseek-core  template:    metadata:      labels:        app: deepseek-core    spec:      hostNetwork: true      nodeSelector:        deepseek: deepseek-7B      containers:        - name: deepseek          image: registry.paas/library/megatron-lm:v1          imagePullPolicy: IfNotPresent          resources:            limits:              nvidia.com/gpu: 4            requests:              nvidia.com/gpu: 4          command:            - &quot;/bin/bash&quot;            - &quot;-c&quot;            - |              cd /data/code/examples/deepseek/conf &amp;&amp;               bash setRank-patch.sh &amp;&amp;              cd /data/ &amp;&amp;              bash pretrain_patch_deepseek.sh          securityContext:            privileged: true          volumeMounts:            - mountPath: /data/code              name: code            - mountPath: /etc/localtime              name: localtime            - mountPath: /dev/shm              name: dshm      volumes:        - name: code          hostPath:            path: /mnt/users/wangxiangbo/Pai-Megatron-Patch        - name: localtime          hostPath:            path: /etc/localtime        - name: dshm          emptyDir:            medium: Memory            sizeLimit: 20G
patchçš„é¢„è®­ç»ƒè„šæœ¬
#!/bin/bash#sh run_pretrain_megatron_deepseek.sh dsw /workspace/Pai-Megatron-Patch 7B 1 8 1e-5 1e-6 2048 2048 0 bf16 1 1 sel true true true false 100000 /mnt/deepseek-datasets/wudao_llamabpe_text_document /mnt/deepseek-ckpts/Llama-2-7b-hf-to-mg-tp1-pp1/ 10000000000 100000000 /mnt/output_patch_testexport NCCL_P2P_DISABLE=1#export NCCL_DEBUG=INFOexport NCCL_SOCKET_IFNAME=eth0export GLOO_SOCKET_IFNAME=eth0export CUDA_DEVICE_MAX_CONNECTIONS=1export CUDA_VISIBLE_DEVICES=0,1,2,3MEGATRON_PATCH_PATH=/data/code MEGATRON_PATH=$&#123;MEGATRON_PATCH_PATH&#125;/Megatron-LM-231007export PYTHONPATH=$&#123;MEGATRON_PATH&#125;:$&#123;MEGATRON_PATCH_PATH&#125;:$PYTHONPATHNNODES=2NODE_RANK=0GPUS_PER_NODE=4MASTER_ADDR=192.168.0.22MASTER_PORT=8889DISTRIBUTED_ARGS=&quot;--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT&quot;MODEL_SIZE=7BBATCH_SIZE=1GLOBAL_BATCH_SIZE=8LR=1e-5MIN_LR=1e-6SEQ_LEN=1024PAD_LEN=1024EXTRA_VOCAB_SIZE=0PR=fp16TP=4PP=1AC=selDO=trueFL=falseSP=trueTE=falseSAVE_INTERVAL=1000DATASET_PATH=/data/code/examples/deepseek/dataset/wudao_deepseekbpe_text_documentPRETRAIN_CHECKPOINT_PATH=/data/code/examples/deepseek/ckptTRAIN_TOKENS=10000000WARMUP_TOKENS=0OUTPUT_BASEPATH=/data/code/examples/deepseek/output/pretrain_outputif [ $MODEL_SIZE = 7B ]; thenNUM_LAYERS=30HIDDEN_SIZE=4096NUM_ATTN_HEADS=32INTERMEDIATE_SIZE=11008gqa_options=&quot;&quot;fiif [ $AC = full ]; then    activation_checkpoint_options=&quot; \		    --recompute-method uniform \		    --recompute-granularity full&quot;elif [ $AC = sel ]; then    activation_checkpoint_options=&quot; \        --recompute-activations&quot;elif [ $AC = none ]; then    activation_checkpoint_options=&quot; \                    &quot;fiif [ $PR = fp16 ]; then    pr_options=&quot; \		    --fp16&quot;elif [ $PR = bf16 ]; then    pr_options=&quot; \        --bf16&quot;elif [ $PR = fp8 ]; then    pr_options=&quot; \        --bf16        --fp8-hybrid \        --fp8-amax-compute-algo max \        --fp8-amax-history-len 1024 \        --transformer-impl transformer_engine&quot;fiif [ $DO = true ]; then    do_options=&quot; \		    --use-distributed-optimizer&quot;elif [ $DO = false ]; then    do_options=&quot; \                    &quot;fiif [ $FL = true ]; then    flash_options=&quot; \		    --use-flash-attn&quot;elif [ $FL = false ]; then    flash_options=&quot; \                    &quot;fiif [ $TE = true ]; then    te_options=&quot; \		    --transformer-impl transformer_engine&quot;elif [ $TE = false ]; then    te_options=&quot; \                    &quot;fiif [ $SP = true ] &amp;&amp; [ $TP -gt 1 ]; then    sp_options=&quot; \		    --sequence-parallel&quot;elif [ $SP = false ]; then    sp_options=&quot; \                    &quot;fiif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then    load_options=&quot; \            --load $PRETRAIN_CHECKPOINT_PATH&quot;fiTRAIN_ITERS=$(( $&#123;TRAIN_TOKENS&#125; / $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))LR_WARMUP_ITERS=$(( $&#123;WARMUP_TOKENS&#125;  / $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))LR_DECAY_ITERS=$(( $&#123;TRAIN_TOKENS&#125; /  $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))NAME=&quot;$&#123;ENV&#125;-pretrain-patch-deepseek-$&#123;MODEL_SIZE&#125;-lr-$&#123;LR&#125;-bs-$&#123;BATCH_SIZE&#125;-seqlen-$&#123;SEQ_LEN&#125;-pr-$&#123;PR&#125;-tp-$&#123;TP&#125;-pp-$&#123;PP&#125;-ac-$&#123;AC&#125;-do-$&#123;DO&#125;-sp-$&#123;SP&#125;-tt-$&#123;TRAIN_TOKENS&#125;-wt-$&#123;WARMUP_TOKENS&#125;&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/log/&quot;current_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)TENSORBOARD_DIR=&quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/$&#123;NAME&#125;_$&#123;current_time&#125;&quot;mkdir -p $&#123;TENSORBOARD_DIR&#125;SAVED_PRETRAIN_CHECKPOINT_PATH=&quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/$&#123;NAME&#125;&quot;megatron_options=&quot;  \        --save $&#123;SAVED_PRETRAIN_CHECKPOINT_PATH&#125; \        --split 99,1,0 \        --train-data-path $&#123;DATASET_PATH&#125;        --lr $&#123;LR&#125; \        --min-lr $&#123;MIN_LR&#125; \        --lr-decay-style linear \        --adam-beta1 0.9 \        --adam-beta2 0.95 \        --weight-decay 0.1 \        --clip-grad 1.0 \        --init-method-std 0.006 \        --lr-decay-iters $&#123;LR_DECAY_ITERS&#125; \        --lr-warmup-iters $&#123;LR_WARMUP_ITERS&#125; \        --train-iters $&#123;TRAIN_ITERS&#125; \        --micro-batch-size $&#123;BATCH_SIZE&#125; \        --global-batch-size $&#123;GLOBAL_BATCH_SIZE&#125; \        --num-layers $&#123;NUM_LAYERS&#125; \        --hidden-size $&#123;HIDDEN_SIZE&#125; \        --num-attention-heads $&#123;NUM_ATTN_HEADS&#125; \        --ffn-hidden-size $&#123;INTERMEDIATE_SIZE&#125; \        --seq-length $&#123;SEQ_LEN&#125; \        --max-position-embeddings $&#123;SEQ_LEN&#125; \        --log-interval 1 \        --eval-interval 10000 \        --eval-iters 10 \        --save-interval $&#123;SAVE_INTERVAL&#125; \        --tensorboard-queue-size 1 \        --tensorboard-dir $&#123;TENSORBOARD_DIR&#125; \        --log-timers-to-tensorboard \        --log-batch-size-to-tensorboard \        --log-validation-ppl-to-tensorboard \        --tensor-model-parallel-size $&#123;TP&#125; \        --pipeline-model-parallel-size $&#123;PP&#125; \        --dataset LLama-Pretrain-Idxmap \        --no-load-optim \        --no-load-rng \        --num-workers 8 \        --seed 1234 \        --max-padding-length $&#123;PAD_LEN&#125; \        --extra-vocab-size $&#123;EXTRA_VOCAB_SIZE&#125; \        --patch-tokenizer-type LLamaTokenizer \        --swiglu \        --normalization RMSNorm \        --use-llama2-rotary-position-embeddings \        --position-embedding-type rope \        --untie-embeddings-and-output-weights \        --rotary-base 10000 \        --rotary-scale-factor 4 \        --loss-scale 16384 \        --disable-bias-linear        &quot;#--no-query-key-layer-scaling \#/data/code/examples/llama2/pretrain_megatron_llama.pyLOG_SAVE=&quot;/data/code/examples/deepseek/output/pretrain_output/log/&quot;$&#123;current_time&#125;-deepseek-patch-$&#123;NODE_RANK&#125;.log&quot;&quot; run_cmd=&quot;torchrun $DISTRIBUTED_ARGS /data/code/examples/llama2/pretrain_megatron_llama.py $&#123;megatron_options&#125; $&#123;pr_options&#125; $&#123;load_options&#125; $&#123;te_options&#125; $&#123;activation_checkpoint_options&#125; $&#123;do_options&#125; $&#123;flash_options&#125; $&#123;sp_options&#125; $&#123;gqa_options&#125; &gt; $&#123;LOG_SAVE&#125; 2&gt;&amp;1&quot;echo $&#123;run_cmd&#125;eval $&#123;run_cmd&#125;set +x
patchçš„é¢„è®­ç»ƒsetrank
#!/bin/bashorigin_shell=&quot;/data/code/examples/deepseek/pretrain_patch_deepseek.sh&quot;conf_dir=&quot;/data/code/examples/deepseek/conf/&quot;local_dir=&quot;/data/&quot;shell_name=pretrain_patch_deepseek.sh## å¤åˆ¶è„šæœ¬åˆ°/workspaceä¸‹cp $origin_shell $local_dir## è¯»å–hostfilereadarray -t ips &lt; &lt;(grep -vE &#x27;^[[:space:]]*$&#x27; &quot;$conf_dir&quot;hostfile)## è·å–rank0 IPrank0_ip=$(echo &quot;$&#123;ips[0]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)nodes=$&#123;#ips[@]&#125;## è·å–hostfileä¸­é…ç½®çš„IPå‰ç¼€## ä½¿ç”¨cutæå–IPåœ°å€çš„å‰ä¸‰ä¸ªæ•°å­—éƒ¨åˆ†ip_prefix=$(echo &quot;$&#123;ips[0]&#125;&quot; | cut -d &#x27;.&#x27; -f 1-3)## è·å–æœ¬æœºIPip=$(hostname -I | grep -oE &quot;$ip_prefix\.[0-9]+&quot;)# åˆå§‹åŒ–ranknode_rank=-1# éå†æ•°ç»„for i in &quot;$&#123;!ips[@]&#125;&quot;; do    # ä½¿ç”¨trå‘½ä»¤å»é™¤ç©ºç™½å­—ç¬¦    clean_string=$(echo &quot;$&#123;ips[$i]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)    if [[ &quot;$clean_string&quot; == &quot;$ip&quot; ]]; then        node_rank=$i        break    fidoneif [ $node_rank -ne -1 ]; then    ## ä¿®æ”¹è„šæœ¬ä¸­MASTER_ADDR    sed -i &quot;s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/&quot; $local_dir$shell_name    ## ä¿®æ”¹NNODES    sed -i &quot;s/^NNODES=.*/NNODES=$nodes/&quot; $local_dir$shell_name    ## ä¿®æ”¹NODE_RANK    sed -i &quot;s/^NODE_RANK=.*/NODE_RANK=$node_rank/&quot; $local_dir$shell_namefi
patchæ¡†æ¶é¢„è®­ç»ƒæµ‹è¯•




]]></content>
      <categories>
        <category>æ¨¡å‹é€‚é…</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>æ¨¡å‹é€‚é…</tag>
        <tag>Nvidia</tag>
        <tag>Deepseek</tag>
        <tag>Megatron-LM</tag>
      </tags>
  </entry>
  <entry>
    <title>2.3 Deepseek-7Bæƒé‡è½¬æ¢</title>
    <url>/2025/11/13/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.3%20Deepseek-7B%E6%9D%83%E9%87%8D%E8%BD%AC%E6%8D%A2/</url>
    <content><![CDATA[Hg-Coreæƒé‡è½¬æ¢
huggingfaceå¼€æºæƒé‡å‡†å¤‡

æƒé‡è½¬æ¢è„šæœ¬
#!/bin/bashexport CUDA_VISIBLE_DEVICES=0export CUDA_DEVICE_MAX_CONNECTIONS=1START_TIME=$SECONDSMASTER_ADDR=localhostMASTER_PORT=6666MODEL_SIZE=7BHG_CKPT_PATH=/mnt/model/deepseek-ai/deepseek-llm-7b-baseMEGATRON_PATCH_PATH=/data/code/MEGATRON_PATH=$&#123;MEGATRON_PATCH_PATH&#125;/Megatron-LMexport PYTHONPATH=$&#123;MEGATRON_PATH&#125;:$&#123;MEGATRON_PATCH_PATH&#125;:$PYTHONPATHSOURCE_CKPT_PATH=/mnt/model/deepseek-ai/deepseek-llm-7b-baseTARGET_CKPT_PATH=/mnt/model/deepseek-ai/deepseek-llm-7b-coreTP=4PP=1EXTRA_VOCAB_SIZE=2400NUM_EXPERTS=0EXPERTS_TOPK=0EP=0NUM_EXPERT_SPLITS=0mg2hf=falseif [ $MODEL_SIZE = 7B ]; thenNUM_LAYERS=30HIDDEN_SIZE=4096NUM_ATTN_HEADS=32INTERMEDIATE_SIZE=11008NUM_KV_HEADS=32VOCAB_SIZE=102400ROPE_THETA=10000RMS_NORM_EPS=1e-6gqa_options=&quot;&quot;fiif [ $NUM_EXPERT_SPLITS -gt 0 ]; thenINTERMEDIATE_SIZE=$(( $&#123;INTERMEDIATE_SIZE&#125; / $&#123;NUM_EXPERT_SPLITS&#125;))fiif [ $NUM_EXPERTS -gt 0 ]; then    expert_options=&quot;                --moe-router-topk $&#123;EXPERTS_TOPK&#125; \                --num-experts $&#123;NUM_EXPERTS&#125; \                --expert-model-parallel-size 1 \                --target_expert_model_parallel_size $&#123;EP&#125; \                --num_expert_split_size $&#123;NUM_EXPERT_SPLITS&#125; \    &quot;fiif [ $mg2hf = true ]; then    convert_options=&quot;                --convert_checkpoint_from_megatron_to_transformers    &quot;elif [ $mg2hf = false ]; then    convert_options=&quot;&quot;fiDISTRIBUTED_ARGS=&quot;--nproc_per_node 1 --nnodes 1 --node_rank 0 --master_addr $MASTER_ADDR --master_port $MASTER_PORT&quot;if [ $MODEL_SIZE != 70B ]; thentorchrun $&#123;DISTRIBUTED_ARGS&#125; hf2mcore.py \    --load_path $&#123;SOURCE_CKPT_PATH&#125; \    --save_path $&#123;TARGET_CKPT_PATH&#125; \    --tokenizer-model $&#123;HG_CKPT_PATH&#125; \    --huggingface_model_path $&#123;HG_CKPT_PATH&#125; \    --megatron-path $&#123;MEGATRON_PATH&#125; \    --target_tensor_model_parallel_size $&#123;TP&#125; \    --target_pipeline_model_parallel_size $&#123;PP&#125; \    --micro-batch-size 1 \    --fp16 \    --swiglu \    --num-layers $&#123;NUM_LAYERS&#125; \    --hidden-size $&#123;HIDDEN_SIZE&#125; \    --ffn-hidden-size $&#123;INTERMEDIATE_SIZE&#125; \    --norm-epsilon $&#123;RMS_NORM_EPS&#125;\    --num-attention-heads $&#123;NUM_ATTN_HEADS&#125; \    --max-position-embeddings 4096 \    --seq-length 1 \    --no-async-tensor-model-parallel-allreduce \    --patch-tokenizer-type DeepseekTokenizer \    --extra-vocab-size $&#123;EXTRA_VOCAB_SIZE&#125; \    --untie-embeddings-and-output-weights \    --no-rope-fusion \    --use-rotary-position-embeddings \    --rotary-base $&#123;ROPE_THETA&#125; \    --transformer-impl transformer_engine \    --disable-bias-linear \    --normalization RMSNorm \    --use-mcore-models \    --attention-dropout 0.0 \    --hidden-dropout 0.0 \    $&#123;expert_options&#125; \    $&#123;convert_options&#125; \    $&#123;gqa_options&#125;elsepython hf2mcore_70b.py \  --load $&#123;HG_CKPT_PATH&#125; \  --megatron-path $&#123;MEGATRON_PATH&#125; \  --load_path $&#123;SOURCE_CKPT_PATH&#125; \  --save_path $&#123;TARGET_CKPT_PATH&#125; \  --target_params_dtype bf16 \  --target_tensor_model_parallel_size $&#123;TP&#125; \  --target_pipeline_model_parallel_size $&#123;PP&#125; \$&#123;convert_options&#125; \fiELAPSED_TIME=$(($SECONDS - $START_TIME))echo &quot;$(($ELAPSED_TIME/60)) min $(($ELAPSED_TIME%60)) sec&quot;
tokenizeré…ç½®
åœ¨/data/code/megatron_patch/tokenizer/init.pyä¸­åŠ å…¥Deepseeké…ç½®
elif args.patch_tokenizer_type == &#x27;DeepseekTokenizer&#x27;:        from megatron.core.datasets.megatron_tokenizer import MegatronTokenizer        class _DeepseekTokenizer(MegatronTokenizer):            def __init__(self, tokenizer_path, extra_vocab_size):                super().__init__(tokenizer_path)                self.tokenizer = AutoTokenizer.from_pretrained(                    tokenizer_path,                    padding_side=&quot;right&quot;,                    use_fast=False,                    trust_remote_code=True,                )                self.extra_vocab_size = extra_vocab_size            def __call__(self, text, return_tensors=None,                         padding=None, max_length=None, truncation=None, add_special_tokens=True):                return self.tokenizer(text, return_tensors=return_tensors, padding=padding,                        max_length=max_length, truncation=truncation, add_special_tokens=add_special_tokens)            @property            def vocab_size(self):                return self.tokenizer.vocab_size + self.extra_vocab_size            @property            def vocab(self):                return self.tokenizer.encoder            @property            def inv_vocab(self):                return self.tokenizer.decoder            def tokenize(self, text):                return self.tokenizer.encode(text)            def detokenize(self, token_ids):                return self.tokenizer.decode(token_ids)            @property            def eod(self):                return self.tokenizer.eos_token_id            @property            def eos_token(self):                return self.tokenizer.eos_token            @property            def pad_token_id(self):                return self.tokenizer.pad_token_id            @property            def eos_token_id(self):                return self.tokenizer.eos_token_id        tokenizer = _DeepseekTokenizer(args.tokenizer_model, args.extra_vocab_size)        # tokenizer = _DeepseekTokenizer(args.load, args.extra_vocab_size)        args.padded_vocab_size = tokenizer.vocab_size
hf2mcore.pyä¿®æ”¹
åœ¨/data/code/toolkits/model_checkpoints_convertor/deepseek_core/hf2mcore.pyä¸­ä¿®æ”¹å¯¼åŒ…ä½ç½®

æ‰§è¡Œè½¬æ¢è„šæœ¬ï¼Œè½¬æ¢æµç¨‹


è½¬æ¢ç»“æŸï¼Œå¾—åˆ°coreæƒé‡

coreæƒé‡çš„æ¨ç†ç»“æœéªŒè¯
coreæƒé‡æ¨ç†ç»“æœ

huggingfaceæƒé‡æ¨ç†ç»“æœ

]]></content>
      <categories>
        <category>æ¨¡å‹é€‚é…</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>æ¨¡å‹é€‚é…</tag>
        <tag>Nvidia</tag>
        <tag>Deepseek</tag>
        <tag>Megatron-LM</tag>
      </tags>
  </entry>
  <entry>
    <title>2.1 æ•°æ®é›†å¤„ç†</title>
    <url>/2025/11/13/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.1%20Deepseek-7B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[ä¸‹è½½åˆå§‹æ•°æ®é›†

ä¸‹è½½WuDaoCorpora2.0å¼€æºæ•°æ®é›†åˆ°/mnt/workspace/llama3-datasetså·¥ä½œç›®å½•ä¸‹

wget https://atp-modelzoo.oss-cn-hangzhou.aliyuncs.com/release/datasets/WuDaoCorpus2.0_base_sample.tgztar zxvf WuDaoCorpus2.0_base_sample.tgz 
Megatron-LMé¢„è®­ç»ƒæ•°æ®æ¸…æ´—
mmapæ•°æ®æ˜¯ä¸€ç§é¢„å…ˆæ‰§è¡Œtokenizeå¤„ç†çš„æ•°æ®æ ¼å¼ï¼Œå¯ä»¥æå¤§å‡å°‘è®­ç»ƒå¾®è°ƒè¿‡ç¨‹ä¸­ç­‰å¾…æ•°æ®è¯»å…¥çš„æ—¶é—´ï¼Œå½“æ•°æ®é‡æå¤§æ—¶ï¼Œä¼˜åŠ¿æ˜¾è‘—ã€‚
#! /bin/bashset -ex# è¯·åœ¨æ­¤å¤„è®¾ç½®åŸå§‹æ•°æ®æ‰€åœ¨è·¯å¾„data_dir=/mnt/users/wangxiangbo/dataset/WuDaoCorpus2.0_base_sample#å¼€å§‹æ•°æ®æ¸…æ´—æµç¨‹dataset_dir=$(dirname $data_dir)mkdir -p $&#123;dataset_dir&#125;/cleaned_wudao_datasetcd $&#123;dataset_dir&#125;/cleaned_wudao_datasetwget https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/llama2-codes/preprocess_wudao2.py# æ­¤å¤„ä¸ä¸Šä¸€èŠ‚ä¸åŒï¼Œå¢åŠ äº†keyå‚æ•°è®¾ä¸ºtextpython3 preprocess_wudao2.py -i $&#123;data_dir&#125; -o $&#123;dataset_dir&#125;/cleaned_wudao_dataset -k text -p 32# åˆå¹¶æ¸…æ´—åçš„æ•°æ®mkdir $&#123;dataset_dir&#125;/wudaocd $&#123;dataset_dir&#125;/wudaofind $&#123;dataset_dir&#125;/cleaned_wudao_dataset -name &quot;*.json&quot; -exec cat &#123;&#125; + &gt; $&#123;dataset_dir&#125;/wudao/merged_wudao_cleaned.jsonrm -rf $&#123;dataset_dir&#125;/cleaned_wudao_dataset
è„šæœ¬æ‰§è¡Œå®Œæˆåï¼Œä¼šå¾—åˆ°ä¸€ä¸ªåˆå¹¶çš„jsonæ•°æ®é›†æ ¼å¼

æ‰§è¡Œä¸‹æ–¹çš„å¤„ç†æ•°æ®è„šæœ¬
#! /bin/bashSTART_TIME=$SECONDSMEGATRON_PATCH_PATH=/data/codeMEGATRON_PATH=$&#123;MEGATRON_PATCH_PATH&#125;/Megatron-LMexport PYTHONPATH=$&#123;MEGATRON_PATH&#125;:$&#123;MEGATRON_PATCH_PATH&#125;:$PYTHONPATHinput_data_dir=/mnt/dataset/wudao/merged_wudao_cleaned.jsontokenizer=DeepseekTokenizerjson_keys=textoutput_data_dir=/data/code/temp/deepseek/deepseek-datasetsload_dir=/data/code/temp/deepseek/deepseek-ckptTOKENIZER_PATH=/data/code/temp/deepseek/deepseek-ckptINPUT=&quot;$&#123;input_data_dir&#125;&quot;if [ $tokenizer = &quot;Qwen2Tokenizer&quot; ]; then  python preprocess_data_megatron.py \  --input $&#123;INPUT&#125; \  --output-prefix $&#123;output_data_dir&#125;/mmap_qwen2_datasets \  --patch-tokenizer-type Qwen2Tokenizer \  --json-keys $&#123;json_keys&#125; \  --load $&#123;load_dir&#125; \  --workers 2 \  --partitions 2 \  --keep-sequential-samples \  --append-eodelif [ $tokenizer = &quot;DeepSeekV2Tokenizer&quot; ]; then  python preprocess_data_megatron.py \  --input $&#123;INPUT&#125; \  --output-prefix $&#123;output_data_dir&#125;/mmap_deepseekv2_datasets \  --patch-tokenizer-type DeepSeekV2Tokenizer \  --json-keys $&#123;json_keys&#125; \  --load $&#123;load_dir&#125; \  --workers 8 \  --partitions 1 \  --keep-sequential-samples \  --append-eodelif [ $tokenizer = &quot;LLamaTokenizer&quot; ]; then  python preprocess_data_megatron.py \  --tokenizer-model $TOKENIZER_PATH \  --input $&#123;INPUT&#125; \  --output-prefix $&#123;output_data_dir&#125;/mmap_llama_datasets \  --patch-tokenizer-type LLamaTokenizer \  --load $&#123;load_dir&#125; \  --workers 16 \  --partitions 1 \  --keep-sequential-samples \  --append-eodelif [ $tokenizer = &quot;DeepseekTokenizer&quot; ]; then  python preprocess_data_megatron.py \  --tokenizer-model $TOKENIZER_PATH \  --input $&#123;INPUT&#125; \  --output-prefix $&#123;output_data_dir&#125;/mmap_deepseek_datasets \  --patch-tokenizer-type LLamaTokenizer \  --load $&#123;load_dir&#125; \  --workers 16 \  --partitions 1 \  --keep-sequential-samples \  --append-eodelif [ $tokenizer = &quot;LLama2Tokenizer&quot; ]; then  python preprocess_data_megatron.py \  --tokenizer-model $TOKENIZER_PATH \  --extra-vocab-size 0 \  --input $&#123;INPUT&#125; \  --output-prefix $&#123;output_data_dir&#125;/mmap_llama2_datasets \  --patch-tokenizer-type LLama2Tokenizer \  --load $&#123;load_dir&#125; \  --workers 16 \  --partitions 1 \  --keep-sequential-samples \  --append-eodelif [ $tokenizer = &quot;LLama3Tokenizer&quot; ]; then  python preprocess_data_megatron.py \  --input $&#123;INPUT&#125; \  --output-prefix $&#123;output_data_dir&#125;/mmap_llama3_datasets \  --patch-tokenizer-type LLama3Tokenizer \  --load $&#123;load_dir&#125; \  --workers 16 \  --partitions 1 \  --keep-sequential-samples \  --append-eodfiELAPSED_TIME=$(($SECONDS - $START_TIME))echo &quot;$(($ELAPSED_TIME/60)) min $(($ELAPSED_TIME%60)) sec&quot;
åœ¨python preprocess_data_megatron.pyä¸­ï¼ŒåŠ å…¥DeepseekTokenizerï¼Œæ­¤å¤–è¿˜éœ€è¦åœ¨/data/code/megatron_patch/tokenizer/init.pyä¸­åŠ å…¥DeepseekTokenizeré…ç½®ã€‚


æ•°æ®å¤„ç†å®Œæˆ


]]></content>
      <categories>
        <category>æ¨¡å‹é€‚é…</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>æ¨¡å‹é€‚é…</tag>
        <tag>Nvidia</tag>
        <tag>Deepseek</tag>
        <tag>Megatron-LM</tag>
      </tags>
  </entry>
  <entry>
    <title>2.4 Deepseek-7Bå¾®è°ƒé€‚é…</title>
    <url>/2025/11/13/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.4%20Deepseek-7B%E5%BE%AE%E8%B0%83%E9%80%82%E9%85%8D/</url>
    <content><![CDATA[å¾®è°ƒç‰©æ–™åŠä»£ç å‡†å¤‡
æ•°æ®é›†å‡†å¤‡
/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-train.json
/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-valid.json

å¾®è°ƒè„šæœ¬
#!/bin/bashexport NCCL_P2P_DISABLE=1#export NCCL_DEBUG=INFOexport NCCL_SOCKET_IFNAME=eth0export GLOO_SOCKET_IFNAME=eth0export CUDA_DEVICE_MAX_CONNECTIONS=1export CUDA_VISIBLE_DEVICES=0,1,2,3MEGATRON_PATCH_PATH=/data/codeMEGATRON_PATH=$&#123;MEGATRON_PATCH_PATH&#125;/Megatron-LMexport PYTHONPATH=$&#123;MEGATRON_PATH&#125;:$&#123;MEGATRON_PATCH_PATH&#125;:$PYTHONPATHNNODES=2NODE_RANK=0GPUS_PER_NODE=4MASTER_ADDR=192.168.0.65MASTER_PORT=8889DISTRIBUTED_ARGS=&quot;--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT&quot;MODEL_SIZE=7BBATCH_SIZE=1GLOBAL_BATCH_SIZE=8 LR=1e-5MIN_LR=1e-6SEQ_LEN=1024PAD_LEN=1024EXTRA_VOCAB_SIZE=2400PR=fp16TP=4PP=1AC=selDO=trueFL=falseSP=trueTE=trueMOE=falseSAVE_INTERVAL=1000DATASET_PATH=/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-train.jsonVALID_DATASET_PATH=/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-valid.jsonPRETRAIN_CHECKPOINT_PATH=/data/code/temp/deepseek/deepseek-ckptTOKENIZER_PATH=/data/code/temp/deepseek/deepseek-ckptTRAIN_TOKENS=100000000   WARMUP_TOKENS=10000OUTPUT_BASEPATH=/data/code/temp/output/deepseek/finetune_outputif [ $MODEL_SIZE = 7B ]; thenNUM_LAYERS=30HIDDEN_SIZE=4096NUM_ATTN_HEADS=32INTERMEDIATE_SIZE=11008MAX_POSITION_EMBEDDINGS=4096gqa_options=&quot;&quot;fiif [ $AC = full ]; then    activation_checkpoint_options=&quot; \		    --recompute-method uniform \		    --recompute-granularity full&quot;elif [ $AC = sel ]; then    activation_checkpoint_options=&quot; \        --recompute-activations&quot;elif [ $AC = none ]; then    activation_checkpoint_options=&quot; \                    &quot;fiif [ $PR = fp16 ]; then    pr_options=&quot; \		    --fp16 \            --apply-query-key-layer-scaling&quot;    export NVTE_APPLY_QK_LAYER_SCALING=1elif [ $PR = bf16 ]; then    pr_options=&quot; \        --bf16&quot;elif [ $PR = fp8 ]; then    pr_options=&quot; \        --bf16 \        --fp8-hybrid \        --fp8-amax-compute-algo max \        --fp8-amax-history-len 1024 \        --transformer-impl transformer_engine&quot;fiif [ $DO = true ]; then    do_options=&quot; \		    --use-distributed-optimizer&quot;elif [ $DO = false ]; then    do_options=&quot; \                    &quot;fiif [ $FL = true ]; then    flash_options=&quot; \		    --use-flash-attn&quot;elif [ $FL = false ]; then    flash_options=&quot; \                    &quot;fiif [ $TE = true ]; then    te_options=&quot; \		    --transformer-impl transformer_engine&quot;elif [ $TE = false ]; then    te_options=&quot; \                    &quot;fiif [ $MOE = true ]; then    moe_options=&quot; \		    --moe-router-topk 1 \		    --num-experts 4 \		    --moe-aux-loss-coeff 1e-2 \		    --expert-model-parallel-size 2&quot;elif [ $MOE = false ]; then    moe_options=&quot; \                    &quot;fiif [ $SP = true ] &amp;&amp; [ $TP -gt 1 ]; then    sp_options=&quot; \		    --sequence-parallel&quot;elif [ $SP = false ]; then    sp_options=&quot; \                    &quot;fiif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then    load_options=&quot; \            --load $PRETRAIN_CHECKPOINT_PATH&quot;fiTRAIN_ITERS=$(( $&#123;TRAIN_TOKENS&#125; / $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))LR_WARMUP_ITERS=$(( $&#123;WARMUP_TOKENS&#125;  / $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))LR_DECAY_ITERS=$(( $&#123;TRAIN_TOKENS&#125; /  $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))# LR_DECAY_ITERS=$(($&#123;TRAIN_ITERS&#125; - $&#123;LR_WARMUP_ITERS&#125;))NAME=&quot;$&#123;ENV&#125;-finetune-mcore-deepseek-$&#123;MODEL_SIZE&#125;-lr-$&#123;LR&#125;-bs-$&#123;BATCH_SIZE&#125;-seqlen-$&#123;SEQ_LEN&#125;-pr-$&#123;PR&#125;-tp-$&#123;TP&#125;-pp-$&#123;PP&#125;-ac-$&#123;AC&#125;-do-$&#123;DO&#125;-sp-$&#123;SP&#125;-tt-$&#123;TRAIN_TOKENS&#125;-wt-$&#123;WARMUP_ITERS&#125;&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/log/&quot;current_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)TENSORBOARD_DIR=&quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/$&#123;NAME&#125;_$&#123;current_time&#125;&quot;mkdir -p $&#123;TENSORBOARD_DIR&#125;SAVED_PRETRAIN_CHECKPOINT_PATH=&quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/$&#123;NAME&#125;&quot;megatron_options=&quot;  \        --tokenizer-model $&#123;TOKENIZER_PATH&#125; \        --save $&#123;SAVED_PRETRAIN_CHECKPOINT_PATH&#125; \        --split 99,1,0 \        --train-data-path $&#123;DATASET_PATH&#125; \        --valid-data-path $&#123;VALID_DATASET_PATH&#125; \        --test-data-path $&#123;VALID_DATASET_PATH&#125; \        --lr $&#123;LR&#125; \        --min-lr $&#123;MIN_LR&#125; \        --lr-decay-style linear \        --adam-beta1 0.9 \        --adam-beta2 0.95 \        --weight-decay 0.1 \        --clip-grad 1.0 \        --init-method-std 0.006 \        --dataloader-type cyclic \        --lr-decay-iters $&#123;LR_DECAY_ITERS&#125; \        --lr-warmup-iters $&#123;LR_WARMUP_ITERS&#125; \        --train-iters $&#123;TRAIN_ITERS&#125; \        --micro-batch-size $&#123;BATCH_SIZE&#125; \        --global-batch-size $&#123;GLOBAL_BATCH_SIZE&#125; \        --num-layers $&#123;NUM_LAYERS&#125; \        --hidden-size $&#123;HIDDEN_SIZE&#125; \        --num-attention-heads $&#123;NUM_ATTN_HEADS&#125; \        --ffn-hidden-size $&#123;INTERMEDIATE_SIZE&#125; \        --seq-length $&#123;SEQ_LEN&#125; \        --max-position-embeddings $&#123;MAX_POSITION_EMBEDDINGS&#125; \        --max-padding-length $&#123;PAD_LEN&#125; \        --log-interval 1 \        --eval-interval 10000 \        --eval-iters 10 \        --save-interval $&#123;SAVE_INTERVAL&#125; \        --tensorboard-queue-size 1 \        --tensorboard-dir $&#123;TENSORBOARD_DIR&#125; \        --log-timers-to-tensorboard \        --log-batch-size-to-tensorboard \        --log-validation-ppl-to-tensorboard \        --tensor-model-parallel-size $&#123;TP&#125; \        --pipeline-model-parallel-size $&#123;PP&#125; \        --no-load-optim \        --no-load-rng \        --num-workers 8 \        --seed 1234 \        --extra-vocab-size $&#123;EXTRA_VOCAB_SIZE&#125; \        --patch-tokenizer-type DeepseekTokenizer \        --dataset LLama-Pretrain-Raw \        --swiglu \        --normalization RMSNorm \        --use-rotary-position-embeddings \        --no-rope-fusion \        --position-embedding-type rope \        --untie-embeddings-and-output-weights \        --disable-bias-linear \        --use-mcore-models \        --rotary-base 10000 \        --attention-dropout 0.0 \        --hidden-dropout 0.0 \        --norm-epsilon 1e-05 \        --eod-mask-loss \        --log-throughput \        --loss-scale 2048              &quot;        # --loss-scale 2048  LOG_SAVE=&quot;/data/code/temp/output/deepseek/finetune_output/log/&quot;$&#123;current_time&#125;-deepseek-finetune-$&#123;NODE_RANK&#125;.log&quot;&quot; run_cmd=&quot;torchrun $DISTRIBUTED_ARGS /data/code/examples/deepseek_core/pretrain_mcore_deepseek.py $&#123;megatron_options&#125; $&#123;pr_options&#125; $&#123;load_options&#125; $&#123;te_options&#125; $&#123;activation_checkpoint_options&#125; $&#123;do_options&#125; $&#123;flash_options&#125; $&#123;sp_options&#125; $&#123;gqa_options&#125; $&#123;moe_options&#125; &gt; $&#123;LOG_SAVE&#125; 2&gt;&amp;1&quot;echo $&#123;run_cmd&#125;eval $&#123;run_cmd&#125;set +x
å¾®è°ƒyaml
apiVersion: apps/v1kind: DaemonSetmetadata:  name: deepseek-core  namespace: deepseekspec:  selector:    matchLabels:      app: deepseek-core  template:    metadata:      labels:        app: deepseek-core    spec:      hostNetwork: true      nodeSelector:        deepseek: deepseek-7B      containers:        - name: deepseek          image: registry.paas/library/megatron-lm:v1          imagePullPolicy: IfNotPresent          resources:            limits:              nvidia.com/gpu: 4            requests:              nvidia.com/gpu: 4          command:            - &quot;/bin/bash&quot;            - &quot;-c&quot;            - |              cd /data/code/temp/conf/deepseek_core/finetune &amp;&amp;              bash setRank.sh &amp;&amp;              cd /data/ &amp;&amp;              bash finetune_mcore_deepseek.sh          securityContext:            privileged: true          volumeMounts:            - mountPath: /data/code              name: code            - mountPath: /etc/localtime              name: localtime            - mountPath: /dev/shm              name: dshm            - name: weight              mountPath: /data/code/temp/deepseek/deepseek-ckpt/      volumes:        - name: code          hostPath:            path: /mnt/users/lihai/distribute/core/megatron-core        - name: weight          hostPath:            path: /mnt/users/wangxiangbo/model/deepseek-ai/deepseek-llm-7b-core        - name: localtime          hostPath:            path: /etc/localtime        - name: dshm          emptyDir:            medium: Memory            sizeLimit: 20G
setrank
#!/bin/bashorigin_shell=&quot;/data/code/examples/deepseek_core/finetune_mcore_deepseek.sh&quot;conf_dir=&quot;/data/code/temp/conf/&quot;local_dir=&quot;/data/&quot;shell_name=finetune_mcore_deepseek.sh## å¤åˆ¶è„šæœ¬åˆ°/workspaceä¸‹cp $origin_shell $local_dir## è¯»å–hostfilereadarray -t ips &lt; &lt;(grep -vE &#x27;^[[:space:]]*$&#x27; &quot;$conf_dir&quot;hostfile-23)## è·å–rank0 IPrank0_ip=$(echo &quot;$&#123;ips[0]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)nodes=$&#123;#ips[@]&#125;## è·å–hostfileä¸­é…ç½®çš„IPå‰ç¼€## ä½¿ç”¨cutæå–IPåœ°å€çš„å‰ä¸‰ä¸ªæ•°å­—éƒ¨åˆ†ip_prefix=$(echo &quot;$&#123;ips[0]&#125;&quot; | cut -d &#x27;.&#x27; -f 1-3)## è·å–æœ¬æœºIPip=$(hostname -I | grep -oE &quot;$ip_prefix\.[0-9]+&quot;)# åˆå§‹åŒ–ranknode_rank=-1# éå†æ•°ç»„for i in &quot;$&#123;!ips[@]&#125;&quot;; do    # ä½¿ç”¨trå‘½ä»¤å»é™¤ç©ºç™½å­—ç¬¦    clean_string=$(echo &quot;$&#123;ips[$i]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)    if [[ &quot;$clean_string&quot; == &quot;$ip&quot; ]]; then        node_rank=$i        break    fidoneif [ $node_rank -ne -1 ]; then    ## ä¿®æ”¹è„šæœ¬ä¸­MASTER_ADDR    sed -i &quot;s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/&quot; $local_dir$shell_name    ## ä¿®æ”¹NNODES    sed -i &quot;s/^NNODES=.*/NNODES=$nodes/&quot; $local_dir$shell_name    ## ä¿®æ”¹NODE_RANK    sed -i &quot;s/^NODE_RANK=.*/NODE_RANK=$node_rank/&quot; $local_dir$shell_namefi
å¤šæœºå¤šå¡æ‹‰èµ·æµ‹è¯•





æ–­ç‚¹ç»­è®­æµ‹è¯•
lossæ›²çº¿å¯¹æ¯”
hugging-patchæƒé‡è½¬æ¢
patchè½¬æ¢è„šæœ¬
#!/bin/bashset -eSTART_TIME=$SECONDSMEGATRON_PATH=/data/code/export PYTHONPATH=$PYTHONPATH:$&#123;MEGATRON_PATH&#125;:$&#123;MEGATRON_PATH&#125;/Megatron-LM-231007SOURCE_CKPT_PATH=/mnt/deepseek-ai/deepseek-llm-7b-baseTARGET_CKPT_PATH=/mnt/deepseek-ai/deepseek-llm-7b-patchTP=4PP=1MN=llama-7b #llama-7b, llama-13b, llama-30b, llama-65b, llama2-7b, llama2-13b, llama2-70bEXTRA_VOCAB_SIZE=2400mg2hf=falseif [ $mg2hf = true ]; then    do_options=&quot;                --convert_checkpoint_from_megatron_to_transformers    &quot;elif [ $mg2hf = false ]; then    do_options=&quot;&quot;fipython hf2megatron.py \    --load_path $&#123;SOURCE_CKPT_PATH&#125; \    --save_path $&#123;TARGET_CKPT_PATH&#125; \    --target_params_dtype fp16 \    --megatron-path $&#123;MEGATRON_PATH&#125; \    --target_tensor_model_parallel_size $&#123;TP&#125; \    --target_pipeline_model_parallel_size $&#123;PP&#125; \    --model_name $&#123;MN&#125; \    --extra_num_vocabs $&#123;EXTRA_VOCAB_SIZE&#125; \$&#123;do_options&#125;ELAPSED_TIME=$(($SECONDS - $START_TIME))echo &quot;$(($ELAPSED_TIME/60)) min $(($ELAPSED_TIME%60)) sec&quot;
patchæƒé‡è½¬æ¢è¿‡ç¨‹

å¾—åˆ°è½¬æˆpatchçš„æƒé‡

patchæ¡†æ¶å¾®è°ƒä»£ç å‡†å¤‡
patchçš„å¾®è°ƒyaml
apiVersion: apps/v1kind: DaemonSetmetadata:  name: deepseek-core  namespace: deepseekspec:  selector:    matchLabels:      app: deepseek-core  template:    metadata:      labels:        app: deepseek-core    spec:      hostNetwork: true      nodeSelector:        deepseek: deepseek-7B      containers:        - name: deepseek          image: registry.paas/library/megatron-lm:v1          imagePullPolicy: IfNotPresent          resources:            limits:              nvidia.com/gpu: 4            requests:              nvidia.com/gpu: 4          command:            - &quot;/bin/bash&quot;            - &quot;-c&quot;            - |              cd /data/code/examples/deepseek/conf &amp;&amp;              bash setRank.sh &amp;&amp;              cd /data/ &amp;&amp;              bash finetune_patch_deepseek.sh          securityContext:            privileged: true          volumeMounts:            - mountPath: /data/code              name: code            - mountPath: /etc/localtime              name: localtime            - mountPath: /dev/shm              name: dshm            - name: weight              mountPath: /data/code/examples/deepseek/ckpt/      volumes:        - name: code          hostPath:            path: /mnt/users/wangxiangbo/Pai-Megatron-Patch        - name: weight          hostPath:            path: /mnt/users/wangxiangbo/model/deepseek-ai/deepseek-llm-7b-patch        - name: localtime          hostPath:            path: /etc/localtime        - name: dshm          emptyDir:            medium: Memory            sizeLimit: 20G
patchçš„å¾®è°ƒè„šæœ¬
#!/bin/bash#sh run_pretrain_megatron_deepseek.sh dsw /workspace/Pai-Megatron-Patch 7B 1 8 1e-5 1e-6 2048 2048 0 bf16 1 1 sel true true true false 100000 /mnt/deepseek-datasets/wudao_llamabpe_text_document /mnt/deepseek-ckpts/Llama-2-7b-hf-to-mg-tp1-pp1/ 10000000000 100000000 /mnt/output_patch_testexport NCCL_P2P_DISABLE=1#export NCCL_DEBUG=INFOexport NCCL_SOCKET_IFNAME=eth0export GLOO_SOCKET_IFNAME=eth0export CUDA_DEVICE_MAX_CONNECTIONS=1export CUDA_VISIBLE_DEVICES=0,1,2,3MEGATRON_PATCH_PATH=/data/code MEGATRON_PATH=$&#123;MEGATRON_PATCH_PATH&#125;/Megatron-LM-231007export PYTHONPATH=$&#123;MEGATRON_PATH&#125;:$&#123;MEGATRON_PATCH_PATH&#125;:$PYTHONPATHNNODES=2NODE_RANK=0GPUS_PER_NODE=4MASTER_ADDR=192.168.0.65MASTER_PORT=8889DISTRIBUTED_ARGS=&quot;--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT&quot;MODEL_SIZE=7BBATCH_SIZE=1GLOBAL_BATCH_SIZE=8LR=1e-5MIN_LR=1e-6SEQ_LEN=1024PAD_LEN=1024EXTRA_VOCAB_SIZE=4800PR=fp16TP=4PP=1AC=selDO=trueFL=falseSP=trueTE=falseSAVE_INTERVAL=1000DATASET_PATH=/data/code/examples/deepseek/dataset/alpaca_zh-deepseek-train.jsonVALID_DATASET_PATH=/data/code/examples/deepseek/dataset/alpaca_zh-deepseek-valid.jsonPRETRAIN_CHECKPOINT_PATH=/data/code/examples/deepseek/ckptTRAIN_ITERS=10000LR_WARMUP_ITERS=0OUTPUT_BASEPATH=/data/code/examples/deepseek/output/finetune_outputif [ $MODEL_SIZE = 7B ]; thenNUM_LAYERS=30HIDDEN_SIZE=4096NUM_ATTN_HEADS=32INTERMEDIATE_SIZE=11008gqa_options=&quot;&quot;fiif [ $AC = full ]; then    activation_checkpoint_options=&quot; \		    --recompute-method uniform \		    --recompute-granularity full \            --recompute-num-layers $&#123;NUM_LAYERS&#125;&quot;elif [ $AC = sel ]; then    activation_checkpoint_options=&quot; \        --recompute-activations&quot;elif [ $AC = none ]; then    activation_checkpoint_options=&quot; \                    &quot;fiif [ $PR = fp16 ]; then    pr_options=&quot; \		    --fp16&quot;elif [ $PR = bf16 ]; then    pr_options=&quot; \        --bf16&quot;elif [ $PR = fp8 ]; then    pr_options=&quot; \        --bf16        --fp8-hybrid \        --fp8-amax-compute-algo max \        --fp8-amax-history-len 1024 \        --transformer-impl transformer_engine&quot;fiif [ $DO = true ]; then    do_options=&quot; \		    --use-distributed-optimizer&quot;elif [ $DO = false ]; then    do_options=&quot; \                    &quot;fiif [ $FL = true ]; then    flash_options=&quot; \		    --use-flash-attn&quot;elif [ $FL = false ]; then    flash_options=&quot; \                    &quot;fiif [ $TE = true ]; then    te_options=&quot; \		    --transformer-impl transformer_engine&quot;elif [ $TE = false ]; then    te_options=&quot; \                    &quot;fiif [ $SP = true ] &amp;&amp; [ $TP -gt 1 ]; then    sp_options=&quot; \		    --sequence-parallel&quot;elif [ $SP = false ]; then    sp_options=&quot; \                    &quot;fiif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then    load_options=&quot; \            --load $PRETRAIN_CHECKPOINT_PATH&quot;fiLR_DECAY_ITERS=$(( $&#123;TRAIN_ITERS&#125; - $&#123;LR_WARMUP_ITERS&#125;))NAME=&quot;$&#123;ENV&#125;-finetune-patch-deepseek-$&#123;MODEL_SIZE&#125;-lr-$&#123;LR&#125;-bs-$&#123;BATCH_SIZE&#125;-seqlen-$&#123;SEQ_LEN&#125;-pr-$&#123;PR&#125;-tp-$&#123;TP&#125;-pp-$&#123;PP&#125;-ac-$&#123;AC&#125;-do-$&#123;DO&#125;-sp-$&#123;SP&#125;-tt-$&#123;TRAIN_TOKENS&#125;-wt-$&#123;WARMUP_TOKENS&#125;&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/log/&quot;current_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)TENSORBOARD_DIR=&quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/$&#123;NAME&#125;_$&#123;current_time&#125;&quot;mkdir -p $&#123;TENSORBOARD_DIR&#125;SAVED_PRETRAIN_CHECKPOINT_PATH=&quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/$&#123;NAME&#125;&quot;megatron_options=&quot;  \        --save $&#123;SAVED_PRETRAIN_CHECKPOINT_PATH&#125; \        --split 99,1,0 \        --train-data-path $&#123;DATASET_PATH&#125; \        --valid-data-path $&#123;VALID_DATASET_PATH&#125; \        --test-data-path $&#123;VALID_DATASET_PATH&#125; \        --lr $&#123;LR&#125; \        --min-lr $&#123;MIN_LR&#125; \        --lr-decay-style linear \        --adam-beta1 0.9 \        --adam-beta2 0.95 \        --weight-decay 0.1 \        --clip-grad 1.0 \        --init-method-std 0.006 \        --dataloader-type cyclic \        --lr-decay-iters $&#123;LR_DECAY_ITERS&#125; \        --lr-warmup-iters $&#123;LR_WARMUP_ITERS&#125; \        --train-iters $&#123;TRAIN_ITERS&#125; \        --micro-batch-size $&#123;BATCH_SIZE&#125; \        --global-batch-size $&#123;GLOBAL_BATCH_SIZE&#125; \        --num-layers $&#123;NUM_LAYERS&#125; \        --hidden-size $&#123;HIDDEN_SIZE&#125; \        --num-attention-heads $&#123;NUM_ATTN_HEADS&#125; \        --ffn-hidden-size $&#123;INTERMEDIATE_SIZE&#125; \        --seq-length $&#123;SEQ_LEN&#125; \        --max-position-embeddings $&#123;SEQ_LEN&#125; \        --log-interval 1 \        --eval-interval 10000 \        --eval-iters 10 \        --save-interval $&#123;SAVE_INTERVAL&#125; \        --tensorboard-queue-size 1 \        --tensorboard-dir $&#123;TENSORBOARD_DIR&#125; \        --log-timers-to-tensorboard \        --log-batch-size-to-tensorboard \        --log-validation-ppl-to-tensorboard \        --tensor-model-parallel-size $&#123;TP&#125; \        --pipeline-model-parallel-size $&#123;PP&#125; \        --dataset LLama-Pretrain-Raw \        --no-save-optim \        --no-load-optim \        --no-load-rng \        --num-workers 8 \        --seed 1234 \        --max-padding-length $&#123;PAD_LEN&#125; \        --extra-vocab-size $&#123;EXTRA_VOCAB_SIZE&#125; \        --patch-tokenizer-type LLamaTokenizer \        --swiglu \        --normalization RMSNorm \        --use-llama2-rotary-position-embeddings \        --position-embedding-type rope \        --untie-embeddings-and-output-weights \        --rotary-base 10000 \        --rotary-scale-factor 4 \        --loss-scale 2048 \        --disable-bias-linear        &quot;LOG_SAVE=&quot;/data/code/examples/deepseek/output/finetune_output/log/&quot;$&#123;current_time&#125;-deepseek-patch-$&#123;NODE_RANK&#125;.log&quot;&quot; run_cmd=&quot;torchrun $DISTRIBUTED_ARGS /data/code/examples/llama2/pretrain_megatron_llama.py $&#123;megatron_options&#125; $&#123;pr_options&#125; $&#123;load_options&#125; $&#123;te_options&#125; $&#123;activation_checkpoint_options&#125; $&#123;do_options&#125; $&#123;flash_options&#125; $&#123;sp_options&#125; $&#123;gqa_options&#125; &gt; $&#123;LOG_SAVE&#125; 2&gt;&amp;1&quot;echo $&#123;run_cmd&#125;eval $&#123;run_cmd&#125;set +x
patchçš„å¾®è°ƒsetrank
#!/bin/bashorigin_shell=&quot;/data/code/examples/deepseek/finetune_patch_deepseek.sh&quot;conf_dir=&quot;/data/code/examples/deepseek/conf/&quot;local_dir=&quot;/data/&quot;shell_name=finetune_patch_deepseek.sh## å¤åˆ¶è„šæœ¬åˆ°/workspaceä¸‹cp $origin_shell $local_dir## è¯»å–hostfilereadarray -t ips &lt; &lt;(grep -vE &#x27;^[[:space:]]*$&#x27; &quot;$conf_dir&quot;hostfile)## è·å–rank0 IPrank0_ip=$(echo &quot;$&#123;ips[0]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)nodes=$&#123;#ips[@]&#125;## è·å–hostfileä¸­é…ç½®çš„IPå‰ç¼€## ä½¿ç”¨cutæå–IPåœ°å€çš„å‰ä¸‰ä¸ªæ•°å­—éƒ¨åˆ†ip_prefix=$(echo &quot;$&#123;ips[0]&#125;&quot; | cut -d &#x27;.&#x27; -f 1-3)## è·å–æœ¬æœºIPip=$(hostname -I | grep -oE &quot;$ip_prefix\.[0-9]+&quot;)# åˆå§‹åŒ–ranknode_rank=-1# éå†æ•°ç»„for i in &quot;$&#123;!ips[@]&#125;&quot;; do    # ä½¿ç”¨trå‘½ä»¤å»é™¤ç©ºç™½å­—ç¬¦    clean_string=$(echo &quot;$&#123;ips[$i]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)    if [[ &quot;$clean_string&quot; == &quot;$ip&quot; ]]; then        node_rank=$i        break    fidoneif [ $node_rank -ne -1 ]; then    ## ä¿®æ”¹è„šæœ¬ä¸­MASTER_ADDR    sed -i &quot;s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/&quot; $local_dir$shell_name    ## ä¿®æ”¹NNODES    sed -i &quot;s/^NNODES=.*/NNODES=$nodes/&quot; $local_dir$shell_name    ## ä¿®æ”¹NODE_RANK    sed -i &quot;s/^NODE_RANK=.*/NODE_RANK=$node_rank/&quot; $local_dir$shell_namefi
patchæ¡†æ¶å¾®è°ƒæµ‹è¯•


]]></content>
      <categories>
        <category>æ¨¡å‹é€‚é…</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>æ¨¡å‹é€‚é…</tag>
        <tag>Nvidia</tag>
        <tag>Deepseek</tag>
        <tag>Megatron-LM</tag>
      </tags>
  </entry>
  <entry>
    <title>2.5 pytorchjob+ekiæ–­ç‚¹ç»­è®­</title>
    <url>/2025/11/13/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.5%20pytorchjob+eki%E6%96%AD%E7%82%B9%E7%BB%AD%E8%AE%AD/</url>
    <content><![CDATA[ç¯å¢ƒå‡†å¤‡
pytorchjobè®­ç»ƒç¯å¢ƒå·²ç”±å°ç»„åŒäº‹æ­å»ºå®Œæˆ

ä»£ç å‡†å¤‡
æ–­ç‚¹ç»­è®­yaml
éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸èƒ½è®¾å®šè‡ªå®šä¹‰çš„namespaceï¼Œéœ€è¦åœ¨defaultä¸‹å¯åŠ¨pod
apiVersion: &quot;ai.cmss.chinamobile.com/v1alpha1&quot;kind: PyTorchJobmetadata:  name: deepseek-core  labels:    service: deepseek-corespec:  nprocPerNode: &quot;4&quot;  pytorchReplicaSpecs:    Master:      replicas: 1      template:        metadata:          labels:            app: ckpt        spec:          nodeSelector:            model-ckpt: deepseek-master          restartPolicy: Always          hostNetwork: true          dnsPolicy: ClusterFirstWithHostNet          affinity:            podAntiAffinity:              requiredDuringSchedulingIgnoredDuringExecution:                - labelSelector:                    matchExpressions:                      - key: app                        operator: In                        values:                          - ckpt                  topologyKey: &quot;kubernetes.io/hostname&quot;          containers:            - name: pytorch              image: registry.paas/library/megatron-lm:v1              imagePullPolicy: IfNotPresent              securityContext:                capabilities:                  add:                    - SYS_ADMIN              resources:                limits:                  nvidia.com/gpu: 4 # Request access to all GPU                requests:                  nvidia.com/gpu: 4              env:                - name: OMP_NUM_THREADS                  value: &quot;4&quot;              command:                - &quot;/bin/bash&quot;                - &quot;-c&quot;                - |                  cd /data/code/temp/conf/deepseek_core/pretrain &amp;&amp;                  bash SetRank.sh &amp;&amp;                  cd /data/ &amp;&amp;                  bash pretrain_ckpt_deepseek.sh              volumeMounts:                - mountPath: /data/code                  readOnly: false                  name: code                - name: shm-volume                  mountPath: /dev/shm                - name: pod-gpu-resources                  mountPath: /tmp                  readOnly: false          volumes:            - name: code              hostPath:                path: /mnt/users/lihai/distribute/core/megatron-core            - name: shm-volume              emptyDir:                medium: Memory                sizeLimit: 1G            - name: pod-gpu-resources              hostPath:                path: /var/lib/kubelet/pod-resources          schedulerName: ai-scheduler    Worker:      replicas: 1      restartPolicy: OnFailure      template:        metadata:          labels:            app: ckpt        spec:          nodeSelector:            model-ckpt: deepseek-worker          restartPolicy: Always          hostNetwork: true          dnsPolicy:            ClusterFirstWithHostNet            #affinty:            #podAntiAffinity:            # requiredDuringSchedulingIgnoredDuringExecution:            # - labelSelector:            #     matchExpressions:            #     - key: app            #       operator: In            #       values:            #       - ckpt            #   topologyKey: &quot;kubernetes.io/hostname&quot;          containers:            - name: pytorch              image: registry.paas/library/megatron-lm:v1              imagePullPolicy: IfNotPresent              # workingDir: /workspace/model/glm3/chatglm3/finetune_demo              securityContext:                capabilities:                  add:                    - SYS_ADMIN              resources:                limits:                  nvidia.com/gpu: 4 # Request access to all GPU                requests:                  nvidia.com/gpu: 4              env:                - name: OMP_NUM_THREADS                  value: &quot;4&quot;              command:                - &quot;/bin/bash&quot;                - &quot;-c&quot;                - |                  cd /data/code/temp/conf/deepseek_core/pretrain &amp;&amp;                  bash SetRank.sh &amp;&amp;                  cd /data/ &amp;&amp;                  bash pretrain_ckpt_deepseek.sh              volumeMounts:                - mountPath: /data/code                  readOnly: false                  name: code                - name: shm-volume                  mountPath: /dev/shm                - name: pod-gpu-resources                  mountPath: /tmp                  readOnly: false          volumes:            - name: code              hostPath:                path: /mnt/users/lihai/distribute/core/megatron-core                type: Directory            - name: shm-volume              emptyDir:                medium: Memory                sizeLimit: 1G            - name: pod-gpu-resources              hostPath:                path: /var/lib/kubelet/pod-resources          schedulerName: ai-scheduler
setrank
#!/bin/bashorigin_shell=&quot;/data/code/examples/deepseek_core/pretrain_ckpt_deepseek.sh&quot;conf_dir=&quot;/data/code/temp/conf/&quot;local_dir=&quot;/data/&quot;shell_name=pretrain_ckpt_deepseek.sh## å¤åˆ¶è„šæœ¬åˆ°/workspaceä¸‹cp $origin_shell $local_dir## è¯»å–hostfilereadarray -t ips &lt; &lt;(grep -vE &#x27;^[[:space:]]*$&#x27; &quot;$conf_dir&quot;hostfile-236)## è·å–rank0 IPrank0_ip=$(echo &quot;$&#123;ips[0]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)nodes=$&#123;#ips[@]&#125;## è·å–hostfileä¸­é…ç½®çš„IPå‰ç¼€## ä½¿ç”¨cutæå–IPåœ°å€çš„å‰ä¸‰ä¸ªæ•°å­—éƒ¨åˆ†ip_prefix=$(echo &quot;$&#123;ips[0]&#125;&quot; | cut -d &#x27;.&#x27; -f 1-3)## è·å–æœ¬æœºIPip=$(hostname -I | grep -oE &quot;$ip_prefix\.[0-9]+&quot;)# åˆå§‹åŒ–ranknode_rank=-1# æ‰“å°è°ƒè¯•ä¿¡æ¯echo &quot;æœ¬æœºIP: $ip&quot;echo &quot;rank0 IP: $rank0_ip&quot;echo &quot;æ‰€æœ‰èŠ‚ç‚¹IP: $&#123;ips[@]&#125;&quot;# éå†æ•°ç»„# for i in &quot;$&#123;!ips[@]&#125;&quot;; do#     # ä½¿ç”¨trå‘½ä»¤å»é™¤ç©ºç™½å­—ç¬¦#     clean_string=$(echo &quot;$&#123;ips[$i]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)#     if [[ &quot;$clean_string&quot; == &quot;$ip&quot; ]]; then#         node_rank=$i#         break#     fi# done# TODO æµ‹è¯•æ–­ç‚¹ç»­è®­for i in &quot;$&#123;!ips[@]&#125;&quot;; do    clean_string=$(echo &quot;$&#123;ips[$i]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)    if [[ &quot;$clean_string&quot; == &quot;$ip&quot; ]]; then        if [[ &quot;$ip&quot; == &quot;$rank0_ip&quot; ]]; then            node_rank=0        else            node_rank=1        fi        break    fidone# æ‰“å°è°ƒè¯•ä¿¡æ¯echo &quot;è®¾ç½®çš„NODE_RANK: $node_rank&quot;if [ $node_rank -ne -1 ]; then    ## ä¿®æ”¹è„šæœ¬ä¸­MASTER_ADDR    sed -i &quot;s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/&quot; $local_dir$shell_name    ## ä¿®æ”¹NNODES    # sed -i &quot;s/^NNODES=.*/NNODES=$nodes/&quot; $local_dir$shell_name    ## ä¿®æ”¹NODE_RANK    sed -i &quot;s/^NODE_RANK=.*/NODE_RANK=$node_rank/&quot; $local_dir$shell_namefi
hostfile
å¤šæœºèŠ‚ç‚¹æ•…éšœéœ€è¦é€‰å–3ä¸ªèŠ‚ç‚¹æ¥æ¨¡æ‹Ÿï¼Œå…¶ä¸­2å°ç”¨æ¥æ‹‰èµ·è®­ç»ƒï¼Œä¹‹åæ¨¡æ‹Ÿå…¶ä¸­1å°å‘ç”Ÿæ•…éšœï¼Œä¹‹åå†è°ƒåº¦åˆ°å¦ä¸€ä¸ªå¤‡ç”¨æœºå™¨ä¸Šç»§ç»­è®­ç»ƒã€‚
å…¶ä¸­å°†192.168.0.65ä½œä¸ºmasterèŠ‚ç‚¹ï¼Œ192.168.0.20å’Œ192.168.0.63ä½œä¸ºworkerèŠ‚ç‚¹ã€‚
192.168.0.65192.168.0.20192.168.0.63
èŠ‚ç‚¹æ•…éšœå®è·µ
è®­ç»ƒæ‹‰èµ·
kubectl apply -f deepseek-ckpt.yamlkubectl get node -owidekubectl get pod -owide

å¯ä»¥çœ‹å‡ºè®­ç»ƒå†åœ¨masterèŠ‚ç‚¹192.168.0.65å’ŒworkerèŠ‚ç‚¹192.168.0.63ä¸Šæ‹‰èµ·ã€‚

ckptä¿å­˜
æŒç»­è®­ç»ƒï¼Œå¹¶åœ¨interval50æ­¥ä¿å­˜ckptä¿¡æ¯ï¼Œä¹‹åç»§ç»­è®­ç»ƒ


æ¨¡æ‹ŸèŠ‚ç‚¹æ•…éšœ
è¿›å…¥192.168.0.63æœºå™¨ï¼Œæ¨¡æ‹ŸèŠ‚ç‚¹æ•…éšœï¼Œåœæ‰k8sæœåŠ¡
systemctl stop kubelet

åœæ‰ååœ¨k8sçš„masterèŠ‚ç‚¹æŸ¥çœ‹nodeçŠ¶æ€ï¼Œæ˜¯å¦ä¸ºNotReady

æ­¤æ—¶192.168.0.63å·²ç»å‡ºç°æ•…éšœï¼Œå†å»æŸ¥çœ‹æ‹‰èµ·è®­ç»ƒçš„podä¿¡æ¯
ç”±äº192.168.0.63å‡ºç°æ•…éšœï¼Œpytorchjobä¼šæ ¹æ®hostfileçš„èŠ‚ç‚¹ä¿¡æ¯ï¼Œå†é‡æ–°å°è¯•è°ƒåº¦åˆ°å¦å¤–çš„æœºå™¨ä¸Š192.168.0.20ï¼Œå¯ä»¥çœ‹å‡ºåœ¨192.168.0.20é‡æ–°è¿›è¡Œäº†Initã€PodInitializingã€Runningçš„è¿‡ç¨‹ã€‚


æŸ¥çœ‹ç»­è®­log


è‡³æ­¤ï¼ŒèŠ‚ç‚¹æ•…éšœæ–­ç‚¹ç»­è®­å®è·µå®Œæˆã€‚
æ¨¡æ‹Ÿå®Œæˆåï¼Œé‡æ–°å°†192.168.0.63èŠ‚ç‚¹æ¢å¤
systemctl start kubelet


]]></content>
      <categories>
        <category>æ¨¡å‹é€‚é…</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>æ¨¡å‹é€‚é…</tag>
        <tag>Nvidia</tag>
        <tag>Deepseek</tag>
        <tag>Megatron-LM</tag>
      </tags>
  </entry>
  <entry>
    <title>A100é›†ç¾¤6æœº48å¡-åŸºäºk8sçš„Colossal-AI llama2 70Bè®­ç»ƒæ“ä½œæ–‡æ¡£</title>
    <url>/2025/11/13/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Llama2/A100%E9%9B%86%E7%BE%A46%E6%9C%BA48%E5%8D%A1-%E5%9F%BA%E4%BA%8Ek8s%E7%9A%84Colossal-AI%20llama2%2070B%E8%AE%AD%E7%BB%83%E6%93%8D%E4%BD%9C%E6%96%87%E6%A1%A3/</url>
    <content><![CDATA[æ³¨ï¼šè¯¥é¡¹ç›®ä¸ºæœ¬äººæ”¯æ’‘å®¢æˆ·é¡¹ç›®ï¼Œæ–‡æ¡£å†…æ¶‰åŠåˆ°çš„å®¢æˆ·ã€è®¾å¤‡ä¿¡æ¯ç­‰å·²è„±æ•å¤„ç†ã€‚
åŸºç¡€ç¯å¢ƒè¯´æ˜
æ¦‚è¿°
æ¨¡å‹åç§°: Colossal-AI llama2 70B
æ¨¡å‹å‚æ•°: 70Bå‚æ•°
ç¡¬ä»¶éœ€æ±‚: 6å°æœåŠ¡å™¨ï¼Œæ¯å°æœåŠ¡å™¨é…å¤‡8å¼ NVIDIA A100 GPU
ç¡¬ä»¶é…ç½®
GPUå‹å·: NVIDIA A100
GPUæ•°é‡: 6å°æœåŠ¡å™¨ x 8å¼ /å° = 48å¼ GPU
å†…å­˜å®¹é‡: æ¯å¼ A100 GPUæ‹¥æœ‰80GBæ˜¾å­˜
CPU: IntelÂ® XeonÂ® Gold 5218R CPU @ 2.10GHzï¼Œx86_64
å­˜å‚¨: é«˜é€ŸSSDå­˜å‚¨ï¼Œç”¨äºæ•°æ®è¯»å†™
ç½‘ç»œ: é«˜é€Ÿç½‘ç»œè¿æ¥ï¼Œæ”¯æŒå¤šæœºè®­ç»ƒ
GPUç¯å¢ƒè¯´æ˜
é©±åŠ¨ç‰ˆæœ¬: 470.141.03
CUDAç‰ˆæœ¬: CUDA 11.4
Nvidia-smiç‰ˆæœ¬ï¼š470.141.03
NCCL: ç”¨äºå¤šGPUå’Œå¤šèŠ‚ç‚¹é€šä¿¡
æŒä¹…æ€§æ¨¡å¼: Persistence-Ml
åŠŸç‡ä½¿ç”¨ä¸Šé™: åŠŸç‡ä¸Šé™ä¸º400W
æ˜¾å­˜ï¼šæ˜¾å­˜æ€»é‡ä¸º81920MiB (80GB)
è½¯ä»¶ç¯å¢ƒ
æ“ä½œç³»ç»Ÿ: Linux (CentOS Linux 7 (Core))
Pythonç‰ˆæœ¬: Python 3.8 æˆ–æ›´é«˜ç‰ˆæœ¬
ä¾èµ–åº“:
PyTorch 2.1
Colossal-AI: æ”¯æŒ70Bå‚æ•°æ¨¡å‹ç‰ˆæœ¬
å…¶ä»–ä¾èµ–: NumPy, SciPy, Pandasç­‰
æ¨¡å‹å‚æ•°å¤§å°
å‚æ•°æ€»æ•°: 70B
æ¨¡å‹å ç”¨å†…å­˜: ç”±äºGPUæ˜¾å­˜å ç”¨ä¼šæ ¹æ®æ¨¡å‹çš„å‚æ•°é‡ã€æ¨¡å‹çš„å†…å­˜å ç”¨ã€è®­ç»ƒæ—¶çš„batch sizeã€å¹¶è¡Œä¼˜åŒ–æŠ€æœ¯ç­‰ä¸åŒè€Œå‘ç”Ÿå˜åŒ–ï¼Œç°å‚æ•°æœªå®šï¼Œæš‚ä¸èƒ½é‡åŒ–GPUå ç”¨æƒ…å†µã€‚
æ¨¡å‹è®­ç»ƒæµç¨‹
æ³¨ï¼šç”±äºxxx.xx.xx.141æœºå™¨æ—¥å¸¸æœ‰ä¸šåŠ¡åœ¨è·‘ï¼Œæ— æ³•ç©ºé—²å‡ºå®Œæ•´çš„8å—gpuèµ„æºï¼Œæ•…ä»¥ä¸‹æœ€å¤šä»¥5æœº40å¡æ¥æ‹‰èµ·è®­ç»ƒã€‚
æŸ¥çœ‹A100æœºå™¨çš„ipä¿¡æ¯
cat /root/a100.txt
æŸ¥çœ‹å¾…è®­ç»ƒèŠ‚ç‚¹çš„GPUå ç”¨æƒ…å†µ
é€šè¿‡137æœºå™¨ï¼Œåˆ†åˆ«sshè¿›å…¥ä»¥ä¸Š6å°æœºå™¨ï¼ˆå·²è®¾ç½®å…å¯†ç™»å½•ï¼‰ï¼ŒæŸ¥çœ‹å¯¹åº”æœºå™¨çš„gpuä½¿ç”¨æƒ…å†µnvidia-smiã€ç½‘å¡é…ç½®ä¿¡æ¯ifconfigç­‰ã€‚
ç¤ºä¾‹ä¸€ï¼š
ssh root@xxx.xx.xx.141 sshè¿›å…¥æœºå™¨141
nvidia-smi141æœºå™¨çš„gpuå·²ç»è¢«å ç”¨ï¼Œè‹¥æ˜¯æœ‰éœ€æ±‚è¦ä½¿ç”¨è¯¥æœºå™¨ï¼Œéœ€è¦æå‰æ²Ÿé€š
exit ä»141æœºå™¨é€€å‡ºåˆ°137æœºå™¨ä¸Š
ç¤ºä¾‹äºŒï¼š
ssh root@xxx.xx.xx.78
nvidia-smi 78æœºå™¨çš„gpuæ²¡æœ‰è¢«ä½¿ç”¨ï¼Œå¯ä»¥è¿›è¡Œæ¨¡å‹è®­ç»ƒä»»åŠ¡
ç‰©æ–™å‡†å¤‡å·¥ä½œ
æ‰€æœ‰çš„ç‰©æ–™ä½ç½®å­˜å‚¨åœ¨xxx.xx.xx137æœºå™¨ä¸Šçš„/home/disk_sdb0/llama2ä¸‹ã€‚
ç‰©æ–™åŒ…å«åŸå§‹æ•°æ®é›†æ–‡ä»¶å¤¹raw_dataï¼Œç¦»çº¿åˆ‡åˆ†å®Œæˆçš„æ•°æ®é›†æ–‡ä»¶å¤¹datasã€é•œåƒllama2-70b.tarã€70Bæ¨¡å‹æƒé‡Llama-2-70b-hfã€13Bæ¨¡å‹æƒé‡Llama-2-13b-hfã€7Bæ¨¡å‹æƒé‡Llama-2-7b-hfã€k8sæ‹‰èµ·é¢„è®­ç»ƒçš„é…ç½®æ–‡ä»¶llama2-70b-test.yamlã€k8sæ‹‰èµ·å¾®è°ƒè®­ç»ƒçš„é…ç½®æ–‡ä»¶llama2-70b-finetune.yamlï¼Œé¢„è®­ç»ƒ/å¾®è°ƒçš„è®­ç»ƒä»£ç train.pyã€é¢„è®­ç»ƒçš„é…ç½®è„šæœ¬train.example.shã€å¾®è°ƒè®­ç»ƒçš„é…ç½®è„šæœ¬finetune.example.shã€èŠ‚ç‚¹ipçš„ä¿¡æ¯é…ç½®æ–‡ä»¶hostfile.txtï¼Œæ­¤å¤–è¿˜éœ€è¦åˆ›å»ºä¸€ä¸ªç›®å½•outputæ¥å­˜æ”¾è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¾“å‡ºæ—¥å¿—åŠcheckpointä¿¡æ¯mkdir outputã€‚

è®­ç»ƒç¯å¢ƒé…ç½®å‡†å¤‡
/etc/hostsé…ç½®
6å°æœºå™¨éƒ½éœ€è¦å¯¹vim /etc/hostsæ–‡ä»¶è¿›è¡Œä¿®æ”¹ï¼ŒåŠ å…¥å¯¹åº”çš„ IP åœ°å€å’Œä¸å…¶å¯¹åº”çš„ä¸»æœºåæˆ–åŸŸåçš„æ˜ å°„å…³ç³»ï¼Œä»¥æ­¤ä¿è¯å„ä¸»æœºä¹‹é—´é€šä¿¡æ­£å¸¸ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦åŠ å…¥ç™¾åº¦é•œåƒä»“åº“åœ°å€çš„ipæ˜ å°„ã€‚å¦åˆ™ä¼šæŠ¥socketé€šä¿¡é”™è¯¯ã€‚
socket.gaierror: [Errno -2] Name or service not known
xxx.xx.xx.103 xxx.xx.xx..baidu.comxxx.xx.xx137 xxx-xx-xx-xxx-137xxx.xx.xx.78 xxx-xx-xx-78.XXX-stack.comxxx.xx.xx.79 xxx-xx-xx-79.XXX-stack.comxxx.xx.xx.80 xxx-xx-xx-80.XXX-stack.comxxx.xx.xx.141 xxx-xx-xx-141.XXX-stack.comxxx.xx.xx.142 xxx-xx-xx-142.XXX-stack.comxxx.xx.xx.143 xxx-xx-xx-143.XXX-stack.com
k8så‘½åç©ºé—´åˆ›å»º
å› ä¸ºæœªä½¿ç”¨ç™¾åº¦çš„æµ‹è¯•å¹³å°æ¥æ‹‰è®­ç»ƒï¼Œé€šè¿‡k8sæ¥æ‹‰èµ·ï¼Œæ‰€ä»¥éœ€è¦è‡ªå®šä¹‰ä¸€ä¸ªnamespaceï¼Œå¹¶åœ¨æ”¹nsä¸‹å¯åŠ¨podæ‹‰èµ·è®­ç»ƒä»»åŠ¡ã€‚è¯¥nséœ€è¦ä¸llama2-70b-test.yamlä¸­çš„namespace: llama2-70b-testå¯¹åº”ã€‚
kubectl create namespace llama2-70b-test
kubectl get ns
é•œåƒpushè‡³ä»“åº“
é•œåƒéœ€pushåˆ°ç™¾åº¦æŒ‡å®šçš„ä»“åº“ä¸­
å°†é•œåƒloadä¹‹åï¼Œå¹¶å°†é•œåƒpushè‡³ xxx.xx.xxâ€¦baidu.com ä»“åº“ä¸­ã€‚
    1. `docker load -i llama2-70b.tar`
    2. `docker images`
    3. `docker tag 9cb8e972304f xxx.xx.xx..baidu.com/public/llama2-70b:v1.0`
    4. `docker login xxx.xx.xx..baidu.com -u admin -p XXXXXXX`
    5. `docker push xxx.xx.xx..baidu.com/public/llama2-70b:v1.0`

hostfileé…ç½®
é€šè¿‡sshç™»å½•åˆ°5å°æœºå™¨ï¼ŒæŸ¥çœ‹æ˜¯å¦æœ‰gpuå ç”¨ï¼Œæ‹‰èµ·è®­ç»ƒæ—¶è¦ä¿è¯èŠ‚ç‚¹çš„8ä¸ªgpuéƒ½æ²¡æœ‰è¢«å ç”¨ã€‚å°†ç©ºé—²çš„æœºå™¨bond4çš„ipå†™å…¥åˆ°hostfileä¸­ã€‚bond4çš„ipåœ°å€å¯ä»¥é€šè¿‡ifconfigæ¥è¿›è¡ŒæŸ¥çœ‹ï¼Œæœ‰çš„æœºå™¨ä½¿ç”¨çš„æ˜¯ä»¥å¤ªç½‘eth0çš„ipé…ç½®ã€‚
xxx.xx.xx.142xxx.xx.xx.143xxx.xx.xx.78xxx.xx.xx.79xxx.xx.xx.80
è®­ç»ƒè„šæœ¬
ä¿®æ”¹é¢„è®­ç»ƒå¯åŠ¨çš„è„šæœ¬train.example.shï¼Œç½‘å¡çš„socketé€šä¿¡é…ç½®è¦ä¿®æ”¹ä¸ºbond4ï¼Œå¹¶åœ¨æ‹‰èµ·çš„è®­ç»ƒçš„èŠ‚ç‚¹ä¸­é€‰å–ä¸€ä¸ªä½œä¸ºmatserä¸»èŠ‚ç‚¹--master_addr
#!/bin/bash# NCCL IB environment variablesexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1export NCCL_IB_DISABLE=0export NCCL_SOCKET_IFNAME=bond4export NCCL_IB_GID_INDEX=3export NCCL_IB_TIMEOUT=23export NCCL_IB_RETRY_CNT=7export OMP_NUM_THREADS=8PROJECT_NAME=&quot;llama2-70b-pt&quot;PARENT_SAVE_DIR=&quot;/opt/tmp/ColossalAI/output/save/&quot;PARENT_TENSORBOARD_DIR=&quot;/opt/tmp/ColossalAI/output/tensorboard/&quot;PARENT_CONFIG_FILE=&quot;/opt/tmp/ColossalAI/output/config/&quot;PRETRAINED_MODEL_PATH=&quot;/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf&quot;declare -a dataset=(    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009&quot;)TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)FULL_PROJECT_NAME=&quot;$&#123;PROJECT_NAME&#125;-$&#123;TIMESTAMP&#125;&quot;SAVE_DIR=&quot;$&#123;PARENT_SAVE_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;TENSORBOARD_DIR=&quot;$&#123;PARENT_TENSORBOARD_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;CONFIG_FILE=&quot;$&#123;PARENT_CONFIG_FILE&#125;$&#123;FULL_PROJECT_NAME&#125;.json&quot;colossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.78 --master_port 30015 train.py \    --pretrained $PRETRAINED_MODEL_PATH \    --dataset $&#123;dataset[@]&#125; \    --plugin &quot;gemini_auto&quot; \    --save_interval 400 \    --save_dir $SAVE_DIR \    --tensorboard_dir $TENSORBOARD_DIR \    --config_file $CONFIG_FILE \    --num_epochs 1 \    --micro_batch_size 1 \    --lr 1e-4 \    --mixed_precision &quot;fp16&quot; \    --grad_clip 1.0 \    --weight_decay 0.01 \    --warmup_steps 100 \    --use_grad_checkpoint \    --max_length 4096 \    --accumulation_steps 1 \    --pad_token &quot;unk&quot;
å¯åŠ¨k8sçš„yaml
ä¿®æ”¹k8sæ‹‰èµ·çš„é…ç½®æ–‡ä»¶llama2-70b-test.yaml
apiVersion: apps/v1kind: Deploymentmetadata:  namespace: llama2-70b-test  name: llama2-70b-pt # Deployment çš„åç§°spec:  replicas: 2 # å‰¯æœ¬æ•°é‡  selector:    matchLabels:      app: llama2-70b-pt  template:    metadata:      labels:        app: llama2-70b-pt    spec:      hostNetwork: true      containers:      - name: llama2-70b-container # å®¹å™¨çš„åç§°        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0        imagePullPolicy: IfNotPresent        command: [&quot;/bin/bash&quot;, &quot;-c&quot;]        args: [&quot;cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ &amp;&amp; chmod +x start.sh &amp;&amp; ./start.sh &amp;&amp; chmod +x train.example.sh &amp;&amp; ./train.example.sh&quot;] # å¯åŠ¨å®¹å™¨åæ‰§è¡Œçš„å‘½ä»¤ï¼Œåˆ‡æ¢ç›®å½•å¹¶è¿è¡Œè®­ç»ƒè„šæœ¬        resources:          limits:            nvidia.com/gpu: 8 # é™åˆ¶ä½¿ç”¨ 8 ä¸ª GPUï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´          requests:            nvidia.com/gpu: 8        volumeMounts:        - name: colossalai-orgdataset          mountPath: /opt/tmp/ColossalAI/datas/org_dataset # å°†å®¹å™¨å†…æ•°æ®ç›®å½•æŒ‚è½½        - name: colossalai-weight          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # å°†å®¹å™¨å†…æƒé‡ç›®å½•æŒ‚è½½        - name: colossalai-train          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py          subPath: train.py          readOnly: true # è¡¨ç¤ºå°†æŒ‚è½½çš„æ–‡ä»¶è®¾ç½®ä¸ºåªè¯»æ¨¡å¼ï¼Œè¿™æ„å‘³ç€å®¹å™¨å†…çš„è¿›ç¨‹åªèƒ½è¯»å–æ–‡ä»¶ï¼Œè€Œä¸èƒ½ä¿®æ”¹å®ƒ        - name: colossalai-output          mountPath: /opt/tmp/ColossalAI/output # å°†å®¹å™¨å†…è¾“å‡ºç›®å½•æŒ‚è½½        - name: host-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # å°†å®¹å™¨å†…hostfile          subPath: hostfile.txt          readOnly: true # è¡¨ç¤ºå°†æŒ‚è½½çš„æ–‡ä»¶è®¾ç½®ä¸ºåªè¯»æ¨¡å¼ï¼Œè¿™æ„å‘³ç€å®¹å™¨å†…çš„è¿›ç¨‹åªèƒ½è¯»å–æ–‡ä»¶ï¼Œè€Œä¸èƒ½ä¿®æ”¹å®ƒ        - name: train-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # å®¹å™¨å†…train_example.sh          subPath: train.example.sh        - name: shm-volume          mountPath: /dev/shm      nodeSelector:        model: llama2-70b      volumes:      - name: colossalai-orgdataset        hostPath:          path: /home/disk_sdb0/llama2/datas/org_dataset # å®¿ä¸»æœºä¸­ ColossalAI æ•°æ®çš„è·¯å¾„          type: Directory      - name: colossalai-weight        hostPath:          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # å®¿ä¸»æœºä¸­ ColossalAI æƒé‡çš„è·¯å¾„          type: Directory      - name: colossalai-train        hostPath:          path: /home/disk_sdb0/llama2          type: Directory      - name: colossalai-output        hostPath:          path: /home/disk_sdb0/llama2/output # å®¿ä¸»æœºä¸­ ColossalAI è¾“å‡ºçš„è·¯å¾„          type: Directory      - name: host-script        hostPath:          path: /home/disk_sdb0/llama2 # å®¿ä¸»æœºä¸­åŒ…å«è®­ç»ƒè„šæœ¬çš„è·¯å¾„          type: Directory      - name: train-script        hostPath:          path: /home/disk_sdb0/llama2 # å®¿ä¸»æœºä¸­åŒ…å«è®­ç»ƒè„šæœ¬çš„è·¯å¾„          type: Directory      - name: shm-volume        emptyDir:          medium: Memory          sizeLimit: 8Gi
nodeèŠ‚ç‚¹æ‰“æ ‡ç­¾
ç»™å¾…æ‹‰èµ·è®­ç»ƒçš„nodeæ‰“ä¸Šæ ‡ç­¾ï¼ˆé¦–å…ˆéœ€è¦ç¡®è®¤å¾…æ‹‰èµ·è®­ç»ƒçš„nodeçš„çŠ¶æ€æ˜¯å¦Readyï¼‰
æ‰“æ ‡ç­¾æ—¶ï¼Œå…¶ä¸­çš„model=llama2-70bå¯¹åº”çš„æ˜¯llama2-70b-test.yamlæ–‡ä»¶ä¸­çš„nodeSelectoréƒ¨åˆ†ï¼Œéœ€ä¿æŒä¸€è‡´ã€‚
kubectl label nodes xxx-xx-xx-141.XXX-stack.com model=llama2-70b
kubectl label nodes xxx-xx-xx-142.XXX-stack.com model=llama2-70b
kubectl label nodes xxx-xx-xx-143.XXX-stack.com model=llama2-70b
kubectl label nodes xxx-xx-xx-78.XXX-stack.com model=llama2-70b
kubectl label nodes xxx-xx-xx-79.XXX-stack.com model=llama2-70b
kubectl label nodes xxx-xx-xx-80.XXX-stack.com model=llama2-70b
è®­ç»ƒç‰©æ–™ä¼ è¾“ä¸åŒæ­¥
å› ä¸ºå¤šæœºæ‹‰èµ·è®­ç»ƒæ—¶ï¼Œæ¯ä¸ªnodeèŠ‚ç‚¹éƒ½éœ€è¦æœ‰ä¸€ä»½è®­ç»ƒæ‰€éœ€çš„ç‰©æ–™ï¼ˆæƒé‡ã€æ•°æ®é›†ã€è®­ç»ƒè„šæœ¬ç­‰æ–‡ä»¶ï¼‰ï¼Œæ•…éœ€è¦é€šè¿‡rsyncæˆ–scpå‘½ä»¤å°†137æœºå™¨ä¸Šçš„ç‰©æ–™æ–‡ä»¶ä¼ è¾“è‡³å¾…è®­ç»ƒçš„èŠ‚ç‚¹ä¸­ã€‚(é™¤å»å¯åŠ¨k8sæ‰€éœ€çš„llama2-70b-test.yamlé…ç½®æ–‡ä»¶è¦åœ¨matserä¸»èŠ‚ç‚¹ä¸Šï¼Œå…¶ä½™çš„æ‰€æœ‰ç‰©æ–™åœ¨å¾…è®­ç»ƒçš„nodeçš„åŒè·¯å¾„ä¸­/home/disk_sdb0/llama2éƒ½éœ€è¦åŒæ­¥æ‹·è´ä¸€ä»½)
137èŠ‚ç‚¹ä¼ è¾“è‡³79èŠ‚ç‚¹çš„ç¤ºä¾‹ï¼š
scp -r /home/disk_sdb0/llama2/Llama-2-70b-hf root@xxx.xx.xx.79:/home/disk_sdb0/llama2/Llama-2-70b-hf
scp -r /home/disk_sdb0/llama2/datas root@xxx.xx.xx.79:/home/disk_sdb0/llama2/datas
scp -r /home/disk_sdb0/llama2/output root@xxx.xx.xx.79:/home/disk_sdb0/llama2/output
scp -r /home/disk_sdb0/llama2/hostfile.txt root@xxx.xx.xx.79:/home/disk_sdb0/llama2
scp -r /home/disk_sdb0/llama2/train.py root@xxx.xx.xx.79:/home/disk_sdb0/llama2
scp -r /home/disk_sdb0/llama2/train.example.sh root@xxx.xx.xx.79:/home/disk_sdb0/llama2
è®­ç»ƒæ‹‰èµ·
æ‰“å®Œæ ‡ç­¾ä¹‹åï¼Œæ³¨æ„åˆ‡æ¢è‡³cd /home/disk_sdb0/llama2ï¼Œapplyå¯åŠ¨è®­ç»ƒè„šæœ¬ã€‚
kubectl apply -f llama2-70b-test.yaml å¯åŠ¨yamlã€‚
kubectl get pod -n llama2-70b-test -owide æŸ¥çœ‹nsä¸ºllama2-70b-testä¸‹çš„podä¿¡æ¯ã€‚
kubectl describe pod -n llama2-70b-test xxx æŸ¥çœ‹æ¯ä¸ªæ­£åœ¨è¿è¡Œçš„podä¿¡æ¯ï¼Œç”¨æ¥è¿½è¸ªpodçš„è¿è¡ŒçŠ¶æ€ï¼Œçœ‹æ˜¯å¦æœ‰erroræŠ¥é”™ã€‚
kubectl logs xxx -n llama2-70b-test -f å®æ—¶è·Ÿè¸ªpodçš„è®­ç»ƒæ—¥å¿—ï¼ˆæœ€ç»ˆçš„è®­ç»ƒæ—¥å¿—ä¼šåœ¨è®¾å®šçš„ä¸»èŠ‚ç‚¹masterä¸Šæ‰“å°ï¼‰ã€‚
kubectl delete -f llama2-70b-test.yamlåˆ é™¤yamlåï¼Œå°†ç»ˆæ­¢è®­ç»ƒä»»åŠ¡ã€‚
æ¨¡å‹è®­ç»ƒæµ‹è¯•è®°å½•
æŠ¥é”™è®°å½•
nvidia driver ä¸ cudaç‰ˆæœ¬é—®é¢˜
ï¼ˆnvidia driver 470 å’Œ cuda11.4ç‰ˆæœ¬è¿‡ä½ï¼Œæ•…ä¸€ç›´è°ƒä¸åˆ°æœºå™¨GPUçš„èµ„æºï¼Œæ‰€ä»¥åœ¨å¹¶è¡Œç­–ç•¥zeroå’Œgeminiæ—¶å€™ä¼šå»è°ƒåº¦CPUçš„èµ„æºï¼Œç„¶åCPUæ’‘ä¸èµ·æ¥ï¼Œç›´æ¥å°±downæ‰äº†ï¼‰è¯¥é”™è¯¯é€šè¿‡å‡çº§nvidia driverè‡³525.125.06ï¼Œcudaå‡çº§ä¸º12.0æ—¶ï¼Œé—®é¢˜è§£å†³ã€‚

dockerå®¹å™¨æ‹‰èµ·æµ‹è¯•é—®é¢˜
å°è¯•é€šè¿‡è¯¥é•œåƒrunä¸€ä¸ªå®¹å™¨æ¥æµ‹è¯•ï¼Œç”±äºç¼ºå¤±nvidia-container-toolkitï¼Œåç»­å®‰è£…å®Œæˆåï¼Œå†æ¬¡runï¼Œå‘ç°â€“gpus all æœªèƒ½è°ƒç”¨ï¼ˆnvidiaé©±åŠ¨å¤ªä½å¯¼è‡´ï¼‰ã€‚å»æ‰â€“gpus allä¹‹åï¼Œå®¹å™¨å¯åŠ¨åï¼Œè¯†åˆ«ä¸åˆ°GPUã€‚
dpkg -l | grep nvidia-container-toolkit æŸ¥çœ‹æ˜¯å¦å®‰è£…äº†nvidia-container-toolkit
A100éœ€è¦nvidia-fabricmanageræ”¯æŒ
é©±åŠ¨å‡çº§å®Œæˆåï¼Œåœ¨xxx.xx.xx.78æœºå™¨ä¸Šå°è¯•æ‹‰èµ·å•æœº8å¡çš„llama2-7Bæ¨¡å‹çš„é¢„è®­ç»ƒï¼ŒæŠ¥äº†é”™è¯¯ç»æŸ¥é˜…èµ„æ–™åå‘ç°A100éœ€è¦nvidia-fabricmanageræ”¯æŒï¼Œåç»­å®‰è£…å¯¹åº”nvidia driveré©±åŠ¨525.125.06ä¸€è‡´çš„nvidia-fabricmanageråæˆåŠŸæ‹‰èµ·è®­ç»ƒã€‚
é”™è¯¯è§£å†³å‚è€ƒåšå®¢ï¼šhttps://www.cnblogs.com/huadongw/p/16504137.html

å•æœº8å¡çš„llama2-7Bçš„é¢„è®­ç»ƒæˆåŠŸæ‹‰èµ·

5æœº40å¡é¢„è®­ç»ƒæ—¶å­˜åœ¨æŸè¿›ç¨‹éšæœºå ç”¨gpuèµ„æºé—®é¢˜
åœ¨æ‹‰èµ·5æœº40å¡æ—¶ï¼Œä¸€ç›´å­˜åœ¨ä¸€ä¸ªnsä¸ºsingle-gpuçš„podè¢«è°ƒåº¦å¹¶å ç”¨ä¸€ä¸ªgpuèµ„æºï¼ˆåˆ é™¤ä¹‹åä¼šéšæœºåœ¨6å°æœºå™¨ä¸Šå ç”¨ä¸€ä¸ªgpuï¼‰
kubectl describe node xxx-xx-xx-142.XXX-stack.comæŸ¥çœ‹142èŠ‚ç‚¹æƒ…å†µ
æ­£å¸¸æƒ…å†µä¸‹ï¼Œä¸€å°æœºå™¨çš„8å°gpuéƒ½å¤„äºç©ºé—²çŠ¶æ€ä¸‹ï¼Œnvidia.com/gpuçš„å ç”¨åº”è¯¥æ˜¯0
è§£å†³æ–¹æ³•ï¼šåˆ é™¤æ‰å¯åŠ¨è¯¥podçš„å¯¹åº”çš„labelï¼ˆgpu-a100ï¼‰å¹¶åˆ é™¤æ‰è¯¥pod
kubectl label nodes xxx-xx-xx-141.XXX-stack.com gpu-
kubectl label nodes xxx-xx-xx-142.XXX-stack.com gpu-
kubectl label nodes xxx-xx-xx-143.XXX-stack.com gpu-
kubectl label nodes xxx-xx-xx-78.XXX-stack.com gpu-
kubectl label nodes xxx-xx-xx-79.XXX-stack.com gpu-
kubectl label nodes xxx-xx-xx-80.XXX-stack.com gpu-
è®­ç»ƒæ‹‰èµ·æµ‹è¯•
é¢„è®­ç»ƒ
llama2-13B 2æœº16å¡

å¯åŠ¨k8sçš„yamlé…ç½®æ–‡ä»¶

apiVersion: apps/v1kind: Deploymentmetadata:  namespace: llama2-70b-test  name: llama2-70b-pt # Deployment çš„åç§°spec:  replicas: 2 # å‰¯æœ¬æ•°é‡  selector:    matchLabels:      app: llama2-70b-pt  template:    metadata:      labels:        app: llama2-70b-pt    spec:      hostNetwork: true      containers:      - name: llama2-70b-container # å®¹å™¨çš„åç§°        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0        imagePullPolicy: IfNotPresent        command: [&quot;/bin/bash&quot;, &quot;-c&quot;]        args: [&quot;cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ &amp;&amp; chmod +x train.example.sh &amp;&amp; ./train.example.sh&quot;] # å¯åŠ¨å®¹å™¨åæ‰§è¡Œçš„å‘½ä»¤ï¼Œåˆ‡æ¢ç›®å½•å¹¶è¿è¡Œè®­ç»ƒè„šæœ¬        resources:          limits:            nvidia.com/gpu: 8 # é™åˆ¶ä½¿ç”¨ 8 ä¸ª GPUï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´          requests:            nvidia.com/gpu: 8        volumeMounts:        - name: colossalai-weight          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # å°†å®¹å™¨å†…æƒé‡ç›®å½•æŒ‚è½½        - name: colossalai-train          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py          subPath: train.py          readOnly: true # è¡¨ç¤ºå°†æŒ‚è½½çš„æ–‡ä»¶è®¾ç½®ä¸ºåªè¯»æ¨¡å¼ï¼Œè¿™æ„å‘³ç€å®¹å™¨å†…çš„è¿›ç¨‹åªèƒ½è¯»å–æ–‡ä»¶ï¼Œè€Œä¸èƒ½ä¿®æ”¹å®ƒ        - name: colossalai-output          mountPath: /opt/tmp/ColossalAI/output # å°†å®¹å™¨å†…è¾“å‡ºç›®å½•æŒ‚è½½        - name: host-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # å°†å®¹å™¨å†…hostfile          subPath: hostfile.txt          readOnly: true # è¡¨ç¤ºå°†æŒ‚è½½çš„æ–‡ä»¶è®¾ç½®ä¸ºåªè¯»æ¨¡å¼ï¼Œè¿™æ„å‘³ç€å®¹å™¨å†…çš„è¿›ç¨‹åªèƒ½è¯»å–æ–‡ä»¶ï¼Œè€Œä¸èƒ½ä¿®æ”¹å®ƒ        - name: train-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # å®¹å™¨å†…train_example.sh          subPath: train.example.sh        - name: shm-volume          mountPath: /dev/shm      nodeSelector:        model: llama2-70b      volumes:      - name: colossalai-weight        hostPath:          path: /home/disk_sdb0/llama2/Llama-2-13b-hf # å®¿ä¸»æœºä¸­ ColossalAI æƒé‡çš„è·¯å¾„          type: Directory      - name: colossalai-train        hostPath:          path: /home/disk_sdb0/llama2          type: Directory      - name: colossalai-output        hostPath:          path: /home/disk_sdb0/llama2/output # å®¿ä¸»æœºä¸­ ColossalAI è¾“å‡ºçš„è·¯å¾„          type: Directory      - name: host-script        hostPath:          path: /home/disk_sdb0/llama2 # å®¿ä¸»æœºä¸­åŒ…å«è®­ç»ƒè„šæœ¬çš„è·¯å¾„          type: Directory      - name: train-script        hostPath:          path: /home/disk_sdb0/llama2 # å®¿ä¸»æœºä¸­åŒ…å«è®­ç»ƒè„šæœ¬çš„è·¯å¾„          type: Directory      - name: train-script        hostPath:          path: /home/disk_sdb0/llama2 # å®¿ä¸»æœºä¸­åŒ…å«è®­ç»ƒè„šæœ¬çš„è·¯å¾„          type: Directory      - name: shm-volume        emptyDir:          medium: Memory          sizeLimit: 8Gi

è®­ç»ƒè„šæœ¬

#!/bin/bash# NCCL IB environment variablesexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1export NCCL_IB_DISABLE=0export NCCL_SOCKET_IFNAME=bond4export NCCL_IB_GID_INDEX=3export NCCL_IB_TIMEOUT=23export NCCL_IB_RETRY_CNT=7export OMP_NUM_THREADS=8PROJECT_NAME=&quot;llama2-70b-pt&quot;PARENT_SAVE_DIR=&quot;/opt/tmp/ColossalAI/output/save/&quot;PARENT_TENSORBOARD_DIR=&quot;/opt/tmp/ColossalAI/output/tensorboard/&quot;PARENT_CONFIG_FILE=&quot;/opt/tmp/ColossalAI/output/config/&quot;PRETRAINED_MODEL_PATH=&quot;/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf&quot;declare -a dataset=(    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009&quot;)TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)FULL_PROJECT_NAME=&quot;$&#123;PROJECT_NAME&#125;-$&#123;TIMESTAMP&#125;&quot;SAVE_DIR=&quot;$&#123;PARENT_SAVE_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;TENSORBOARD_DIR=&quot;$&#123;PARENT_TENSORBOARD_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;CONFIG_FILE=&quot;$&#123;PARENT_CONFIG_FILE&#125;$&#123;FULL_PROJECT_NAME&#125;.json&quot;colossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.78 --master_port 30015 train.py \    --pretrained $PRETRAINED_MODEL_PATH \    --dataset $&#123;dataset[@]&#125; \    --plugin &quot;gemini_auto&quot; \    --save_interval 400 \    --save_dir $SAVE_DIR \    --tensorboard_dir $TENSORBOARD_DIR \    --config_file $CONFIG_FILE \    --num_epochs 1 \    --micro_batch_size 8 \    --lr 1e-4 \    --mixed_precision &quot;fp16&quot; \    --grad_clip 1.0 \    --weight_decay 0.01 \    --warmup_steps 100 \    --use_grad_checkpoint \    --max_length 512 \    --accumulation_steps 1 \    --pad_token &quot;unk&quot;

hostfile.txt

xxx.xx.xx.78xxx.xx.xx.79

è®­ç»ƒæ‹‰èµ·æˆªå›¾




78ï¼Œ79æ˜¾å­˜å ç”¨æƒ…å†µ



llama2-70B 2æœº16å¡

å‚æ•°é…ç½®




micro_batch_size
max_length
plugin
mixed_precision
tflops




8
512
gemini_auto
fp16
64.4685




å¯åŠ¨k8sçš„yamlé…ç½®æ–‡ä»¶

apiVersion: apps/v1kind: Deploymentmetadata:  namespace: llama2-70b-test  name: llama2-70b-pt # Deployment çš„åç§°spec:  replicas: 2 # å‰¯æœ¬æ•°é‡  selector:    matchLabels:      app: llama2-70b-pt  template:    metadata:      labels:        app: llama2-70b-pt    spec:      hostNetwork: true      containers:      - name: llama2-70b-container # å®¹å™¨çš„åç§°        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0        imagePullPolicy: IfNotPresent        command: [&quot;/bin/bash&quot;, &quot;-c&quot;]        args: [&quot;cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ &amp;&amp; chmod +x train.example.sh &amp;&amp; ./train.example.sh&quot;] # å¯åŠ¨å®¹å™¨åæ‰§è¡Œçš„å‘½ä»¤ï¼Œåˆ‡æ¢ç›®å½•å¹¶è¿è¡Œè®­ç»ƒè„šæœ¬        resources:          limits:            nvidia.com/gpu: 8 # é™åˆ¶ä½¿ç”¨ 4 ä¸ª GPUï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´          requests:            nvidia.com/gpu: 8        volumeMounts:        - name: colossalai-weight          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # å°†å®¹å™¨å†…æƒé‡ç›®å½•æŒ‚è½½        - name: colossalai-train          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py          subPath: train.py          readOnly: true # è¡¨ç¤ºå°†æŒ‚è½½çš„æ–‡ä»¶è®¾ç½®ä¸ºåªè¯»æ¨¡å¼ï¼Œè¿™æ„å‘³ç€å®¹å™¨å†…çš„è¿›ç¨‹åªèƒ½è¯»å–æ–‡ä»¶ï¼Œè€Œä¸èƒ½ä¿®æ”¹å®ƒ        - name: colossalai-output          mountPath: /opt/tmp/ColossalAI/output # å°†å®¹å™¨å†…è¾“å‡ºç›®å½•æŒ‚è½½        - name: host-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # å°†å®¹å™¨å†…hostfile          subPath: hostfile.txt          readOnly: true # è¡¨ç¤ºå°†æŒ‚è½½çš„æ–‡ä»¶è®¾ç½®ä¸ºåªè¯»æ¨¡å¼ï¼Œè¿™æ„å‘³ç€å®¹å™¨å†…çš„è¿›ç¨‹åªèƒ½è¯»å–æ–‡ä»¶ï¼Œè€Œä¸èƒ½ä¿®æ”¹å®ƒ        - name: train-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # å®¹å™¨å†…train_example.sh          subPath: train.example.sh        - name: profiler-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/performance_evaluator.py  # å®¹å™¨å†…train_example.sh          subPath: performance_evaluator.py        - name: shm-volume          mountPath: /dev/shm      nodeSelector:        model: llama2-70b      volumes:      - name: colossalai-weight        hostPath:          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # å®¿ä¸»æœºä¸­ ColossalAI æƒé‡çš„è·¯å¾„          type: Directory      - name: colossalai-train        hostPath:          path: /home/disk_sdb0/llama2          type: Directory      - name: colossalai-output        hostPath:          path: /home/disk_sdb0/llama2/output # å®¿ä¸»æœºä¸­ ColossalAI è¾“å‡ºçš„è·¯å¾„          type: Directory      - name: host-script        hostPath:          path: /home/disk_sdb0/llama2 # å®¿ä¸»æœºä¸­åŒ…å«è®­ç»ƒè„šæœ¬çš„è·¯å¾„          type: Directory      - name: train-script        hostPath:          path: /home/disk_sdb0/llama2 # å®¿ä¸»æœºä¸­åŒ…å«è®­ç»ƒè„šæœ¬çš„è·¯å¾„          type: Directory      - name: profiler-script        hostPath:            path: /home/disk_sdb0/llama2 # å®¿ä¸»æœºä¸­åŒ…å«è®­ç»ƒè„šæœ¬çš„è·¯å¾„            type: Directory      - name: shm-volume        emptyDir:          medium: Memory          sizeLimit: 8Gi

è®­ç»ƒè„šæœ¬

#!/bin/bash# NCCL IB environment variablesexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1export NCCL_IB_DISABLE=0export NCCL_SOCKET_IFNAME=bond4export NCCL_IB_GID_INDEX=3export NCCL_IB_TIMEOUT=23export NCCL_IB_RETRY_CNT=7export OMP_NUM_THREADS=8PROJECT_NAME=&quot;llama2-70b-pt&quot;PARENT_SAVE_DIR=&quot;/opt/tmp/ColossalAI/output/save/&quot;PARENT_TENSORBOARD_DIR=&quot;/opt/tmp/ColossalAI/output/tensorboard/&quot;PARENT_CONFIG_FILE=&quot;/opt/tmp/ColossalAI/output/config/&quot;PRETRAINED_MODEL_PATH=&quot;/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf&quot;declare -a dataset=(    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009&quot;)TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)FULL_PROJECT_NAME=&quot;$&#123;PROJECT_NAME&#125;-$&#123;TIMESTAMP&#125;&quot;SAVE_DIR=&quot;$&#123;PARENT_SAVE_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;TENSORBOARD_DIR=&quot;$&#123;PARENT_TENSORBOARD_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;CONFIG_FILE=&quot;$&#123;PARENT_CONFIG_FILE&#125;$&#123;FULL_PROJECT_NAME&#125;.json&quot;colossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.78 --master_port 30015 train.py \    --pretrained $PRETRAINED_MODEL_PATH \    --dataset $&#123;dataset[@]&#125; \    --plugin &quot;gemini_auto&quot; \    --save_interval 400 \    --save_dir $SAVE_DIR \    --tensorboard_dir $TENSORBOARD_DIR \    --config_file $CONFIG_FILE \    --num_epochs 1 \    --micro_batch_size 8 \    --lr 1e-4 \    --mixed_precision &quot;fp16&quot; \    --grad_clip 1.0 \    --weight_decay 0.01 \    --warmup_steps 100 \    --use_grad_checkpoint \    --max_length 512 \    --accumulation_steps 1 \    --pad_token &quot;unk&quot;

hostfile.txt

xxx.xx.xx.78xxx.xx.xx.79

è®­ç»ƒæ‹‰èµ·æˆªå›¾




78ï¼Œ79æ˜¾å­˜å ç”¨æƒ…å†µ



llama2-70B 5æœº40å¡

é¢„è®­ç»ƒæ•°æ®é›†


ç”±äºæ¨¡å‹éœ€è¦å¯¹é€šè¿‡è„šæœ¬å¯¹rawåŸå§‹æ•°æ®é›†è¿›è¡Œåˆ‡åˆ†ï¼Œè¿™ä¸ªæ­¥éª¤ä¼šç‰¹åˆ«è€—æ—¶ï¼Œä¼šéšç€æ•°æ®é›†å¤§å°çš„å¢åŠ è€Œå¢åŠ åˆ‡åˆ†æ—¶é•¿ï¼Œç”±äºæ¯æ¬¡æ‹‰èµ·è®­ç»ƒæ—¶éƒ½éœ€è¦åˆ‡åˆ†å¹¶æ¶ˆè€—å¾ˆå¤šæ—¶é—´ï¼Œæ‰€ä»¥é‡‡ç”¨äº†ç¦»çº¿åˆ‡åˆ†çš„æ–¹å¼å°†æ•°æ®é›†ç¦»çº¿åˆ‡åˆ†å¥½å¹¶ç›´æ¥æŒ‚è½½è¯»å–ä½¿ç”¨ã€‚
ç¦»çº¿åˆ‡åˆ†å¥½çš„æ•°æ®é›†æ–‡ä»¶å…±211Gï¼Œå…¶ä¸­ä¼šå°†åŸå§‹æ•°æ®é›†åˆ‡åˆ†æˆ10ä»½å¹¶ä¸ºarrowæ ¼å¼æ–‡ä»¶ã€‚


å¯åŠ¨k8sçš„yamlé…ç½®æ–‡ä»¶

apiVersion: apps/v1kind: Deploymentmetadata:  namespace: llama2-70b-test  name: llama2-70b-pt # Deployment çš„åç§°spec:  replicas: 5 # å‰¯æœ¬æ•°é‡  selector:    matchLabels:      app: llama2-70b-pt  template:    metadata:      labels:        app: llama2-70b-pt    spec:      hostNetwork: true      containers:      - name: llama2-70b-container # å®¹å™¨çš„åç§°        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0        imagePullPolicy: IfNotPresent        command: [&quot;/bin/bash&quot;, &quot;-c&quot;]        args: [&quot;cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ &amp;&amp; chmod +x train.example.sh &amp;&amp; ./train.example.sh&quot;] # å¯åŠ¨å®¹å™¨åæ‰§è¡Œçš„å‘½ä»¤ï¼Œåˆ‡æ¢ç›®å½•å¹¶è¿è¡Œè®­ç»ƒè„šæœ¬        resources:          limits:            nvidia.com/gpu: 8 # é™åˆ¶ä½¿ç”¨ 8 ä¸ª GPUï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´          requests:            nvidia.com/gpu: 8        volumeMounts:        - name: colossalai-orgdataset          mountPath: /opt/tmp/ColossalAI/datas/org_dataset # å°†å®¹å™¨å†…æ•°æ®ç›®å½•æŒ‚è½½        - name: colossalai-predataset          mountPath: /opt/tmp/ColossalAI/datas/pre_dataset2        - name: colossalai-weight          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # å°†å®¹å™¨å†…æƒé‡ç›®å½•æŒ‚è½½        - name: colossalai-train          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py          subPath: train.py          readOnly: true # è¡¨ç¤ºå°†æŒ‚è½½çš„æ–‡ä»¶è®¾ç½®ä¸ºåªè¯»æ¨¡å¼ï¼Œè¿™æ„å‘³ç€å®¹å™¨å†…çš„è¿›ç¨‹åªèƒ½è¯»å–æ–‡ä»¶ï¼Œè€Œä¸èƒ½ä¿®æ”¹å®ƒ        - name: colossalai-output          mountPath: /opt/tmp/ColossalAI/output # å°†å®¹å™¨å†…è¾“å‡ºç›®å½•æŒ‚è½½        - name: host-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # å°†å®¹å™¨å†…hostfile          subPath: hostfile.txt          readOnly: true # è¡¨ç¤ºå°†æŒ‚è½½çš„æ–‡ä»¶è®¾ç½®ä¸ºåªè¯»æ¨¡å¼ï¼Œè¿™æ„å‘³ç€å®¹å™¨å†…çš„è¿›ç¨‹åªèƒ½è¯»å–æ–‡ä»¶ï¼Œè€Œä¸èƒ½ä¿®æ”¹å®ƒ        - name: train-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # å®¹å™¨å†…train_example.sh          subPath: train.example.sh        - name: shm-volume          mountPath: /dev/shm      nodeSelector:        model: llama2-70b      volumes:      - name: colossalai-orgdataset        hostPath:          path: /home/disk_sdb0/llama2/datas/org_dataset # å®¿ä¸»æœºä¸­ ColossalAI æ•°æ®çš„è·¯å¾„          type: Directory      - name: colossalai-predataset        hostPath:          path: /home/disk_sdb0/llama2/datas/pre_dataset2          type: Directory      - name: colossalai-weight        hostPath:          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # å®¿ä¸»æœºä¸­ ColossalAI æƒé‡çš„è·¯å¾„          type: Directory      - name: colossalai-train        hostPath:          path: /home/disk_sdb0/llama2          type: Directory      - name: colossalai-output        hostPath:          path: /home/disk_sdb0/llama2/output # å®¿ä¸»æœºä¸­ ColossalAI è¾“å‡ºçš„è·¯å¾„          type: Directory      - name: host-script        hostPath:          path: /home/disk_sdb0/llama2 # å®¿ä¸»æœºä¸­åŒ…å«è®­ç»ƒè„šæœ¬çš„è·¯å¾„          type: Directory      - name: train-script        hostPath:          path: /home/disk_sdb0/llama2 # å®¿ä¸»æœºä¸­åŒ…å«è®­ç»ƒè„šæœ¬çš„è·¯å¾„          type: Directory      - name: profiler-script        hostPath:            path: /home/disk_sdb0/llama2 # å®¿ä¸»æœºä¸­åŒ…å«è®­ç»ƒè„šæœ¬çš„è·¯å¾„            type: Directory      - name: shm-volume        emptyDir:          medium: Memory          sizeLimit: 8Gi

è®­ç»ƒè„šæœ¬

#!/bin/bash# NCCL IB environment variablesexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1export NCCL_IB_DISABLE=0export NCCL_SOCKET_IFNAME=bond4export NCCL_IB_GID_INDEX=3export NCCL_IB_TIMEOUT=23export NCCL_IB_RETRY_CNT=7export OMP_NUM_THREADS=8PROJECT_NAME=&quot;llama2-70b-pt&quot;PARENT_SAVE_DIR=&quot;/opt/tmp/ColossalAI/output/save/&quot;PARENT_TENSORBOARD_DIR=&quot;/opt/tmp/ColossalAI/output/tensorboard/&quot;PARENT_CONFIG_FILE=&quot;/opt/tmp/ColossalAI/output/config/&quot;PRETRAINED_MODEL_PATH=&quot;/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf&quot;declare -a dataset=(    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009&quot;)TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)FULL_PROJECT_NAME=&quot;$&#123;PROJECT_NAME&#125;-$&#123;TIMESTAMP&#125;&quot;SAVE_DIR=&quot;$&#123;PARENT_SAVE_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;TENSORBOARD_DIR=&quot;$&#123;PARENT_TENSORBOARD_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;CONFIG_FILE=&quot;$&#123;PARENT_CONFIG_FILE&#125;$&#123;FULL_PROJECT_NAME&#125;.json&quot;colossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.142 --master_port 30015 train.py \    --pretrained $PRETRAINED_MODEL_PATH \    --dataset $&#123;dataset[@]&#125; \    --plugin &quot;gemini_auto&quot; \    --save_interval 1600 \    --save_dir $SAVE_DIR \    --tensorboard_dir $TENSORBOARD_DIR \    --config_file $CONFIG_FILE \    --num_epochs 1 \    --micro_batch_size 16 \    --lr 1e-4 \    --mixed_precision &quot;fp16&quot; \    --grad_clip 1.0 \    --weight_decay 0.01 \    --warmup_steps 100 \    --use_grad_checkpoint \    --max_length 512 \    --accumulation_steps 1 \    --pad_token &quot;unk&quot;

hostfile.txt

xxx.xx.xx.142xxx.xx.xx.143xxx.xx.xx.78xxx.xx.xx.79xxx.xx.xx.80

å‚æ•°é…ç½®è°ƒä¼˜




micro_batch_size
max_length
plugin
mixed_precision
tflops




1
512
gemini_auto
fp16
10.8231


2
512
gemini_auto
fp16
20.4685


4
512
gemini_auto
fp16
45.6885


8
512
gemini_auto
fp16
78.6885


16
512
gemini_auto
fp16
136.5975


16
512
zero2
fp16
OOM


16
512
3d
fp16
OOM


1
1024
gemini_auto
fp16
OOM


4
2048
gemini_auto
fp16
OOM


16
1024
gemini_auto
fp16
OOM




è®­ç»ƒæ‹‰èµ·æˆªå›¾


æ€»ä½“step1519


142ï¼Œ143ï¼Œ78ï¼Œ79ï¼Œ80çš„GPUæ˜¾å­˜å ç”¨







XXXå¹³å°ç›‘æ§

6å°æœºå™¨çš„GPUæ€§èƒ½ç›‘æ§
æ¶‰åŠå®¢æˆ·ä¿¡æ¯ï¼Œå›¾ç‰‡ç•¥å»ã€‚
å•æœºå™¨141çš„GPUæ€§èƒ½ç›‘æ§
æ¶‰åŠå®¢æˆ·ä¿¡æ¯ï¼Œå›¾ç‰‡ç•¥å»ã€‚

æ–­ç‚¹ç»­è®­

7.4æ—¥æ™šä¸Šç”±äº142ã€143ã€78ã€79ã€80äº”å°æœºå™¨çš„sshæ²¡æœ‰è®¾ç½®å…å¯†ç™»å½•ï¼Œæ¨¡å‹é¢„è®­ç»ƒè¿‡ç¨‹ä¸­åœ¨ä¸»èŠ‚ç‚¹142ä¿å­˜çš„checkpointæ–‡ä»¶ä¸èƒ½åŠæ—¶åŒæ­¥ç»™å…¶ä»–143ã€78ã€79ã€80å››ä¸ªèŠ‚ç‚¹ï¼Œæ•…å¯¼è‡´äº†è®­ç»ƒä¸­æ–­ï¼Œç°äº”å°æœºå™¨çš„sshå·²äº’ç›¸å…å¯†ï¼Œcheckpointä¿¡æ¯å¯ä»¥é€šè¿‡ä¸»èŠ‚ç‚¹142æ¥ä¸‹å‘å…±äº«ã€‚
ä¹‹åè®¾ç½®è¯»å–æ–­ç‚¹400æ­¥åä¿å­˜çš„checkpointä¸æƒé‡ä¿¡æ¯ï¼Œæ‹‰èµ·è®­ç»ƒæ—¶ä¼šä»ç¬¬400æ­¥å¼€å§‹ç»§ç»­è®­ç»ƒã€‚
åŠ å…¥--load_checkpoint $PARENT_LOAD_DIR \å‚æ•°ï¼Œè®¾å®šåŠ è½½çš„checkpointä½ç½®PARENT_LOAD_DIR=&quot;/opt/tmp/ColossalAI/output/save/llama2-70b-pt-2024-07-03-14-11-44/epoch-0_step-400&quot;
#!/bin/bash# NCCL IB environment variablesexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1export NCCL_IB_DISABLE=0export NCCL_SOCKET_IFNAME=bond4export NCCL_IB_GID_INDEX=3export NCCL_IB_TIMEOUT=23export NCCL_IB_RETRY_CNT=7export OMP_NUM_THREADS=8PROJECT_NAME=&quot;llama2-70b-pt&quot;PARENT_SAVE_DIR=&quot;/opt/tmp/ColossalAI/output/save/&quot;PARENT_LOAD_DIR=&quot;/opt/tmp/ColossalAI/output/save/llama2-70b-pt-2024-07-03-14-11-44/epoch-0_step-400&quot;PARENT_TENSORBOARD_DIR=&quot;/opt/tmp/ColossalAI/output/tensorboard/&quot;PARENT_CONFIG_FILE=&quot;/opt/tmp/ColossalAI/output/config/&quot;PRETRAINED_MODEL_PATH=&quot;/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf&quot;declare -a dataset=(    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009&quot;)TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)FULL_PROJECT_NAME=&quot;$&#123;PROJECT_NAME&#125;-$&#123;TIMESTAMP&#125;&quot;SAVE_DIR=&quot;$&#123;PARENT_SAVE_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;TENSORBOARD_DIR=&quot;$&#123;PARENT_TENSORBOARD_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;CONFIG_FILE=&quot;$&#123;PARENT_CONFIG_FILE&#125;$&#123;FULL_PROJECT_NAME&#125;.json&quot;colossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.142 --master_port 30015 train.py \    --pretrained $PRETRAINED_MODEL_PATH \    --dataset $&#123;dataset[@]&#125; \    --plugin &quot;gemini_auto&quot; \    --save_interval 1600 \    --save_dir $SAVE_DIR \    --load_checkpoint $PARENT_LOAD_DIR \    --tensorboard_dir $TENSORBOARD_DIR \    --config_file $CONFIG_FILE \    --num_epochs 1 \    --micro_batch_size 16 \    --lr 1e-4 \    --mixed_precision &quot;fp16&quot; \    --grad_clip 1.0 \    --weight_decay 0.01 \    --warmup_steps 100 \    --use_grad_checkpoint \    --max_length 512 \    --accumulation_steps 1 \    --pad_token &quot;unk&quot;




é¢„è®­ç»ƒç»“æŸ

ç»“æŸåä¼šåœ¨è®¾å®šçš„ä¸»èŠ‚ç‚¹142ä¸Šç”Ÿæˆå¯¹åº”æƒé‡æ–‡ä»¶


å¾®è°ƒ
llama2-70B 5æœº40å¡

æ•°æ®é›†å‡†å¤‡

å¾®è°ƒæ•°æ®é›†å‚ç…§Colossalaiå®˜ç½‘è¦æ±‚çš„æ•°æ®å¯¹è¯æ ¼å¼è¿›è¡Œå‡†å¤‡
æ•°æ®æ ·å¼

æ•°æ®é›†çš„ç¦»çº¿åˆ‡åˆ†ï¼Œéœ€è¦é€šè¿‡kubectlè¿›å…¥podå†…æ‰§è¡Œåˆ‡åˆ†è„šæœ¬prepare_sft_dataset.pyè¿›è¡Œåˆ‡åˆ†
#!/bin/bashpython3 prepare_sft_dataset.py \--data_input_dirs &quot;/opt/tmp/ColossalAI/applications/Colossal-LLaMA/data&quot; \--tokenizer_dir &quot;/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf&quot; \--data_output_dirs &quot;/opt/tmp/ColossalAI/applications/Colossal-LLaMA/sftdata&quot; \--max_length 4096 \--num_spliced_dataset_bins 10 \--llama_version 2
æ‰§è¡Œåˆ‡åˆ†è„šæœ¬åï¼Œå¼€å§‹è¿›è¡Œåˆ‡åˆ†ï¼Œä¼šå°†åŸå§‹æ•°æ®é›†æ ¼å¼jsonlè½¬ä¸ºarrowæ ¼å¼

tar -cvf sftdata.tar sftdata å‹ç¼©åˆ‡åˆ†å¥½çš„æ•°æ®é›†sftdata
kubectl cp llama2-70b-test/llama2-70b-pt-f5877d7c9-n7448:/opt/tmp/ColossalAI/applications/Colossal-LLaMA/sftdata.tar /home/disk_sdb0/llama2/sftdata.taré€šè¿‡kubectl cpå°†å‹ç¼©å¥½çš„sftdata.tarä¼ è¾“è‡³å®¿ä¸»æœº137ä¸Š
tar -xvf sftdata.tarè§£å‹
åˆ‡åˆ†å¥½çš„æ•°æ®é›†ä¸º52G


å¯åŠ¨k8sçš„yamlçš„é…ç½®æ–‡ä»¶

apiVersion: apps/v1kind: Deploymentmetadata:  namespace: llama2-70b-test  name: llama2-70b-pt # Deployment çš„åç§°spec:  replicas: 5 # å‰¯æœ¬æ•°é‡  selector:    matchLabels:      app: llama2-70b-pt  template:    metadata:      labels:        app: llama2-70b-pt    spec:      hostNetwork: true      containers:      - name: llama2-70b-container # å®¹å™¨çš„åç§°        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0        imagePullPolicy: IfNotPresent        command: [&quot;/bin/bash&quot;, &quot;-c&quot;]        args: [&quot;cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ &amp;&amp; chmod +x finetune.example.sh &amp;&amp; ./finetune.example.sh&quot;] # å¯åŠ¨å®¹å™¨åæ‰§è¡Œçš„å‘½ä»¤ï¼Œåˆ‡æ¢ç›®å½•å¹¶è¿è¡Œè®­ç»ƒè„šæœ¬        resources:          limits:            nvidia.com/gpu: 8 # é™åˆ¶ä½¿ç”¨ 8 ä¸ª GPUï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´          requests:            nvidia.com/gpu: 8        volumeMounts:        - name: colossalai-sftdataset          mountPath: /opt/tmp/ColossalAI/datas/sftdata        - name: colossalai-weight          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # å°†å®¹å™¨å†…æƒé‡ç›®å½•æŒ‚è½½        - name: colossalai-train          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py          subPath: train.py          readOnly: true # è¡¨ç¤ºå°†æŒ‚è½½çš„æ–‡ä»¶è®¾ç½®ä¸ºåªè¯»æ¨¡å¼ï¼Œè¿™æ„å‘³ç€å®¹å™¨å†…çš„è¿›ç¨‹åªèƒ½è¯»å–æ–‡ä»¶ï¼Œè€Œä¸èƒ½ä¿®æ”¹å®ƒ        - name: colossalai-output          mountPath: /opt/tmp/ColossalAI/output # å°†å®¹å™¨å†…è¾“å‡ºç›®å½•æŒ‚è½½        - name: host-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # å°†å®¹å™¨å†…hostfile          subPath: hostfile.txt          readOnly: true # è¡¨ç¤ºå°†æŒ‚è½½çš„æ–‡ä»¶è®¾ç½®ä¸ºåªè¯»æ¨¡å¼ï¼Œè¿™æ„å‘³ç€å®¹å™¨å†…çš„è¿›ç¨‹åªèƒ½è¯»å–æ–‡ä»¶ï¼Œè€Œä¸èƒ½ä¿®æ”¹å®ƒ        - name: finetune-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/finetune.example.sh  # å®¹å™¨å†…finetune.example.sh          subPath: finetune.example.sh        - name: shm-volume          mountPath: /dev/shm      nodeSelector:        model: llama2-70b      volumes:      - name: colossalai-sftdataset        hostPath:          path: /home/disk_sdb0/llama2/datas/sftdata          type: Directory      - name: colossalai-weight        hostPath:          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # å®¿ä¸»æœºä¸­ ColossalAI æƒé‡çš„è·¯å¾„          type: Directory      - name: colossalai-train        hostPath:          path: /home/disk_sdb0/llama2          type: Directory      - name: colossalai-output        hostPath:          path: /home/disk_sdb0/llama2/output # å®¿ä¸»æœºä¸­ ColossalAI è¾“å‡ºçš„è·¯å¾„          type: Directory      - name: host-script        hostPath:          path: /home/disk_sdb0/llama2 # å®¿ä¸»æœºä¸­åŒ…å«è®­ç»ƒè„šæœ¬çš„è·¯å¾„          type: Directory      - name: finetune-script        hostPath:          path: /home/disk_sdb0/llama2 # å®¿ä¸»æœºä¸­åŒ…å«è®­ç»ƒè„šæœ¬çš„è·¯å¾„          type: Directory      - name: shm-volume        emptyDir:          medium: Memory          sizeLimit: 8Gi

å¾®è°ƒè„šæœ¬

#!/bin/bash# NCCL IB environment variablesexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1export NCCL_IB_DISABLE=0export NCCL_SOCKET_IFNAME=bond4export NCCL_IB_GID_INDEX=3export NCCL_IB_TIMEOUT=23export NCCL_IB_RETRY_CNT=7export OMP_NUM_THREADS=8#export NCCL_P2P_DISABLE=1export NCCL_DEBUG=INFOPROJECT_NAME=&quot;llama2-70b-ft&quot;PARENT_SAVE_DIR=&quot;/opt/tmp/ColossalAI/output/save/&quot;PARENT_TENSORBOARD_DIR=&quot;/opt/tmp/ColossalAI/output/tensorboard/&quot;PARENT_CONFIG_FILE=&quot;/opt/tmp/ColossalAI/output/config/&quot;PRETRAINED_MODEL_PATH=&quot;/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf&quot;declare -a dataset=(    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00000&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00001&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00002&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00003&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00004&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00005&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00006&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00007&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00008&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00009&quot;)TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)FULL_PROJECT_NAME=&quot;$&#123;PROJECT_NAME&#125;-$&#123;TIMESTAMP&#125;&quot;SAVE_DIR=&quot;$&#123;PARENT_SAVE_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;TENSORBOARD_DIR=&quot;$&#123;PARENT_TENSORBOARD_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;CONFIG_FILE=&quot;$&#123;PARENT_CONFIG_FILE&#125;$&#123;FULL_PROJECT_NAME&#125;.json&quot;colossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.142 --master_port 30013 train.py \    --pretrained $PRETRAINED_MODEL_PATH \    --dataset $&#123;dataset[@]&#125; \    --plugin &quot;gemini&quot; \    --save_interval 400 \    --save_dir $SAVE_DIR \    --tensorboard_dir $TENSORBOARD_DIR \    --config_file $CONFIG_FILE \    --num_epochs 1 \    --accumulation_steps 1 \    --micro_batch_size 1 \    --lr 5e-5 \    --mixed_precision &quot;bf16&quot; \    --grad_clip 1.0 \    --weight_decay 0.01 \    --warmup_steps 100 \    --use_grad_checkpoint \    --use_neft \    --pad_token &quot;eos&quot;

hostfile.txt

xxx.xx.xx.142xxx.xx.xx.143xxx.xx.xx.78xxx.xx.xx.79xxx.xx.xx.80

æ‹‰èµ·å¾®è°ƒè®­ç»ƒ

#æŸ¥çœ‹podçš„è¯¦ç»†ä¿¡æ¯kubectl get pod -n llama2-70b-test -owide#åˆ é™¤å¯åŠ¨å¾®è°ƒè®­ç»ƒçš„yamlkubectl delete -f llama2-70b-finetune.yaml#å¯åŠ¨å¾®è°ƒè®­ç»ƒçš„yamlkubectl apply -f llama2-70b-finetune.yaml#æŸ¥çœ‹podæ—¥å¿—kubectl logs -n llama2-70b-test llama2-70b-pt-7b5884d87c-pmd76 -f#æŸ¥çœ‹podçŠ¶æ€ä¿¡æ¯kubectl describe pod -n llama2-70b-test llama2-70b-pt-6599d788c7-7nngz


å‚æ•°é…ç½®åŠè°ƒä¼˜




micro_batch_size
accumulation_steps
plugin
mixed_precision
tflops




1
1
gemini
bf16
38.8488


4
1
gemini
bf16
OOM


8
1
gemini
bf16
OOM


1
1
gemini_auto
bf16
OOM


8
1
gemini_auto
bf16
OOM


16
1
gemini_auto
bf16
OOM


1
1
zero2
bf16
OOM


1
1
3d
bf16
OOM


1
4
gemini
bf16
OOM


1
8
gemini
bf16
OOM




142ï¼Œ143ï¼Œ78ï¼Œ79ï¼Œ80çš„GPUæ˜¾å­˜å ç”¨







å¤‡æ³¨

åœ¨æ‹‰èµ·å¾®è°ƒè®­ç»ƒæ—¶ï¼Œå°è¯•è°ƒæ•´äº†è®­ç»ƒå‚æ•°micro_batch_sizeã€accumulation_stepsã€å¹¶è¡Œç­–ç•¥geminiã€gemini_autoã€zero2ç­‰å‚æ•°æ¥å°½é‡ç¼©å°æ€»çš„è¿­ä»£stepæ­¥æ•°ï¼ˆæ•°æ®é›†çš„å¤§å°ä¹Ÿä¼šå½±å“stepæ•°ï¼‰ï¼Œä½†å°è¯•å¢åŠ äº†micro_batch_sizeç­‰å‚æ•°åï¼Œå‘ç°è®­ç»ƒä¼šæ‹‰ä¸èµ·æ¥ï¼ˆout of memoryï¼‰ï¼Œå½“å‰5æœº40å¡å¯ä»¥æˆåŠŸæ‹‰èµ·çš„å‚æ•°åªèƒ½å°†micro_batch_sizeè°ƒæ•´ä¸º1ï¼Œaccumulation_stepsä¸º1ã€å¹¶è¡Œç­–ç•¥ä¸ºgeminiï¼Œæ€»çš„è¿­ä»£æ­¥æ•°ä¸º35132æ­¥ï¼ˆæ¯æ­¥è¿­ä»£è€—æ—¶éœ€è¦3åˆ†é’Ÿå·¦å³ï¼Œåœ¨æ¯ä¸€æ­¥è¿­ä»£ä¸­ï¼Œé¦–å…ˆè¿›è¡Œå‰å‘ä¼ æ’­æ¥è®¡ç®—é¢„æµ‹è¾“å‡ºå’ŒæŸå¤±ï¼Œç„¶åè¿›è¡Œåå‘ä¼ æ’­æ¥è®¡ç®—æ¢¯åº¦ï¼Œæœ€åæ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°ã€‚æ¯ä¸ªæ‰¹æ¬¡éƒ½ä¼šè¿›è¡Œä¸€æ¬¡æ¢¯åº¦å’ŒæŸå¤±çš„è®¡ç®—ï¼Œç›´åˆ°å®Œæˆä¸€ä¸ªepochçš„è®­ç»ƒã€‚ï¼‰
ç±»ä¼¼å‚ç…§æŸæ¨¡å‹175Bï¼ˆåƒå¡è§„æ¨¡ï¼‰çš„æ‹‰èµ·çš„è®­ç»ƒè¿›åº¦ï¼Œæ€»çš„è¿­ä»£æ­¥æ•°ä¸º574833ã€‚

ç±»ä¼¼å‚ç…§æŸæ¨¡å‹57Bï¼ˆåƒå¡è§„æ¨¡ï¼‰çš„æ‹‰èµ·è®­ç»ƒè¿›åº¦ï¼Œæ€»çš„è¿­ä»£æ­¥æ•°574636ã€‚

æ•…å®Œæˆæ•´å¥—å¤§æ¨¡å‹çš„è®­ç»ƒä¼šæŒç»­è¾ƒé•¿çš„è®­ç»ƒå‘¨æœŸï¼Œå¹¶åœ¨ç°æœ‰çš„å¡æ•°é™åˆ¶ä¸Šï¼Œåœ¨çŸ­æœŸå†…å®ç°æ‰€æœ‰æµç¨‹å­˜åœ¨éš¾åº¦ã€‚
ç»“è®ºä¸è¯´æ˜
ç°æœ‰ç¯å¢ƒè®­ç»ƒæ¨¡å‹å‚æ•°å¤§å°åŠæ˜¾å­˜å ç”¨
ç°æœ‰çš„5æœº40å¡é¢„è®­ç»ƒ/å¾®è°ƒllama2-70Bï¼Œæ¨¡å‹å‚æ•°åŠæ˜¾å­˜å ç”¨å¦‚ä¸‹è¡¨æ ¼æ‰€ç¤ºï¼ˆæ³¨ï¼šç”±äºxxx.xx.xx.141æœºå™¨æ—¥å¸¸æœ‰ä¸šåŠ¡åœ¨è·‘ï¼Œæ— æ³•ç©ºé—²å‡ºå®Œæ•´çš„8å—gpuèµ„æºï¼Œæ•…æœ€å¤šä»¥5æœº40å¡æ¥æ‹‰èµ·è®­ç»ƒã€‚ï¼‰




æ¨¡å‹å‚æ•°
æœºå™¨
GPUå¡
æ˜¾å­˜å ç”¨
GPUåˆ©ç”¨ç‡




é¢„è®­ç»ƒ
70B
142
0
77886MiB/81920MiB
99%





1
77898MiB/81920MiB
99%





2
77898MiB/81920MiB
99%





3
77898MiB/81920MiB
99%





4
79946MiB/81920MiB
99%





5
77898MiB/81920MiB
99%





6
77898MiB/81920MiB
99%





7
77874MiB/81920MiB
100%




143
0
77886MiB/81920MiB
100%





1
77898MiB/81920MiB
100%





2
77898MiB/81920MiB
100%





3
77898MiB/81920MiB
99%





4
77898MiB/81920MiB
99%





5
77886MiB/81920MiB
99%





6
77898MiB/81920MiB
99%





7
77886MiB/81920MiB
99%




78
0
78764MiB/81920MiB
99%





1
78776MiB/81920MiB
99%





2
78776MiB/81920MiB
99%





3
78776MiB/81920MiB
99%





4
78776MiB/81920MiB
100%





5
78776MiB/81920MiB
100%





6
78776MiB/81920MiB
100%





7
78764MiB/81920MiB
99%




79
0
77886MiB/81920MiB
99%





1
77898MiB/81920MiB
99%





2
77886MiB/81920MiB
99%





3
77898MiB/81920MiB
100%





4
77898MiB/81920MiB
99%





5
77898MiB/81920MiB
99%





6
77898MiB/81920MiB
100%





7
77886MiB/81920MiB
100%




80
0
77886MiB/81920MiB
99%





1
77898MiB/81920MiB
99%





2
79948MiB/81920MiB
99%





3
77898MiB/81920MiB
99%





4
78350MiB/81920MiB
99%





5
79386MiB/81920MiB
99%





6
79374MiB/81920MiB
100%





7
78338MiB/81920MiB
99%


å¾®è°ƒ
70B
142
0
53278MiB/81920MiB
100%





1
52160MiB/81920MiB
100%





2
53660MiB/81920MiB
100%





3
53164MiB/81920MiB
100%





4
52382MiB/81920MiB
100%





5
53260MiB/81920MiB
100%





6
53660MiB/81920MiB
100%





7
54652MiB/81920MiB
100%




143
0
53494MiB/81920MiB
100%





1
53254MiB/81920MiB
100%





2
53950MiB/81920MiB
100%





3
53172MiB/81920MiB
100%





4
67234MiB/81920MiB
100%





5
53212MiB/81920MiB
100%





6
53160MiB/81920MiB
100%





7
53604MiB/81920MiB
100%




78
0
51978MiB/81920MiB
100%





1
51992MiB/81920MiB
100%





2
51410MiB/81920MiB
100%





3
51492MiB/81920MiB
100%





4
51992MiB/81920MiB
100%





5
51488MiB/81920MiB
100%





6
52154MiB/81920MiB
100%





7
47638MiB/81920MiB
100%




79
0
48468MiB/81920MiB
100%





1
49280MiB/81920MiB
100%





2
49182MiB/81920MiB
100%





3
48980MiB/81920MiB
100%





4
49160MiB/81920MiB
100%





5
49966MiB/81920MiB
100%





6
49160MiB/81920MiB
100%





7
49858MiB/81920MiB
100%




80
0
49366MiB/81920MiB
100%





1
48740MiB/81920MiB
100%





2
49400MiB/81920MiB
99%





3
47470MiB/81920MiB
99%





4
36662MiB/81920MiB
100%





5
37262MiB/81920MiB
100%





6
37660MiB/81920MiB
99%





7
37148MiB/81920MiB
100%



AIå¹³å°è®­ç»ƒå¯è§†åŒ–åˆ†æèƒ½åŠ›
4.1ä¸­æè¿°çš„å…³äºGPUçš„æ˜¾å­˜ä½¿ç”¨åŠåˆ©ç”¨ç‡æƒ…å†µï¼Œä¸»è¦æ˜¯é€šè¿‡nvidia-smiNVIDIAæä¾›çš„å‘½ä»¤è¡Œå·¥å…·ç›‘æ§åˆ†æGPUè®¾å¤‡ä¿¡æ¯ï¼Œå…¶ä¸­çš„ä¿¡æ¯åŒ…æ‹¬

GPU åˆ©ç”¨ç‡ï¼ˆGPU-Utilï¼‰ï¼šæ˜¾ç¤º GPU æ ¸å¿ƒçš„åˆ©ç”¨ç‡ç™¾åˆ†æ¯”ã€‚
æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼ˆMemory-Usageï¼‰ï¼šæ˜¾ç¤º GPU å½“å‰æ˜¾å­˜çš„ä½¿ç”¨é‡/æ€»æ˜¾å­˜é‡ã€‚
åŠŸè€—ï¼ˆPwr:Usage/Capï¼‰ï¼šæ˜¾ç¤º GPU çš„å½“å‰/æœ€å¤§åŠŸè€—ã€‚
æ¸©åº¦ï¼ˆGPU Temperatureï¼‰ï¼šæ˜¾ç¤º GPU çš„å½“å‰æ¸©åº¦ã€‚
è¿›ç¨‹ IDï¼ˆPIDï¼‰ï¼šæ˜¾ç¤ºå½“å‰ä½¿ç”¨ GPU çš„è¿›ç¨‹çš„è¿›ç¨‹ IDã€‚
è¿›ç¨‹åç§°ï¼šæ˜¾ç¤ºä½¿ç”¨ GPU çš„è¿›ç¨‹çš„åç§°ã€‚
ä½¿ç”¨ GPU çš„åº”ç”¨ç¨‹åºï¼šæ˜¾ç¤ºå“ªäº›åº”ç”¨ç¨‹åºæ­£åœ¨ä½¿ç”¨ GPUã€‚
GPU å‹å·ï¼šæ˜¾ç¤º GPU çš„å‹å·å’Œç‰ˆæœ¬ä¿¡æ¯ã€‚
é©±åŠ¨ç‰ˆæœ¬ï¼šæ˜¾ç¤ºå½“å‰å®‰è£…çš„ NVIDIA é©±åŠ¨ç‰ˆæœ¬ã€‚
CUDA ç‰ˆæœ¬ï¼šæ˜¾ç¤ºå½“å‰å®‰è£…çš„ CUDA ç‰ˆæœ¬ã€‚
ECC çŠ¶æ€ï¼šæ˜¾ç¤ºé”™è¯¯æ ¡æ­£ç ï¼ˆECCï¼‰çš„çŠ¶æ€ã€‚

å¯¹æ¯”XXXç›‘æ§å¹³å°çš„æ•°æ®ï¼Œä¸¤è€…åœ¨GPUæ˜¾å­˜ä½¿ç”¨å’ŒGPUåˆ©ç”¨ç‡ä¸Šå¯èƒ½ä¼šå­˜åœ¨å·®å¼‚ï¼Œç”±äºnvidia-smiæ˜¯å®æ—¶ç›‘æ§å½“å‰æ—¶é—´ä¸‹çš„GPUä¿¡æ¯ï¼ŒXXXç›‘æ§å¹³å°çš„ä¿¡æ¯ä¸nvidia-smiç›‘æ§ä¿¡æ¯å­˜åœ¨æ—¶é—´å·®ï¼Œæ•…å­˜åœ¨æŸäº›æ•°æ®ä¸åŒæ­¥ï¼Œè¿™ä¹Ÿåœ¨åˆç†çš„èŒƒå›´å†…ã€‚
XXXç›‘æ§å¹³å°åŒ…å«äº†å„æœºå™¨çš„å•ä¸ªGPUçš„æ˜¾å­˜ä½¿ç”¨ã€GPUåˆ©ç”¨ç‡ã€æ¸©åº¦ã€åŠŸè€—ç­‰ä¿¡æ¯ï¼ŒåŸºæœ¬è¦†ç›–äº†æ¨¡å‹è®­ç»ƒç›‘æ§æ‰€éœ€çš„ä¸€äº›æŒ‡æ ‡ã€‚æ­¤å¤–ï¼ŒXXXä¸­è¿˜æ˜¾ç¤ºäº†GPUçš„æ—¶é’Ÿé¢‘ç‡ï¼Œä¹Ÿä»£è¡¨äº†GPUæ€§èƒ½çš„å…³é”®æŒ‡æ ‡ã€‚
XXXç›‘æ§å¹³å°å¯ä»¥è¿›ä¸€æ­¥ç›‘æ§æœºé—´ç½‘å¡çš„é€šä¿¡å¸¦å®½ã€æœºå†…å•å‘é€šä¿¡å¸¦å®½ã€Leafå±‚/spineå±‚äº¤æ¢æœºä¿¡æ¯ã€æ¨¡å‹è®­ç»ƒå‚æ•°ç­‰ç­‰å¤§æ¨¡å‹è®­ç»ƒçš„ç‰©ç†å±‚ä¸è½¯ä»¶å±‚ä¿¡æ¯ã€‚
ç”¨æˆ·çš„æ˜“ç”¨æ€§,é•¿æ—¶é—´è®­ç»ƒ,ä»»åŠ¡æ–­ç‚¹,ç›‘æ§å‘Šè­¦,è‡ªåŠ¨åŒ–æ‹‰èµ·

æœ¬æ¬¡è®­ç»ƒé‡‡ç”¨k8sçš„æ–¹å¼æ¥ç®¡ç†å’Œæ‹‰èµ·é›†ç¾¤podèŠ‚ç‚¹ï¼Œå…¶ä¸­æ¶‰åŠåˆ°ä¸€äº›æ‹‰èµ·k8sçš„yamlé…ç½®æ–‡ä»¶ã€ä»¥åŠé›†ç¾¤æœºå™¨çš„sshé€šä¿¡é…ç½®ã€etc/hostsä¸»æœºipåæ˜ å°„é…ç½®ã€æ ‡ç­¾ç®¡ç†ç­‰æ­¥éª¤ï¼Œå¯èƒ½éœ€è¦ç”¨æˆ·ç†Ÿæ‚‰å¹¶æ“ä½œã€‚æ­¤å¤–ï¼Œæœºå™¨çš„é©±åŠ¨ç¯å¢ƒå¯ä»¥æŒ‰æœŸæ›´æ–°è¿­ä»£ï¼Œä»¥æ­¤é€‚åº”å’ŒåŒ¹é…æœ€æ–°ç³»åˆ—å¤§æ¨¡å‹è®­ç»ƒæ‰€éœ€è¦çš„æ–°ç‰¹æ€§ã€‚
æœ¬æ¬¡é¢„è®­ç»ƒä»»åŠ¡é•¿è¾¾70h+ï¼Œå…¶ä¸­å­˜åœ¨è®­ç»ƒä¸­æ–­é—®é¢˜ï¼Œå½“å‰çš„æ–¹æ³•æ˜¯é€šè¿‡æ‰‹åŠ¨æ‹‰èµ·å’Œåˆ©ç”¨è®­ç»ƒè„šæœ¬ä¸­save_intervalçš„é—´éš”æ­¥æ•°ä¿å­˜çš„checkpointä¿¡æ¯æ¥æ¢å¤è‡³é—´éš”æ­¥æ•°çš„è®­ç»ƒçŠ¶æ€ã€‚
è®­ç»ƒçš„ç›‘æ§å‘Šè­¦å¯ä»¥é€šè¿‡kubectlå‘½ä»¤æŸ¥çœ‹podkubectl describe podã€nodekubectl describe nodeã€è®­ç»ƒæ—¥å¿—kubectl logsé‡Œçš„é”™è¯¯ä¿¡æ¯è¿›è¡Œæ’é”™å¤„ç†ã€‚
è‡ªåŠ¨åŒ–æ‹‰èµ·å¯èƒ½éœ€è¦EKIï¼ˆk8sç®¡ç†ï¼‰å¹³å°ä¸PyTorchJob æ”¯æŒï¼ŒEKIå¯ä»¥è‡ªåŠ¨åŒ–å¤„ç†podèŠ‚ç‚¹å¼‚å¸¸å¹¶è‡ªåŠ¨æ‹‰èµ·æ–°çš„èŠ‚ç‚¹ã€‚

]]></content>
      <categories>
        <category>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</tag>
        <tag>Nvidia</tag>
        <tag>Llama2</tag>
        <tag>A100</tag>
        <tag>Colossal-AI</tag>
      </tags>
  </entry>
  <entry>
    <title>1.2-Qwenæ¨¡å‹é•œåƒåˆ¶ä½œ</title>
    <url>/2025/11/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Qwen/1.2-Qwen%E6%A8%A1%E5%9E%8B%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/</url>
    <content><![CDATA[ç”±äºæ–°æœºå™¨æŒ‚è½½æ–‡ä»¶å­˜å‚¨é€Ÿåº¦éå¸¸æ…¢ï¼Œé€šè¿‡dockerfileæ–‡ä»¶æ¥ç›´æ¥ç”Ÿæˆé•œåƒéå¸¸æ…¢ï¼Œæ‰€ä»¥æœ¬é•œåƒåœ¨è‡ªå·±çš„V100äº‘ä¸»æœºä¸­è¿›è¡Œæ‰“åŒ…ã€‚
1. é¦–å…ˆdocker pullæ‹‰å–ä¸€ä¸ªubuntuåŸºç¡€ç¯å¢ƒ
https://hub.docker.com/r/nvidia/cuda/tags?page=11&amp;page_size=&amp;name=&amp;ordering=
docker pull nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04
2. å®‰è£…nvidia-container-toolkitï¼Œä½¿dockerå¯ä»¥è°ƒç”¨å®¿ä¸»æœºgpuèµ„æº
2.1 ä¸‹è½½nvidia-container-toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) &amp;&amp; \ curl -fsSL [https://nvidia.github.io/libnvidia-container/gpgkey](https://nvidia.github.io/libnvidia-container/gpgkey) | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg &amp;&amp; \ curl -s -L [https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list](https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list) | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
2.2 å®‰è£…nvidia-container-toolkit
sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit
2.3 æ·»åŠ nvidia-dockeræº
curl -s -L [https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list](https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list) |   sudo tee /etc/apt/sources.list.d/nvidia-docker.list
2.4 æ›´æ–°å¹¶é‡æ–°æ‰§è¡Œå®‰è£…
sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit
å¦‚æœæ‰§è¡Œè¿‡ç¨‹ä¸­æŠ¥W: GPG error: https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64  InRelease: The following signatures couldnâ€™t be verified because the public key is not available: NO_PUBKEY DDCAE044F796ECB0ï¼Œåˆ™éœ€è¦ç¡®è®¤ä½ çš„ç³»ç»Ÿæ˜¯å¦ä¿¡ä»»NVIDIAçš„GPGå¯†é’¥ã€‚å¦‚æœæ²¡æœ‰ï¼Œä½ éœ€è¦å¯¼å…¥å®ƒã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å¯¼å…¥GPGå¯†é’¥ï¼š
curl -s [https://nvidia.github.io/libnvidia-container/gpgkey](https://nvidia.github.io/libnvidia-container/gpgkey) | sudo apt-key add -
2.5 å®Œæˆ nvidia-container-toolkit çš„å®‰è£…ä¹‹åï¼Œæˆ‘ä»¬ç»§ç»­æ‰§è¡Œ nvidia-ctk runtime configure å‘½ä»¤ï¼Œä¸º Docker æ·»åŠ  nvidia è¿™ä¸ªè¿è¡Œæ—¶ã€‚å®Œæˆåï¼Œæˆ‘ä»¬çš„åº”ç”¨å°±èƒ½åœ¨å®¹å™¨ä¸­ä½¿ç”¨æ˜¾å¡èµ„æºäº†
sudo nvidia-ctk runtime configure --runtime=docker

2.6 é‡å¯docker
sudo systemctl restart docker
2.7 æŸ¥çœ‹æ˜¯å¦å®‰è£…æˆåŠŸ
dpkg -l | grep nvidia-container-toolkit

3. æ‹‰å–åŸºç¡€é•œåƒæˆåŠŸåï¼Œåˆ›å»ºä¸€ä¸ªdockerå®¹å™¨
docker images
sudo docker run -it --name qwen --gpus all nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04
exit
4. é€€å‡ºå®¹å™¨åï¼Œå°†æœ¬åœ°è·‘åŒçš„qwenæ¨¡å‹ä»£ç /æƒé‡/æ•°æ®é›†/ç¯å¢ƒcpåˆ°åˆ›å»ºçš„qwené•œåƒä¸­
docker cp /opt/tmp/Qwen/ 02649afd9710:/qwen
5. é‡å¯dockerï¼Œexecæ‰§è¡Œ
docker ps -a
docker start qwen
docker exec -it 02649afd9710 bash
6. å› ä¸ºæƒ³åœ¨å®¹å™¨ä¸­æ‰§è¡Œè‡ªèº«çš„pythonç¯å¢ƒï¼Œä¸å€Ÿç”¨å®¿ä¸»æœºçš„ç¯å¢ƒï¼Œæ‰€ä»¥éœ€è¦å•ç‹¬å†å®‰è£…condaã€pytorchç­‰ç¯å¢ƒã€‚
7. å®‰è£…å®ŒåŸºç¡€ç¯å¢ƒåï¼Œéœ€è¦å®‰è£…qwenæ¨¡å‹çš„ä¾èµ–
pip install -r requirements.txt
8. æ²¡æœ‰vimç¼–è¾‘å™¨è¿˜éœ€å®‰è£…vim
apt-get update
apt-get install -y vim
9. deepspeedå®‰è£…
pip install &quot;peft&lt;0.8.0&quot; deepspeed
10. æ­¤æ—¶åœ¨qwenå®¹å™¨ä¸­æ‰§è¡Œè®­ç»ƒè„šæœ¬ï¼Œæ‹‰èµ·è®­ç»ƒã€‚
bash finetune/finetune_lora_single_gpu.sh
11. å°†æ­¤æ—¶qwenå®¹å™¨æ‰“æˆé•œåƒ
docker commit -a &quot;wangxiangbo&quot; -m &quot;qwen 7B&quot; 02649afd9710 qwen-7b:v1.0

12. å°†æ‰“å¥½çš„é•œåƒè½¬æˆtaråŒ…ï¼Œä¾›æ–°æœºå™¨è§£å‹ä½¿ç”¨
docker save -o qwen-7b.tar qwen-7b:v1.0
13. åŠ è½½taré•œåƒ, ä½¿ç”¨loadè¿›è¡Œä»taræ–‡ä»¶å¯¼å‡ºé•œåƒ
docker load -i qwen-7b.tar
14. ç”±äºæ–°æœºå™¨æŒ‚è½½æ–‡ä»¶å­˜å‚¨çš„åŸå› ï¼Œé•œåƒè§£å‹é€Ÿåº¦å¤ªæ…¢ï¼Œäºæ˜¯æ‰“ç®—å°†æ‰“å¥½çš„qwené•œåƒpushåˆ°é˜¿é‡Œäº‘ä¸ªäººä»“åº“ä¸­ï¼Œåœ¨æ–°æœºå™¨ä¸­ç›´æ¥pullè¯¥é•œåƒã€‚
15. å°†é•œåƒæ¨é€åˆ°Registry
docker login --username=aliyun9599911612 registry.cn-shanghai.aliyuncs.com
docker tag 37c7b97b67f6 registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0
docker push registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0
16. åœ¨3å·æœºå™¨ä¸­ï¼Œæ‹‰å–è¯¥é•œåƒ
docker pull registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0

17. é€šè¿‡è¯¥é•œåƒï¼Œrunä¸€ä¸ªå®¹å™¨
docker run -it --name qwen --gpus all registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0 bash
]]></content>
      <categories>
        <category>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</tag>
        <tag>Nvidia</tag>
        <tag>Qwen</tag>
        <tag>Deepspeed</tag>
      </tags>
  </entry>
  <entry>
    <title>1.1-Qwenæ¨¡å‹æœ¬åœ°éƒ¨ç½²ä¸å•æœºå•å¡/å¤šå¡è®­ç»ƒ</title>
    <url>/2025/11/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Qwen/1.1-Qwen%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%8D%95%E6%9C%BA%E5%8D%95%E5%8D%A1_%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[ä¸€ã€V100ç¯å¢ƒéƒ¨ç½²

é¡¹ç›®åœ°å€ï¼š https://github.com/QwenLM/Qwen
ä¸‹è½½åˆ°æœ¬åœ°
git clone Â https://github.com/QwenLM/Qwen.git 
åŸºç¡€ç¯å¢ƒæ­å»º
conda create -n qwen python=3.10
conda activate qwen
å®‰è£…pytorch
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
å®‰è£…ä¾èµ–ç¯å¢ƒ
pip install -r requirements.txt 
7Bæ¨¡å‹ä¸‹è½½
git clone [https://www.modelscope.cn/qwen/Qwen-7B-Chat.git](https://www.modelscope.cn/qwen/Qwen-7B-Chat.git)
åœ¨ä½¿ç”¨sdkçš„pythonè„šæœ¬ä¸‹è½½æƒé‡æ—¶ï¼Œéœ€è¦pipå®‰è£…modelscope
pip install modelscope
ä½¿ç”¨git cloneå‘ç°æƒé‡æœªä¸‹è½½æˆåŠŸï¼Œä½¿ç”¨modelscopeå®˜æ–¹sdkè„šæœ¬ä¸‹è½½ã€‚
å°†ä»¥ä¸‹ä»£ç å†™å…¥download.pyæ–‡ä»¶ä¸­ï¼Œå¹¶æ‰§è¡Œpython download.py

#æ¨¡å‹ä¸‹è½½from modelscope import snapshot_downloadmodel_dir = snapshot_download(&#x27;qwen/Qwen-7B-Chat&#x27;, cache_dir=&#x27;/opt/tmp/Qwen&#x27;, revision=&#x27;v1.1.9&#x27;)

æ•°æ®é›†ä¸‹è½½
è¿›å…¥modelscopeä¸‹è½½æ•°æ®é›†
https://modelscope.cn/datasets/Robin021/DISC-Law-SFT/files
æ•°æ®æ ¼å¼å¤„ç†
æ•°æ®é›†å¤„ç†åä¼šç”Ÿæˆtrain_data_law.jsonæ–‡ä»¶
head -n 20 train_data_law.json
å¤„ç†ä¹‹åçš„æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š


import json# è¯»å–ä»¥.jsonlç»“å°¾çš„æ–‡ä»¶json_data = []with open(&#x27;/opt/tmp/Qwen/dataset/DISC-Law-SFT-Triplet-released.jsonl&#x27;, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as file:    for line in file:        data = json.loads(line)        json_data.append(data)# å¾…å¡«å…¥çš„æ¨¡æ¿template = []# éå†jsonæ•°æ®é›†for idx, data in enumerate(json_data[:]):    conversation = [        &#123;            &quot;from&quot;:&quot;user&quot;,            &quot;value&quot;: data[&quot;input&quot;]        &#125;,        &#123;            &quot;from&quot;: &quot;assistant&quot;,            &quot;value&quot;: data[&quot;output&quot;]        &#125;    ]    template.append(&#123;        &quot;id&quot;: f&quot;identity_&#123;idx&#125;&quot;,        &quot;conversations&quot;: conversation    &#125;)print(len(template))# è¾“å‡ºå¡«å……æ•°æ®åçš„æ¨¡æ¿print(json.dumps(template[2], ensure_ascii=False, indent=2))# å°†templateå†™å…¥åˆ°æœ¬åœ°æ–‡ä»¶output_file_path = &quot;/opt/tmp/Qwen/train_data_law.json&quot;with open(output_file_path, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as f:    json.dump(template, f, ensure_ascii=False, indent=2)print(f&quot;å¤„ç†å¥½çš„æ•°æ®å·²å†™å…¥åˆ°æœ¬åœ°æ–‡ä»¶: &#123;output_file_path&#125;&quot;)

è®­ç»ƒä¾èµ–å®‰è£…
deepspeedå®‰è£…
pip install &quot;peft&lt;0.8.0&quot; deepspeed
ä¿®æ”¹æ¨¡å‹å¾®è°ƒè„šæœ¬å‚æ•°
ä¿®æ”¹MODELå’ŒDATAçš„è·¯å¾„ï¼ŒåŠper_device_train_batch_size

#!/bin/bashexport CUDA_DEVICE_MAX_CONNECTIONS=1MODEL=&quot;/opt/tmp/Qwen/Qwen-7B-Chat&quot; # Set the path if you do not want to load from huggingface directly# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.# See the section for finetuning in README for more information.DATA=&quot;/opt/tmp/Qwen/train_data_law.json&quot;function usage() &#123;    echo &#x27;Usage: bash finetune/finetune_lora_single_gpu.sh [-m MODEL_PATH] [-d DATA_PATH]&#x27;&#125;while [[ &quot;$1&quot; != &quot;&quot; ]]; do    case $1 in        -m | --model )            shift            MODEL=$1            ;;        -d | --data )            shift            DATA=$1            ;;        -h | --help )            usage            exit 0            ;;        * )            echo &quot;Unknown argument $&#123;1&#125;&quot;            exit 1            ;;    esac    shiftdoneexport CUDA_VISIBLE_DEVICES=0python finetune.py \  --model_name_or_path $MODEL \  --data_path $DATA \  --bf16 False \  --output_dir output_qwen \  --num_train_epochs 5 \  --per_device_train_batch_size 2 \  --per_device_eval_batch_size 1 \  --gradient_accumulation_steps 8 \  --evaluation_strategy &quot;no&quot; \  --save_strategy &quot;steps&quot; \  --save_steps 100 \  --save_total_limit 10 \  --learning_rate 3e-4 \  --weight_decay 0.1 \  --adam_beta2 0.95 \  --warmup_ratio 0.01 \  --lr_scheduler_type &quot;cosine&quot; \  --logging_steps 1 \  --report_to &quot;none&quot; \  --model_max_length 512 \  --lazy_preprocess True \  --gradient_checkpointing \  --use_lora# If you use fp16 instead of bf16, you should use deepspeed# --fp16 True --deepspeed finetune/ds_config_zero2.json

å¼€å¯å•æœºå•å¡è®­ç»ƒ
bash finetune/finetune_lora_single_gpu.sh

å•æœºå¤šå¡è®­ç»ƒï¼ˆåœ¨åˆ†é…çš„3å·æœºå™¨ä¸Šæ‰§è¡Œï¼‰

gpuæ˜¾å­˜åˆ©ç”¨æƒ…å†µ


]]></content>
      <categories>
        <category>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</tag>
        <tag>Nvidia</tag>
        <tag>Qwen</tag>
        <tag>Deepspeed</tag>
      </tags>
  </entry>
  <entry>
    <title>1.3-åŸºäºdockerçš„Qwenå•æœºå•å¡_å¤šå¡è®­ç»ƒ</title>
    <url>/2025/11/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Qwen/1.3-%E5%9F%BA%E4%BA%8Edocker%E7%9A%84Qwen%E5%8D%95%E6%9C%BA%E5%8D%95%E5%8D%A1_%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[1. æŸ¥çœ‹å®¹å™¨
docker ps
2. execè¿›å…¥å®¹å™¨
docker exec -it containerid bash
3. è¿›å…¥qwenç›®å½•å¹¶ä¿®æ”¹finetune_lora_single_gpu.shå‚æ•°
#!/bin/bashexport CUDA_DEVICE_MAX_CONNECTIONS=1MODEL=&quot;/qwen/Qwen-7B-Chat&quot; # Set the path if you do not want to load from huggingface directly# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.# See the section for finetuning in README for more information.DATA=&quot;/qwen/train_data_law.json&quot;function usage() &#123;    echo &#x27;Usage: bash finetune/finetune_lora_single_gpu.sh [-m MODEL_PATH] [-d DATA_PATH]&#x27;&#125;while [[ &quot;$1&quot; != &quot;&quot; ]]; do    case $1 in        -m | --model )            shift            MODEL=$1            ;;        -d | --data )            shift            DATA=$1            ;;        -h | --help )            usage            exit 0            ;;        * )            echo &quot;Unknown argument $&#123;1&#125;&quot;            exit 1            ;;    esac    shiftdoneexport CUDA_VISIBLE_DEVICES=0python finetune.py \  --model_name_or_path $MODEL \  --data_path $DATA \  --bf16 True \  --output_dir output_qwen \  --num_train_epochs 5 \  --per_device_train_batch_size 1 \  --per_device_eval_batch_size 1 \  --gradient_accumulation_steps 8 \  --evaluation_strategy &quot;no&quot; \  --save_strategy &quot;steps&quot; \  --save_steps 100 \  --save_total_limit 10 \  --learning_rate 3e-4 \  --weight_decay 0.1 \  --adam_beta2 0.95 \  --warmup_ratio 0.01 \  --lr_scheduler_type &quot;cosine&quot; \  --logging_steps 1 \  --report_to &quot;none&quot; \  --model_max_length 512 \  --lazy_preprocess True \  --gradient_checkpointing \  --use_lora# If you use fp16 instead of bf16, you should use deepspeed# --fp16 True --deepspeed finetune/ds_config_zero2.json
4. æ‰§è¡Œfinetune_lora_single_gpu.shå•æœºå•å¡
5. ä¿®æ”¹finetune_lora_ds.shå‚æ•°
#!/bin/bashexport CUDA_DEVICE_MAX_CONNECTIONS=1DIR=`pwd`# Guide:# This script supports distributed training on multi-gpu workers (as well as single-worker training).# Please set the options below according to the comments.# For multi-gpu workers training, these options should be manually set for each worker.# After setting the options, please run the script on each worker.# Number of GPUs per GPU workerGPUS_PER_NODE=$(python -c &#x27;import torch; print(torch.cuda.device_count())&#x27;)# Number of GPU workers, for single-worker training, please set to 1NNODES=$&#123;NNODES:-1&#125;# The rank of this worker, should be in &#123;0, ..., WORKER_CNT-1&#125;, for single-worker training, please set to 0NODE_RANK=$&#123;NODE_RANK:-0&#125;# The ip address of the rank-0 worker, for single-worker training, please set to localhostMASTER_ADDR=$&#123;MASTER_ADDR:-localhost&#125;# The port for communicationMASTER_PORT=$&#123;MASTER_PORT:-6001&#125;MODEL=&quot;/qwen/Qwen-7B-Chat&quot; # Set the path if you do not want to load from huggingface directly# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.# See the section for finetuning in README for more information.DATA=&quot;/qwen/train_data_law.json&quot;DS_CONFIG_PATH=&quot;finetune/ds_config_zero2.json&quot;function usage() &#123;    echo &#x27;Usage: bash finetune/finetune_lora_ds.sh [-m MODEL_PATH] [-d DATA_PATH] [--deepspeed DS_CONFIG_PATH]&#x27;&#125;while [[ &quot;$1&quot; != &quot;&quot; ]]; do    case $1 in        -m | --model )            shift            MODEL=$1            ;;        -d | --data )            shift            DATA=$1            ;;        --deepspeed )            shift            DS_CONFIG_PATH=$1            ;;        -h | --help )            usage            exit 0            ;;        * )            echo &quot;Unknown argument $&#123;1&#125;&quot;            exit 1            ;;    esac    shiftdoneDISTRIBUTED_ARGS=&quot;    --nproc_per_node $GPUS_PER_NODE \    --nnodes $NNODES \    --node_rank $NODE_RANK \    --master_addr $MASTER_ADDR \    --master_port $MASTER_PORT&quot;#export CUDA_VISIBLE_DEVICES=2,3torchrun $DISTRIBUTED_ARGS finetune.py \    --model_name_or_path $MODEL \    --data_path $DATA \    --bf16 False \    --output_dir output_qwen \    --num_train_epochs 5 \    --per_device_train_batch_size 2 \    --per_device_eval_batch_size 1 \    --gradient_accumulation_steps 8 \    --evaluation_strategy &quot;no&quot; \    --save_strategy &quot;steps&quot; \    --save_steps 100 \    --save_total_limit 10 \    --learning_rate 3e-4 \    --weight_decay 0.1 \    --adam_beta2 0.95 \    --warmup_ratio 0.01 \    --lr_scheduler_type &quot;cosine&quot; \    --logging_steps 1 \    --report_to &quot;none&quot; \    --model_max_length 512 \    --lazy_preprocess True \    --use_lora \    --gradient_checkpointing \    --deepspeed $&#123;DS_CONFIG_PATH&#125;
6. ä¿®æ”¹deepspeedä¸­ds_config_zero2.jsoné…ç½®æ–‡ä»¶ï¼Œå¢åŠ TFlopsæ˜¾ç¤º
å…¶ä¸­å…·ä½“å¢åŠ çš„å‚æ•°é…ç½®ä¸ºflops_profiler
&quot;flops_profiler&quot;: &#123;        &quot;enabled&quot;: true,        &quot;profile_step&quot;: 1,        &quot;module_depth&quot;: -1,        &quot;top_modules&quot;: 1,        &quot;detailed&quot;: false,        &quot;output_file&quot;: null    &#125;,
&#123;    &quot;fp16&quot;: &#123;        &quot;enabled&quot;: &quot;auto&quot;,        &quot;loss_scale&quot;: 0,        &quot;loss_scale_window&quot;: 1000,        &quot;initial_scale_power&quot;: 16,        &quot;hysteresis&quot;: 2,        &quot;min_loss_scale&quot;: 1    &#125;,    &quot;bf16&quot;: &#123;        &quot;enabled&quot;: &quot;auto&quot;    &#125;,    &quot;optimizer&quot;: &#123;        &quot;type&quot;: &quot;AdamW&quot;,        &quot;params&quot;: &#123;            &quot;lr&quot;: &quot;auto&quot;,            &quot;betas&quot;: &quot;auto&quot;,            &quot;eps&quot;: &quot;auto&quot;,            &quot;weight_decay&quot;: &quot;auto&quot;        &#125;    &#125;,    &quot;scheduler&quot;: &#123;        &quot;type&quot;: &quot;WarmupLR&quot;,        &quot;params&quot;: &#123;            &quot;warmup_min_lr&quot;: &quot;auto&quot;,            &quot;warmup_max_lr&quot;: &quot;auto&quot;,            &quot;warmup_num_steps&quot;: &quot;auto&quot;        &#125;    &#125;,    &quot;zero_optimization&quot;: &#123;        &quot;stage&quot;: 2,        &quot;offload_optimizer&quot;: &#123;            &quot;device&quot;: &quot;none&quot;,            &quot;pin_memory&quot;: true        &#125;,        &quot;allgather_partitions&quot;: true,        &quot;allgather_bucket_size&quot;: 2e8,        &quot;overlap_comm&quot;: true,        &quot;reduce_scatter&quot;: true,        &quot;reduce_bucket_size&quot;: 2e8,        &quot;contiguous_gradients&quot;: true    &#125;,    &quot;flops_profiler&quot;: &#123;        &quot;enabled&quot;: true,        &quot;profile_step&quot;: 1,        &quot;module_depth&quot;: -1,        &quot;top_modules&quot;: 1,        &quot;detailed&quot;: false,        &quot;output_file&quot;: null    &#125;,    &quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,    &quot;gradient_clipping&quot;: &quot;auto&quot;,    &quot;steps_per_print&quot;: 100,    &quot;train_batch_size&quot;: &quot;auto&quot;,    &quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;,    &quot;wall_clock_breakdown&quot;: false&#125;
7. æ‰§è¡Œfinetune_lora_ds.shå•æœºå¤šå¡è®­ç»ƒ
å…¶ä¸­æŠ¥äº†Error while creating shared memory segment /dev/shm/nccl-KXWrmA (size 9637888)å¯¼è‡´åœ¨dockerä¸­å•æœºå¤šå¡æ‹‰èµ·å¤±è´¥
é—®é¢˜åŸå› ï¼šdockerçš„shmå…±äº«å†…å­˜ä¸è¶³ï¼Œå¯ä»¥é€šè¿‡å‘½ä»¤
df -h | grep shmæŸ¥çœ‹å½“å‰å®¹å™¨çš„shmå¤§å°ï¼Œé»˜è®¤ä¸º64Mï¼Œè¿™æ˜¯è¿œè¿œä¸å¤Ÿçš„ï¼Œæ‰€ä»¥è¦å¢åŠ è¯¥å®¹å™¨çš„shmå…±äº«å†…å­˜å¤§å°ã€‚å‚è€ƒåšæ–‡https://blog.csdn.net/gg864461719/article/details/112466585
è§£å†³æ–¹æ³•1ï¼šåˆ›å»ºå®Œå®¹å™¨ä¹‹åï¼Œæ‰‹åŠ¨ä¿®æ”¹shmå…±äº«å†…å­˜å¤§å°
a. é¦–å…ˆè¦å…³é—­docker, å¦åˆ™ä¸‹é¢çš„æ“ä½œæ­¥éª¤ä¼šæ— æ•ˆ.
service docker stop
b. è¿›å…¥å®¿ä¸»æœºä¸­/docker/containers/å®¹å™¨id ä¿®æ”¹è¯¥å®¹å™¨çš„hostconfig.jsonæ–‡ä»¶ï¼ŒæŠŠå…¶ä¸­çš„ShmSizeçš„å¤§å°åé¢å¢åŠ 22ï¼ˆå°±å˜ä¸ºäº†6.3Gï¼‰å…¶é»˜è®¤çš„æ˜¯67108864_KB_ å°±çº¦ç­‰äº64Mã€‚
c. é‡å¯dockeræœåŠ¡
systemctl start docker
d. è§£å†³å®Œä¹‹åï¼Œé‡æ–°æŸ¥çœ‹shmçš„å…±äº«å†…å­˜å¤§å°ï¼Œæ­¤æ—¶å·²ç»å˜ä¸ºäº†6.3G
è§£å†³æ–¹æ³•2ï¼šåœ¨é€šè¿‡é•œåƒrunå®¹å™¨æ—¶ï¼Œå°±ç›´æ¥æŒ‡å®šâ€“shm-size 6G
docker run -it --name qwen --gpus all --shm-size 6G registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0 bash
é‡æ–°æ‹‰èµ·è®­ç»ƒ
bash finetune/finetune_lora_ds.sh
å…¶ä¸­tflopsåœ¨9.5~10.5ä¹‹é—´



]]></content>
      <categories>
        <category>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</tag>
        <tag>Nvidia</tag>
        <tag>Qwen</tag>
        <tag>Deepspeed</tag>
      </tags>
  </entry>
  <entry>
    <title>1.4-åŸºäºk8sæ‹‰èµ·Qwenæ¨¡å‹çš„å¤šæœºå¤šå¡å¾®è°ƒ</title>
    <url>/2025/11/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Qwen/1.4-%E5%9F%BA%E4%BA%8Ek8s%E6%8B%89%E8%B5%B7Qwen%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E5%BE%AE%E8%B0%83/</url>
    <content><![CDATA[ä¸€ã€é•œåƒå‡†å¤‡
1. æŸ¥çœ‹é•œåƒ
docker images

2. ä¿®æ”¹é•œåƒæ ‡ç­¾
docker tag ec99659d9677 registry.paas/library/qwen:v3.0
3. å°†é•œåƒæ¨è‡³ä»“åº“
docker push registry.paas/library/qwen:v3.0
4. å¦‚æœå‡ºç°ç­¾åè®¤è¯å¤±è´¥ï¼Œéœ€è¦ä¿®æ”¹dockerå®ˆæŠ¤è¿›ç¨‹é…ç½®æ–‡ä»¶
vim /etc/docker/daemon.json
å¢åŠ å¦‚ä¸‹é…ç½®ï¼š
&#123;&quot;insecure-registries&quot;:[&quot;registry.paas&quot;]&#125;
é‡å¯docker
systemctl daemon-reload &amp;&amp; systemctl restart docker
5. é‡æ–°pushè‡³registry.paas/library/xxx:tagsä»“åº“
äºŒã€ä¿®æ”¹é…ç½®æ–‡ä»¶
1. qwentest.yaml
apiVersion: apps/v1kind: DaemonSetmetadata:  name: qwentestspec:  selector:    matchLabels:      app: qwentest  template:    metadata:      labels:        app: qwentest    spec:      hostNetwork: true      nodeSelector:        model: qwen-7b      containers:      - name: qwentest        image: registry.paas/library/qwen:v3.0        imagePullPolicy: IfNotPresent        resources:         limits:           nvidia.com/gpu: &quot;4&quot;         requests:           nvidia.com/gpu: &quot;4&quot;        command:                                  # training command, which can be modified              - &quot;/bin/bash&quot;              - &quot;-c&quot;                #- sleep 10000              - |                cd /mnt/ &amp;&amp;                cp setRank.sh /qwen/ &amp;&amp;                cd /qwen/ &amp;&amp;                chmod +x setRank.sh &amp;&amp;                bash setRank.sh &amp;&amp;                chmod +x finetune_lora_ds.sh &amp;&amp;                bash finetune_lora_ds.sh        securityContext:          privileged: true        volumeMounts:        - name: processeddata          mountPath: /mnt        - name: dshm          mountPath: /dev/shm        - name: tmp-volume          mountPath: /tmp      volumes:      - name: processeddata        hostPath:          path: /mnt/users/wangxiangbo/runk8s      - name: dshm        emptyDir:          medium: Memory          sizeLimit: 1G      - name: tmp-volume        hostPath:          path: /tmp
2. hostfile
192.168.0.20192.168.0.58
3. setRank.sh
#!/bin/bashshell_name=&quot;finetune_lora_ds.sh&quot;shell_dir=&quot;/mnt/&quot;local_dir=&quot;/qwen/&quot;## å¤åˆ¶è„šæœ¬åˆ°/qwen/ä¸‹cp $shell_dir$shell_name $local_dir## è¯»å–hostfilereadarray -t ips &lt; &lt;(grep -vE &#x27;^[[:space:]]*$&#x27; &quot;$shell_dir&quot;hostfile)## è·å–rank0 IPrank0_ip=$(echo &quot;$&#123;ips[0]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)nodes=$&#123;#ips[@]&#125;## è·å–hostfileä¸­é…ç½®çš„IPå‰ç¼€## ä½¿ç”¨cutæå–IPåœ°å€çš„å‰ä¸‰ä¸ªæ•°å­—éƒ¨åˆ†ip_prefix=$(echo &quot;$&#123;ips[0]&#125;&quot; | cut -d &#x27;.&#x27; -f 1-3)## è·å–æœ¬æœºIPip=$(hostname -I | grep -oE &quot;$ip_prefix\.[0-9]+&quot;)ls# åˆå§‹åŒ–ranknode_rank=-1# éå†æ•°ç»„for i in &quot;$&#123;!ips[@]&#125;&quot;; do    # ä½¿ç”¨trå‘½ä»¤å»é™¤ç©ºç™½å­—ç¬¦    clean_string=$(echo &quot;$&#123;ips[$i]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)    if [[ &quot;$clean_string&quot; == &quot;$ip&quot; ]]; then        node_rank=$i        break    fidoneif [ $node_rank -ne -1 ]; then    ## ä¿®æ”¹è„šæœ¬ä¸­MASTER_ADDR    sed -i &quot;s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/&quot; $local_dir$shell_name    ## ä¿®æ”¹NNODES    sed -i &quot;s/^NNODES=.*/NNODES=$nodes/&quot; $local_dir$shell_name    ## ä¿®æ”¹NODE_RANK    sed -i &quot;s/^NODE_RANK=.*/NODE_RANK=$node_rank/&quot; $local_dir$shell_namefi
4. sh
#!/bin/bash/bin/bash -i &lt;&lt;&#x27;EOF&#x27;export NCCL_IB_DISABLE=1export NCCL_SOCKET_IFNAME=eth0export NCCL_P2P_DISABLE=1export NCCL_DEBUG=INFOsource ~/.bashrc. /opt/miniconda/etc/profile.d/conda.shconda activate qwenexport CUDA_DEVICE_MAX_CONNECTIONS=1export CUDA_VISIBLE_DEVICES=0,1,2,3DIR=`pwd`# Number of GPUs per GPU workerGPUS_PER_NODE=4# Number of GPU workers, for single-worker training, please set to 1NNODES=2# The rank of this worker, should be in &#123;0, ..., WORKER_CNT-1&#125;, for single-worker training, please set to 0NODE_RANK=0# The ip address of the rank-0 worker, for single-worker training, please set to localhostMASTER_ADDR=192.168.0.20# The port for communicationMASTER_PORT=6003MODEL=&quot;/qwen/Qwen-7B-Chat&quot; # Set the path if you do not want to load from huggingface directly# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.# See the section for finetuning in README for more information.DATA=&quot;/qwen/train_data_law.json&quot;DS_CONFIG_PATH=&quot;/qwen/finetune/ds_config_zero2.json&quot;function usage() &#123;    echo &#x27;Usage: bash finetune_lora_ds.sh [-m MODEL_PATH] [-d DATA_PATH] [--deepspeed DS_CONFIG_PATH]&#x27;&#125;while [[ &quot;$1&quot; != &quot;&quot; ]]; do    case $1 in        -m | --model )            shift            MODEL=$1            ;;        -d | --data )            shift            DATA=$1            ;;        --deepspeed )            shift            DS_CONFIG_PATH=$1            ;;        -h | --help )            usage            exit 0            ;;        * )            echo &quot;Unknown argument $&#123;1&#125;&quot;            exit 1            ;;    esac    shiftdoneDISTRIBUTED_ARGS=&quot;    --nproc_per_node $GPUS_PER_NODE \    --nnodes $NNODES \    --node_rank $NODE_RANK \    --master_addr $MASTER_ADDR \    --master_port $MASTER_PORT&quot;torchrun $DISTRIBUTED_ARGS finetune.py \    --model_name_or_path $MODEL \    --data_path $DATA \    --bf16 False \    --output_dir output_qwen \    --num_train_epochs 5 \    --per_device_train_batch_size 8 \    --per_device_eval_batch_size 1 \    --gradient_accumulation_steps 8 \    --evaluation_strategy &quot;no&quot; \    --save_strategy &quot;steps&quot; \    --save_steps 100 \    --save_total_limit 10 \    --learning_rate 3e-4 \    --weight_decay 0.1 \    --adam_beta2 0.95 \    --warmup_ratio 0.01 \    --lr_scheduler_type &quot;cosine&quot; \    --logging_steps 1 \    --report_to &quot;none&quot; \    --model_max_length 512 \    --lazy_preprocess True \    --use_lora \    --gradient_checkpointing \    --ddp_find_unused_parameters False \EOF
ä¸‰ã€æ‹‰èµ·è®­ç»ƒï¼ˆ3å·å’Œ4å·ä¸¤æœº8å¡ï¼‰
1. åˆ‡æ¢åˆ°1å·æœºå™¨masterèŠ‚ç‚¹ä¸Šï¼Œç»™å¸¦è®­ç»ƒå¾—3å·å’Œ4å·æœºå™¨æ‰“ä¸Šæ ‡ç­¾
kubectl label nodes ecs-jhjs-1234-003 model=qwen-7b
kubectl label nodes ecs-jhjs-1234-004 model=qwen-7b
2. å‡†å¤‡å¥½å¯åŠ¨è„šæœ¬ç­‰æ–‡ä»¶åï¼Œåœ¨masterèŠ‚ç‚¹1å·æœºå™¨ä¸Šï¼Œåˆ©ç”¨ä¿®æ”¹å¥½çš„qwentest.yamlæ–‡ä»¶æ‹‰èµ·è®­ç»ƒä»»åŠ¡
3. kubectl apply -f qwentest.yaml
4. é€šè¿‡kubectlæŸ¥çœ‹podèŠ‚ç‚¹å¯åŠ¨ä¿¡æ¯

5. æŸ¥çœ‹ä¸¤ä¸ªèŠ‚ç‚¹podçš„logsæ—¥å¿—
kubectl logs qwentest-czm8n -f

kubectl logs qwentest-qthsf -f

]]></content>
      <categories>
        <category>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</tag>
        <tag>Nvidia</tag>
        <tag>Qwen</tag>
        <tag>Deepspeed</tag>
      </tags>
  </entry>
  <entry>
    <title>Tulu3-8B-åè®­ç»ƒå®è·µ</title>
    <url>/2025/11/12/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Tulu3/Tulu3-8B-%E5%90%8E%E8%AE%AD%E7%BB%83%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[è®­ç»ƒç¯å¢ƒ
é•œåƒ
registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:tulu3
é•œåƒä¸­å·²åŒ…å«tulu3åè®­ç»ƒæ‰€éœ€è¦çš„æ‰€æœ‰ä¾èµ–ç¯å¢ƒã€‚
ä»£ç ç»“æ„
/mnt/users/wangxiangbo/model/tulu3/open-instruct-main

data
åŒ…å«äº†sftã€dpoã€rlvræ‰€éœ€çš„æ•°æ®é›†

evalç›¸å…³

scripts/configs
è®­ç»ƒè„šæœ¬


tulu3_sft.sh #tulu3 sftå•æœºè„šæœ¬

hostfilehostfile-12setRank.shtulu3_dpo.sh #å¤šæœºdpoè„šæœ¬tulu3_dpo_single.sh #å•æœº4å¡dpoè„šæœ¬tulu3_dpo.yaml #å¤šæœºyamltulu3_sft.sh #å¤šæœºsftè„šæœ¬

tulu3_rlvr_single.sh #tulu3 rlvrå•æœºè„šæœ¬

loss_plot.py #lossæ›²çº¿ç»˜å›¾è„šæœ¬
open_instruct
sftã€dpoã€rlvrç›¸å…³è®­ç»ƒä»£ç 

sftå®è·µ
å•æœº4å¡æµ‹è¯•
sftæ•°æ®é›†
sftæ•°æ®é›†é‡‡ç”¨tulu3å®˜ç½‘æä¾›çš„æ•°æ®é›†

sftæ•°æ®é›†æ ·ä¾‹ï¼š
&#123;&quot;id&quot;:&quot;oasst1_2644&quot;,&quot;messages&quot;:[&#123;&quot;content&quot;:&quot;æœ‰ç¼“è§£ç¤¾äº¤ææƒ§ç—‡çš„å®ç”¨æ–¹æ³•å—ï¼Ÿ&quot;,&quot;role&quot;:&quot;user&quot;&#125;,&#123;&quot;content&quot;:&quot;ä»¥ä¸‹æ˜¯ä¸€äº›å®ç”¨çš„æ–¹æ³•ï¼Œå¯èƒ½æœ‰åŠ©äºç¼“è§£ç¤¾äº¤ææƒ§ç—‡ï¼š\n\n1.  å¯»æ±‚ä¸“ä¸šå¸®åŠ©ï¼šå¯»æ±‚å¿ƒç†åŒ»ç”Ÿæˆ–å¿ƒç†æ²»ç–—å¸ˆçš„å¸®åŠ©æ˜¯ç¼“è§£ç¤¾äº¤ææƒ§ç—‡çš„æœ€ä½³æ–¹æ³•ä¹‹ä¸€ã€‚ä¸“ä¸šçš„æ²»ç–—å¸ˆå¯ä»¥å¸®åŠ©ä½ è¯†åˆ«å’Œå¤„ç†ææƒ§çš„æºå¤´ï¼Œå¹¶æä¾›æœ‰æ•ˆçš„æ²»ç–—æ–¹æ¡ˆã€‚\n\n2. ç»ƒä¹ æ·±å‘¼å¸å’Œæ”¾æ¾æŠ€å·§ï¼šå½“ä½ æ„Ÿåˆ°ç„¦è™‘å’Œç´§å¼ æ—¶ï¼Œå°è¯•é€šè¿‡æ·±å‘¼å¸å’Œæ”¾æ¾æŠ€å·§æ¥å¸®åŠ©ä½ å†·é™ä¸‹æ¥ã€‚æ·±å‘¼å¸å¯ä»¥å¸®åŠ©ä½ å‡è½»èº«ä½“ä¸Šçš„å‹åŠ›å’Œç´§å¼ æ„Ÿï¼Œä»è€Œä½¿ä½ æ›´æ”¾æ¾å’Œé•‡å®šã€‚\n\n3. å­¦ä¹ ç§¯ææ€è€ƒï¼šé¿å…æ¶ˆæçš„è‡ªæˆ‘å¯¹è¯ï¼Œä¾‹å¦‚â€œæˆ‘ä¼šå¤±è´¥â€ã€â€œæˆ‘ä¸è¡Œâ€ç­‰ç­‰ã€‚æ”¹å˜è‡ªå·±çš„æ€è€ƒæ–¹å¼ï¼Œå°è¯•ä»¥ç§¯æã€è‡ªä¿¡çš„æ€åº¦é¢å¯¹ç¤¾äº¤åœºåˆã€‚\n\n4.  æ¡å–æŸ”æ€§æ–¹æ³•ï¼šç‚ºäº†å°æŠ—ç„¦æ…®ç—‡ç‹€ï¼Œå»ºè­°ä½¿ç”¨ä¸€äº›æ–¹æ³•ä¾†æé†’ä½ çš„èº«é«”æ˜¯å®‰å…¨çš„ï¼Œä¾‹å¦‚ï¼š  \n\nï¼Š å‘¼å¸è¨“ç·´ï¼ˆç”¨é¼»å­æ·±æ·±çš„å¸ä¸€å£æ°£å†ç”¨å˜´å·´å°‡æ°£æ…¢æ…¢åœ°åå‡ºï¼Œç•¶ä½ å°ˆæ³¨åœ¨åæ°£æ™‚ï¼Œä½ å°±æœƒè¢«å¸¶å‡ºæŠµæŠ—æˆ–é€ƒé¿çš„æƒ…ç·’ä¸­ã€‚ï¼‰ \n\n ï¼Š å°ˆæ³¨æ–¼ç¾åœ¨ï¼ˆé€™éº¼åšè®“ä½ èƒ½å¤ å°ˆæ³¨æ–¼è‡ªå·±æ‰€ç¶“æ­·çš„æ¯åˆ†æ¯ç§’ã€‚è©¦è‘—èªªå‡ºä½ ç¾åœ¨èƒ½çœ‹è¦‹ã€èåˆ°ã€ç¢°è§¸åˆ°ä¸¦è½å¾—åˆ°çš„æ±è¥¿ï¼Œæˆ–æ˜¯ååœ¨æ¤…å­ä¸Šä¸¦æ„Ÿå—è‡ªå·±çš„è…³è§¸ç¢°åœ°é¢çš„æ„Ÿè¦ºï¼Œæˆ–èº«é«”æ­£è¢«æ”¯æ’è‘—çš„æ„Ÿè¦ºã€‚ï¼‰\n\n5. çµ¦è‡ªå·±ä¸€é»å¾å®¹ï¼šåœ¨éæ¸¡æ™‚æœŸç·´ç¿’è‡ªæˆ‘åŒæƒ…ï¼Œä¸¦æé†’è‡ªå·±æ„Ÿåˆ°ä¸ç¢ºå®šèˆ‡ä¸å®‰æ˜¯å¾ˆæ­£å¸¸çš„ã€‚\n\nè¿™äº›æ–¹æ³•å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç¤¾äº¤ææƒ§ç—‡ï¼Œä½†ä¸åŒçš„äººå¯èƒ½éœ€è¦ä¸åŒçš„æ–¹æ³•å’Œæ²»ç–—æ–¹æ¡ˆã€‚å¦‚æœä½ æ„Ÿåˆ°ä¸¥é‡çš„ç„¦è™‘å’Œææƒ§ï¼Œè¯·åŠæ—¶å¯»æ±‚ä¸“ä¸šå¸®åŠ©ã€‚&quot;,&quot;role&quot;:&quot;assistant&quot;&#125;,&#123;&quot;content&quot;:&quot;å¥½å§ï¼Œå¯æ˜¯æˆ‘è¦å»å“ªè£¡å°‹æ±‚å°ˆæ¥­å¹«åŠ©ï¼Ÿ&quot;,&quot;role&quot;:&quot;user&quot;&#125;,&#123;&quot;content&quot;:&quot;æ‚¨å¯ä»¥å…ˆåƒè€ƒç¾åœ‹ç²¾ç¥ç–¾ç—…è¨ºæ–·æ¨™æº– **DSM-5** ä¸­çš„6é»æ¨™æº–ï¼Œæˆ–è€…é€²è¡Œä¸€äº›å°ˆæ¥­çš„å¿ƒç†æ¸¬è©¦ã€‚å¦‚æœæ‚¨ç¬¦åˆé€™äº›æ¨™å‡†æˆ–æ¸¬è©¦çµæœé¡¯ç¤ºæ‚¨æœ‰ç¤¾äº¤ææ‡¼ç—‡ï¼Œæ‚¨æ‡‰å„˜æ—©å°‹æ±‚å°ˆæ¥­çš„å¹«åŠ©ã€‚\n\nå°‹æ±‚å°ˆæ¥­å¹«åŠ©çš„é€”å¾‘æœ‰ä»¥ä¸‹å¹¾ç¨®ï¼š\n\n- **è«®è©¢é†«ç”Ÿæˆ–å¿ƒç†å¥åº·å°ˆå®¶**ã€‚ä»–å€‘å¯ä»¥ç‚ºæ‚¨æä¾›è¨ºæ–·ï¼Œè—¥ç‰©æ²»ç™‚ï¼Œå¿ƒç†æ²»ç™‚ç­‰æœå‹™ã€‚\n- **åƒåŠ å¿ƒç†è«®è©¢æˆ–æ²»ç™‚æ©Ÿæ§‹**ã€‚é€™äº›æ©Ÿæ§‹é€šå¸¸æœ‰å°ˆæ¥­çš„å¿ƒç†è«®è©¢å¸«æˆ–æ²»ç™‚å¸«ï¼Œå¯ä»¥ç‚ºæ‚¨æä¾›å€‹äººæˆ–åœ˜é«”çš„å¿ƒç†è«®è©¢æˆ–æ²»ç™‚æœå‹™ã€‚\n- **åŠ å…¥äº’åŠ©å°çµ„æˆ–ç¤¾å€**ã€‚é€™äº›å°çµ„æˆ–ç¤¾å€é€šå¸¸ç”±æœ‰ç›¸åŒæˆ–é¡ä¼¼å•é¡Œçš„äººçµ„æˆï¼Œå¯ä»¥ç‚ºæ‚¨æä¾›æ”¯æŒï¼Œé¼“å‹µï¼Œäº¤æµå’Œåˆ†äº«çš„å¹³å°ã€‚\n\nä»¥ä¸Šæ˜¯æˆ‘ç‚ºæ‚¨æœç´¢åˆ°çš„ä¸€äº›ä¿¡æ¯ï¼Œå¸Œæœ›å°æ‚¨æœ‰æ‰€å¹«åŠ©ã€‚å¦‚æœæ‚¨é‚„æœ‰å…¶ä»–å•é¡Œï¼Œæ­¡è¿ç¹¼çºŒæå•ã€‚&quot;,&quot;role&quot;:&quot;assistant&quot;&#125;],&quot;source&quot;:&quot;ai2-adapt-dev\/oasst1_converted&quot;&#125;
ç”±äºæ˜¯.parquetæ ¼å¼æ•°æ®ï¼Œè½¬æˆjsonåä¸€å…±æœ‰93wæ¡æ•°æ®ã€‚

ç”±äºæœºå™¨æ•°é‡æœ‰é™ï¼Œè®­ç»ƒå…¶å®Œæ•´æ•°æ®é›†éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œæ•…å°†.parquetè½¬ä¸ºjsonåå°†æ•°æ®é›†ç¼©å‡è‡³5kæ¡ï¼Œå¹¶å†æ¬¡è½¬å›.parquetæ ¼å¼æ•°æ®ã€‚

å•æœºè„šæœ¬é…ç½®
export CUDA_VISIBLE_DEVICES=0,1,2,3export WANDB_MODE=disabledcurrent_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)LOG_SAVE=&quot;/mnt/open-instruct-main/output/sft/$&#123;current_time&#125;-tulu3-sft.log&quot;MODEL_SIZE=8BNUM_GPUS=4BATCH_SIZE_PER_GPU=2TOTAL_BATCH_SIZE=8GRADIENT_ACC_STEPS=$(($TOTAL_BATCH_SIZE/($NUM_GPUS * $BATCH_SIZE_PER_GPU)))echo &quot;Training llama model $&#123;MODEL_SIZE&#125; using $NUM_GPUS GPUs, $BATCH_SIZE_PER_GPU batch size per GPU, $GRADIENT_ACC_STEPS gradient accumulation steps&quot;accelerate launch \    --mixed_precision fp16 \    --num_machines 1 \    --num_processes $NUM_GPUS \    --use_deepspeed \    --deepspeed_config_file /mnt/open-instruct-main/configs/ds_configs/stage3_offloading_accelerate.conf \    /mnt/open-instruct-main/open_instruct/finetune.py \    --model_name_or_path /mnt/LLM-Research/Meta-Llama-31-8B \    --tokenizer_name /mnt/LLM-Research/Meta-Llama-31-8B \    --use_slow_tokenizer \    --train_file /mnt/open-instruct-main/data/sft_data_json/sft_dataset_5k.json \    --max_seq_length 4096 \    --preprocessing_num_workers 4 \    --per_device_train_batch_size $BATCH_SIZE_PER_GPU \    --gradient_accumulation_steps $GRADIENT_ACC_STEPS \    --learning_rate 2e-5 \    --lr_scheduler_type linear \    --warmup_ratio 0.03 \    --weight_decay 0. \    --num_train_epochs 1 \    --output_dir /mnt/open-instruct-main/output/sft \    --gradient_checkpointing true \    --report_to none \    --use_flash_attn false 2&gt;&amp;1 | tee -a &quot;$LOG_SAVE&quot;
ä¸ºäº†å°½å¯èƒ½åœ°è¿˜åŸtulu3åŸè®ºæ–‡çš„å®éªŒç»“æœï¼Œå…¶ä¸­éƒ¨åˆ†è¶…å‚ä¸è®ºæ–‡ä¸­æœ€ä¼˜ä¿æŒä¸€è‡´ã€‚
--max_seq_length 4096
--learning_rate 2e-5
--lr_scheduler_type linear
--warmup_ratio 0.03
å…¶ä¸­çš„æ¨¡å‹æƒé‡åˆ©ç”¨llama3.1-8B-baseæ¨¡å‹ã€‚
è®­ç»ƒæµ‹è¯•
è®­ç»ƒç»“æŸ





èµ„æºç±»å‹
åˆ©ç”¨ç‡
å†…å­˜/æ˜¾å­˜å ç”¨é‡




CPU
53.2%
80.7%


GPU
85.5%
66.4%



lossæ›²çº¿

eval
llama3.1-8Bé€šè¿‡tulu3æä¾›çš„sftæ•°æ®é›†ï¼ˆåˆ å‡è‡³5kæ¡ï¼‰åï¼Œå¾—åˆ°çš„tulu3_sftæƒé‡




ç±»åˆ«
Llama-31-8B
tulu3-sft-weight
Llama-31-Tulu-3-8B-SFT




å¹³å‡å‡†ç¡®ç‡
0.6014
0.5729
0.6356


å­ç±»åˆ«å‡†ç¡®ç‡
-
-
-


æ•°å­¦
0.4098
0.3778
0.4192


å¥åº·
0.6561
0.6183
0.6720


ç‰©ç†
0.5063
0.4891
0.5328


å•†ä¸š
0.7643
0.7346
0.8101


ç”Ÿç‰©
0.7555
0.6982
0.7775


åŒ–å­¦
0.4851
0.4851
0.5186


è®¡ç®—æœºç§‘å­¦
0.5752
0.5194
0.6092


ç»æµ
0.5930
0.5809
0.6213


å·¥ç¨‹
0.5517
0.5172
0.5517


å“²å­¦
0.4881
0.4627
0.5775


å…¶ä»–
0.6524
0.6532â¬†
0.7090


å†å²
0.7452
0.7172
0.7774


åœ°ç†
0.7323
0.7273
0.7677


æ”¿æ²»
0.7546
0.7099
0.7762


å¿ƒç†å­¦
0.7174
0.6845
0.7485


æ–‡åŒ–
0.8102
0.7440
0.7922


æ³•å¾‹
0.4878
0.4589
0.5020


ç±»åˆ«å‡†ç¡®ç‡
-
-
-


STEM
0.5192
0.4864
0.5394


äººæ–‡å­¦ç§‘
0.5388
0.5116
0.5887


ç¤¾ä¼šç§‘å­¦
0.7062
0.6740
0.7296


å…¶ä»–ï¼ˆå•†ä¸šã€å¥åº·ç­‰ï¼‰
0.6693
0.6465
0.7039



evalç»“æœåˆ†æ

SFTåæƒé‡çš„å¹³å‡å‡†ç¡®ç‡: 0.5729
SFTå‰æƒé‡çš„å¹³å‡å‡†ç¡®ç‡: 0.6014

æ•´ä½“å¹³å‡å‡†ç¡®ç‡ä¸‹é™äº† ~2.85%ã€‚
åŸå› ï¼šæœ¬æ¬¡å®éªŒï¼ˆç”±äºæœºå™¨ç¯å¢ƒé™åˆ¶ï¼‰å°†åŸå§‹çš„tulu3 SFTæ•°æ®é›†ï¼ˆ93wï¼‰åˆ å‡åˆ°5kæ¡ï¼Œå¯¼è‡´æ¨¡å‹æœªèƒ½å……åˆ†å­¦ä¹ å„ä¸ªé¢†åŸŸçš„çŸ¥è¯†å’Œä»»åŠ¡ç‰¹æ€§ï¼Œæ³›åŒ–èƒ½åŠ›ä¸‹é™ã€‚
dpoå®è·µ
å•æœº4å¡æµ‹è¯•
åå¥½æ•°æ®é›†
åå¥½æ•°æ®é›†é‡‡ç”¨tulu3å®˜ç½‘æä¾›çš„æ•°æ®é›†ã€‚

åå¥½æ•°æ®é›†æ ·ä¾‹ï¼š
&#123;&quot;id&quot;:&quot;allenai\/tulu-3-sft-reused-off-policy_0&quot;,&quot;prompt&quot;:&quot;Peter has 5 cages with rabbits (the cages are arranged in a row). It is known that there is at least one rabbit in each cage. We will call two rabbits neighbors if they are either in the same cage or in adjacent cages. It turned out that each rabbit has either 3 or 7 neighbors. How many rabbits are in the central cage?&quot;,&quot;chosen&quot;:[&#123;&quot;content&quot;:&quot;Peter has 5 cages with rabbits (the cages are arranged in a row). It is known that there is at least one rabbit in each cage. We will call two rabbits neighbors if they are either in the same cage or in adjacent cages. It turned out that each rabbit has either 3 or 7 neighbors. How many rabbits are in the central cage?&quot;,&quot;role&quot;:&quot;user&quot;&#125;,&#123;&quot;content&quot;:&quot;Here&#x27;s how to solve this problem:\n\n**Understanding the Information**\n\n* **Each rabbit has 3 or 7 neighbors:** This means there must be a pattern in how the rabbits are distributed within the cages.\n* **At least one rabbit in each cage:**  We can&#x27;t have cages without any rabbits.\n\n**Reasoning**\n\n* **Corner Cages:**  Rabbits in the corner cages (cages 1 and 5) can only have 3 neighbors (two adjacent cages and one within their own cage).\n* **Middle Cages:** Rabbits in the middle cages (cages 2, 3, and 4) must have 7 neighbors to balance out the setup. \n\n**The Solution**\n\nSince rabbits in the middle cages need 7 neighbors, and there&#x27;s at least one rabbit in each cage, the central cage (cage 3) must have **4 rabbits**. \n\n\n\nLet me know if you&#x27;d like to explore other logic puzzles! \n&quot;,&quot;role&quot;:&quot;assistant&quot;&#125;],&quot;rejected&quot;:[&#123;&quot;content&quot;:&quot;Peter has 5 cages with rabbits (the cages are arranged in a row). It is known that there is at least one rabbit in each cage. We will call two rabbits neighbors if they are either in the same cage or in adjacent cages. It turned out that each rabbit has either 3 or 7 neighbors. How many rabbits are in the central cage?&quot;,&quot;role&quot;:&quot;user&quot;&#125;,&#123;&quot;content&quot;:&quot;The central cage has the same number of rabbits as any other cage, so it also has either 3 or 7 neighbors.\nThere are (at most) 7 rabbits in the central cage and (at most) 3 rabbits in any cage on either side. Hence, there are (at most) 7 rabbits in each of those cages. The total number of rabbits in the 5 cages is (at most) 5 * 7 + 1 = 36 + 1 = 37.\nOne cage must have exactly the right number of neighbors, but two cages cannot have the right number.\nIf each cage has exactly 3 neighbors, the total number of rabbits in all 5 cages is 1 + 5 * 3 = 16\nIf each cage has exactly 7 neighbors, the total number of rabbits in all 5 cages is 1 + 5 * 7 = 32\nNow that we know there are at most 36 rabbits, we can safely conclude that there is 1 rabbit in the central cage, that there are 5 cages with 3 rabbits each and one cage with 7 rabbits and that the central cage has 3 neighbors.\nThe central cage has exactly 3 rabbits and 3 neighbors.\nIn the other 4 cages, there are (at most) 5 * 3 = 15 rabbits.\nIn the other 5 cages, there are (at least) 32 â€“ 16 = 16 rabbits.&quot;,&quot;role&quot;:&quot;assistant&quot;&#125;],&quot;source&quot;:&quot;allenai\/tulu-3-sft-reused-off-policy&quot;&#125;
ç”±äºæ˜¯.parquetæ ¼å¼æ•°æ®ï¼Œè½¬æˆjsonåä¸€å…±æœ‰67wæ¡æ•°æ®ã€‚

ç”±äºæœºå™¨æ•°é‡æœ‰é™ï¼Œè®­ç»ƒå…¶å®Œæ•´æ•°æ®é›†éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œæ•…å°†.parquetè½¬ä¸ºjsonåå°†æ•°æ®é›†ç¼©å‡è‡³5kæ¡ï¼Œå¹¶å†æ¬¡è½¬å›.parquetæ ¼å¼æ•°æ®ã€‚

å•æœºè„šæœ¬é…ç½®
å•æœºè„šæœ¬ï¼Œéƒ¨åˆ†é…ç½®OOMï¼Œé‡‡ç”¨deepseedä¸­zero3ä»¥åŠcpu_offloadingï¼Œå¼€å¯æ¢¯åº¦ç´¯ç§¯ã€‚
# you need 8 GPUs for full finetuningexport CUDA_VISIBLE_DEVICES=0,1,2,3export WANDB_MODE=disabledNUM_GPUS=4BATCH_SIZE_PER_GPU=1TOTAL_BATCH_SIZE=8GRADIENT_ACC_STEPS=$(($TOTAL_BATCH_SIZE/$NUM_GPUS/$BATCH_SIZE_PER_GPU))current_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)LOG_SAVE=&quot;/mnt/open-instruct-main/output/dpo/$&#123;current_time&#125;-tulu3-dpo.log&quot;echo &quot;Training model using $NUM_GPUS GPUs, $BATCH_SIZE_PER_GPU batch size per GPU, $GRADIENT_ACC_STEPS gradient accumulation steps&quot;#stage3_no_offloading_accelerate.confaccelerate launch \    --mixed_precision fp16 \    --num_machines 1 \    --num_processes $NUM_GPUS \    --use_deepspeed \    --deepspeed_config_file /mnt/open-instruct-main/configs/ds_configs/stage3_offloading_accelerate.conf \    /mnt/open-instruct-main/open_instruct/dpo_tune.py \    --model_name_or_path /mnt/LLM-Research/Llama-31-Tulu-3-8B-SFT \    --use_flash_attn  false\    --gradient_checkpointing \    --tokenizer_name /mnt/LLM-Research/Llama-31-Tulu-3-8B-SFT \    --use_slow_tokenizer \    --dataset_name /mnt/open-instruct-main/data/dpo_data_5k \    --max_seq_length 2048 \    --preprocessing_num_workers 4 \    --per_device_train_batch_size $BATCH_SIZE_PER_GPU \    --gradient_accumulation_steps $GRADIENT_ACC_STEPS \    --learning_rate 5e-7 \    --lr_scheduler_type linear \    --warmup_ratio 0.1 \    --weight_decay 0. \    --num_train_epochs 1 \    --output_dir /mnt/open-instruct-main/output/dpo \    --with_tracking False \    --logging_steps 1 2&gt;&amp;1 | tee -a &quot;$LOG_SAVE&quot;
ä¸ºäº†å°½å¯èƒ½åœ°è¿˜åŸtulu3åŸè®ºæ–‡çš„å®éªŒç»“æœï¼Œå…¶ä¸­éƒ¨åˆ†è¶…å‚ä¸è®ºæ–‡ä¸­æœ€ä¼˜ä¿æŒä¸€è‡´ã€‚
--max_seq_length 2048
--learning_rate 5e-7
--lr_scheduler_type linear
--warmup_ratio 0.1
å…¶ä¸­çš„æ¨¡å‹æƒé‡åˆ©ç”¨tulu3å¼€æºçš„ç»è¿‡llama3.1-8B sftå¾—åˆ°çš„æƒé‡ã€‚

è®­ç»ƒæµ‹è¯•
å…³é—­cpu_offloading
æ³¨ï¼šå…³é—­cpu_offloadingä¹‹åï¼Œè¿­ä»£10æ­¥å·¦å³ä¼šOOM




èµ„æºç±»å‹
åˆ©ç”¨ç‡
å†…å­˜/æ˜¾å­˜å ç”¨é‡




CPU
16.5%
12.1%


GPU
100%
93.6%



å¼€å¯cpu_offloading




èµ„æºç±»å‹
åˆ©ç”¨ç‡
å†…å­˜/æ˜¾å­˜å ç”¨é‡




CPU
55.0%
58.2%


GPU
85.8%
68.7%



è®­ç»ƒå®Œæˆ

å¾—åˆ°çš„dpoæƒé‡

lossæ›²çº¿

eval
è¯„æµ‹æ•°æ®é›†é‡‡ç”¨MMLUï¼Œä¸€ä¸ªåŒ…å«æ¥è‡ªå„ä¸ªçŸ¥è¯†é¢†åŸŸçš„å¤šé¡¹é€‰æ‹©é¢˜çš„å·¨å¤§å¤šä»»åŠ¡æµ‹è¯•ã€‚ æµ‹è¯•æ¶µç›–äº†äººæ–‡å­¦ç§‘ã€ç¤¾ä¼šç§‘å­¦ã€è‡ªç„¶ç§‘å­¦ä»¥åŠå…¶ä»–å¯¹æŸäº›äººæ¥è¯´é‡è¦çš„å­¦ä¹ é¢†åŸŸã€‚ æ•°æ®é›†ä¸­çš„é—®é¢˜æ˜¯ç”±ç ”ç©¶ç”Ÿå’Œæœ¬ç§‘ç”Ÿä»åœ¨çº¿å…è´¹èµ„æºä¸­æ‰‹åŠ¨æ”¶é›†çš„ã€‚è¿™åŒ…æ‹¬ç ”ç©¶ç”Ÿå…¥å­¦è€ƒè¯•å’Œç¾å›½åŒ»å­¦æ‰§ç…§è€ƒè¯•ç­‰è€ƒè¯•çš„ç»ƒä¹ é¢˜ã€‚è¿˜åŒ…æ‹¬ä¸ºæœ¬ç§‘ç”Ÿè¯¾ç¨‹è®¾è®¡çš„é¢˜ç›®ï¼Œä»¥åŠä¸ºç‰›æ´¥å¤§å­¦å‡ºç‰ˆç¤¾ä¹¦ç±è¯»è€…è®¾è®¡çš„é¢˜ç›®ã€‚ ä¸€äº›ä»»åŠ¡æ¶µç›–ä¸€ä¸ªä¸»é¢˜ï¼Œå¦‚å¿ƒç†å­¦ï¼Œä½†éš¾åº¦çº§åˆ«ç‰¹å®šï¼Œä¾‹å¦‚â€œåˆé˜¶â€ã€â€œé«˜ä¸­â€ã€â€œå¤§å­¦â€æˆ–â€œä¸“ä¸šâ€ã€‚ ä¾‹å¦‚ï¼Œâ€œä¸“ä¸šå¿ƒç†å­¦â€ä»»åŠ¡å€Ÿé‰´äº†å¿ƒç†å­¦ä¸“ä¸šå®è·µè€ƒè¯•çš„å…è´¹ç»ƒä¹ é¢˜ä¸­çš„é—®é¢˜ï¼Œè€Œâ€œé«˜ä¸­å¿ƒç†å­¦â€ä»»åŠ¡åˆ™åŒ…å«ç±»ä¼¼é«˜çº§ Placement å¿ƒç†å­¦è€ƒè¯•ä¸­çš„é—®é¢˜ã€‚
:::tips
MMLUæ•°æ®é›†æ ·ä¾‹å¦‚ä¸‹ï¼Œæ‘˜è‡ªMMLUè®ºæ–‡https://arxiv.org/abs/2009.03300
:::

egï¼šä»¥ä¸‹æ˜¯MMLUä¸ŠCollege Biologyç±»é—®é¢˜åŠç­”æ¡ˆï¼š
&quot;Based on the characteristic population curves that result from plotting population growth of a species, the most effective means of controlling the mosquito population is to&quot;,maintain the population at a point corresponding to the midpoint of its logistic curve,opt for zero population control once the K value of the curve has been reached,reduce the carrying capacity cif the environment to lower the K value,increase the mortality rate,C
egï¼šä»¥ä¸‹æ˜¯dpo_weightåœ¨MMLUä¸ŠCollege Biologyç±»é—®é¢˜çš„å›ç­”ï¼š
0,1,2,3,4,5,correct,choiceA_probs,choiceB_probs,choiceC_probs,choiceD_probs&quot;Based on the characteristic population curves that result from plotting population growth of a species, the most effective means of controlling the mosquito population is to&quot;,maintain the population at a point corresponding to the midpoint of its logistic curve,opt for zero population control once the K value of the curve has been reached,reduce the carrying capacity cif the environment to lower the K value,increase the mortality rate,C,True,0.07324660569429398,0.0071399579755961895,0.6528399586677551,0.1929791122674942
æœ¬æ¬¡dpoåå¾—åˆ°çš„æƒé‡dpo_weightå¯¹æ¯”tulu3å¼€æºçš„sftæƒé‡Llama-31-Tulu-3-8B-SFTã€‚
Llama-31-Tulu-3-8B-SFTåœ¨MMLUä¸Šçš„è¡¨ç°ï¼š

dpo_weightåœ¨MMLUä¸Šçš„è¡¨ç°ï¼š




ç±»åˆ«
Llama-31-8B
Llama-31-Tulu-3-8B-SFT
dpo_weight
Llama-31-Tulu-3-8B-DPO




å¹³å‡å‡†ç¡®ç‡
0.6014
0.6356
**0.6377**â¬†
0.6352


å­ç±»åˆ«å‡†ç¡®ç‡
-
-
-
-


æ•°å­¦
0.4098
0.4192
0.4352â¬†
0.4258


å¥åº·
0.6561
0.6720
0.6726â¬†
0.6915


ç‰©ç†
0.5063
0.5328
0.5500â¬†
0.5391


å•†ä¸š
0.7643
0.8101
0.7941
0.7849


ç”Ÿç‰©
0.7555
0.7775
0.7775â¬†
0.7731


åŒ–å­¦
0.4851
0.5186
0.5182
0.5248


è®¡ç®—æœºç§‘å­¦
0.5752
0.6092
0.6189â¬†
0.6117


ç»æµ
0.5930
0.6213
0.6173
0.6429


å·¥ç¨‹
0.5517
0.5517
0.5655â¬†
0.5586


å“²å­¦
0.4881
0.5775
0.5626
0.5388


å…¶ä»–
0.6524
0.7090
0.7107â¬†
0.7082


å†å²
0.7452
0.7774
0.7828â¬†
0.7720


åœ°ç†
0.7323
0.7677
0.7778â¬†
0.7778


æ”¿æ²»
0.7546
0.7762
0.7793â¬†
0.7762


å¿ƒç†å­¦
0.7174
0.7485
0.7519â¬†
0.7476


æ–‡åŒ–
0.8102
0.7922
0.7892
0.8042


æ³•å¾‹
0.4878
0.5020
0.5133â¬†
0.5156


çˆ¶ç±»åˆ«å‡†ç¡®ç‡
-
-
-
-


STEM
0.5192
0.5394
0.5507â¬†
0.5437


äººæ–‡å­¦ç§‘
0.5388
0.5887
0.5877
0.5762


ç¤¾ä¼šç§‘å­¦
0.7062
0.7296
0.7309â¬†
0.7364


å…¶ä»–ï¼ˆå•†ä¸šã€å¥åº·ã€æ‚é¡¹ï¼‰
0.6693
0.7039
0.7027
0.7101



dpoä»£ç å®ç°åˆ†æ
Length-Normalized DPO çš„å®ç°
_get_batch_logps å‡½æ•°
tulu3è®ºæ–‡ä¸­è¯´æ˜äº†ï¼Œtulu3-8Bçš„dpoä½¿ç”¨äº†é•¿åº¦å½’ä¸€åŒ–çš„DPOï¼ˆLength-Normalized DPOï¼‰ï¼Œä»¥æ­¤æ¥æ¶ˆé™¤å› å›ç­”çš„åºåˆ—é•¿åº¦ï¼ˆå¯¹æ¯”æ¥è¯´å¾ˆé•¿çš„è¯ï¼‰å¸¦æ¥çš„æ¦‚ç‡åå¥½å½±å“ã€‚ å½“ average_log_prob=True æ—¶ï¼Œä¼šå°†æ¯ä¸ª token çš„ log æ¦‚ç‡è¿›è¡Œå¹³å‡å¤„ç†ï¼Œå®ç°åºåˆ—é•¿åº¦å½’ä¸€åŒ–ã€‚ å¦‚æœaverage_log_prob=Falseï¼Œåˆ™æ˜¯ç›´æ¥æ±‚å’Œã€‚
ä»£ç å…·ä½“å®ç°å¦‚ä¸‹ï¼š
def _get_batch_logps(    logits: torch.FloatTensor, labels: torch.LongTensor, average_log_prob: bool = False) -&gt; torch.FloatTensor:    # ç¡®ä¿logitså’Œlabelsçš„å½¢çŠ¶åœ¨é™¤äº†æœ€åä¸€ä¸ªç»´åº¦å¤–æ˜¯ç›¸åŒçš„ã€‚    assert logits.shape[:-1] == labels.shape    # ä¸¤è¡Œä»£ç åˆ†åˆ«å¯¹labelså’Œlogitsè¿›è¡Œåˆ‡ç‰‡æ“ä½œï¼Œå»æ‰äº†æ¯ä¸ªåºåˆ—çš„ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªå…ƒç´ ã€‚    labels = labels[:, 1:].clone()    logits = logits[:, :-1, :]    # è¿™è¡Œä»£ç åˆ›å»ºäº†ä¸€ä¸ªæ©ç loss_maskï¼Œç”¨äºæ ‡è¯†å“ªäº›æ ‡ç­¾ä¸æ˜¯-100ï¼ˆå³ä¸æ˜¯è¦è¢«å¿½ç•¥çš„æ ‡ç­¾ï¼‰ã€‚    loss_mask = labels != -100    # è¿™è¡Œä»£ç å°†labelsä¸­å€¼ä¸º-100çš„å…ƒç´ æ›¿æ¢ä¸º0ï¼Œå› ä¸ºåœ¨PyTorchä¸­ï¼Œ0å¯ä»¥ä½œä¸ºæ— æ•ˆç´¢å¼•ã€‚    labels[labels == -100] = 0    # è®¡ç®—æ¯ä¸ªtokençš„å¯¹æ•°æ¦‚ç‡ã€‚é¦–å…ˆå¯¹logitsåº”ç”¨log_softmaxå‡½æ•°ï¼Œç„¶ååœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šä½¿ç”¨torch.gatheræ ¹æ®labelsç´¢å¼•æ¥é€‰æ‹©å¯¹åº”çš„å¯¹æ•°æ¦‚ç‡ã€‚    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)    # æ ¹æ®average_log_probçš„å€¼å†³å®šè¿”å›å€¼ã€‚å¦‚æœaverage_log_probä¸ºTrueï¼Œåˆ™è¿”å›æ¯ä¸ªæ ·æœ¬çš„å¹³å‡å¯¹æ•°æ¦‚ç‡ï¼›å¦åˆ™ï¼Œè¿”å›æ¯ä¸ªæ ·æœ¬çš„å¯¹æ•°æ¦‚ç‡ä¹‹å’Œ    if average_log_prob:        return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)    else:        return (per_token_logps * loss_mask).sum(-1)
:::tips
å‡½æ•°_get_batch_logpsç”¨äºè®¡ç®—ç»™å®šæ ‡ç­¾ï¼ˆlabelsï¼‰åœ¨ç»™å®šçš„æ¨¡å‹è¾“å‡ºï¼ˆlogitsï¼‰ä¸‹çš„å¯¹æ•°æ¦‚ç‡ï¼ˆlog probabilitiesï¼‰ã€‚
å‚æ•°è¯´æ˜ï¼š

logitsï¼šæ¨¡å‹çš„è¾“å‡ºï¼ˆæœªå½’ä¸€åŒ–çš„ï¼‰ã€‚å½¢çŠ¶ä¸º(batch_size, sequence_length, vocab_size)ï¼Œå…¶ä¸­batch_sizeæ˜¯æ‰¹æ¬¡å¤§å°ï¼Œsequence_lengthæ˜¯åºåˆ—é•¿åº¦ï¼Œvocab_sizeæ˜¯è¯æ±‡è¡¨çš„å¤§å°ã€‚
labelsï¼šè¦è®¡ç®—å¯¹æ•°æ¦‚ç‡çš„æ ‡ç­¾ã€‚å€¼ä¸º-100çš„æ ‡ç­¾æ ‡è®°å°†è¢«å¿½ç•¥ã€‚å½¢çŠ¶ä¸º(batch_size, sequence_length)ã€‚
average_log_probï¼šä¸€ä¸ªå¸ƒå°”å€¼ï¼Œé»˜è®¤ä¸ºFalseã€‚å¦‚æœä¸ºTrueï¼Œåˆ™è¿”å›æ¯ä¸ªï¼ˆæœªè¢«æ©ç çš„ï¼‰tokençš„å¹³å‡å¯¹æ•°æ¦‚ç‡ï¼›å¦‚æœä¸ºFalseï¼Œåˆ™è¿”å›ï¼ˆæœªè¢«æ©ç çš„ï¼‰tokençš„å¯¹æ•°æ¦‚ç‡ä¹‹å’Œã€‚

è¿”å›å€¼ï¼š

è¿”å›ä¸€ä¸ªå½¢çŠ¶ä¸º(batch_size,)çš„å¼ é‡ï¼ŒåŒ…å«ç»™å®šæ ‡ç­¾åœ¨ç»™å®šlogitsä¸‹çš„å¯¹æ•°æ¦‚ç‡çš„å¹³å‡å€¼æˆ–æ€»å’Œã€‚

è¯¥å‡½æ•°æ˜¯ç”¨æ¥è®¡ç®—æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡ï¼Œå¹¶æ ¹æ®æ ‡ç­¾æ¥ç¡®å®šå“ªäº›é¢„æµ‹æ˜¯æœ‰æ•ˆçš„ï¼ˆå³æ ‡ç­¾å€¼ä¸ä¸º-100ï¼‰ã€‚å¦‚æœaverage_log_probå‚æ•°ä¸ºTrueï¼Œåˆ™å‡½æ•°è¿”å›çš„æ˜¯å¹³å‡å¯¹æ•°æ¦‚ç‡ï¼›å¦‚æœä¸ºFalseï¼Œåˆ™è¿”å›çš„æ˜¯æ€»å’Œã€‚
:::
concatenated_forward å‡½æ•°
åœ¨è®­ç»ƒä¸­ï¼Œconcatenated_forward è´Ÿè´£å¯¹æ¨¡å‹çš„ logps è¿›è¡Œè®¡ç®—ï¼š
def concatenated_forward(    model: nn.Module,    batch: Dict[str, Union[List, torch.LongTensor]],    average_log_prob: bool = False,    output_router_logits: bool = False,) -&gt; Tuple[torch.FloatTensor, torch.FloatTensor]:    concatenated_batch = concatenated_inputs(batch)    if output_router_logits:        outputs = model(            input_ids=concatenated_batch[&quot;concatenated_input_ids&quot;],            attention_mask=concatenated_batch[&quot;concatenated_attention_mask&quot;],            output_router_logits=True,        )        logits = outputs.logits.to(torch.float32)        aux_loss = outputs.aux_loss    else:        logits = model(            input_ids=concatenated_batch[&quot;concatenated_input_ids&quot;],            attention_mask=concatenated_batch[&quot;concatenated_attention_mask&quot;],        ).logits.to(torch.float32)        aux_loss = None    all_logps = _get_batch_logps(logits, concatenated_batch[&quot;concatenated_labels&quot;], average_log_prob=average_log_prob)    chosen_logps = all_logps[: batch[&quot;chosen_input_ids&quot;].shape[0]]    rejected_logps = all_logps[batch[&quot;chosen_input_ids&quot;].shape[0] :]    return chosen_logps, rejected_logps, aux_loss
å‚æ•° average_log_prob æ˜¯ä»ä¸»è„šæœ¬dpo_tune.pyä¸­ä¼ é€’çš„ï¼Œå½“è®¾ç½®ä¸º True æ—¶ï¼Œå¯ç”¨é•¿åº¦å½’ä¸€åŒ–ã€‚
dpo_loss å‡½æ•°
dpo_loss ä¸­è®¡ç®— logits çš„å·®å€¼ï¼š
pi_logratios = policy_chosen_logps - policy_rejected_logpsref_logratios = reference_chosen_logps - reference_rejected_logpslogits = pi_logratios - ref_logratios
å¦‚æœ average_log_prob=Trueï¼Œåˆ™ policy_chosen_logps å’Œ policy_rejected_logps éƒ½æ˜¯å½’ä¸€åŒ–çš„å€¼ï¼Œä»è€Œå½±å“æœ€ç»ˆçš„ logits å’ŒæŸå¤±è®¡ç®—ã€‚
æ§åˆ¶å‚æ•°
åœ¨ä¸»è„šæœ¬dpo_tune.pyä¸­ï¼Œé€šè¿‡ä»¥ä¸‹ä»£ç æ§åˆ¶æ˜¯å¦å¯ç”¨é•¿åº¦å½’ä¸€åŒ–ï¼š
average_log_prob_loss_types = [&quot;simpo&quot;, &quot;dpo_norm&quot;]average_log_prob = args.dpo_loss_type in average_log_prob_loss_types
å½“ dpo_loss_type è®¾ç½®ä¸º &quot;dpo_norm&quot; æ—¶ï¼Œaverage_log_probå½’ä¸€åŒ–è¢«å¯ç”¨ï¼Œè¿›è€Œåœ¨ _get_batch_logps å’Œ concatenated_forward ä¸­è§¦å‘é•¿åº¦å½’ä¸€åŒ–é€»è¾‘ã€‚
rlvrå®è·µ
å•æœº4å¡æµ‹è¯•
è®­ç»ƒæ•°æ®é›†

.parquetè½¬ä¸ºjsonåçš„æ•°æ®é›†æ ·ä¾‹ï¼š
&#123;&quot;messages&quot;:[&#123;&quot;content&quot;:&quot;Question: Find the domain of the expression $\\frac&#123;\\sqrt&#123;x-2&#125;&#125;&#123;\\sqrt&#123;5-x&#125;&#125;$.&#125;\nAnswer:The expressions inside each square root must be non-negative.\nTherefore, $x-2 \\ge 0$, so $x\\ge2$, and $5 - x \\ge 0$, so $x \\le 5$.\nAlso, the denominator cannot be equal to zero, so $5-x&gt;0$, which gives $x&lt;5$.\nTherefore, the domain of the expression is $\\boxed&#123;[2,5)&#125;$.\n\nQuestion: If $\\det \\mathbf&#123;A&#125; = 2$ and $\\det \\mathbf&#123;B&#125; = 12,$ then find $\\det (\\mathbf&#123;A&#125; \\mathbf&#123;B&#125;).$\nAnswer:We have that $\\det (\\mathbf&#123;A&#125; \\mathbf&#123;B&#125;) = (\\det \\mathbf&#123;A&#125;)(\\det \\mathbf&#123;B&#125;) = (2)(12) = \\boxed&#123;24&#125;.$\n\nQuestion: Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight?\nAnswer:If Terrell lifts two 20-pound weights 12 times, he lifts a total of $2\\cdot 12\\cdot20=480$ pounds of weight.  If he lifts two 15-pound weights instead for $n$ times, he will lift a total of $2\\cdot15\\cdot n=30n$ pounds of weight.  Equating this to 480 pounds, we can solve for $n$: \\begin&#123;align*&#125;\n30n&amp;=480\\\\\n\\Rightarrow\\qquad n&amp;=480\/30=\\boxed&#123;16&#125;\n\\end&#123;align*&#125;\n\nQuestion: If the system of equations\n\n\\begin&#123;align*&#125;\n6x-4y&amp;=a,\\\\\n6y-9x &amp;=b.\n\\end&#123;align*&#125;has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\frac&#123;a&#125;&#123;b&#125;,$ assuming $b$ is nonzero.\nAnswer:If we multiply the first equation by $-\\frac&#123;3&#125;&#123;2&#125;$, we obtain\n\n$$6y-9x=-\\frac&#123;3&#125;&#123;2&#125;a.$$Since we also know that $6y-9x=b$, we have\n\n$$-\\frac&#123;3&#125;&#123;2&#125;a=b\\Rightarrow\\frac&#123;a&#125;&#123;b&#125;=\\boxed&#123;-\\frac&#123;2&#125;&#123;3&#125;&#125;.$$\n\nQuestion: What is the modulo $13$ residue of $247+5 \\cdot 39 + 7 \\cdot 143 +4 \\cdot 15?$&quot;,&quot;role&quot;:&quot;user&quot;&#125;],&quot;ground_truth&quot;:&quot;8&quot;,&quot;dataset&quot;:&quot;MATH&quot;,&quot;constraint_type&quot;:null,&quot;constraint&quot;:null&#125;

éªŒè¯æ•°æ®é›†

&#123;&quot;messages&quot;:[&#123;&quot;content&quot;:&quot;Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\nAnswer:There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\n\nQuestion: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nAnswer:There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\n\nQuestion: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nAnswer:Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\n\nQuestion: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\nAnswer:Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\n\nQuestion: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\nAnswer:Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\n\nQuestion: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\nAnswer:There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\n\nQuestion: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\nAnswer:Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33.\n\nQuestion: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nAnswer:Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is 8.\n\nQuestion: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?&quot;,&quot;role&quot;:&quot;user&quot;&#125;],&quot;ground_truth&quot;:&quot;3&quot;,&quot;dataset&quot;:&quot;gsm8k&quot;&#125;&#123;&quot;messages&quot;:[&#123;&quot;content&quot;:&quot;Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\nAnswer:There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\n\nQuestion: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nAnswer:There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\n\nQuestion: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nAnswer:Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\n\nQuestion: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\nAnswer:Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\n\nQuestion: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\nAnswer:Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\n\nQuestion: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\nAnswer:There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\n\nQuestion: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\nAnswer:Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33.\n\nQuestion: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nAnswer:Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is 8.\n\nQuestion: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?&quot;,&quot;role&quot;:&quot;user&quot;&#125;],&quot;ground_truth&quot;:&quot;70000&quot;,&quot;dataset&quot;:&quot;gsm8k&quot;&#125;

å•æœºè„šæœ¬é…ç½®
export WANDB_MODE=disabledexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:Truecurrent_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)LOG_SAVE=&quot;/mnt/open-instruct-main/output/rlvr/$&#123;current_time&#125;-tulu3-rlvr.log&quot;python /mnt/open-instruct-main/open_instruct/ppo_vllm_thread_ray_gtrl.py \    --dataset_mixer &#x27;&#123;&quot;/mnt/open-instruct-main/data/gsm8k_math_ground_truth_mixed&quot;: 1.0&#125;&#x27; \    --dataset_train_splits train \    --dataset_eval_mixer &#x27;&#123;&quot;/mnt/open-instruct-main/data/gsm8k_math_ground_truth&quot;: 1.0&#125;&#x27; \    --dataset_eval_splits test \    --max_token_length 2048 \    --max_prompt_token_length 2048 \    --response_length 2048 \    --model_name_or_path /mnt/LLM-Research/Llama-31-Tulu-3-8B-DPO \    --reward_model_path /mnt/LLM-Research/LLama-31-Tulu3-8B-RM \    --non_stop_penalty \    --stop_token eos \    --temperature 1.0 \    --ground_truths_key ground_truth \    --chat_template tulu \    --sft_messages_key messages \    --learning_rate 3e-7 \    --total_episodes 10000000 \    --penalty_reward_value -10.0 \    --deepspeed_stage 3 \    --per_device_train_batch_size 1 \    --local_rollout_forward_batch_size 1 \    --local_mini_batch_size 16 \    --local_rollout_batch_size 16 \    --actor_num_gpus_per_node 3 \    --vllm_tensor_parallel_size 2 \    --beta 0.05 \    --apply_verifiable_reward true \    --output_dir /mnt/open-instruct-main/output/rlvr \    --seed 3 \    --num_evals 3 \    --save_freq 100 \    --reward_model_multiplier 0.0 \    --gradient_checkpointing \    --with_tracking False 2&gt;&amp;1 | tee -a &quot;$LOG_SAVE&quot;
ä¸ºäº†å°½å¯èƒ½åœ°è¿˜åŸtulu3åŸè®ºæ–‡çš„å®éªŒç»“æœï¼Œå…¶ä¸­éƒ¨åˆ†è¶…å‚ä¸è®ºæ–‡ä¸­æœ€ä¼˜ä¿æŒä¸€è‡´ã€‚
--max_token_length 2048
--max_prompt_token_length 2048
--response_length 2048
--learning_rate 3e-7
--penalty_reward_value -10.0
--reward_model_multiplier 0.0
å…¶ä¸­çš„æ¨¡å‹æƒé‡åˆ©ç”¨tulu3å¼€æºçš„ç»è¿‡tulu3-sft dpoå¾—åˆ°çš„Llama-31-Tulu-3-8B-DPOæƒé‡ï¼Œå¥–åŠ±æ¨¡å‹é‡‡ç”¨tulu3å¼€æºçš„LLama-31-Tulu3-8B-RMã€‚
è®­ç»ƒæµ‹è¯•
åŠ è½½ckptæ—¶ä¸€ç›´å¡ä½ä¸åŠ¨ï¼Œå°šæœªè§£å†³ã€‚

]]></content>
      <categories>
        <category>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</tag>
        <tag>Nvidia</tag>
        <tag>Tulu3</tag>
      </tags>
  </entry>
  <entry>
    <title>1.5-Qwenå¤šæœºå¤šå¡è°ƒä¼˜</title>
    <url>/2025/11/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Qwen/1.5-Qwen%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%B0%83%E4%BC%98/</url>
    <content><![CDATA[1. lora
1.1 per_device_train_batch_sizeæµ‹è¯•ï¼Œæœ€ä¼˜ä¸º16



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
bf16
tflops




2
1
512
8
true
8.37


4
1
512
8
true
9.07


8
1
512
8
true
9.88


16
1
512
8
true
10.32


32
1
512
8
true
OOM



å•ä¸ªGPUæ‰¹æ¬¡å¤§å°å¢åŠ æ—¶ï¼Œéœ€è¦çš„å†…å­˜ä¹Ÿä¼šå¢åŠ ï¼ŒGPUå†…å­˜ä¸è¶³ä»¥æ”¯æŒæ›´å¤§çš„æ‰¹æ¬¡ï¼Œå¯èƒ½ä¼šå¯¼è‡´æº¢å‡ºæˆ–æ•ˆç‡é™ä½ã€‚
1.2 per_device_eval_batch_sizeæµ‹è¯•ï¼Œæœ€ä¼˜ä¸º2



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
bf16
tflops




16
1
512
8
true
10.32


16
2
512
8
true
10.33


16
4
512
8
true
10.26



1.3 model_max_lengthæµ‹è¯•ï¼Œæœ€ä¼˜ä¸º512



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
bf16
flops




16
2
128
8
true
8.95


16
2
256
8
true
9.56


16
2
512
8
true
10.33


16
2
1024
8
true
OOM



1.4 gradient_accumulation_stepsæµ‹è¯•ï¼Œæœ€ä¼˜ä¸º16



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
bf16
tflops




16
2
512
1
true
9.07


16
2
512
2
true
9.24


16
2
512
4
true
9.89


16
2
512
8
true
10.33


16
2
512
16
true
11.63



1.5 å¼€å¯Fp16æµ‹è¯•



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
tflops




16
2
512
16
true
67.13



1.6 å…³é—­gradient_checkpointing



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
gradient_checkpointing
tflops




16
2
512
16
true
true
67.13


16
2
512
16
true
false
OOM






å‚æ•°é…ç½®
per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
bf16/fp16
gradient_checkpointing
tflops




é»˜è®¤
2
1
512
8
bf16
true
8.32


è°ƒä¼˜å
16
2
512
16
fp16
true
67.13



åŸºäº2æœº8å¡çš„V100ï¼ŒQwen-7Bæ¨¡å‹çš„loraå¾®è°ƒè®­ç»ƒä¸­ï¼ˆé‡‡ç”¨deepspeedçš„zero2çš„å†…å­˜ä¼˜åŒ–å¹¶è¡Œæ–¹å¼ï¼‰ï¼ŒTflopsçš„å€¼æœ€é«˜ä¸º67.13**ï¼ˆ8.32ï¼‰
æœ€ä½³å‚æ•°é…ç½®ï¼ˆper_device_train_batch_sizeï¼š16ï¼Œper_device_eval_batch_sizeï¼š2ï¼Œmodel_max_lengthï¼š512ï¼Œgradient_accumulation_stepsï¼š16ï¼ŒFp16ç²¾åº¦ï¼Œgradient_checkpointingï¼šTrueï¼‰
2. qlora
qloraä½¿ç”¨4æ¯”ç‰¹é‡åŒ–æ¨¡å‹ä»¥åŠpaged attentionç­‰æŠ€æœ¯å®ç°æ›´å°çš„æ˜¾å­˜å¼€é”€
2.1 per_device_train_batch_sizeæµ‹è¯•ï¼Œæœ€ä¼˜ä¸º32



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
tflops




2
1
512
8
true
31.71


4
1
512
8
true
42.91


8
1
512
8
true
51.14


16
1
512
8
true
55.75


32
1
512
8
true
58.44


64
1
512
8
true
OOM



2.2 per_device_eval_batch_sizeæµ‹è¯•ï¼Œæœ€ä¼˜ä¸º4



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
tflops




32
1
512
8
true
58.44


32
2
512
8
true
58.84


32
4
512
8
true
59.05


32
8
512
8
true
58.42



2.3 model_max_lengthæµ‹è¯•ï¼Œæœ€ä¼˜ä¸º512



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
tflops




32
4
128
8
true
48.68


32
4
256
8
true
53.02


32
4
512
8
true
59.05


32
4
1024
8
true
OOM



2.4 gradient_accumulation_stepsæµ‹è¯•ï¼Œæœ€ä¼˜ä¸º



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
tflops




32
4
512
1
true
52.66


32
4
512
2
true
52.18


32
4
512
4
true
54.99


32
4
512
8
true
59.05


32
4
512
16
true
64.26


32
4
512
32
true
80.37


32
4
512
64
true
103.89



2.5 å¼€å¯BF16æµ‹è¯•



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
bf16/fp16
tflops




32
4
512
64
bf16
14.23


32
4
512
64
fp16
103.89



2.6 å…³é—­gradient_checkpointing



gradient_checkpointing
per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
tflops




True
32
4
512
64
true
103.89


False
32
4
512
64
true
OOM



DeepSpeed ZeRO 3 å¯¹èŠ‚ç‚¹é—´é€šä¿¡é€Ÿç‡çš„è¦æ±‚è¿œå¤§äº ZeRO 2ï¼Œåœ¨å¤šæœºå¾®è°ƒçš„æƒ…å†µä¸‹ä¼šå¤§å¹…é™ä½è®­ç»ƒé€Ÿåº¦ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸å»ºè®®åœ¨å¤šæœºå¾®è°ƒçš„æƒ…å†µä¸‹ä½¿ç”¨ DeepSpeed ZeRO 3 é…ç½®ã€‚



å‚æ•°é…ç½®
per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
tflops




é»˜è®¤
2
1
512
8
true
31.71


è°ƒä¼˜å
32
4
512
64
true
103.89



åŸºäº2æœº8å¡çš„V100ï¼ŒQwen-7Bæ¨¡å‹çš„qloraå¾®è°ƒè®­ç»ƒä¸­ï¼ˆé‡‡ç”¨deepspeedçš„zero2çš„å†…å­˜ä¼˜åŒ–å¹¶è¡Œæ–¹å¼ï¼‰ï¼ŒTflopsçš„å€¼æœ€é«˜ä¸º103.89 
é»˜è®¤å‚æ•°é…ç½®
æœ€ä½³å‚æ•°é…ç½®ï¼ˆgradient_checkpointingï¼šTrueï¼Œper_device_train_batch_sizeï¼š32ï¼Œper_device_eval_batch_sizeï¼š4ï¼Œmodel_max_lengthï¼š512ï¼Œgradient_accumulation_stepsï¼š64ï¼ŒFp16ç²¾åº¦ï¼‰
qloraåœ¨æ‹‰èµ·è®­ç»ƒæ—¶ï¼Œéœ€è¦å¯¹æ¨¡å‹æƒé‡è¿›è¡ŒINT-4é‡åŒ–ï¼Œä¼šæŸå¤±æ‰æƒé‡å°æ•°éƒ¨åˆ†ç²¾åº¦ï¼Œé‡åŒ–åçš„æƒé‡ç”±äºä½¿ç”¨æ›´å°‘çš„ä½æ•°æ¥è¡¨ç¤ºï¼Œå­˜å‚¨ä¸Šæ¯”åŸå§‹æƒé‡é«˜æ•ˆï¼Œåœ¨æ‹‰èµ·æ¨¡å‹è®­ç»ƒè¿‡ç¨‹è¿‡ä¸­tflopsæœ‰æ˜æ˜¾æå‡ã€‚
]]></content>
      <categories>
        <category>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>æ¨¡å‹è®­ç»ƒè°ƒä¼˜</tag>
        <tag>Nvidia</tag>
        <tag>Qwen</tag>
        <tag>Deepspeed</tag>
      </tags>
  </entry>
</search>
