<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Git1_git命令</title>
    <url>/2025/11/04/Git/Git1_git%20%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[git commmit -v和git push origin develop之后，我怎么撤销本次commit提交的代码，将origin develop的仓库返回值提交之前的状态？
在Git中，如果你想要撤销最近的一次提交并将仓库状态回退到提交之前，你可以使用git reset命令。这里有几种方法可以撤销提交：

使用git reset --soft：

这个命令会将HEAD指针移动到指定的提交，但是不会改变工作目录和索引的状态。这意味着你的更改仍然在暂存区（staged），你可以重新提交。
命令：git reset --soft HEAD~1（将HEAD指针回退到上一个提交）


使用git reset --mixed（默认选项）：

这个命令会将HEAD指针移动到指定的提交，并且将索引（staged changes）重置到HEAD的状态，但是不会改变工作目录。
命令：git reset --mixed HEAD~1（将HEAD和索引回退到上一个提交，但工作目录不变）


使用git reset --hard：

这个命令会将HEAD指针、索引和工作目录都重置到指定的提交状态。这意味着所有未提交的更改都会丢失。
命令：git reset --hard HEAD~1（将HEAD、索引和工作目录都回退到上一个提交）



如果你已经将代码推送到了远程仓库（比如origin develop），并且想要撤销这次推送，你可以使用git push命令配合--force选项来强制推送本地的更改到远程仓库。但是请注意，这会覆盖远程仓库的历史，所以只有在你确定远程仓库没有其他人的提交或者你不介意丢失这些提交时才应该使用。
命令如下：
git push origin develop --force
或者，如果你使用的是Git 2.0或更高版本，可以使用更安全的--force-with-lease选项，它会在远程分支没有新的提交时才允许强制推送。
命令如下：
git push origin develop --force-with-lease
警告：使用--force选项会覆盖远程仓库的历史，这是一个破坏性的操作。在团队协作的环境中，这可能会导致其他协作者的工作丢失，所以请谨慎使用。
]]></content>
      <categories>
        <category>开发</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>开发</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title>Git2_gerrit仓库代码提交流程</title>
    <url>/2025/11/04/Git/Git2_gerrit%E4%BB%93%E5%BA%93%E4%BB%A3%E7%A0%81%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[
生成个人密钥

ssh-keygen -C wangxiangbo_JTAGI@cmss.chinamobile.com

修改config文件，增加gerrit仓库配置，增加刚生成的个人密钥id_ed25519

host gerritport 29418hostname gerrit.cmss.comuser wangxiangbo_JTAGIIdentityFile ~/.ssh/id_ed25519

在gerrit中setting设置中，增加ssh的公钥，即id_ed25519.pub
设置邮箱，邮箱名wangxiangbo@cmss.chinamobile.com为不带JTAGI后缀的邮箱。
用ssh clone代码仓库

git clone &quot;ssh://wangxiangbo_JTAGI@gerrit.cmss.com:29418/AGI/CM_OPTIMUS&quot; &amp;&amp; scp -p -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg &quot;CM_OPTIMUS/.git/hooks/&quot;

查看分支

git branch -a

查看远端分支

git branch -r

通过远端origin/develop仓库，创建一个本地develop开发分支

git checkout -b develop origin/develop

修改代码后，add之后并commit提交

git commit -v

推送远端origin/develop仓库

git push origin HEAD:refs/for/develop

在推送过程中会出现缺失 Change-Id 的错误

gitdir=$(git rev-parse --git-dir); scp -p -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg $&#123;gitdir&#125;/hooks/
执行完毕后如果出现subsystem request failed on channel 0，则将-p修改为-O
gitdir=$(git rev-parse --git-dir); scp -O -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg $&#123;gitdir&#125;/hooks/
将本次commit提交的末尾加上Change-Id
git commit --amend --no-edit
再次push
git push origin HEAD:refs/for/develop

提交完成之后，打开gerrit，找到develop分支的gitweb
选择review
找到刚刚提交的代码
点击add reviewer，评审人要最少要两个以上
第一次之后提交代码步骤

git pull origingit add xxxgit commit -v git push origin HEAD:refs/for/develop
]]></content>
      <categories>
        <category>开发</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>开发</tag>
        <tag>gerrit</tag>
      </tags>
  </entry>
  <entry>
    <title>Git3_gitlab仓库代码提交流程</title>
    <url>/2025/11/04/Git/Git3_gitlab%E4%BB%93%E5%BA%93%E4%BB%A3%E7%A0%81%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/</url>
    <content><![CDATA[
gitlab上fork主仓库，生成个人的远端仓库origin/develop
修改git提交用户配置为九天账号

git config --global --listgit config --global user.name wangxiangbo_JTAGIgit config --global user.email wangxiangbo_JTAGI@cmss.chinamobile.com

拉取个人的远端仓库

git clone http://gitlab.cmss.com/wangxiangbo/CM_OPTIMUS.git 

查看分支

git branch -a

查看远程仓库分支

git branch -r

通过个人的远端origin/develop仓库，创建一个本地develop开发分支

git checkout -b develop origin/develop

列出所有的远程仓库以及对应的 URL

git remote -v

将远程仓库地址添加到本地Git仓库的远程仓库列表中，本地的upstream/develop仓库会和gitlab远程仓库关联起来

git remote add upstream http://gitlab.cmss.com/AGI/CM_OPTIMUS.git

upstream/develop远端仓库拉取最新的代码

git fetch upstream

将upstream/develop远端仓库的最新代码合并到本地的develop分支中

git merge upstream/develop

将本地develop最新的代码推到个人的远端仓库origin/develop中

git push origin develop

提交develop到origin/develop代码之前，保证自己的本地个人的远端仓库origin/develop和本地gitlab的远端仓库upstream/develop保持一致再提交，防止后续origin/develop合并打upstream/develop上出现冲突。

#本地代码修改后拉取最新代码#1.拉取源代码git fetch upstream #2.暂存本地修改的代码git stash#3.合并源代码到当前的develop仓库git merge upstream/develop#4.将本地修改的推送到远程的origin分支，使origin/develop和upstream/develop保持一致git push origin develop#5.将暂存的代码弹出来git stash pop

souretree上add需要提交的代码
提交add之后的代码

git commit -v
#修改下方的提交信息，第一行为提交描述信息deepseek-7B预训练Code Source From: Self CodeDescription: deepseek-7B预训练Jira: #CMOPTIMUS-1089市场项目编号（名称）：CM_OPTIMUS

将提交完之后的代码合并到origin/develop上

git push origin develop

gitlab中找的到自己仓库下CM_OPTIMUS
找到Merge requests，然后点击开始一个合并
然后选择自己的分支为develop，还有旁边的分支develop，然后点击最下方的merge即可。该操作是将自己个人远端的orgin/develop仓库提交修改后的代码合并到gitlab远端的主仓库中（upstream/develop已和其关联，并通过git fetch保持最新）
选中自己刚刚提交到origin/develop上的代码，合并到AGI/CM_OPTIMUS的develop分支上
在Write里添加合并人信息

]]></content>
      <categories>
        <category>开发</category>
        <category>git</category>
      </categories>
      <tags>
        <tag>开发</tag>
        <tag>gitlab</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux1_kimi问题</title>
    <url>/2025/11/04/Linux/Linux1_kimi%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[
权重转换代码换行符
Kimi: 出现这些错误通常是因为脚本文件在不同的操作系统环境下被编辑或创建，导致文件的换行符不一致。Unix/Linux系统使用LF（Line Feed，\n）作为换行符，而Windows系统使用CR+LF（Carriage Return + Line Feed，\r\n）作为换行符。
错误信息中的 $'\r': command not found 表示脚本尝试执行一个以 \r 开头的命令，这通常是因为脚本文件中的换行符是 \r\n 而不是 \n。
此外，syntax error near unexpected token 'elif' 错误表明脚本解析器在遇到 elif 时遇到了语法错误，这可能是因为 elif 之前的行（可能是第59行）以 \r\n 结尾，导致 elif 行实际上是从一个新的行开始，而不是在预期的位置。
要解决这个问题，你可以使用 dos2unix 工具将脚本文件从Windows格式转换为Unix/Linux格式。如果你没有 dos2unix 工具，可以使用 sed 命令来移除 \r 字符：

sed -i &#x27;s/\r$//&#x27; hf2mcore_convertor.sh
这条命令会查找文件中的所有行，并移除每行末尾的 \r 字符。
如果你有 dos2unix 工具，可以使用以下命令：
dos2unix hf2mcore_convertor.sh
这将把文件从Windows格式转换为Unix/Linux格式。
在运行这些命令之后，再次执行脚本应该就不会出现这些错误了。
2. 修改文件遇到Read-only file system
Kimi: 当你遇到 “Read-only file system” 错误时，通常意味着你的文件系统出于某种原因被挂载为只读模式。这可能是由于文件系统损坏、磁盘硬件问题、或者挂载时指定了错误的参数。以下是一些解决步骤：

检查文件系统挂载状态：
使用 mount 命令查看所有挂载的文件系统及其状态。你可以使用如下命令：

mount | grep /path/to/mount/point
查看挂载点是否为只读（ro）。
2. 重新挂载文件系统为读写模式：
如果文件系统被挂载为只读，你可以尝试重新挂载它为读写模式。使用以下命令：
sudo mount -o remount,rw /path/to/mount/point
请将 /path/to/mount/point 替换为实际的挂载点路径。
3. 检查和修复文件系统：
如果重新挂载不起作用，可能是文件系统损坏。你可以使用 fsck 命令来检查和修复文件系统。例如，对于 ext4 文件系统，可以使用：
sudo fsck -f /dev/sda1
请确保替换 /dev/sda1 为你的实际分区。
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>运维</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux2_Linux命令</title>
    <url>/2025/11/04/Linux/Linux2_linux%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[
当执行训练任务时，手动关闭训练，此时gup资源仍然占用，需要手动kill掉进程
查询正在运行的进程并gerp finetune_lora_single_gpu.sh
ps aux | grep finetune_lora_single_gpu.sh
ps aux | grep finetune_lora_ds.sh
kill -9 372813 根据上一步查出来的进程号，用kill -9强制删除
nvidia-smi 查看是否gpu显存已经清空
查看某个文件的前20行
head -n 20 train_data_law.json
查看当前文件夹的大小
du -sh .
查看当前文件夹挂载的是哪一个盘
df -h .
从大到小查看当前文件夹的下的文件大小
du -sh * | sort -rh
查看当前文件下所有文件的大小
ls -lh
把一个名为Yuan2的文件夹的所有内容（包括该文件夹名），cp到/mnt/users/wangxiangbo/nemo/model目录下，使最后的目录为 /mnt/users/wangxiangbo/nemo/model/Yuan2
cp -a Yuan2 /mnt/users/wangxiangbo/nemo/model/
通过pid查看某个进程的详细信息
&lt;font style=&quot;color:rgb(56, 58, 66);background-color:rgb(250, 250, 250);&quot;&gt;ps -fp 167891&lt;/font&gt;
查看当前目录下所有文件的磁盘占用情况
du -ah | sort -hr | head -n 20
使用以下命令将megatron-core文件夹压缩成一个名为megatron-core.zip的zip文件
zip -r megatron-core.zip megatron-core 
使用以下命令将megatron-core.zip解压缩
unzip megatron-core.zip 
hg上面下载模型

pip install -U huggingface_hubhuggingface-cli download bigscience/bloom-560m --local-dir bloom-560mhuggingface-cli download Qwen/Qwen2-7B-Instruct --local-dir Qwen2-7B-Instructhuggingface-cli download Qwen/Qwen2.5-7B-Instruct --local-dir Qwen2.5-7B-Instructhuggingface-cli download BAAI/IndustryCorpus_computer --repo-type dataset --local-dir IndustryCorpus_computerhuggingface-cli download BAAI/IndustryCorpus2_current_affairs_government_administration --repo-type dataset --local-dir government_administrationhuggingface-cli download ShengbinYue/DISC-Law-SFT --repo-type dataset --local-dir DISC-Law-SFThuggingface-cli download TigerResearch/sft_zh

归档压缩文件

#-c：创建一个新的压缩文件。#-z：通过 gzip 压缩文件。#-v：显示详细的压缩过程。#-f：指定压缩后的文件名，这里是 colossalai.tar.gztar -czvf colossalai.tar.gz colossalai/  #-x：表示解压。#-z：表示解压 .gz 格式的文件。#-v：显示解压过程。#-f：指定解压的文件tar -xzvf file.tar.gz
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>运维</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>运维1_docker命令</title>
    <url>/2025/11/04/%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%90%E7%BB%B4/%E8%BF%90%E7%BB%B41_docker%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[
启动一个容器

sudo docker run -it --name qwen --gpus all nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04
启动一个容器后，又通过exit关闭。发现docker ps没有了，该如何重新启动这个qwen容器？
docker ps -adocker start qwendocker exec -it 容器id bash

拉取一个新镜像后，通过这个镜像创建一个容器。

docker run -it --name qwen --gpus all 镜像名:镜像tags bash

将此时qwen容器打成镜像

docker commit -a &quot;wangxiangbo&quot; -m &quot;qwen 7B&quot; 02649afd9710 qwen-7b:v1.0
]]></content>
      <categories>
        <category>运维</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>运维</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>前端1_调用api接口的写法</title>
    <url>/2025/11/06/%E5%89%8D%E7%AB%AF/%E5%89%8D%E7%AB%AF1_%E8%B0%83%E7%94%A8api%E6%8E%A5%E5%8F%A3%E7%9A%84%E5%86%99%E6%B3%95/</url>
    <content><![CDATA[Get请求写法
1. 看Parameters中是否有需要带参数，其中Authorization为授权认证的token可以不用考虑。
1.1 若是没有其余的参数限定，那么SyncRequestFuncType&lt;请求参数类型,响应返回参数类型&gt;，第一个参数就为void或者undefined。返回类型可以使用any，等获取到响应参数之后，在对其通过Interface/type进行明确。
export const getApi: SyncRequestFuncType&lt;void,responseType&gt; = () =&gt; &#123;return javaAxios(&#123; method: &quot;get&quot;, url: &quot;xxx/xxxx&quot;,&#125;);&#125;;
1.2 若是有其余参数限定，这时需要箭头函数中需要带着params来进行请求，且reuturn中也需要将params带着。
1.2.1 `单个参数`，例如id，这时在请求参数类型中，需要明确出请求参数的类型&lt;&#123;id:string&#125;,responseType&gt;，这时请求参数的类型为对象&#123;&#125;的形式，可以直接在尖括号中写出来，也可以通过引入定义的Interface/type类型来写&lt;IdType,responseType&gt;。

export const getApi: SyncRequestFuncType&lt; &#123;id:string&#125;, void&gt; = (params) =&gt; &#123;return javaAxios(&#123; method: &quot;get&quot;, url: &quot;xxx/xxxx&quot;, params,&#125;);&#125;;
1.2.2 `多个参数`，例如email，type...，因为参数比较多，最好通过Interface/type的方式SendEmailCaptchaForUserInfoModifyType来明确请求参数类型。

export type SendEmailCaptchaForUserInfoModifyType = &#123;type: ModifyType;email?: string;&#125;;
export const getApi: SyncRequestFuncType&lt;SendEmailCaptchaForUserInfoModifyType,void&gt; = (params) =&gt; &#123;return javaAxios(&#123; method: &quot;get&quot;, url: &quot;xxx/xxx&quot;, params,&#125;);&#125;;
Post请求写法
1. 看看Parameters中是否有需要带参数，其中Authorization为授权认证的token可以不用考虑。
1.1 若是没有其余的参数限定，那么SyncRequestFuncType&lt;请求参数类型,响应返回参数类型&gt;，第一个参数就为void或者undefined。返回类型可以使用any，等获取到响应参数之后，在对其通过Interface/type进行明确。
export const clearVideoRecycle: SyncRequestFuncType&lt;void, void&gt; = () =&gt; &#123;return javaAxios(&#123; method: &quot;post&quot;, url: &quot;xxx/xxx&quot;,&#125;);&#125;;
1.2 若是有其余参数限定，这时需要箭头函数中需要带着params来进行请求，且reuturn中也需要将params带着。
1.2.1 单个参数时

export const cancelSubscribeVideo: SyncRequestFuncType&lt;&#123; themeId: string &#125;,any&gt; = (params) =&gt; &#123;return javaAxios(&#123; method: &quot;post&quot;, url: &quot;xxx/xxx&quot;, params,&#125;);&#125;;
1.2.2 多个参数时

export type SendEmailCaptchaForUserInfoModifyType = &#123;type: ModifyType;email?: string;&#125;;
export const sendEmailCaptchaForUserInfoModify: SyncRequestFuncType&lt;SendEmailCaptchaForUserInfoModifyType,void&gt; = (params) =&gt; &#123;return javaAxios(&#123; method: &quot;get&quot;, url: &quot;xxx/xxx&quot;, params,&#125;);&#125;;
2. 当出现请求体Request body时，这时就需要在return的javaAxios中添加键值对data: params，来将参数添加到请求体中传递过去。
2.1 Request body为：
[&quot;string&quot;]
export const batchDeleteVideo: SyncRequestFuncType&lt;string[], void&gt; = (params) =&gt; &#123;return javaAxios(&#123; method: &quot;post&quot;, url: &quot;videos/themes/batchDel&quot;, data: params,&#125;);&#125;;
2.2 Request body为：这时候需要在data中进一步在约束一下，使其对应api的请求体的格式。
&#123;&quot;themes&quot;: [ &quot;string&quot;]&#125;
export const sortVideos: SyncRequestFuncType&lt;string[], void&gt; = (params) =&gt; &#123;return javaAxios(&#123; method: &quot;post&quot;, url: &quot;videos/themes/changeVideoThemesSort&quot;, data: &#123; themes: params &#125;,&#125;);&#125;;
]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>前端</tag>
        <tag>Javascript</tag>
      </tags>
  </entry>
  <entry>
    <title>测试 Hexo 标签分类2</title>
    <url>/2025/10/31/%E9%A1%B5%E9%9D%A2%E6%B5%8B%E8%AF%95/test%20copy/</url>
    <content><![CDATA[这是文章正文。
]]></content>
      <categories>
        <category>测试</category>
      </categories>
      <tags>
        <tag>测试2</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>运维2_k8s命令</title>
    <url>/2025/11/04/%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%90%E7%BB%B4/%E8%BF%90%E7%BB%B42_k8s%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[
获取特定命名空间的详细信息

kubectl get ns namespace-name -o wide

kubectl通过ns获取所有的pods的详细信息

kubectl get pods -n &lt;ns_name&gt; -o wide

删除启动失败的pod，可以直接删除启动时的yaml，否则pod会自动重启

kubectl delete -f qwentest.yaml

查看所有的node的lable标签信息

kubectl get node --show-labelskubectl get node xxx --show-labelskubectl get node --show-labels | grep model

打标签

kubectl label nodes ecs-jhjs-1234-003 key=vaule

删除某节点的lable标签

kubectl label nodes cce100-64-29-79.cce-stack.com model-

创建一个命名空间namespace

kubectl create namespace xxx

查看所有节点的标签

kubectl get nodes --show-labelskubectl get nodes --show-labels | grep model=llama2-70b
]]></content>
      <categories>
        <category>运维</category>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>运维</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>xinference推理实践</title>
    <url>/2026/01/08/LLM/%E6%8E%A8%E7%90%86/xinference%E6%8E%A8%E7%90%86%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[NV生态
容器创建
docker run -d --restart=always --name=xinference_wxb \ -v /mnt/users/wangxiangbo/xinference:/opt/xinference -e XINFERENCE_HOME=/opt/xinference -e XINFERENCE_MODEL_SRC=modelscope \ -p 9998:9997 --gpus all registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:latest xinference-local -H 0.0.0.0
docker run -d --restart=always --name=xinference_wxb \ -v /mnt/users/wangxiangbo/xinference:/opt/xinference -e XINFERENCE_HOME=/opt/xinference -e XINFERENCE_MODEL_SRC=hf-mirror.com \ -p 9998:9997 --gpus all registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:latest xinference-local -H 0.0.0.0
docker run -d --restart=always --name=xinference_wxb \ -v /mnt/users/wangxiangbo/xinference:/opt/xinference -e XINFERENCE_HOME=/opt/xinference -p 9998:9997 --gpus all registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:latest xinference-local -H 0.0.0.0
进入容器
docker exec -it xinference_wxb bash
服务创建
开启xinference-local服务
root@979723eca011:/opt/inference/examples# xinference-local


web端调用


选择待推理的LLM模型


控制台开始下载对应的Qwen1.5-Chat-1.8B模型










终端API调用
from gradio_client import Clientclient = Client(&quot;http://127.0.0.1:54696/qwen1.5-chat/&quot;)result = client.predict(		message=&quot;Hello!!&quot;,		request=512,		param_3=1,		param_4=&quot;Hello!!&quot;,		api_name=&quot;/chat&quot;)print(result)


HW生态
容器创建
docker run -d --restart=always --name=xinference_wxb \ -v /reason-sharedata/training_inference/wangxiangbo/xinference:/opt/xinference -e XINFERENCE_HOME=/opt/xinference -e XINFERENCE_MODEL_SRC=modelscope \ -p 9998:9997 registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:nightly-main-cpu xinference-local -H 0.0.0.0




由于Xinference框架的最新镜像需要配合GPU来使用，故本次利用cpu版本进行推理验证。




服务创建
]]></content>
      <categories>
        <category>模型推理</category>
        <category>xinference</category>
      </categories>
      <tags>
        <tag>模型推理</tag>
        <tag>NVIDIA</tag>
        <tag>V100</tag>
        <tag>xinference</tag>
      </tags>
  </entry>
  <entry>
    <title>ColossalAI训练框架解读</title>
    <url>/2026/01/08/LLM/%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6/ColossalAI/</url>
    <content><![CDATA[框架特点
Colossal-AI 框架中的 Gemini 是一种内存优化策略，旨在通过智能的内存管理和计算优化来减少显存占用，从而支持更大规模的模型训练。Gemini 的核心思想是通过 动态内存管理 和 计算优化 来最大化显存利用率，同时减少内存碎片和冗余计算。
以llama2为例的验证
增加每部迭代打印以及tflops打印
ColossalAI初始打印效果






脚本
#!/bin/bash# NCCL IB environment variablesexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1export NCCL_IB_DISABLE=0export NCCL_SOCKET_IFNAME=eth0export NCCL_IB_GID_INDEX=3export NCCL_IB_TIMEOUT=23export NCCL_IB_RETRY_CNT=7export OMP_NUM_THREADS=4current_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)LOG_SAVE=&quot;/mnt/colossalai/output/log/$&#123;current_time&#125;-llama2-pretrain.log&quot;PROJECT_NAME=&quot;llama2-7B-pretrain&quot;PARENT_SAVE_DIR=&quot;/mnt/colossalai/output/save/&quot;PARENT_TENSORBOARD_DIR=&quot;/mnt/colossalai/output/tensorboard/&quot;PARENT_CONFIG_FILE=&quot;/mnt/colossalai/output/config/&quot;PRETRAINED_MODEL_PATH=&quot;/mnt/colossalai/weight/Llama-2-7b-hf&quot;declare -a dataset=( &quot;/mnt/colossalai/dataset/llama2_pretraindata/arrow/part-00000&quot;)TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)FULL_PROJECT_NAME=&quot;$&#123;PROJECT_NAME&#125;-$&#123;TIMESTAMP&#125;&quot;SAVE_DIR=&quot;$&#123;PARENT_SAVE_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;TENSORBOARD_DIR=&quot;$&#123;PARENT_TENSORBOARD_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;CONFIG_FILE=&quot;$&#123;PARENT_CONFIG_FILE&#125;$&#123;FULL_PROJECT_NAME&#125;.json&quot;colossalai run --nproc_per_node 4 --master_port 30012 train.py \    --pretrained $PRETRAINED_MODEL_PATH \    --dataset $&#123;dataset[@]&#125; \    --plugin &quot;gemini&quot; \    --microbatch_size 1 \    --batch_size 4 \    --save_interval 400 \    --save_dir $SAVE_DIR \    --tensorboard_dir $TENSORBOARD_DIR \    --config_file $CONFIG_FILE \    --num_epochs 1 \    --accumulation_steps 1 \    --lr 1e-4 \    --mixed_precision &quot;fp16&quot; \    --grad_clip 1.0 \    --weight_decay 0.01 \    --warmup_steps 100 \    --use_grad_checkpoint \    --pad_token &quot;unk&quot; \    --max_length 1024 2&gt;&amp;1 | tee -a &quot;$LOG_SAVE&quot;
超参设置/GPU利用率/TFlops
mixed_precision：fp16/plugin：gemini /mbs：1/gbs：4




mixed_precision：fp16/plugin：gemini_auto /mbs：1/gbs：4




mixed_precision：bf16/plugin：gemini_auto /mbs：1/gbs：4




mixed_precision：fp16/plugin：zero2  /mbs：1/gbs：4




Gemini并行策略分析
传统并行策略 vs Gemini



并行策略
主要特点
主要问题




数据并行（DP）
每个 GPU 计算整个模型的一部分数据，最终聚合梯度
计算冗余，显存消耗大


张量并行（TP）
不同 GPU 处理矩阵乘法的不同部分，减少单卡负担
通信开销大，负载不均衡


流水线并行（PP）
将模型层划分到多个 GPU，流水线方式进行计算
依赖严格的同步，调度复杂


ZeRO（Zero Redundancy Optimizer）
ZeRO-1: 仅切分优化器状态   ZeRO-2: 切分优化器+梯度   ZeRO-3: 进一步切分模型参数
计算和通信开销较高


Gemini
混合 ZeRO-2/3 + 计算和数据自动调度
适用于更大规模模型



Gemini的关键优化点




优化了参数切分策略
在 GeminiPlugin 里，shard_param_frac 和 offload_optim_frac 控制了参数切分和优化器状态的存储：
shard_param_frac: float = 1.0  # 1.0 相当于 ZeRO-3, 0.0 相当于 ZeRO-2offload_optim_frac: float = 0.0  # 控制优化器状态是否下放到 CPU

**如果 ****shard_param_frac=1.0**，那么所有参数都会被 shard（类似 ZeRO-3）。
**如果 ****shard_param_frac=0.0**，则不会对参数进行 shard（类似 ZeRO-2）。
**如果 ****offload_optim_frac=1.0**，优化器状态可以下放到 CPU，减少 GPU 占用。

计算图的智能分配
Gemini 结合 TP（张量并行）+ DP（数据并行）+ ZeRO 进行计算优化：
self.pg_mesh = ProcessGroupMesh(self.zero_size, self.extra_dp_size, self.tp_size)
其中：

self.zero_group 负责 ZeRO 参数管理（类似 ZeRO-2/3）。
self.extra_dp_group 控制 额外的数据并行组。
self.tp_group 负责 张量并行（TP）。

这样，Gemini 允许 动态切换并行方式，避免静态分配导致的资源浪费。
自适应内存管理
GPU-CPU 混合存储
Gemini 提供了 placement_policy 参数：
placement_policy: str = &quot;static&quot;  # 也可设为 &quot;auto&quot;

&quot;static&quot; 模式：手动设定 shard_param_frac 和 offload_optim_frac 来控制 ZeRO 级别。
&quot;auto&quot; 模式：Gemini 自动检测显存使用情况，智能分配参数到 CPU/GPU，防止 OOM。

GPU 显存利用率优化
warmup_non_model_data_ratio: float = 0.8  # 训练前预留显存steady_cuda_cap_ratio: float = 0.9  # 训练稳定后，模型最大使用显存比例

训练初期，预留 80% 显存给非模型数据（如激活值缓存）。
训练稳定后，Gemini 允许模型占用 90% 显存，提高利用率。

混合精度训练
Gemini 通过 precision 参数支持 fp16/bf16 训练：
precision: str = &quot;fp16&quot;

**fp16**：减少显存占用，提升计算速度。
**bf16**：训练稳定性更好，适用于 NVIDIA A100/H100。

此外，它还支持 master_weights（优化器保留 FP32 权重）：
master_weights: bool = True

这样可以在 fp16 计算时避免精度下降。

梯度同步优化
Gemini 允许 异步梯度同步，减少通信瓶颈：
enable_async_reduce: bool = True
当 enable_async_reduce=True 时：

反向传播时，梯度不会立即同步，而是 累积多个 step 之后再同步。
这样可以 减少通信次数，提高 GPU 计算效率。

ZeRO-3 vs Gemini



特性
ZeRO-3
Gemini




参数切分
仅参数
参数 + 计算状态（更全面）


优化器状态存储
GPU / CPU（手动配置）
动态调度（自动调整 GPU/CPU）


梯度同步
每步梯度同步
支持异步梯度同步


显存利用率优化
需要手动调整 stage
自动分配显存


计算图优化
仅限 DP/TP/PP
自动适配 DP/TP/PP


适用场景
静态任务
适用于动态任务



代码示例
（1）传统 ZeRO-3
plugin = LowLevelZeroPlugin(stage=3, precision=&quot;fp16&quot;)
(2) Gemini 自动优化
plugin = GeminiPlugin(precision=&quot;fp16&quot;, placement_policy=&quot;auto&quot;, enable_async_reduce=True)

placement_policy=&quot;auto&quot;自动调整参数在 CPU/GPU 之间的存放。
enable_async_reduce=True开启异步梯度同步，减少通信开销。

Gemini vs Gemini_auto
主要区别


**gemini**

placement_policy=&quot;static&quot;：在这个模式下，Gemini 使用固定的参数切分策略，用户需要手动设置 shard_param_frac、offload_optim_frac 等参数来控制模型参数和优化器状态的存放位置（比如是否存放在 CPU 或 GPU 上）。
在这种模式下，所有的参数切分、存储和计算资源的调度都由用户指定。虽然能获得较高的精细控制，但也需要用户进行更多的调优工作。

**gemini_auto**:

placement_policy=&quot;auto&quot;：在这种模式下，Gemini 会自动根据硬件资源（如 GPU 显存、CPU 显存等）的情况动态调整参数切分和存放，而不需要用户手动指定。例如，当 GPU 内存不足时，它会自动将部分参数转移到 CPU。
自动的内存管理策略可以减少用户的配置工作，同时提高训练过程中的显存利用率，避免 OOM（内存溢出）错误。

内存和显存分配策略
在 gemini 模式下，用户需要手动设置内存和计算资源的分配。例如：
shard_param_frac: float = 1.0  # 控制参数是否进行切分offload_optim_frac: float = 0.0  # 控制优化器状态是否转移到 CPU

shard_param_frac=1.0 会将所有参数切分到不同的 GPU 上，类似于 ZeRO-3。
offload_optim_frac=0.0 则意味着优化器的状态仍然保留在 GPU 上。

需要手动调节显存利用率（例如，通过设置 shard_param_frac 和 offload_optim_frac 来控制参数和优化器的存放位置）。适合那些 对内存使用有较高控制需求 的场景，但需要更多的人工调试和优化。
在 gemini_auto 模式下，Gemini 会自动优化内存的使用。例如：
placement_policy=&quot;auto&quot;

自动根据硬件状态调整内存和参数的存放位置。
适合在 没有足够硬件信息或不希望进行手动调优 的情况下使用。特别是在多 GPU 或多个机器的训练场景中，自动的内存管理可以有效避免 OOM 和提升显存利用率。

这种模式会根据当前 GPU 的负载、显存的剩余情况等因素，动态决定是否将部分模型参数或优化器状态从 GPU 转移到 CPU 上，减少 GPU 内存的占用，避免显存溢出。这种模式通常会在训练开始时进行预热，即根据模型和数据的特点自动调整参数的分配方式，以保证训练过程中内存的高效利用。
适用场景

**gemini**：适合 **精确控制 **内存使用和训练过程的场景，尤其是当用户对硬件资源有深入了解并能进行手动调优时。适用于超大规模训练，特别是需要显式控制模型切分和存储位置的情况。
**gemini_auto**：适合那些 **希望减少人工调优 **的用户，特别是在训练过程中动态调整资源和内存时。它使得训练过程更加灵活，特别是在面对 GPU 内存瓶颈 和 多卡训练 时，自动调整内存和计算资源分配能够提高训练效率。

总结
Gemini 的关键改进点：

参数切分更加灵活：支持 ZeRO-2/3 之间的动态调整。
计算图智能优化：结合 TP + DP + ZeRO 进行负载均衡。
显存管理更高效：自动调整参数在 CPU/GPU 之间的存放，防止 OOM。
混合精度支持：FP16/BF16 计算，提高计算效率。
异步梯度同步：减少通信开销，提高 GPU 利用率。

]]></content>
      <categories>
        <category>模型训练框架</category>
        <category>ColossalAI</category>
      </categories>
      <tags>
        <tag>NVIDIA</tag>
        <tag>V100</tag>
        <tag>模型训练调优</tag>
        <tag>ColossalAI</tag>
      </tags>
  </entry>
  <entry>
    <title>vllm+mindie推理实践</title>
    <url>/2026/01/08/LLM/%E6%8E%A8%E7%90%86/vllm+mindie%E6%8E%A8%E7%90%86%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[vllm
镜像
registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:vllm或registry.paas/library/ray-vllm-inference:v2.0
容器
docker run -it \--name vllm-wxb \--gpus all \--network host -p 8010:8010 \--shm-size 11g \-v /mnt/users/wangxiangbo/:/mnt \-v /usr/local/cuda-12.2:/usr/local/cuda-12.2 \-w /mnt \registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:vllm
DeepSeek-R1-Distill-Qwen-1.5B
开启vllm推理服务
为了探究所需的最低GPU显存占用，gpu-memory-utilization从0.1开始测试。
python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.1 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 1


python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.2 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 1


api测试
curl &quot;http://127.0.0.1:8010/v1/chat/completions&quot; \-H &quot;Content-Type: application/json&quot; \-d &#x27;&#123;&quot;model&quot;: &quot;/mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B&quot;,&quot;messages&quot;: [    &#123;        &quot;role&quot;: &quot;user&quot;,        &quot;content&quot;: &quot;I like Beijing, because&quot;    &#125;],&quot;max_tokens&quot;:512,&quot;presence_penalty&quot;: 1.2,&quot;frequency_penalty&quot;: 1.0,&quot;temperature&quot;: 0.7,&quot;top_p&quot;: 0.9&#125;&#x27;


evalscope压测
evalscope安装
#安装eval-scope测试环境，使用源码安装git clone https://github.com/modelscope/evalscope.gitpip install -e .
启动压测服务
:::tips
参考eval-scope文档：https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/parameters.html
:::
evalscope perf --url &#x27;http://127.0.0.1:8010/v1/chat/completions&#x27; \--parallel 10 \--model &#x27;/mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B&#x27; \--log-every-n-query 10 \--read-timeout=120 \--max-tokens 512 \--dataset-path &#x27;/mnt/script/open_qa.jsonl&#x27; \-n 30 \--max-prompt-length 12800 \--api openai \--stream \--stop &#x27;&lt;｜end▁of▁sentence｜&gt;&#x27; \--dataset openqa \--debug
DeepSeek-R1-Distill-Qwen-7B
python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.2 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/deepseek_qwen_hf \--trust-remote-code \--dtype float16 \--tensor-parallel-size 1


python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.5 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/deepseek_qwen_hf \--trust-remote-code \--dtype float16 \--tensor-parallel-size 1


DeepSeek-R1-Distill-Llama-8B
python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.5 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Llama-8B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 1


python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.6 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Llama-8B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 1


DeepSeek-R1-Distill-Qwen-14B
python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.4 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 2


python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.5 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 2


DeepSeek-R1-Distill-Qwen-32B
python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.5 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 4


python3 -m vllm.entrypoints.openai.api_server \--host 0.0.0.0 --port 8010 \--gpu-memory-utilization 0.6 \--max-model-len=1024 \--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \--trust-remote-code \--dtype float16 \--tensor-parallel-size 4


压测结果

mindie
镜像
aiops-8af5363b.ecis.huhehaote-1.cmecloud.cn/public/mindie-develop-inference-model:1.0.rc3-pytorch2.1.0-py3.10-npu-cann8.0.rc3-ubuntu22.04
容器
sudo docker run --name mindie-wxb -dit --net=host --shm-size=100g \--privileged=true \-w /workspace \--device=/dev/davinci_manager \--device=/dev/hisi_hdc \--device=/dev/devmm_svm \--entrypoint=bash \-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \-v /usr/local/dcmi:/usr/local/dcmi \-v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \-v /usr/local/sbin:/usr/local/sbin \-v /as-sharedata-1/wangxiangbo/mindie:/workspace \-v /mnt/weight-1:/job \-v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime \-e http_proxy=$http_proxy \-e https_proxy=$https_proxy \aiops-8af5363b.ecis.huhehaote-1.cmecloud.cn/public/mindie-develop-inference-model:1.0.rc3-pytorch2.1.0-py3.10-npu-cann8.0.rc3-ubuntu22.04
激活环境
source /usr/local/Ascend/nnal/atb/set_env.shsource /usr/local/Ascend/mindie/set_env.shsource /usr/local/Ascend/ascend-toolkit/set_env.shsource /usr/local/Ascend/llm_model/set_env.sh
config推理配置修改
vim /usr/local/Ascend/mindie/latest/mindie-service/conf/config.json
&#123;  &quot;Version&quot;: &quot;1.0.0&quot;,  &quot;LogConfig&quot;: &#123;    &quot;logLevel&quot;: &quot;Info&quot;,    &quot;logFileSize&quot;: 20,    &quot;logFileNum&quot;: 20,    &quot;logPath&quot;: &quot;logs/mindservice.log&quot;  &#125;,  &quot;ServerConfig&quot;: &#123;    &quot;ipAddress&quot;: &quot;10.208.200.67&quot;,    &quot;managementIpAddress&quot;: &quot;127.0.0.2&quot;,    &quot;port&quot;: 16384,    &quot;managementPort&quot;: 1026,    &quot;metricsPort&quot;: 1027,    &quot;allowAllZeroIpListening&quot;: false,    &quot;maxLinkNum&quot;: 1000,    &quot;httpsEnabled&quot;: false,    &quot;fullTextEnabled&quot;: false,    &quot;tlsCaPath&quot;: &quot;security/ca/&quot;,    &quot;tlsCaFile&quot;: [      &quot;ca.pem&quot;    ],    &quot;tlsCert&quot;: &quot;security/certs/server.pem&quot;,    &quot;tlsPk&quot;: &quot;security/keys/server.key.pem&quot;,    &quot;tlsPkPwd&quot;: &quot;security/pass/key_pwd.txt&quot;,    &quot;tlsCrl&quot;: &quot;security/certs/server_crl.pem&quot;,    &quot;managementTlsCaFile&quot;: [      &quot;management_ca.pem&quot;    ],    &quot;managementTlsCert&quot;: &quot;security/certs/management/server.pem&quot;,    &quot;managementTlsPk&quot;: &quot;security/keys/management/server.key.pem&quot;,    &quot;managementTlsPkPwd&quot;: &quot;security/pass/management/key_pwd.txt&quot;,    &quot;managementTlsCrl&quot;: &quot;security/certs/management/server_crl.pem&quot;,    &quot;kmcKsfMaster&quot;: &quot;tools/pmt/master/ksfa&quot;,    &quot;kmcKsfStandby&quot;: &quot;tools/pmt/standby/ksfb&quot;,    &quot;inferMode&quot;: &quot;standard&quot;,    &quot;interCommTLSEnabled&quot;: false,    &quot;interCommPort&quot;: 1121,    &quot;interCommTlsCaFile&quot;: &quot;security/grpc/ca/ca.pem&quot;,    &quot;interCommTlsCert&quot;: &quot;security/grpc/certs/server.pem&quot;,    &quot;interCommPk&quot;: &quot;security/grpc/keys/server.key.pem&quot;,    &quot;interCommPkPwd&quot;: &quot;security/grpc/pass/key_pwd.txt&quot;,    &quot;interCommTlsCrl&quot;: &quot;security/certs/server_crl.pem&quot;,    &quot;openAiSupport&quot;: &quot;vllm&quot;  &#125;,  &quot;BackendConfig&quot;: &#123;    &quot;backendName&quot;: &quot;mindieservice_llm_engine&quot;,    &quot;modelInstanceNumber&quot;: 1,    &quot;npuDeviceIds&quot;: [      [        0      ]    ],    &quot;tokenizerProcessNumber&quot;: 8,    &quot;multiNodesInferEnabled&quot;: false,    &quot;multiNodesInferPort&quot;: 1120,    &quot;interNodeTLSEnabled&quot;: true,    &quot;interNodeTlsCaFile&quot;: &quot;security/grpc/ca/ca.pem&quot;,    &quot;interNodeTlsCert&quot;: &quot;security/grpc/certs/server.pem&quot;,    &quot;interNodeTlsPk&quot;: &quot;security/grpc/keys/server.key.pem&quot;,    &quot;interNodeTlsPkPwd&quot;: &quot;security/grpc/pass/mindie_server_key_pwd.txt&quot;,    &quot;interNodeTlsCrl&quot;: &quot;security/grpc/certs/server_crl.pem&quot;,    &quot;interNodeKmcKsfMaster&quot;: &quot;tools/pmt/master/ksfa&quot;,    &quot;interNodeKmcKsfStandby&quot;: &quot;tools/pmt/standby/ksfb&quot;,    &quot;ModelDeployConfig&quot;: &#123;      &quot;maxSeqLen&quot;: 2048,      &quot;maxInputTokenLen&quot;: 2048,      &quot;truncation&quot;: false,      &quot;ModelConfig&quot;: [        &#123;          &quot;modelInstanceType&quot;: &quot;Standard&quot;,          &quot;modelName&quot;: &quot;deepseek-r1-distill-qwen-7b&quot;,          &quot;modelWeightPath&quot;: &quot;/job/huggingface/DeepSeek-R1-Distill-qwen-7B&quot;,          &quot;worldSize&quot;: 1,          &quot;cpuMemSize&quot;: 5,          &quot;npuMemSize&quot;: 1,          &quot;backendType&quot;: &quot;atb&quot;        &#125;      ]    &#125;,    &quot;ScheduleConfig&quot;: &#123;      &quot;templateType&quot;: &quot;Standard&quot;,      &quot;templateName&quot;: &quot;Standard_LLM&quot;,      &quot;cacheBlockSize&quot;: 128,      &quot;maxPrefillBatchSize&quot;: 50,      &quot;maxPrefillTokens&quot;: 48000,      &quot;prefillTimeMsPerReq&quot;: 150,      &quot;prefillPolicyType&quot;: 0,      &quot;decodeTimeMsPerReq&quot;: 50,      &quot;decodePolicyType&quot;: 0,      &quot;maxBatchSize&quot;: 200,      &quot;maxIterTimes&quot;: 512,      &quot;maxPreemptCount&quot;: 0,      &quot;supportSelectBatch&quot;: false,      &quot;maxQueueDelayMicroseconds&quot;: 5000    &#125;  &#125;&#125;
启动推理服务
增加权限
CONFIG_FILE=/usr/local/Ascend/mindie/latest/mindie-service/conf/config.jsonmodelWeightPath=/job/huggingface/DeepSeek-R1-Distill-qwen-7Bchmod -R 640 $modelWeightPathchmod 640 $modelWeightPathchmod 640 $CONFIG_FILE
启动推理
cd /usr/local/Ascend/mindie/latest/mindie-servicenohup ./bin/mindieservice_daemon &gt; output.log 2&gt;&amp;1 &amp;tail -f output.log










evalscope压测
evalscope安装




启动压测服务
]]></content>
      <categories>
        <category>模型推理</category>
        <category>vllm+mindie</category>
      </categories>
      <tags>
        <tag>模型推理</tag>
        <tag>NVIDIA</tag>
        <tag>V100</tag>
        <tag>昇腾</tag>
        <tag>910B</tag>
        <tag>vllm+mindie</tag>
      </tags>
  </entry>
  <entry>
    <title>ms-swift框架微调测试</title>
    <url>/2026/01/08/LLM/%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6/ms-swift%E6%A1%86%E6%9E%B6/</url>
    <content><![CDATA[ms-swift 框架微调
环境搭建
训练脚本
基于政企类数据集的 qwen2.5-7B-Ins-lora 微调
# 22GBCUDA_VISIBLE_DEVICES=0,1,2,3 \swift sft \    --model /mnt/model/Qwen/Qwen2.5-7B-Instruct \    --train_type lora \    --dataset /mnt/ms-swift/data/train.jsonl \    --torch_dtype float16 \    --num_train_epochs 6 \    --per_device_train_batch_size 1 \    --per_device_eval_batch_size 1 \    --learning_rate 1e-4 \    --lora_rank 8 \    --lora_alpha 32 \    --target_modules all-linear \    --gradient_accumulation_steps 1 \    --eval_steps 50 \    --save_steps 50 \    --save_total_limit 5 \    --logging_steps 1 \    --max_length 2048 \    --output_dir /mnt/ms-swift/output \    --system &#x27;你是一个苏州政企问答小助手&#x27; \    --warmup_ratio 0.05 \    --dataloader_num_workers 4 \    --model_author swift \    --model_name swift-qwen2.5-7b
基于政企类数据集的 qwen2.5-0.5B-Ins-全参微调
# 76GiBCUDA_VISIBLE_DEVICES=0,1,2,3 \swift sft \    --model /mnt/model/Qwen2.5-0.5B-Instruct \    --train_type full \    --dataset /mnt/ms-swift/data/train.jsonl \    --torch_dtype float32 \    --num_train_epochs 6 \    --per_device_train_batch_size 1 \    --per_device_eval_batch_size 1 \    --learning_rate 1e-5 \    --gradient_accumulation_steps 16 \    --eval_steps 100 \    --save_steps 100 \    --save_total_limit 5 \    --logging_steps 1 \    --max_length 2048 \    --output_dir /mnt/ms-swift/output \    --system &#x27;You are a helpful assistant.&#x27; \    --warmup_ratio 0.05 \    --dataloader_num_workers 4 \    --model_author swift \    --model_name swift-qwen2.5-0.5b
训练
日志


显存占用情况


loss 曲线


推理测试
开启 swift web 界面服务
swift web-ui --lang zh --server_port 16686







]]></content>
      <categories>
        <category>模型微调</category>
        <category>ms-swift</category>
      </categories>
      <tags>
        <tag>NVIDIA</tag>
        <tag>V100</tag>
        <tag>模型微调</tag>
        <tag>ms-swift</tag>
      </tags>
  </entry>
  <entry>
    <title>tulu3论文解读</title>
    <url>/2026/01/08/LLM/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/tulu3%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[语言模型的后训练可以改进其性能并解锁新技能，为了缩小开源方法在这方面通常落后于专有闭源技术这一差距，研究者推出了TÜLU3，一个完全开源的高级后训练模型系列，包括其数据、代码和训练方法。基于Llama 3.1基础模型，TÜLU3的表现优于多个知名模型，如Llama 3.1、Qwen 2.5、Mistral等，并且在某些方面甚至超过了GPT-4o-mini和Claude 3.5-Haiku这样的闭源模型。TÜLU3采用**监督微调(SFT)、直接偏好优化(DPO)以及一种新的可验证奖励强化学习(RLVR)**方法进行训练。此外，还建立了一个全面的多任务评估框架，涵盖了开发与未见测试、标准基准及对现有开源数据集的大规模清理。最后，项目提供了模型权重、示例、数据集、工具包、训练代码和详细报告，以便他人复制结果并将这些技术应用于更广泛的领域。
TÜLU3 70B
TÜLU3 8B
TÜLU3 Data
TÜLU3 Code
TÜLU3 Eval
Introduction
“正如骆驼与商队中的其他骆驼分担负担一样，智者也会分享他们的见解，以减轻无知的负担。” – TÜLU3产生的谚语。


为了缩小开源式和闭源式后训练之间的差距，作者们推出了这是一系列开源的先进后训练模型，以及所有数据、训练方法、代码、基础设施和评估框架。
为了构建 TÜLU3，作者们确定了一组后训练需要改进的核心技能（例如推理、数学、编码、安全、精确遵循指令、知识回忆等），并建立了一个评估框架，以建立明确的绩效目标并指导模型在一系列开发和未见过的任务上的改进。TÜLU3受益匪浅，它充分利用了公开可用的开源数据，在各个训练阶段生成多样化、特定技能的合成数据，并根据作者们的评估套件积极地净化这些数据。
TÜLU3训练Recipe涉及多个阶段，每个阶段都以之前的模型为基础，并侧重于不同类型的数据——即监督微调的快速完成实例、偏好调整以及强化学习的可验证奖励。作者们的方法有助于识别技能缺陷并改进数据组合、方法和参数，确保在整个训练过程中核心技能的平衡表现。通过严格、有原则的实验，作者们确定了监督微调的最佳数据组合，从而得出 TÜLU3SFT 检查点。利用偏好调整方面的最新进展，作者们随后通过比较 TÜLU3SFT 完成情况与其他语言模型的输出，使用精心策划的策略偏好数据来训练模型。此外，作者们引入了一个新的最终微调阶段——可验证奖励的强化学习 (RLVR)——它采用了一种新颖的 RL 目标，旨在增强特定技能，具有可验证的答案，例如数学和精确的指令。
作者们表现最佳的方案产生的 TÜLU3模型的表现优于同等规模的最先进的后训练开源权重模型，例如 Llama 3.1 Instruct、Qwen2.5 Instruct 或 Mistral-Instruct，并且在 70B 规模下，TÜLU可与 Claude 3.5 Haiku 和 GPT-4o mini 等闭源提供商的产品相媲美。
闭源模型版本：GPT-3.5-Turbo-0125、GPT-4o-mini-2024-07-18、Claude 3.5 Haiku 20241022
总而言之，TÜLU3代表了一系列最先进的开源语言模型，具有现代的后训练框架、完全开源的数据 TÜLU3D ATA 、评估 TÜLU3E VAL 、训练代码 TÜLU3C ODE 和开发Recipe TÜLU3R ECIPE 。以下是 TÜLU开发的一些关键贡献：

为评估、净化和Recipe设计提供广泛的指导和工具，
扩展的、新的合成教学数据集，
使用基于策略的生成来扩展偏好数据，
具有可验证奖励的强化学习，这是一种基于 RL 的方法，只有在模型的完成被验证为正确时才会获得奖励，
先进的基础设施、细节和代码，以促进大型模型的成功实施

作者们的工作成果是完全开源的语言模型微调pipeline。作者们发布了在 Llama 3.1 基础版本基础上训练的最终模型，其中包含中间检查点、训练数据、训练代码和评估代码（发布的完整工件列表见表 1）。借助所有发布的资源，其他人可以采用开源的基础模型并对其进行微调，以在任何感兴趣的任务上实现高性能，为复杂、多目标和多阶段训练方案中的后训练研究奠定基础。


TÜLU3  Overview
表 2：TÜLU3 EVAL由开发和隐藏的分割组成，用于评估核心技能。借助 TÜLU3 EVAL ，作者们发布了统一的标准化评估套件和工具包，用于根据基准对训练数据进行净化。下标显示了作者们用于评估的指标。TÜLU3 Safety 是安全评估的集合，详情请参阅第 7.2.1 节。


TÜLU3突破了后训练研究的界限，缩小了开源式和闭源式微调方法之间的差距。借助 TÜLU3，作者们希望发现开源社区的哪些路径将取得成功，哪些路径不会（通过报告负面结果）。这是一个复杂的训练过程，将专有方法的部分细节与新技术和成熟的学术研究相结合。mdelname 3 成功的关键因素在于仔细的数据管理、严格的实验和评估、创新的方法和改进的训练基础设施。作者们遵循系统的指导方针，通过创建开发和测试集进行评估，对这一过程进行科学评估，并对公开可用的数据集进行仔细的净化。开源后训练的TÜLU3 不仅仅是一个工件，而是一套全面的数据和工具，旨在推动前沿技术的发展。通过公开分享作者们的数据、Recipe和发现，作者们旨在使社区能够探索新的和创新的后训练方法。
TÜLU3 Data
核心技能TÜLU3 的工作首先确定了开源的后训练方法经常落后的关键领域，这些领域是通用语言模型所需的能力。表 3 概述了作者们旨在增强的核心能力以及为涵盖这些技能而选择的评估基准。在 TÜLU3 中，作者们专注于知识回忆、推理、数学、编码、指令遵循、一般聊天和安全等核心技能。
作者们通过从公共数据中获取数据并综合整理数据来整理和收集 TÜLU3D ATA以针对这些核心技能。作者们在训练的不同阶段使用各种数据格式。表 6 概述了用于训练作者们模型的数据集集合，更多详细信息请参见第 3 节。
TÜLU3  Evaluation
作者们的后训练方法取得成功的一个关键因素是建立明确的绩效目标和评估工具，以指导这些阶段的改进。借助 TÜLU3E VAL ，作者们发布了统一的标准化评估套件和工具包，以指导开发和评估最终模型并根据评估基准净化训练数据。
作者们的框架包括一个用于可重复评估的开源评估工具包（第 7.1 节）、一套用于评估指令调整模型中核心技能的套件（具有单独开发（第 7.2 节）和保留评估（第 7.3 节），以及一组基于作者们对各种模型的实验在作者们的评估套件上进行评估的推荐设置。这两种划分都涵盖了所有已识别的技能。至关重要的是，作者们没有检查作者们未见集上的分数
TÜLU3  Recipe
在本节中，作者们概述了 TÜLU3 Recipe  ，以获得最先进的后训练模型。作者们在预训练语言模型的基础上通过四阶段后训练Recipe生成 TÜLU3模型（见图 1）。TÜLU3RECIPE是一个先进的多阶段训练pipeline，结合了强化学习中的新算法进步、尖端基础设施和严格的实验，以整理数据并优化各个训练阶段的数据组合、方法和参数。在所有阶段，作者们使用精心选择的评估套件来衡量模型性能。阶段如下：

**第 1 阶段：数据管理 (§3)，**作者们管理各种Prompts，以分配到多个优化阶段。作者们创建新的合成Prompts，或者在可用时从现有数据集中获取源Prompts，以针对特定功能。作者们确保Prompts不会受到作者们的评估套件 TÜLU3 EVAL 的污染。
**第 2 阶段：监督微调 (§4)，**作者们对精心挑选的Prompts和完成内容进行监督微调 (SFT)。通过彻底的实验，在作者们的评估框架的指导下，确定最终的 SFT 数据和训练超参数，以增强目标核心技能，而不会显著影响其他技能的表现。
**第 3 阶段：偏好调整 (§5)，**作者们将偏好调整（特别是 DPO）应用于新整理的基于策略的合成偏好数据（从选定的Prompts以及非策略数据中生成）。与 SFT 阶段一样，作者们通过彻底的实验确定最佳偏好数据组合，发现哪些格式的数据、方法或超参数可以带来改进。
第 4 阶段：具有可验证奖励的强化学习（§6），作者们引入了一个新的基于 RL 的后训练阶段，该阶段使用可验证奖励而不是奖励模型来训练模型，这在传统 RLHF PPO 训练中很常见。作者们选择具有可验证结果的任务，例如数学问题解决，并且仅在模型的生成被验证为正确时才提供奖励。然后作者们使用 RL 来训练这些奖励。

作者们的 TÜLU3流程的主要贡献在于改进了数据、方法、基础设施和严格的评估。作者们流程的关键要素包括：

**数据质量、来源和规模（§3）**作者们通过仔细调查可用的开源数据集、分析其来源并对其进行净化，以及策划针对核心技能的合成Prompts来获得Prompts。为了确保有效性，作者们进行了彻底的实验来研究它们对作者们的开发评估套件的影响。
**创建多技能 SFT 数据集(§4.1) **通过对各种数据mixtures进行多轮监督微调，改进了“一般”和“特定技能”类别中Prompts的分布。例如，为了提高数学推理能力，作者们首先通过创建数学专业模型在评估套件中建立上限，然后混合数据以使一般模型更接近这个上限。
整理基于策略的偏好数据集(§5.2) 作者们开发了基于策略的数据整理流程来扩展作者们的偏好数据集生成。具体来说，作者们从 TÜLU3-SFT 和其他模型中为给定的Prompts生成回答，并通过它们的成对比较获得偏好标签。作者们的方法扩展并改进了 Cui 等人提出的基于策略的偏好数据生成方法 [2023]。通过精心选择偏好数据，作者们获得了 354,192 个用于偏好调整的实例，在一系列任务中表现出显著的改进。
**偏好调整算法设计(§5.4) **作者们试验了几种偏好调整算法，并观察到使用长度归一化的直接偏好优化可以提高性能。作者们在实验中优先考虑简单性和效率，并在整个开发过程和训练最终模型时使用长度归一化的 DPO，而不是对基于 PPO 的方法进行成本更高的研究。
**具有可验证奖励的技能特定强化学习(§6) **作者们采用一种新方法，利用标准强化学习范式来针对可以根据真实结果（例如数学）进行评估的技能。作者们将此算法称为具有可验证奖励的强化学习 (RLVR)；如果完成成功，它将获得恒定的奖励值。作者们的结果表明，RLVR 可以提高 GSM8K、MATH 和 IFEval 性能。
**强化学习的训练基础设施（§6.3）**作者们实施了异步 RL 设置：作者们通过 vLLM 高效运行 LLM 推理，同时学习者同时执行梯度更新。作者们的 RL 代码库也具有高度可扩展性，可以训练 70B RLVR 策略模型。
**评估框架：TÜLU3EVAL（§7）**除了评估最终模型之外，作者们的评估框架是一个开源的评估工具包，旨在通过精心挑选的评估套件和净化工具来指导开发进度。

Evaluation and Results
在整个工作过程中报告分数时，作者们使用表 3 中确定的指标；分数越高越好。在计算整体性能时，作者们只是对所有评估取平均分数，对每个评估一视同仁。在作者们的开发评估套件中，在 Llama 3 基础模型上训练的 TÜLU3的表现优于其大小类别中的所有其他开源权重模型。与闭源模型相比，TÜLU370B 甚至超越了 GPT-3.5-Turbo-0125 或 GPT-4o-mini-2024-07-18 等闭源模型，同时接近 Claude 3.5 Haiku 20241022 的性能。表 3显示了在 80 亿和 700 亿个参数的 Llama 3 上训练的 TÜLU3与它们大小类别中的领先模型的总结。表 4 显示了 8B 版本按训练阶段的性能细分，表 5 显示了 70B 版本按训练阶段的性能细分。






TÜLU3 Data
Prompts代表用户与模型交互的各种方式，是所有后训练阶段的重要组成部分。作者们精心挑选了数百万个Prompts作为 TÜLU3后训练Recipe的起点。从这些Prompts中选择了用于下一阶段训练的数据。表 6 总结了这些Prompts的关键信息。在本节中，作者们描述了作者们的Prompts策划过程和净化工作，以确保作者们的评估不会泄露在这些Prompts中。在以下部分中，作者们将描述如何使用Prompts进行监督微调 §4 和偏好调整 §5。
Prompt 拓展
为了瞄准所需的核心技能，作者们从具有明确来源的公开数据集中挑选出一组多样化、高质量的Prompts，并综合生成Prompts以填补任何空白。
从公共数据集获取数据
自作者们发布 TÜLU2 以来，社区见证了大量工作为后期训练创建数据集，包括监督微调和偏好调整。TÜLU3旨在整合和扩展这些资源以构建更强大的模型。作者们从对公共数据集的广泛调查开始这一过程，包括由专职人员注释的数据集、来自真实用户的数据集以及与模型合成的数据集。然后，作者们手动审查每个数据集，并根据以下考虑因素进行选择。


针对目标技能进行合成
为了满足对多样化和特定技能数据集日益增长的需求，作者们采用了合成数据生成作为补充方法。合成数据生成已成为人工编写数据的一种有前途的替代方案，因为它更容易获取、可针对不同目的进行定制，并且反映了底层模型的丰富知识 [Dubey et al., 2024]。然而，大规模生成多样化和高质量的数据并非易事，因为 LM 很容易陷入重复的模式或模式，称为“模式崩溃”。为了确保生成的多样性，作者们遵循 Chan 等人 [2024] 中最近采用的角色驱动方法来生成合成数据。关键思想是使用不同的角色（例如，“专注于神经网络的机器学习研究人员”）和数据合成Prompts（例如，“创建编码问题”）来引导 LLM 合成具有相应视角的数据。具体来说，作者们以 Persona Hub [Chan et al., 2024] 中的 ∼250K 个角色为条件，生成针对特定技能（例如精确遵循指令、数学和编码）的Prompts。

**精确的指令遵循，**精确的指令遵循是指遵循自然语言中可验证的指令（例如“写 3 段话”）的能力，这些指令可以通过启发式方法自动验证。作者们使用角色驱动的方法综合生成可验证指令，涵盖 IFEval 基准中定义的 25 种不同约束类型。更具体地说，作者们首先为每个约束手动编写 1-2 个示例指令（用作seedPrompts。然后，作者们使用例如单词数来生成新指令），从而得到总共33 条可验证指令，作者们给出了数据合成Prompts、角色和单个可验证指令作为示例。
**数学和编码，**作者们采用类似的角色驱动方法来综合生成各种数学单词和编码问题。数学问题包括需要高等数学技能的问题以及小学问题。对于编码，作者们生成入门到中级程序员都可以解决的 Python 编程问题。与精确的指令遵循不同，作者们零样本Prompts GPT-4o 生成特定于给定角色输入的独特问题。生成问题后，作者们生成多步骤使用 GPT-4o 编写数学解决方案，使用 claude-3-5-sonnet 编写 Python 程序。
不合规和安全，随着作者们增强模型有效协助用户的能力，确保它们能够可靠地拒绝不安全查询并适当处理细微和超出范围的查询至关重要。

Prompt  清洗
在拓展作者们的训练组合时，一个重要的考虑因素是训练集和评估集之间可能存在重叠。作者们按如下方式量化这种重叠，并根据需要从作者们的训练组合中删除实例，以防止测试集污染。

**匹配方法。**作者们尝试了全字符串、n-gram 和基于嵌入的匹配，发现 n-gram 匹配产生了最有用的结果——虽然基于嵌入的方法原则上可以识别由于释义而产生的非平凡污染 [Yang et al., 2023]，但作者们发现很难区分单纯的分布相似性和实际的释义。此外，使用 n-gram 匹配的部分表面重叠成功地识别了污染的情况，其中实例略有不同，例如数学问题仅数字不同。
**识别匹配实例。**由于训练数据集中的补全通常使用语言模型重新生成，因此作者们选择仅计算Prompts中的重叠（或更一般地计算多轮对话中的用户轮次）。
去污。 如果训练集中的任何实例数量与作者们开发和未见套件中的任何评估中超过 2% 的实例重叠，则作者们认为该训练集受到污染。作者们会删除所有受到未见评估污染的训练集。对于受到开发评估污染的训练集，如果这样做不会显著影响最终模型的性能，作者们会删除整个数据集；否则，作者们会删除与任何测试实例匹配的特定实例。

SFT
将预训练的基础模型调整为各种任务和用户请求通常依赖于监督微调 (SFT)，也称为指令微调。此过程中的一个关键挑战是平衡代表不同技能的混合训练数据集的比例。对于 TÜLU3，作者们进行了数据混合消融并探索了模型合并数据最初用于构建 Vicuna 模型，但确切的数据集尚未发布。


SFT 数据
从Prompts到 SFT 数据
为了创建作者们的 SFT 组合，作者们通过两种方式收集或创建第 3 节中描述的Prompts的回答：过滤现有回答和创建新回答。
对于有现有回答的Prompts，如果原始回答是由人类或前沿模型（如 GPT-4o）编写的，作者们通常会保留原始回答。对于包含来自前沿模型子集的大型数据集（例如 WildChat），作者们使用来自最佳模型的子集。作者们还会过滤空回答和包含有关模型或其开发人员的信息的回答。如果一组Prompts没有回答（例如作者们的 Persona Prompts），或者原始回答来自较弱的模型（例如 WildGuardMix），作者们会使用 GPT-4o 生成新回答。作者们还手写了对硬编码Prompts的回答。
TÜLU3 SFT Mix
为了开发作者们的 SFT 组合，作者们首先使用在 TÜLU2 上训练的 Llama3.1 作为基线，确定了落后于最先进模型的技能。针对每一项技能，作者们收集了高质量的公开数据集并创建了合成数据集（如第 3.1.2 节所述），同时删除了一些作者们的发现与其他较新的数据集相比，其质量相对较低的数据集。
为了设计最终的 SFT 组合，作者们首先构建了特定技能的数据组合和模型，保留了在单个技能上表现最佳的混合数据，忽略了其他评估。这样做是为了根据作者们的设置估算出每个评估的上限。
然后，作者们将这些mixtures组合起来，创建了作者们的初始 TÜLU3预览组合。然后，作者们继续通过添加或删除数据集来迭代mixtures，以提高滞后技能，并根据作者们的评估进行净化混合，以及作者们的中间和最终 TÜLU3SFT 混合。




关键数据实验
在开发最终的 SFT mixtures之后，作者们还进行了一系列控制实验，以探索在数据混合和训练期间做出的不同决策的重要性。

**多样化的聊天数据。**在作者们的组合中，作者们还强调添加多样化的聊天数据，主要来自 WildChat。作者们在表9中展示了删除 WildChat 的影响，作者们发现大多数技能都出现了轻微但明显的退化，最明显的是 Alpaca Eval，这凸显了多样化现实世界数据的重要性。
安全性是正交的。作者们发现，作者们的安全性 SFT 数据通常与作者们的其他数据集正交。作者们在表9中报告了删除特定于安全性的数据集的效果，并且作者们看到，除了安全性平均值之外，大多数技能大致保持不变。作者们还发现，添加诸如 CoCoNot 中的那些构造性Prompts有助于防止作者们的模型过度拒绝安全Prompts。
**新人物角色数据。**作者们新的人物角色数据集是针对特定技能而构建的：数学、编码和遵循指令。在表9中，作者们显示删除人物角色数据集后，HumanEval(+)、GSM8K、MATH 和 IFEval 上的性能下降，显示了创建多样化、特定技能的 SFT 数据集的价值。针对特定技能。作者们还在表9 中展示了针对特定技能的影响，其中作者们展示了删除所有特定数学数据的效果。与作者们的最终组合相比，GSM8K 和 MATH 都显着下降，强调了在数据收集期间关注特定技能的积极影响。
**针对特定技能。**作者们很大一部分精力都集中在收集或创建针对特定能力的数据集上。以数学推理为例，作者们在表8中展示了数学特定数据对 GSM8K 和 MATH 的影响。作者们发现数学特定的 SFT 数据显著提高了 GSM8K 和 MATH，显示了最终组合中包含的数据的价值。
**SFT 数据量。**在图 4 中，作者们展示了对 SFT 组合进行分层子抽样的效果。作者们发现，随着更多 SFT 数据被纳入，作者们的模型平均水平继续提高，并且随着作者们将数据量增加到完整组合，作者们看到 GSM8K 等指标的巨大改进。有趣的是，随着组合中的数据量增加，TruthfulQA 性能实际上会下降。作者们不会将 SFT 数据大小增加到当前组合之外，因为作者们分配了其他Prompts以进行偏好优化。



SFT  Recipe and Analyses
**训练设置 **为了训练作者们的 TÜLU 3 模型，作者们使用了 4 到 16 个 8xH100 节点，并进行了高速互连。作者们使用了 128 的batch size  和 4,096 个 token 的最大序列长度。作者们训练了两个时期，对于 8B 模型，作者们使用 5e-6 的学习率，对于 70B 模型，作者们使用 2e-6 的学习率，这是作者们在超参数搜索后发现的。作者们的超参数设置也总结在表 10 中。


关键训练实验
**基础模型的选择。**作者们还使用完整的 SFT 组合测试了训练不同的基础预训练模型对数学性能的影响。在表 11 中，作者们展示了通过在 Llama 3.1 8B 和 70B 上进行训练来改变模型大小的影响，以及通过在 Qwen 2.5 7B和 Qwen 2.5 Math 7B 上进行训练来添加特定领域的预训练数据的影响。在这两种情况下，作者们都看到 GSM8K 和 MATH 都有了显著的提升，凸显了模型大小和预训练数据对于下游技能的重要性。


**聊天模板变化。**在创建 TÜLU3 期间，作者们探索了更改用于指导生成微调模型的聊天模板。作者们对以前 TÜLU版本中使用的聊天模板做了一些小改动，具体来说是删除了模板末尾的新行（在模型回答之前）。表 12 显示了作者们 SFT 设置的早期版本中对聊天模板的不同更改之间的性能。作者们发现用 eos token替换助手消息末尾的换行符可以获得最佳性能，但作者们选择不使用它，以避免与后训练的pipeline中的后续步骤生成不一致。


**随机seed和模型soups。**作者们还探索了在 SFT 期间更改随机seed，然后使用这些模型创建模型soups。在表 13 中，作者们将使用多个不同seed训练的 8B 和 70B 模型与最佳模型soups进行了比较。作者们发现 SFT 性能因seed的不同而明显不同，这凸显了多次训练运行的重要性，并且最佳模型soups并不总是优于最佳单次训练运行。因此，作者们将每种模型大小（8B和70B）的最佳单次 SFT 训练运行用作最终的 SFT 模型。


批量聚合
在训练 Tframework 和在其他设置（例如 TPU）上训练的模型的早期阶段。TÜLU 3 中，作者们注意到在作者们的 Open-Instruct 8上训练的 SFT 模型之间存在性能差距。作者们发现这个问题主要是由于Transformers 内部损失聚合问题造成的：在不考虑梯度累积或分布式训练设置的情况下对填充标记之间的损失进行平均。
这里作者们用一个例子来说明这个问题。假设作者们在一个批次中有两个样本,其中有 n1n_1n1​、n2n_2n2​ 个非填充标记和 m1m_1m1​、m2m_2m2​ 个填充标记。如果作者们同时将两个样本传递到默认的 Transformers 前向传递中,作者们会得到:
L=ln1+ln2n1+n2L = \frac{l_{n1} + l_{n2}}{n_1 + n_2}
L=n1​+n2​ln1​+ln2​​
当两个样本同时通过Transformer模型的前向传播时,损失函数 L 是两个样本的损失值之和除以两个样本中非填充tokens的总数。这里的 ln1l_{n1}ln1​ 和 ln2l_{n2}ln2​ 分别是第一个和第二个样本的损失值,它们是根据各自样本中的非填充tokens计算得出的。
然而,如果作者们应用梯度积累,分别输入两个样本,计算损失,然后除以损失就会这样计算:
L=ln1n1+ln2n2L = \frac{l_{n1}}{n_1} + \frac{l_{n2}}{n_2}
L=n1​ln1​​+n2​ln2​​
当使用梯度累积时,每个样本单独通过模型,分别计算损失,然后再将这两个损失值平均。在这种情况下,每个样本的损失值是按照其非填充tokens的数量来加权的。
也就是说，在第二种情况下，作者们对每个示例赋予相同的权重，而在第一种情况下，作者们对每个标记赋予相同的权重。因此，改变梯度累积会对性能产生很大影响，因为实际上会改变样本权重。由于跨设备平均，分布式训练中也会出现类似的问题。
为了解决这个问题，作者们通常选择在训练时使用总损失而不是平均损失。这只需从上述方程中删除分母即可消除问题，并需要调整学习率。这实际上对所有标记赋予了相同的权重（作者们发现这通常会使初始混合的性能更好）。作者们通过在 TÜLU2 SFT 混合上使用各种学习率、时期和损失类型对 Llama 3.0 进行微调来验证作者们设置的性能，如图 5 和图 6 所示。最终，作者们发现使用学习率为 5.00E-06 的总损失效果最好。令人惊讶的是，作者们还发现更长时间的训练并没有带来进一步的改进，因此使用了 2 个时期进行训练。




Preference Finetuning
对于TÜLU3，作者们探索了许多偏好微调方法，目的是改进整个评估套件。作者们探索了多种训练算法，从直接偏好优化 (DPO) 及其衍生算法到强化学习算法，例如近端策略优化 (PPO)。在本节中，作者们详细介绍了从人类偏好和作者们的优化器中学习的问题表述。接下来，作者们将解释如何将他们的Prompts转换为来自在线策略（TÜLU3套件）和离线策略模型（其他指导模型）的合成偏好数据。作者们展示了如何为感兴趣的特定技能创建偏好数据，以及如何使用 DPO 稳健地改进他们的模型。
背景
先前的研究已将偏好数据训练确立为提高模拟人类或合成偏好的基准模型性能的关键步骤 。典型的程序是从人类或合成反馈中进行强化学习。
设置
偏好数据


图 7展示了偏好数据集制作的流程，主要分为三个阶段：Prompt Selection（提示选择）、Response Generation（回答生成） 和 **Preference Annotation（偏好标注）**在这个阶段，会从以下三类来源中选择提示（Prompts）：

Prompts used in SFT（监督微调中使用的提示）：用于模型的监督微调训练的数据集中的提示。
Prompts from datasets subsampled for SFT（从用于SFT的数据集中采样的提示）：从更大的训练数据集中挑选出一部分提示。
New OOD prompts（超出分布的提示，例如Ultrafeedback和Persona）：设计新的、超出模型训练分布的提示，以测试模型在新领域上的表现。

**Response Generation（回答生成）**这里提到了一个模型池，包含22个不同的模型，其中回答生成的过程包括：

Off-policy data（离策略数据）：由其他模型生成的回答（如Meta、Gemma、技术创新研究所等）。这些模型生成的回答可能会被标注为“较差”或“被拒绝”。
On-policy data（策略内数据）：由Tülu 3（SFT 8B或SFT 70B）生成的回答，这些回答可能是高质量、符合偏好的答案。

Preference Annotation（偏好标注）  在这个阶段，使用GPT-4o协助标注回答质量，具体操作包括：

标注维度：对每个生成的回答，从以下四个方面进行评分（1到5分）：

Helpfulness（有用性）：回答是否对用户有帮助。
Instruction Following（指令遵循性）：是否严格按照提示指令进行回答。
Truthfulness（真实性）：回答内容是否真实、准确。
Honesty（诚实性）：回答是否有误导性或隐瞒信息。


Binarize（二值化）：将评分结果转化为“Chosen（选择）”或“Rejected（拒绝）”：

Chosen：得分较高的回答。
Rejected：得分较低的回答。



**GPT-4o 不直接生成回答，而是作为标注助手，为每个回答在不同维度上给出具体评分。**除了判断“好”或“坏”，还可以提供更精细的质量维度（如有用性、指令遵循性等），以便后续更细粒度地优化模型。


奖励模型
给定偏好数据集,训练奖励模型 (RM) rϕr_{\phi}rϕ​,目标如下:
max⁡ϕE(x,yc,yr)∼D[log⁡σ(rϕ(x,yc)−rϕ(x,yr))]\max_{\phi} \mathbb{E}_{(x,y_c,y_r)\sim D} \left[ \log \sigma(r_{\phi}(x,y_c) - r_{\phi}(x,y_r)) \right]
ϕmax​E(x,yc​,yr​)∼D​[logσ(rϕ​(x,yc​)−rϕ​(x,yr​))]

数据集 D\mathcal{D}D 包含了一些提示（prompts） xxx 和两个候选回答 yyy 和 y′y&#x27;y′。
对于每个提示 xxx，一些评审者会从 yyy 和 y′y&#x27;y′ 中选择一个作为更好的回答 ycy_cyc​，另一个会被标记为被拒绝的回答 yry_ryr​。
rϕ(x,y)r_{\phi}(x, y)rϕ​(x,y)：奖励模型对于输入 xxx 和回答 yyy 的评分。
σ(z)\sigma(z)σ(z)：激活函数 Sigmoid，σ(z)=11+e−z\sigma(z)=\frac{1}{1+e^{-z}}σ(z)=1+e−z1​，将分数差转化为一个概率值。
(x,yc,yr)∼D(x,y_c,y_r)\sim \mathcal{D}(x,yc​,yr​)∼D：从偏好数据集中采样的一个三元组，其中 ycy_cyc​ 是选择的回答，yry_ryr​ 是被拒绝的回答。
rϕ(x,yc)−rϕ(x,yr)r_{\phi}(x,y_c)-r_{\phi}(x,y_r)rϕ​(x,yc​)−rϕ​(x,yr​)：表示奖励模型认为选择回答 ycy_cyc​ 与被拒绝回答 yry_ryr​ 的分数差。
log⁡σ(rϕ(x,yc)−rϕ(x,yr))\log \sigma(r_{\phi}(x,y_c)-r_{\phi}(x,y_r))logσ(rϕ​(x,yc​)−rϕ​(x,yr​))：表示根据分数差的对数似然，描述了模型将 ycy_cyc​ 视为选择回答的概率，它越接近 0，意味着对应的原始概率 σ(z)\sigma(z)σ(z) 越高。

优化目标：最大化这一对数似然，强化奖励模型对选择回答与被拒绝回答的区分能力。




5.1.2	策略优化
PPO
用 KL 散度作为正则化项,限制模型更新的步长。模型希望最大化奖励(生成更符合人类偏好的内容),但模型也需要受限于“不要偏离预训练模型太远”,以避免生成奇怪、不稳定的内容。KL 散度提供了一个度量工具,β\betaβ 则决定了偏离的允许程度。
R(x,y)=rϕ(x,y)−βKL[πθ(y∣x)∥πref(y∣x)]R(x,y) = r_{\phi}(x,y) - \beta \text{KL}[\pi_{\theta}(y|x) \| \pi_{\text{ref}}(y|x)]
R(x,y)=rϕ​(x,y)−βKL[πθ​(y∣x)∥πref​(y∣x)]

πθ(y∣x)\pi_{\theta}(y \mid x)πθ​(y∣x)：当前训练的语言模型的策略（生成 yyy 的概率分布）。
πref(y∣x)\pi_{\text{ref}}(y \mid x)πref​(y∣x)：初始参考策略（预训练模型的策略，用于保持生成结果的稳定性）。
KL[πθ(y∣x) ∥ πref(y∣x)]\mathrm{KL}\big[\pi_{\theta}(y \mid x)\,\|\,\pi_{\text{ref}}(y \mid x)\big]KL[πθ​(y∣x)∥πref​(y∣x)]：Kullback–Leibler（KL）散度，用来衡量当前策略 πθ\pi_\thetaπθ​ 偏离参考策略 πref\pi_\text{ref}πref​ 的程度。
rϕ(x,y)r_\phi(x, y)rϕ​(x,y): 奖励模型的输出，表示对输入 xxx 和生成结果 yyy 的质量评分。
β\betaβ: 控制 KL 散度的权重，平衡模型对奖励最大化和与参考策略保持接近之间的取舍。

目标：

目标是优化策略 πθ\pi_\thetaπθ​，使得生成的结果 yyy 在给定 xxx 时能获得最高的奖励 R(x,y)R(x,y)R(x,y)。
奖励 R(x,y) 被分为两部分：

rϕ(x,y)r_\phi(x, y)rϕ​(x,y): 奖励模型给出的偏好评分。
βKL[πθ∥πref]\beta \text{KL}[\pi_\theta \| \pi_\text{ref}]βKL[πθ​∥πref​]: 惩罚项，限制 πθ\pi_\thetaπθ​偏离πref\pi_\text{ref}πref​ 太远。



作用：

奖励最大化：通过 rϕ(x,y)r_\phi(x, y)rϕ​(x,y) 让策略生成更符合人类偏好的内容。
稳定性控制：通过 KL 散度，确保模型不会偏离初始策略太远，避免生成质量崩塌或出现极端行为。
**权重 **β\betaβ：
β\betaβ 控制两部分的权衡：

如果 β\betaβ 较大，模型更倾向于保持和参考策略接近。
如果 β\betaβ 较小，模型更倾向于追求奖励最大化，但可能会导致生成不稳定。



DPO
直接用偏好数据优化策略,避免大幅偏离初始参考策略,其等效目标如下:
max⁡θE(x,yc,yr)∼D[log⁡σ(βlog⁡πθ(yc∣x)πref(yc∣x)−βlog⁡πθ(yr∣x)πref(yr∣x))]\max_{\theta} \mathbb{E}_{(x,y_c,y_r)\sim D} \left[ \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)} - \beta \log \frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)} \right) \right]
θmax​E(x,yc​,yr​)∼D​[logσ(βlogπref​(yc​∣x)πθ​(yc​∣x)​−βlogπref​(yr​∣x)πθ​(yr​∣x)​)]
输入数据：

数据集D\mathcal{D}D包含一对偏好标注 $ (y_c, y_r) $：

ycy_{c}yc​：优选项（chosen）。
yry_{r}yr​：非优选项（rejected）。


xxx是输入上下文。

目标函数的核心：

分数差：分数的计算基于模型 πθ\pi_\thetaπθ​ 和参考策略 πref\pi_\text{ref}πref​ 的对数比值（即对数概率比）。优选项 ycy_cyc​ 和非优选项 yry_ryr​ 的分数差定义为：

Δ=βlog⁡πθ(yc∣x)πref(yc∣x)−βlog⁡πθ(yr∣x)πref(yr∣x).\Delta = \beta \log \frac{\pi_\theta(y_c\mid x)}{\pi_\text{ref}(y_c\mid x)} - \beta \log \frac{\pi_\theta(y_r\mid x)}{\pi_\text{ref}(y_r\mid x)}.Δ=βlogπref​(yc​∣x)πθ​(yc​∣x)​−βlogπref​(yr​∣x)πθ​(yr​∣x)​.
β\betaβ 是一个权重参数，用于调整策略和参考策略之间的偏离程度。



上面的 Δ\DeltaΔ 可进一步简化,最终为:
log⁡πθ(yc∣x)=∑t=1Tclog⁡πθ(yc,t∣x,yc,1:t−1)\log \pi_{\theta}(y_c|x) = \sum_{t=1}^{T_c} \log \pi_{\theta}(y_{c,t}|x, y_{c,1:t-1})
logπθ​(yc​∣x)=t=1∑Tc​​logπθ​(yc,t​∣x,yc,1:t−1​)
其中,假设 ycy_cyc​ 是由多个 token 组成的序列: y=(y1,y2,…,yT)y = (y_1, y_2, \ldots, y_T)y=(y1​,y2​,…,yT​),那么条件概率可以被分解为:
πθ(y∣x)=∏t=1Tπθ(yt∣x,y1:t−1)\pi_{\theta}(y|x) = \prod_{t=1}^{T} \pi_{\theta}(y_t|x, y_{1:t-1})
πθ​(y∣x)=t=1∏T​πθ​(yt​∣x,y1:t−1​)
在生成第 t个 token yty_tyt​ 时,模型不仅参考输入 x,还参考之前已经生成的 token y1,…,yt−1y_{1},\ldots,y_{t-1}y1​,…,yt−1​。
在 DPO 的过程中，希望模型学会对好的回答 ycy_cyc​ 赋予 更高的概率，也就是 πθ(yc∣x)\pi_\theta(y_c\mid x)πθ​(yc​∣x) 更大。同时，对差的回答 yry_ryr​ 赋予 更低的概率，也就是 πθ(yr∣x)\pi_\theta(y_r\mid x)πθ​(yr​∣x) 更小。在优化模型参数 θ\thetaθ 的过程中，逐步学习从输入 xxx 到优质输出 yyy 的映射关系，即优化 πθ(y∣x)\pi_\theta(y\mid x)πθ​(y∣x) 的分布，使其更贴合偏好数据，逐步让 优质回答的生成概率 πθ(yc∣x)\pi_\theta(y_c\mid x)πθ​(yc​∣x) 超过劣质回答的生成概率 πθ(yr∣x)\pi_\theta(y_r\mid x)πθ​(yr​∣x)。
Length-Normalized DPO
作者在上述DPO的基础上,进一步加入了归一化来消除,因回答的序列长度(对比来说很长的话)带来的概率偏好影响。
max⁡θE(x,yc,yr)∼D[log⁡σ(β1∣yc∣log⁡πθ(yc∣x)πref(yc∣x)−β1∣yr∣log⁡πθ(yr∣x)πref(yr∣x))]\max_{\theta} \mathbb{E}_{(x,y_c,y_r)\sim D} \left[ \log \sigma \left( \beta \frac{1}{|y_c|} \log \frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)} - \beta \frac{1}{|y_r|} \log \frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)} \right) \right]
θmax​E(x,yc​,yr​)∼D​[logσ(β∣yc​∣1​logπref​(yc​∣x)πθ​(yc​∣x)​−β∣yr​∣1​logπref​(yr​∣x)πθ​(yr​∣x)​)]
其中,假设 ycy_cyc​ 是由多个 token 组成的序列: y=(y1,y2,…,yT)y = (y_1, y_2, \ldots, y_T)y=(y1​,y2​,…,yT​),那么条件概率可以被分解为:
πθ(y∣x)=∏t=1Tπθ(yt∣x,y1:t−1)\pi_{\theta}(y|x) = \prod_{t=1}^{T} \pi_{\theta}(y_t|x, y_{1:t-1})
πθ​(y∣x)=t=1∏T​πθ​(yt​∣x,y1:t−1​)
其对数概率为:
log⁡πθ(y∣x)=∑t=1Tlog⁡πθ(yt∣x,y1:t−1)\log \pi_{\theta}(y|x) = \sum_{t=1}^{T} \log \pi_{\theta}(y_t|x, y_{1:t-1})
logπθ​(y∣x)=t=1∑T​logπθ​(yt​∣x,y1:t−1​)

较长的序列 y 会累积更多的概率项，即累积更多的对数值，即使单个词的概率较低，其整体对数概率可能仍然较高。因此，较长的序列可能会在优化目标中获得更大的得分，进而被模型优先选择。

DOP和Length-Normalized DPO 的区别
长度归一化：

在分数计算中,加入了长度归一化项 1∣y∣\frac{1}{|y|}∣y∣1​,分别对 ycy_cyc​ 和 yry_ryr​ 的分数进行长度调整。
直观理解：较长的输出序列通常会累积更大的对数概率，从而可能产生长度偏差。长度归一化可以减轻这种偏差。

优化目标：

在 DPO 的基础上，引入长度归一化，适配更广泛的人类偏好标注数据，特别是在输出长度差异较大的情况下。

DPO 和 Length-Normalized DPO 的特点
DPO：

不需要显式利用训练奖励模型评分，直接优化策略 $ \pi_\theta $。
通过分数差 Δ\DeltaΔ表达偏好关系。
简洁高效，但可能受输出长度偏差影响。

Length-Normalized DPO：

在 DPO 基础上加入长度归一化，减轻模型生成偏好较长输出的倾向。
更加适配包含长度偏差的标注数据。

DPO和PPO对比



对比维度
DPO
PPO




否需要奖励模型
不需要（直接用偏好对）
需要训练一个奖励模型


训练复杂度
较低（只优化策略模型）
较高（需要优化策略、奖励、价值等多个组件）


依赖强化学习
不需要
需要


对人类偏好的利用
更直接（通过概率对比）
间接（通过奖励模型评估）


实现难度
实现简单
实现复杂


KL 控制
内置在偏好对比中（更直观）
需要显式控制 KL 惩罚项



TÜLU3偏好数据
从Prompts到偏好数据
根据 §3 中的Prompts，作者们通过调整和改进偏好数据的pipeline创建符合策略的偏好数据 ( 𝑥,𝑦,𝑦 ′,𝑙𝑎𝑏𝑒𝑙 ) 。作者们的早期实验表明，该流程在创建偏好数据方面具有优势，可以生成高质量的合成偏好数据集。作者们的数据创建流程（如图 7 所示）包括三个阶段：Prompts选择、从模型池生成回答以及使用 LLM-as-a-judge 进行偏好注释以创建（偏好、拒绝）对。

第 1 阶段：Prompts选择。准备数据集以进行偏好微调的第一步是选择Prompts或用户指令以生成回答并获取偏好。鉴于表 6 中的Prompts集，作者们精心挑选了作者们的选择，以包括 SFT 期间使用的Prompts，以及从相同来源抽样但未用于 SFT 的Prompts。作者们还包括来自其他来源的Prompts，例如没有 TruthfulQA 实例的 Ultrafeedback 版本，或者通过向Prompts添加新的 IF 约束。
**第 2 阶段：回答生成。**对于给定的Prompts，作者们从模型池中随机抽取四个模型来生成回答。作者们的模型选择受到 Ultrafeedback pipeline的启发，该pipeline由开源和专有模型组成，这些模型在参数大小和模型系列上有所不同。作者们通过使用某些模型的最新版本（Llama 2 → Llama 3.1）来更新 Ultrafeedback 的模型池，添加性能最佳的模型以增加池大小，并用开源替代方案替换当前无法访问的模型（例如 WizardLM）。最后，作者们还通过从 TÜLU SFT 模型中抽样完成来包含在线策略数据。作者们通过添加一系列Prompts来实现这一点，其中一个回答来自在线策略模型，另一个回答来自非策略模型。
第 3 阶段：偏好注释。在为每个Prompts生成四个回答后，作者们使用 LLM-as-a-judge，特别是 GPT-4o-2024-0806，从四个不同方面对每个回答从 1 到 5 进行评分：有帮助性、遵循指示、诚实和真实性。

TÜLU3偏好组合




作者们为 8b 和 70b 模型选择了最终的偏好组合，这可以最大限度地提高开发评估的平均表现，同时在目标技能方面也表现出色。大多数偏好数据组合消融都是针对 8b 模型运行的，作者们从用于 SFT 的Prompts开始，生成在策略和离策略偏好数据，从而产生 96911 个（拒绝策略）和 19444 个（接受策略）偏好实例。根据这个偏好基础，作者们向组合中添加其他Prompts源，以及这些添加如何影响下游评估性能，特别是针对诸如精确指令遵循、数学和 alpacaeval 上的一般聊天表现等技能。总之，作者们的偏好组合来自不同的Prompts源，例如 SFT 数据、WildChat 和 Persona IF。它包括 SFT 训练期间看到的Prompts，也包括新的、未见过的Prompts。最后，它包含在线和离线策略完成的混合。
数据消融的主要发现
]]></content>
      <categories>
        <category>论文解读</category>
        <category>tulu3</category>
      </categories>
      <tags>
        <tag>NVIDIA</tag>
        <tag>V100</tag>
        <tag>论文解读</tag>
        <tag>tulu3</tag>
      </tags>
  </entry>
  <entry>
    <title>基于llama-factory的法律模型微调</title>
    <url>/2026/01/08/LLM/%E5%AE%A2%E6%88%B7%E6%94%AF%E6%92%91/%E6%9F%90%E5%BE%8B%E6%89%80%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/%E5%9F%BA%E4%BA%8Ellama-factory%E7%9A%84%E5%BE%8B%E6%89%80%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/</url>
    <content><![CDATA[数据集




微调
lora
deepseek-r1-distill-qwen-7b
训练 log




loss 曲线


lora 优化器与权重合并




qwen3-8b
训练 log




loss 曲线


lora 优化器与权重合并




推理
]]></content>
      <categories>
        <category>模型微调</category>
        <category>llama-factory</category>
      </categories>
      <tags>
        <tag>昇腾</tag>
        <tag>910B</tag>
        <tag>模型微调</tag>
        <tag>llama-factory</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Easy-Dataset的律所数据集处理</title>
    <url>/2026/01/08/LLM/%E5%AE%A2%E6%88%B7%E6%94%AF%E6%92%91/%E6%9F%90%E5%BE%8B%E6%89%80%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/%E5%9F%BA%E4%BA%8EEasy-Dataset%E7%9A%84%E5%BE%8B%E6%89%80%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[环境
git clone https://github.com/ConardLi/easy-dataset.gitcd easy-datasetnpm installnpm run buildnpm run start


模型测试


]]></content>
      <categories>
        <category>数据集处理</category>
        <category>Easy-Dataset</category>
      </categories>
      <tags>
        <tag>昇腾</tag>
        <tag>910B</tag>
        <tag>数据集处理</tag>
        <tag>Easy-Dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>2.1 数据集处理</title>
    <url>/2025/11/13/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.1%20Deepseek-7B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[下载初始数据集

下载WuDaoCorpora2.0开源数据集到/mnt/workspace/llama3-datasets工作目录下

wget https://atp-modelzoo.oss-cn-hangzhou.aliyuncs.com/release/datasets/WuDaoCorpus2.0_base_sample.tgztar zxvf WuDaoCorpus2.0_base_sample.tgz 
Megatron-LM预训练数据清洗
mmap数据是一种预先执行tokenize处理的数据格式，可以极大减少训练微调过程中等待数据读入的时间，当数据量极大时，优势显著。
#! /bin/bashset -ex# 请在此处设置原始数据所在路径data_dir=/mnt/users/wangxiangbo/dataset/WuDaoCorpus2.0_base_sample#开始数据清洗流程dataset_dir=$(dirname $data_dir)mkdir -p $&#123;dataset_dir&#125;/cleaned_wudao_datasetcd $&#123;dataset_dir&#125;/cleaned_wudao_datasetwget https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/llama2-codes/preprocess_wudao2.py# 此处与上一节不同，增加了key参数设为textpython3 preprocess_wudao2.py -i $&#123;data_dir&#125; -o $&#123;dataset_dir&#125;/cleaned_wudao_dataset -k text -p 32# 合并清洗后的数据mkdir $&#123;dataset_dir&#125;/wudaocd $&#123;dataset_dir&#125;/wudaofind $&#123;dataset_dir&#125;/cleaned_wudao_dataset -name &quot;*.json&quot; -exec cat &#123;&#125; + &gt; $&#123;dataset_dir&#125;/wudao/merged_wudao_cleaned.jsonrm -rf $&#123;dataset_dir&#125;/cleaned_wudao_dataset
脚本执行完成后，会得到一个合并的json数据集格式

执行下方的处理数据脚本
#! /bin/bashSTART_TIME=$SECONDSMEGATRON_PATCH_PATH=/data/codeMEGATRON_PATH=$&#123;MEGATRON_PATCH_PATH&#125;/Megatron-LMexport PYTHONPATH=$&#123;MEGATRON_PATH&#125;:$&#123;MEGATRON_PATCH_PATH&#125;:$PYTHONPATHinput_data_dir=/mnt/dataset/wudao/merged_wudao_cleaned.jsontokenizer=DeepseekTokenizerjson_keys=textoutput_data_dir=/data/code/temp/deepseek/deepseek-datasetsload_dir=/data/code/temp/deepseek/deepseek-ckptTOKENIZER_PATH=/data/code/temp/deepseek/deepseek-ckptINPUT=&quot;$&#123;input_data_dir&#125;&quot;if [ $tokenizer = &quot;Qwen2Tokenizer&quot; ]; then  python preprocess_data_megatron.py \  --input $&#123;INPUT&#125; \  --output-prefix $&#123;output_data_dir&#125;/mmap_qwen2_datasets \  --patch-tokenizer-type Qwen2Tokenizer \  --json-keys $&#123;json_keys&#125; \  --load $&#123;load_dir&#125; \  --workers 2 \  --partitions 2 \  --keep-sequential-samples \  --append-eodelif [ $tokenizer = &quot;DeepSeekV2Tokenizer&quot; ]; then  python preprocess_data_megatron.py \  --input $&#123;INPUT&#125; \  --output-prefix $&#123;output_data_dir&#125;/mmap_deepseekv2_datasets \  --patch-tokenizer-type DeepSeekV2Tokenizer \  --json-keys $&#123;json_keys&#125; \  --load $&#123;load_dir&#125; \  --workers 8 \  --partitions 1 \  --keep-sequential-samples \  --append-eodelif [ $tokenizer = &quot;LLamaTokenizer&quot; ]; then  python preprocess_data_megatron.py \  --tokenizer-model $TOKENIZER_PATH \  --input $&#123;INPUT&#125; \  --output-prefix $&#123;output_data_dir&#125;/mmap_llama_datasets \  --patch-tokenizer-type LLamaTokenizer \  --load $&#123;load_dir&#125; \  --workers 16 \  --partitions 1 \  --keep-sequential-samples \  --append-eodelif [ $tokenizer = &quot;DeepseekTokenizer&quot; ]; then  python preprocess_data_megatron.py \  --tokenizer-model $TOKENIZER_PATH \  --input $&#123;INPUT&#125; \  --output-prefix $&#123;output_data_dir&#125;/mmap_deepseek_datasets \  --patch-tokenizer-type LLamaTokenizer \  --load $&#123;load_dir&#125; \  --workers 16 \  --partitions 1 \  --keep-sequential-samples \  --append-eodelif [ $tokenizer = &quot;LLama2Tokenizer&quot; ]; then  python preprocess_data_megatron.py \  --tokenizer-model $TOKENIZER_PATH \  --extra-vocab-size 0 \  --input $&#123;INPUT&#125; \  --output-prefix $&#123;output_data_dir&#125;/mmap_llama2_datasets \  --patch-tokenizer-type LLama2Tokenizer \  --load $&#123;load_dir&#125; \  --workers 16 \  --partitions 1 \  --keep-sequential-samples \  --append-eodelif [ $tokenizer = &quot;LLama3Tokenizer&quot; ]; then  python preprocess_data_megatron.py \  --input $&#123;INPUT&#125; \  --output-prefix $&#123;output_data_dir&#125;/mmap_llama3_datasets \  --patch-tokenizer-type LLama3Tokenizer \  --load $&#123;load_dir&#125; \  --workers 16 \  --partitions 1 \  --keep-sequential-samples \  --append-eodfiELAPSED_TIME=$(($SECONDS - $START_TIME))echo &quot;$(($ELAPSED_TIME/60)) min $(($ELAPSED_TIME%60)) sec&quot;
在python preprocess_data_megatron.py中，加入DeepseekTokenizer，此外还需要在/data/code/megatron_patch/tokenizer/init.py中加入DeepseekTokenizer配置。


数据处理完成


]]></content>
      <categories>
        <category>模型适配</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>模型适配</tag>
        <tag>Nvidia</tag>
        <tag>Deepseek</tag>
        <tag>Megatron-LM</tag>
      </tags>
  </entry>
  <entry>
    <title>微调数据集处理全流程解析</title>
    <url>/2026/01/08/LLM/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/%E5%BE%AE%E8%B0%83%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%84%E7%90%86%E5%85%A8%E6%B5%81%E7%A8%8B%E8%A7%A3%E6%9E%90/</url>
    <content><![CDATA[行业模型微调流程图

大模型微调全流程解决方案


pdf数据提取处理
政企类数据集.pdf


pdf数据提取
采用mineru方式
mineru环境安装
https://github.com/opendatalab/MinerU
conda create -n mineru python=3.10conda activate minerupip install -U &quot;magic-pdf[full]&quot; --extra-index-url https://wheels.myhloli.com -i https://mirrors.aliyun.com/pypi/simple


magic-pdf.json配置文件修改
&#123;    &quot;bucket_info&quot;: &#123;        &quot;bucket-name-1&quot;: [            &quot;ak&quot;,            &quot;sk&quot;,            &quot;endpoint&quot;        ],        &quot;bucket-name-2&quot;: [            &quot;ak&quot;,            &quot;sk&quot;,            &quot;endpoint&quot;        ]    &#125;,    &quot;models-dir&quot;: &quot;/mnthp/users/wangxiangbo/model/opendatalab/PDF-Extract-Kit-1___0/models&quot;,    &quot;layoutreader-model-dir&quot;: &quot;/mnthp/users/wangxiangbo/model/ppaanngggg/layoutreader&quot;,    &quot;device-mode&quot;: &quot;cpu&quot;,    &quot;layout-config&quot;: &#123;        &quot;model&quot;: &quot;doclayout_yolo&quot;    &#125;,    &quot;formula-config&quot;: &#123;        &quot;mfd_model&quot;: &quot;yolo_v8_mfd&quot;,        &quot;mfr_model&quot;: &quot;unimernet_small&quot;,        &quot;enable&quot;: true    &#125;,    &quot;table-config&quot;: &#123;        &quot;model&quot;: &quot;rapid_table&quot;,        &quot;sub_model&quot;: &quot;slanet_plus&quot;,        &quot;enable&quot;: false,        &quot;max_time&quot;: 400    &#125;,    &quot;llm-aided-config&quot;: &#123;        &quot;formula_aided&quot;: &#123;            &quot;api_key&quot;: &quot;your_api_key&quot;,            &quot;base_url&quot;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,            &quot;model&quot;: &quot;qwen2.5-7b-instruct&quot;,            &quot;enable&quot;: false        &#125;,        &quot;text_aided&quot;: &#123;            &quot;api_key&quot;: &quot;your_api_key&quot;,            &quot;base_url&quot;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,            &quot;model&quot;: &quot;qwen2.5-7b-instruct&quot;,            &quot;enable&quot;: false        &#125;,        &quot;title_aided&quot;: &#123;            &quot;api_key&quot;: &quot;your_api_key&quot;,            &quot;base_url&quot;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,            &quot;model&quot;: &quot;qwen2.5-32b-instruct&quot;,            &quot;enable&quot;: false        &#125;    &#125;,    &quot;config_version&quot;: &quot;1.1.1&quot;&#125;
py脚本方式处理pdf文件
import osfrom magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReaderfrom magic_pdf.data.dataset import PymuDocDatasetfrom magic_pdf.model.doc_analyze_by_custom_model import doc_analyzefrom magic_pdf.config.enums import SupportedPdfParseMethod# argspdf_file_name = &quot;/mnthp/users/wangxiangbo/data/pdf2.pdf&quot;  # replace with the real pdf path# name_without_suff = pdf_file_name.split(&quot;.&quot;)[0]name_without_suff = os.path.basename(pdf_file_name).split(&quot;.&quot;)[0]# prepare envlocal_image_dir, local_md_dir = &quot;/mnthp/users/wangxiangbo/mineru/images&quot;, &quot;/mnthp/users/wangxiangbo/mineru/data&quot;image_dir = str(os.path.basename(local_image_dir))os.makedirs(local_image_dir, exist_ok=True)image_writer, md_writer = FileBasedDataWriter(local_image_dir), FileBasedDataWriter(    local_md_dir)# read bytesreader1 = FileBasedDataReader(&quot;&quot;)pdf_bytes = reader1.read(pdf_file_name)  # read the pdf content# proc## Create Dataset Instanceds = PymuDocDataset(pdf_bytes)## inferenceif ds.classify() == SupportedPdfParseMethod.OCR:    infer_result = ds.apply(doc_analyze, ocr=True)    ## pipeline    pipe_result = infer_result.pipe_ocr_mode(image_writer)else:    infer_result = ds.apply(doc_analyze, ocr=False)    ## pipeline    pipe_result = infer_result.pipe_txt_mode(image_writer)### draw model result on each pageinfer_result.draw_model(os.path.join(local_md_dir, f&quot;&#123;name_without_suff&#125;_model.pdf&quot;))### get model inference resultmodel_inference_result = infer_result.get_infer_res()### draw layout result on each pagepipe_result.draw_layout(os.path.join(local_md_dir, f&quot;&#123;name_without_suff&#125;_layout.pdf&quot;))### draw spans result on each pagepipe_result.draw_span(os.path.join(local_md_dir, f&quot;&#123;name_without_suff&#125;_spans.pdf&quot;))### get markdown contentmd_content = pipe_result.get_markdown(image_dir)### dump markdownpipe_result.dump_md(md_writer, f&quot;&#123;name_without_suff&#125;.md&quot;, image_dir)### get content list contentcontent_list_content = pipe_result.get_content_list(image_dir)### dump content listpipe_result.dump_content_list(md_writer, f&quot;&#123;name_without_suff&#125;_content_list.json&quot;, image_dir)### get middle jsonmiddle_json_content = pipe_result.get_middle_json()### dump middle jsonpipe_result.dump_middle_json(md_writer, f&#x27;&#123;name_without_suff&#125;_middle.json&#x27;)


处理结果


利用Langchain切分md数据，并转为jsonl格式
Langchain环境安装
pip install langchain


py脚本切分转好的md数据
LangChain 的 **RecursiveCharacterTextSplitter** 会 按优先级 依次尝试 分割文本：

**\n**（换行符）：如果能按段落分割，就不继续往下。
**&quot;. &quot;**** / ****&quot;。&quot;**（句号）：如果没有换行符，就按句号分割。
**&quot;!&quot;**** / **&quot;?&quot;** / **&quot;；&quot;** / ****&quot;;&quot;**（其他标点）：如果句号也没有，就按这些符号切分。

🚀** 这样能保证**：

先按段落分割，如果段落太长，再按 句号 细分；
如果句号也不够用，就继续用其他标点分割，确保切片合理。

from langchain.text_splitter import RecursiveCharacterTextSplitterimport json # 读取 Markdown 文件with open(&quot;/mnthp/users/wangxiangbo/mineru/data/pdf2.md&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:    markdown_text = f.read()# 使用 RecursiveCharacterTextSplitter，按句号分割splitter = RecursiveCharacterTextSplitter(    chunk_size=300,   # 每个块最大长度    chunk_overlap=0, # 允许的重叠字符    separators=[&quot;\n&quot;, &quot;. &quot;, &quot;。&quot;, &quot;!&quot;, &quot;?&quot;, &quot;；&quot;, &quot;;&quot;],  # 先按换行符分割，再按句号、感叹号等)chunks = splitter.split_text(markdown_text)# 保存为 JSONL 文件output_path = &quot;/mnthp/users/wangxiangbo/mineru/data/output6.jsonl&quot;with open(output_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:    for chunk in chunks:        f.write(json.dumps(&#123;&quot;text&quot;: chunk&#125;, ensure_ascii=False) + &quot;\n&quot;)print(f&quot;文本已按句号分割并保存到 &#123;output_path&#125;&quot;)
切分结果


Data-juicer进行数据清洗与质量分析
data-juicer环境安装
https://github.com/modelscope/data-juicer
依赖环境步骤略去，已推至镜像
registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:data-juicer
数据清洗
清洗配置
# Process config example for dataset# global parametersproject_name: &#x27;test-process&#x27;dataset_path: &#x27;/mnt/mineru/data/output6.jsonl&#x27;  # path to your dataset directory or filenp: 4  # number of subprocess to process your datasetexport_path: &#x27;./outputs/demo-process/test-processed4.jsonl&#x27;# process schedule# a list of several process operators with their argumentsprocess:  - language_id_score_filter:      lang: &#x27;zh&#x27;      min_score: 0.8  - clean_email_mapper:                                     # remove emails from text.  - clean_html_mapper:                                      # remove html formats form text.  - clean_ip_mapper:                                        # remove ip addresses from text.  - clean_links_mapper:                                     # remove web links from text.  - average_line_length_filter:                             # filter text with the average length of lines out of specific range.      min_len: 50                                             # the min length of filter range      max_len: 10000                                          # the max length of filter range#   - perplexity_filter:#       lang: &#x27;zh&#x27;#       max_ppl: 5000                                             #过滤困惑度大于5000的文本  #   - alphanumeric_filter:                                    # filter text with alphabet/numeric ratio out of specific range.#       tokenization: false                                     # whether to count the ratio of alphanumeric to the total number of tokens.#       min_ratio: 0.0                                          # 确保文本包含足够多的字母数字内容，字母数字/总字符数 &gt;0.7#       max_ratio: 0.9                                         # 允许高达90%的字母数字，过滤掉接近纯字母数字的异常文本。
python tools/process_data.py --config configs/demo/process.yaml




数据分析（质量）
分析配置
# Process config example for dataset# global parametersproject_name: &#x27;demo-analyzer&#x27;dataset_path: &#x27;./outputs/demo-process/test-processed4.jsonl&#x27;  # path to your dataset directory or filenp: 4  # number of subprocess to process your datasetexport_path: &#x27;./outputs/demo-analyzer/demo-analyzer-result4.jsonl&#x27;# process schedule# a list of several process operators with their argumentsprocess:  - language_id_score_filter:      lang: &#x27;zh&#x27;  - perplexity_filter:      lang: &#x27;zh&#x27;    #   max_ppl: 5000                                             #过滤困惑度大于5000的文本                                        #   - llm_difficulty_score_filter:                            # filter to keep sample with high difficulty score estimated by LLM.#       api_or_hf_model: &#x27;gpt-4o&#x27;                               # API or huggingface model name.#       min_score: 0.5                                          # The lowest difficulty score threshold to keep the sample.#       api_endpoint: null                                      # URL endpoint for the API.#       response_path: null                                     # Path to extract content from the API response. Defaults to &#x27;choices.0.message.content&#x27;.#       input_keys: [&#x27;text&#x27;]                                    # Sub set of keys in the sample. Support data with multi fields such as &#x27;query&#x27;, &#x27;analysis&#x27; and &#x27;answer&#x27; in RFT data.#       field_names: [&#x27;text&#x27;]                                   # Corresponding field names for input keys.#       system_prompt: null                                     # System prompt for the task.#       input_template: null                                    # The input template.#       field_template: null                                    # Template for each field in the prompt.#       try_num: 3                                              # The number of retry attempts when there is an API call error or output parsing error.#       enable_vllm: false                                      # If true, use VLLM for loading hugging face or local llm. Otherwise, use API for reference.#       model_params: &#123;&#125;                                        # Parameters for initializing the API model.#       sampling_params: &#123;&#125;                                     # Extra parameters passed to the API call. e.g &#123;&#x27;temperature&#x27;: 0.9, &#x27;top_p&#x27;: 0.95&#125;#   - llm_quality_score_filter:                               # filter to keep sample with high quality score estimated by LLM.#       api_or_hf_model: &#x27;gpt-4o&#x27;                               # API or huggingface model name.#       min_score: 0.5                                          # The lowest quality score threshold to keep the sample.#       api_endpoint: https://dashscope.aliyuncs.com/compatible-mode/v1                                      # URL endpoint for the API.#       response_path: null                                     # Path to extract content from the API response. Defaults to &#x27;choices.0.message.content&#x27;.#       input_keys: [&#x27;text&#x27;]                                    # Sub set of keys in the sample. Support data with multi fields such as &#x27;query&#x27;, &#x27;analysis&#x27; and &#x27;answer&#x27; in RFT data.#       field_names: [&#x27;text&#x27;]                                   # Corresponding field names for input keys.#       system_prompt: null                                     # System prompt for the task.#       input_template: null                                    # The input template.#       field_template: null                                    # Template for each field in the prompt.#       try_num: 3                                              # The number of retry attempts when there is an API call error or output parsing error.#       enable_vllm: false                                      # If true, use VLLM for loading hugging face or local llm. Otherwise, use API for reference.#       model_params: &#123;#         &#x27;api_key&#x27;: &quot;sk-312f14f4e65144f1b9f9240a31b584f9&quot;,  # 你的 OpenAI API Key#         &#x27;base_url&#x27;: &quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;  # OpenAI API Base URL#       &#125;                                        # Parameters for initializing the API model.#       sampling_params: &#123;#         &#x27;temperature&#x27;: 0.9,#         &#x27;top_p&#x27;: 0.95#       &#125;                                     # Extra parameters passed to the API call. e.g &#123;&#x27;temperature&#x27;: 0.9, &#x27;top_p&#x27;: 0.95&#125;
python tools/analyze_data.py --config configs/demo/analyzer.yaml




1. 语言检测（lang）

count: 19.0（19 个样本都有语言标记）。
unique: 1.0（只有一种语言）。
top: zh（最常见的语言是中文）。
freq: 19.0（所有 19 个样本都是中文）。
结论：这个数据集是纯中文数据集，没有其他语言混入。

2. 语言得分（lang_score）
定义：语言得分通常表示文本被识别为某种语言的可信度，分数范围一般是 0 到 1，越高表示越可能是该语言。
统计结果：

mean: 0.949412（平均得分约为 0.95，表明文本被识别为中文的置信度很高）。
std: 0.02133（标准差较小，说明得分分布较为集中，样本间差异不大）。
min: 0.909303（最低得分仍高于 0.9，说明即使是最差的样本，中文识别置信度也很高）。
25%: 0.937739（25% 分位数，说明大部分样本得分高于 0.94）。
50%: 0.95398（中位数，接近均值，分布较为对称）。
75%: 0.958543（75% 分位数，大部分样本得分集中在 0.96 以下）。
max: 0.98278（最高得分接近 1，表明有些样本几乎被完美识别为中文）。

结论：语言得分整体较高且分布稳定，说明数据集的文本语言一致性非常好，几乎可以确定为高质量的中文文本。
3. 困惑度（perplexity）
定义：困惑度是衡量语言模型对文本预测难度的指标，通常与文本的复杂性、可预测性相关。值越低，文本越容易被模型理解；值越高，文本可能更复杂或噪声更大。
统计结果：

mean: 4351.347368（平均困惑度约为 4351，偏高）。
std: 1360.413693（标准差较大，说明样本间困惑度差异明显）。
min: 1270.3（最低困惑度，表明有些样本较为简单或规律性强）。
25%: 3974.1（25% 分位数，大多数样本困惑度高于 3974）。
50%: 4415.3（中位数，与均值接近，分布相对均匀）。
75%: 5062.0（75% 分位数，表明 25% 的样本困惑度高于 5062）。
max: 7357.8（最高困惑度，说明某些样本可能非常复杂或包含噪声）。

结论：困惑度均值和范围都较高（1270.3 到 7357.8），且标准差较大，表明数据集文本复杂度差异显著。部分样本可能包含不规则内容、噪声或过于复杂的表达，这可能会影响模型训练效果。
jsonl格式数据-&gt;Q&amp;A问答对格式
jsonl格式数据集
采用huggingface开源的BAAI/IndustryCorpus和BAAI/IndustryCorpus2
计算机数据集：https://huggingface.co/datasets/BAAI/IndustryCorpus_computer
国家政治类数据集：https://huggingface.co/datasets/BAAI/IndustryCorpus2_current_affairs_government_administration
清洗处理后的政企类数据集：/mnthp/users/wangxiangbo/data-juicer-main/outputs/demo-process/test-processed4.jsonl
计算机类数据集jsonl抽取
为了测试代码和节约api，仅测试其中10条数据
&#123;&quot;text&quot;:&quot;推特怎样查看评论(推特怎么看评论区)\n去年22月开始,推特开始测试类似于Reddit的对话树用户界面,以帮助用户更轻松地追踪转发和后续推文.在经过5个多月的测试之后,该功能终于面向所有用户开放.\n类似于Reddit的对话树现在可以在Windows10的Twitter PWA和网页端中使用.如果你使用的是Android手机,并且手机上安装了Twitter Lite,你也会看到这个变化.值得注意的是,由于这是一个服务器端的更新,所以不需要你访问Google Play Store或Microsoft Store,也不需要你去更新Twitter web应用.&quot;,&quot;alnum_ratio&quot;:0.9172661871,&quot;avg_line_length&quot;:92.6666666667,&quot;char_rep_ratio&quot;:0.029739777,&quot;flagged_words_ratio&quot;:0.0,&quot;industry_type&quot;:&quot;计算机&quot;,&quot;lang&quot;:&quot;zh&quot;,&quot;lang_score&quot;:0.8875751495,&quot;max_line_length&quot;:180,&quot;num_words&quot;:73,&quot;perplexity&quot;:1061.4,&quot;special_char_ratio&quot;:0.1079136691,&quot;word_rep_ratio&quot;:0.0,&quot;id&quot;:3822520893459&#125;


国家政治类数据集parquet格式转为jsonl格式
import pandas as pd# 读取 parquet 文件df = pd.read_parquet(&#x27;/mnt/data/government_administration/chinese/middle/rank_00410.parquet&#x27;)# 转换为 JSON Lines 格式并保存df.to_json(&quot;data.json&quot;, orient=&quot;records&quot;, lines=True, force_ascii=False)
&#123;&quot;text&quot;:&quot;中国外交部发言人耿爽27日在北京表示,中方参与打击索马里海盗国际合作的行动,得到了有关国家的积极评价,为维护国际和地区的和平与安全作出重要贡献,也展现了中国负责任,有担当的大国形象.\n中新社记者 侯宇 摄\n  在当天举行的外交部例行记者会上,有记者提问:日前,联合国秘书长向安理会提交索马里海盗问题报告.报告多次提及中国在亚丁湾和索马里海域执行护航任务,为应对海盗威胁发挥了重要作用.中方对此有何评论?\n  耿爽回应说,2008年12月,经联合国安理会授权,中国海军派出首批舰艇编队,赴亚丁湾,索马里海域执行护航任务.截至2017年7月,中国海军先后派出了26批护航编队,为中外6400余艘船只提供护航,驱离疑似海盗船只3000余艘次,有力保障了国际重要贸易航道安全.中方参与打击索马里海盗国际合作的行动,得到了有关国家的积极评价,为维护国际和地区的和平与安全作出重要贡献,也展现了中国负责任,有担当的大国形象.\n  \&quot;联合国秘书长的有关报告对中方所作工作和贡献给予了积极评价,我们对此表示赞赏.\&quot;耿爽说,正如习近平总书记在十九大报告中所指出的,中国将恪守维护世界和平,促进共同发展的外交政策宗旨,始终做世界和平的建设者,全球发展的贡献者,国际秩序的维护者.中方将继续积极参与亚丁湾和索马里海域护航行动,拓展国际护航合作,履行国际责任和义务,为维护国际和地区和平与稳定,保障国际重要通道安全发挥更大作用.(完)&quot;,&quot;alnum_ratio&quot;:0.9024793388,&quot;avg_line_length&quot;:121.0,&quot;char_rep_ratio&quot;:0.0771812081,&quot;flagged_words_ratio&quot;:0.0,&quot;max_line_length&quot;:204,&quot;num_words&quot;:213,&quot;perplexity&quot;:701.0,&quot;quality_score&quot;:3.79296875,&quot;special_char_ratio&quot;:0.1371900826,&quot;word_rep_ratio&quot;:0.1848184818,&quot;_id&quot;:1005000038194,&quot;industry_type&quot;:&quot;时政_政务_行政&quot;&#125;


通过阿里api_key分析原始jsonl并生成QA微调类数据集
py脚本
import osimport jsonfrom openai import OpenAI# 初始化客户端try:    client = OpenAI(        api_key=&quot;sk-312f14f4e65144f1b9f9240a31b584f9&quot;,  # 请替换为你的实际API Key        base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,    )    # 输入文件路径    input_file = &quot;/mnt/data/IndustryCorpus_computer/10.jsonl&quot;        # 输出文件路径（只保存Q&amp;A问答对）    output_file = &quot;/mnt/data/IndustryCorpus_computer/10_qa_pairs.jsonl&quot;    # 计数器    processed_count = 0    # 打开输入文件并逐行处理    with open(input_file, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f:        for line in f:            try:                # 解析每一行的JSON数据                data = json.loads(line.strip())                                # 构造请求消息，要求返回特定JSON格式                completion = client.chat.completions.create(                    model=&quot;qwen-plus&quot;,                    messages=[                        &#123;                            &#x27;role&#x27;: &#x27;system&#x27;,                             &#x27;content&#x27;: &#x27;&#x27;&#x27;你是一个擅长信息提取和问答对转换的助手。请根据提供的文本，分析其内容并转换为适合训练的问答对格式，一对问答对即可。输出必须是以下JSON格式：                            &#123;                                &quot;instruction&quot;: &quot;问题&quot;,                                &quot;output&quot;: &quot;答案&quot;                            &#125;                            确保问题和答案内容准确反映输入文本信息。&#x27;&#x27;&#x27;                        &#125;,                        &#123;                            &#x27;role&#x27;: &#x27;user&#x27;,                             &#x27;content&#x27;: json.dumps(data, ensure_ascii=False)                        &#125;                    ]                )                                # 获取大模型的输出（JSON格式的问答对）                qa_pair = completion.choices[0].message.content                                # 打印结果                print(f&quot;处理第 &#123;processed_count + 1&#125; 行:&quot;)                print(qa_pair)                print(&quot;-&quot; * 50)                                # 保存JSON格式的问答对到文件                with open(output_file, &#x27;a&#x27;, encoding=&#x27;utf-8&#x27;) as out_f:                    out_f.write(qa_pair + &quot;\n&quot;)                                processed_count += 1                            except Exception as line_error:                print(f&quot;处理第 &#123;processed_count + 1&#125; 行时出错：&#123;line_error&#125;&quot;)                continue        print(f&quot;处理完成，总共处理 &#123;processed_count&#125; 行数据&quot;)except Exception as e:    print(f&quot;初始化或主程序错误：&#123;e&#125;&quot;)    print(&quot;请参考文档：https://help.aliyun.com/zh/model-studio/developer-reference/error-code&quot;)
处理结果
计算机类数据集


通过移动云api_key分析原始jsonl并生成QA微调类数据集
py脚本
from openai import OpenAIimport jsonimport osimport reimport logging# 设置日志logging.basicConfig(    level=logging.INFO,    format=&#x27;%(asctime)s - %(levelname)s - %(message)s&#x27;,    handlers=[        logging.FileHandler(&quot;/mnt/api/log/government.log&quot;, encoding=&#x27;utf-8&#x27;),        logging.StreamHandler()    ])logger = logging.getLogger(__name__)# 初始化OpenAI客户端client = OpenAI(    api_key=&quot;jn8P1B1vl7AbKZanw-rDQ9ahJmUH3eML_hAaKGP9wkA&quot;,    base_url=&quot;https://zhenze-huhehaote.cmecloud.cn/inference-api/exp-api/inf-1336781912337387520/v1&quot;)# 文件路径input_file_path = &quot;/mnt/data-juicer-main/outputs/demo-process/test-processed2.jsonl&quot;output_file_path = &quot;/mnt/api/res/government_qa.jsonl&quot;# 确保输出目录存在output_dir = os.path.dirname(output_file_path)if not os.path.exists(output_dir):    os.makedirs(output_dir)# 系统提示system_prompt = &#123;    &quot;role&quot;: &quot;system&quot;,    &quot;content&quot;: &quot;&quot;&quot;你是一个擅长信息提取和问答对转换的助手。请根据提供的文本，分析其内容并转换为一个问答对，格式必须为：    &#123;&quot;instruction&quot;: &quot;问题&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;答案&quot;&#125;    只生成一对问答对，返回JSON字符串。    &quot;&quot;&quot;&#125;# 计数器processed_count = 0error_count = 0# 从响应中提取JSON的函数def extract_json_from_response(response):    # 尝试直接解析整个响应为JSON    try:        qa_json = json.loads(response)        if all(key in qa_json for key in [&quot;instruction&quot;, &quot;input&quot;, &quot;output&quot;]):            if qa_json[&quot;input&quot;] != &quot;&quot;:                qa_json[&quot;input&quot;] = &quot;&quot;  # 强制设为空字符串            return qa_json    except json.JSONDecodeError:        pass        # 使用更精确的正则表达式匹配JSON    json_match = re.search(r&#x27;```json\s*(&#123;.*?&#125;)\s*```&#x27;, response, re.DOTALL)    if json_match:        json_str = json_match.group(1)        try:            qa_json = json.loads(json_str)            if all(key in qa_json for key in [&quot;instruction&quot;, &quot;input&quot;, &quot;output&quot;]):                if qa_json[&quot;input&quot;] != &quot;&quot;:                    qa_json[&quot;input&quot;] = &quot;&quot;  # 强制设为空字符串                return qa_json        except json.JSONDecodeError:            pass        # 如果仍然失败，返回None    return None# 读取输入文件并处理with open(input_file_path, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as input_file, \     open(output_file_path, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as output_file:        for line_num, line in enumerate(input_file, 1):        try:            # 解析每一行的JSON数据            data = json.loads(line.strip())            user_input = json.dumps(data, ensure_ascii=False)                        # 构造消息            messages = [                system_prompt,                &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input&#125;            ]                        # 调用API            response = client.chat.completions.create(                model=&quot;default&quot;,                messages=messages,                max_tokens=1024,                temperature=0.6,                stream=False            )                        # 获取结果            qa_result = response.choices[0].message.content.strip()            logger.info(f&quot;Line &#123;line_num&#125; - Raw response: &#123;qa_result&#125;&quot;)                        # 从响应中提取JSON            qa_json = extract_json_from_response(qa_result)                        # 如果提取失败，记录原始响应            if qa_json is None:                qa_json = &#123;                    &quot;instruction&quot;: &quot;问题提取失败&quot;,                    &quot;input&quot;: &quot;&quot;,                    &quot;output&quot;: f&quot;原始响应: &#123;qa_result&#125;&quot;                &#125;                logger.warning(f&quot;Line &#123;line_num&#125; - Failed to extract JSON, using fallback&quot;)            else:                logger.info(f&quot;Line &#123;line_num&#125; - Successfully extracted JSON: &#123;json.dumps(qa_json, ensure_ascii=False)&#125;&quot;)                        # 写入文件            output_file.write(json.dumps(qa_json, ensure_ascii=False) + &#x27;\n&#x27;)            processed_count += 1                        # 控制台显示进度            print(f&quot;已处理: &#123;processed_count&#125; 行&quot;)                    except json.JSONDecodeError as e:            error_count += 1            logger.error(f&quot;Line &#123;line_num&#125; - JSON解析错误: &#123;e&#125;, Input: &#123;line.strip()&#125;&quot;)            print(f&quot;Line &#123;line_num&#125; - JSON解析错误: &#123;e&#125;&quot;)            print(&quot;-&quot; * 50)        except Exception as e:            error_count += 1            logger.error(f&quot;Line &#123;line_num&#125; - 处理出错: &#123;e&#125;, Input: &#123;line.strip()&#125;&quot;)            print(f&quot;Line &#123;line_num&#125; - 处理出错: &#123;e&#125;&quot;)            print(&quot;-&quot; * 50)# 打印总结print(&quot;=&quot; * 50)print(f&quot;处理完成！&quot;)print(f&quot;总计处理行数: &#123;processed_count + error_count&#125;&quot;)print(f&quot;成功生成问答对: &#123;processed_count&#125;&quot;)print(f&quot;错误行数: &#123;error_count&#125;&quot;)print(f&quot;结果已保存至: &#123;output_file_path&#125;&quot;)print(f&quot;日志已保存至: /mnt/api/log/government.log&quot;)
处理结果
国家政治类数据集


&#123;&quot;instruction&quot;: &quot;中国海军首批赴亚丁湾、索马里海域执行护航任务的舰艇编队是在何时派出的？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;2008年12月，经联合国安理会授权，中国海军派出首批舰艇编队赴亚丁湾、索马里海域执行护航任务。&quot;&#125;&#123;&quot;instruction&quot;: &quot;闵行区七宝镇党委在提升党群服务阵地能级方面采取了哪些措施？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;七宝镇党委通过织好&#x27;理想信念网&#x27;强化政治建设，采用&#x27;线上+线下&#x27;相结合的立体化学习教育方式，推出包含七号桥碉堡、张充仁纪念馆等18个红色点位组成的行知路线，并创新开展情景党课，全面提升党群服务阵地能级。&quot;&#125;&#123;&quot;instruction&quot;: &quot;汪辜会谈在两岸关系发展中具有怎样的历史意义？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;汪辜会谈是海峡两岸授权的民间机构最高负责人之间的首次会晤，也是两岸高层人士在长期隔断后的首次正式接触，标志着两岸走向和解的历史性突破，成为两岸关系发展进程中的\&quot;重要里程碑\&quot;。尽管会谈范围限于民间性、经济性和事务性领域，但成功启动了两岸对话谈判机制，推动两岸关系向更深层次发展，为后续两岸关系和平发展进入巩固深化期奠定了基础。&quot;&#125;&#123;&quot;instruction&quot;: &quot;泰州市五届人大二次会议选举产生的市人大常委会主任和市监察委员会主任分别是谁？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;曲福田当选为市人大常委会主任，汤成快当选为市监察委员会主任。&quot;&#125;
清洗处理后的政企类数据集






&#123;&quot;instruction&quot;: &quot;优化政务服务、提升行政效能的重要抓手是什么？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;把\&quot;高效办成一件事\&quot;作为优化政务服务、提升行政效能的重要抓手，通过加强整体设计、推动模式创新、注重改革引领和数字赋能双轮驱动，实现办事方式多元化、流程最优化、材料最简化和成本最小化。&quot;&#125;&#123;&quot;instruction&quot;: &quot;到2027年我国高效政务服务体系建设要实现什么目标？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;基本形成泛在可及、智慧便捷、公平普惠的高效政务服务体系，实现企业和个人两个全生命周期重要阶段&#x27;高效办成一件事&#x27;重点事项落地见效，大幅提升企业和群众办事满意度、获得感。&quot;&#125;&#123;&quot;instruction&quot;: &quot;如何推进线下办事“只进一门”？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;通过完善集约高效的线下政务服务体系，县级以上政务服务中心应统一设置综合/专业服务窗口集中办理政务服务事项；整合部门单设窗口至本级中心或实行一体化管理；统筹建设乡镇(街道)便民服务中心和村(社区)便民服务站；鼓励在银行网点、邮政网点、园区设置便民服务点，并利用集成式自助终端提供24小时服务。&quot;&#125;&#123;&quot;instruction&quot;: &quot;如何推进线上办事“一网通办”？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;加强国家政务服务平台为总枢纽的全国政务服务“一张网”建设，推动高频事项网上办、掌上办、一次办；建设省级政务服务平台统一受理端实现办件信息实时共享；整合各级平台资源推动政务服务事项和应用“应接尽接、应上尽上”，除法定或涉密情况外所有事项均纳入同级政务服务平台办理。&quot;&#125;&#123;&quot;instruction&quot;: &quot;如何推进企业和群众诉求的“一线应答”？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;通过依托12345政务服务便民热线加强热线归并，设置重点领域专席提升接办效率，建立“接诉即办”机制及时响应诉求，推动12345与110高效对接联动，并加强热线与政务服务平台投诉建议体系、“好差评”体系的业务协同，打造智慧政务服务“总客服”。&quot;&#125;&#123;&quot;instruction&quot;: &quot;推进关联事项集成办需要采取哪些具体措施？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;1. 建立&#x27;一件事&#x27;牵头部门与配合部门的职责分工体系；2. 重构跨部门业务流程，实现多表合一、一网通办和材料一次提交；3. 开展线上线下并联审批、联合评审和联合验收；4. 通过系统协同和数据共享压缩办理时间，降低办事成本。&quot;&#125;&#123;&quot;instruction&quot;: &quot;推进容缺事项承诺办中提到的“告知承诺+容缺办理”审批服务模式的主要目标是什么？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;在风险可控、纠错成本低且能够通过事中事后监管有效防范风险的前提下，推进政务服务便利化，通过申请材料后补/免交、实质审查后置/豁免等方式实现政务服务事项承诺办理，并建立差异化的告知承诺事后核查和风险防范机制。&quot;&#125;&#123;&quot;instruction&quot;: &quot;如何推进政务服务异地事项跨域办理？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;通过推动政务服务事项省内通办、跨省通办，依托全国一体化政务服务平台实现数据共享和网上办理；优化线下代收代办服务模式，建立跨域窗口协同机制；运用远程虚拟窗口技术提供帮办服务，并支持京津冀等重点区域探索更多跨区域办理事项。&quot;&#125;&#123;&quot;instruction&quot;: &quot;如何推进政策服务免申办？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;通过全面梳理政策条件与规则，依托数据归集共享、模型算法和大数据分析，精准匹配符合条件的企业与群众，推动政策“免申即享”；对需申请的政策自动生成申请表并精准推送申请材料，同时优化政务服务平台专属服务空间，实现政策精准直达。&quot;&#125;&#123;&quot;instruction&quot;: &quot;根据文本内容，推动政务服务平台的整合中，关于地市级以下政务服务平台的建设有什么规定？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;原则上不再单独建设地市级以下政务服务平台。&quot;&#125;&#123;&quot;instruction&quot;: &quot;如何提升政务数据共享实效？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;通过完善政务数据共享责任清单机制，依托全国一体化政务服务平台数据共享枢纽推动部门数据回流和直达基层，编制&#x27;一企一档、一人一档&#x27;数据规范实现&#x27;一数一源一标准&#x27;，深化电子证照跨地区跨部门互通互认，推动政府部门核发材料免提交、共享核验事项免证明、电子证照免实体证照。&quot;&#125;&#123;&quot;instruction&quot;: &quot;如何推动政务服务由人力服务型向人机交互型和数据分析型转变？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;通过创新应用大数据、区块链、人工智能等新技术，优化重构服务流程并完善智能预填、预审功能，同时在具体办事场景中应用自然语言大模型技术提升智能客服能力，推动政务服务向人机交互型和数据分析型转变。&quot;&#125;&#123;&quot;instruction&quot;: &quot;如何增强在高新区、产业园区的项目帮办代办能力？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;通过加强项目全流程帮办代办，结合实际建立专班服务、专员跟进等机制，及时解决项目推进中的难点问题。&quot;&#125;&#123;&quot;instruction&quot;: &quot;在丰富公共服务供给方面，新增了哪些与企业和群众生产生活密切相关的服务领域？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;新增了水电气热、网络通信等公用事业领域的高频办理服务事项。&quot;&#125;&#123;&quot;instruction&quot;: &quot;如何拓展涉企增值服务内容？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;通过统筹行业协会、市场化专业服务机构等资源，一站式提供政策服务、法律服务、金融服务、人才服务、科创服务和国际贸易服务，具体包括政策推荐/申报、公证/合规指导、融资担保/产业基金对接、人才认定/住房安居、科技企业培育、报关退税咨询等服务。&quot;&#125;&#123;&quot;instruction&quot;: &quot;如何健全政务服务标准体系？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;健全政务服务标准体系需加强总体设计，制定全国一体化政务服务平台、政务服务中心建设及12345热线运行标准，统一政务服务事项基础标准，编制集成办理\&quot;一件事\&quot;指南，并完善跨区域办事业务标准与操作规程。&quot;&#125;&#123;&quot;instruction&quot;: &quot;如何强化政务服务制度供给？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;强化政务服务制度供给需通过三方面措施：1.加强国家层面立法研究并清理修订不适应性法规；2.破除集成办/承诺办/跨域办/免申办的制度障碍；3.完善数字化应用配套政策保障电子证照共享和电子档案效力。&quot;&#125;&#123;&quot;instruction&quot;: &quot;如何健全政务服务工作体系？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;推进国家、省、市、县、乡五级政务服务体系建设，健全一体联动、高效便捷、权责清晰的工作机制；加强各级政务服务窗口从业人员配备、管理、培训和考核，推进综合服务窗口人员统一配备和职业化发展；创新政务服务人才引进、培养、选拔和评价机制，提升干部队伍法治思维、服务意识和数字素养，强化专业化队伍建设。&quot;&#125;&#123;&quot;instruction&quot;: &quot;如何确保“高效办成一件事”取得实效？&quot;, &quot;input&quot;: &quot;&quot;, &quot;output&quot;: &quot;1.各地区要强化省级统筹，逐项制定工作方案并明确目标任务、改革措施和职责分工；\n2.国务院各部门需加强条块联动，提供业务指导和数据共享支撑；\n3.国务院办公厅要统筹协调，开展试点示范并逐年推出重点事项清单，推动政务服务提质增效。&quot;&#125;


微调测试
数据集
按上述方法，重新增加了政企类数据集。





提取并切分后的数据




清洗结果如下




清洗结果对比








问答对生成






ms-swift 框架微调
环境搭建
ms-swift 训练环境已传至镜像
registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:swift
训练脚本
基于政企类数据集的 qwen2.5-7B-Ins-lora 微调
# 22GBCUDA_VISIBLE_DEVICES=0,1,2,3 \swift sft \    --model /mnt/model/Qwen/Qwen2.5-7B-Instruct \    --train_type lora \    --dataset /mnt/ms-swift/data/train.jsonl \    --torch_dtype float16 \    --num_train_epochs 6 \    --per_device_train_batch_size 1 \    --per_device_eval_batch_size 1 \    --learning_rate 1e-4 \    --lora_rank 8 \    --lora_alpha 32 \    --target_modules all-linear \    --gradient_accumulation_steps 1 \    --eval_steps 50 \    --save_steps 50 \    --save_total_limit 5 \    --logging_steps 1 \    --max_length 2048 \    --output_dir /mnt/ms-swift/output \    --system &#x27;你是一个苏州政企问答小助手&#x27; \    --warmup_ratio 0.05 \    --dataloader_num_workers 4 \    --model_author swift \    --model_name swift-qwen2.5-7b
基于政企类数据集的 qwen2.5-0.5B-Ins-全参微调
# 76GiBCUDA_VISIBLE_DEVICES=0,1,2,3 \swift sft \    --model /mnt/model/Qwen2.5-0.5B-Instruct \    --train_type full \    --dataset /mnt/ms-swift/data/train.jsonl \    --torch_dtype float32 \    --num_train_epochs 6 \    --per_device_train_batch_size 1 \    --per_device_eval_batch_size 1 \    --learning_rate 1e-5 \    --gradient_accumulation_steps 16 \    --eval_steps 100 \    --save_steps 100 \    --save_total_limit 5 \    --logging_steps 1 \    --max_length 2048 \    --output_dir /mnt/ms-swift/output \    --system &#x27;You are a helpful assistant.&#x27; \    --warmup_ratio 0.05 \    --dataloader_num_workers 4 \    --model_author swift \    --model_name swift-qwen2.5-0.5b
训练
日志




显存占用情况


loss 曲线


推理测试
开启 swift web 界面服务
swift web-ui --lang zh --server_port 16686







]]></content>
      <categories>
        <category>模型微调</category>
        <category>数据集处理</category>
      </categories>
      <tags>
        <tag>NVIDIA</tag>
        <tag>V100</tag>
        <tag>模型微调</tag>
        <tag>数据集处理</tag>
      </tags>
  </entry>
  <entry>
    <title>llama-factory环境搭建-昇腾</title>
    <url>/2026/01/08/LLM/%E5%AE%A2%E6%88%B7%E6%94%AF%E6%92%91/%E6%9F%90%E5%BE%8B%E6%89%80%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/llama-factory%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA-%E6%98%87%E8%85%BE/</url>
    <content><![CDATA[训练
conda create --name llamafactory python=3.10conda activate llamafactorypip install -e &quot;.[torch-npu,metrics]&quot;pip install --upgrade torch==2.6.0 -i https://pypi.tuna.tsinghua.edu.cn/simplepip install --upgrade torch-npu==2.6.0rc1 -i https://pypi.tuna.tsinghua.edu.cn/simplepip uninstall scikit-learnsource /usr/local/Ascend/ascend-toolkit/set_env.shllamafactory-cli train examples/train_lora/qwen3_lora_sft.yaml
推理
llamafactory-cli chat examples/inference/qwen3_lora_sft.yamlvllm推理sudo apt updatesudo apt install libnuma-devpip install vllm==0.8.5 -i https://pypi.tuna.tsinghua.edu.cn/simplepip install vllm-ascend==ASCEND_RT_VISIBLE_DEVICES=2,4 llamafactory-cli chat examples/inference/qwen3_lora_sft.yamlbash Ascend-cann-nnal_8.0.0_linux-aarch64.run --installsource /usr/local/Ascend/nnal/atb/set_env.sh
]]></content>
      <categories>
        <category>训练环境搭建</category>
        <category>llama-factory</category>
      </categories>
      <tags>
        <tag>昇腾</tag>
        <tag>910B</tag>
        <tag>llama-factory</tag>
        <tag>训练环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title>A100集群6机48卡-基于k8s的Colossal-AI llama2 70B训练操作文档</title>
    <url>/2025/11/13/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Llama2/A100%E9%9B%86%E7%BE%A46%E6%9C%BA48%E5%8D%A1-%E5%9F%BA%E4%BA%8Ek8s%E7%9A%84Colossal-AI%20llama2%2070B%E8%AE%AD%E7%BB%83%E6%93%8D%E4%BD%9C%E6%96%87%E6%A1%A3/</url>
    <content><![CDATA[注：该项目为本人支撑客户项目，文档内涉及到的客户、设备信息等已脱敏处理。
基础环境说明
概述
模型名称: Colossal-AI llama2 70B
模型参数: 70B参数
硬件需求: 6台服务器，每台服务器配备8张NVIDIA A100 GPU
硬件配置
GPU型号: NVIDIA A100
GPU数量: 6台服务器 x 8张/台 = 48张GPU
内存容量: 每张A100 GPU拥有80GB显存
CPU: Intel® Xeon® Gold 5218R CPU @ 2.10GHz，x86_64
存储: 高速SSD存储，用于数据读写
网络: 高速网络连接，支持多机训练
GPU环境说明
驱动版本: 470.141.03
CUDA版本: CUDA 11.4
Nvidia-smi版本：470.141.03
NCCL: 用于多GPU和多节点通信
持久性模式: Persistence-Ml
功率使用上限: 功率上限为400W
显存：显存总量为81920MiB (80GB)
软件环境
操作系统: Linux (CentOS Linux 7 (Core))
Python版本: Python 3.8 或更高版本
依赖库:
PyTorch 2.1
Colossal-AI: 支持70B参数模型版本
其他依赖: NumPy, SciPy, Pandas等
模型参数大小
参数总数: 70B
模型占用内存: 由于GPU显存占用会根据模型的参数量、模型的内存占用、训练时的batch size、并行优化技术等不同而发生变化，现参数未定，暂不能量化GPU占用情况。
模型训练流程
注：由于xxx.xx.xx.141机器日常有业务在跑，无法空闲出完整的8块gpu资源，故以下最多以5机40卡来拉起训练。
查看A100机器的ip信息
cat /root/a100.txt
查看待训练节点的GPU占用情况
通过137机器，分别ssh进入以上6台机器（已设置免密登录），查看对应机器的gpu使用情况nvidia-smi、网卡配置信息ifconfig等。
示例一：
ssh root@xxx.xx.xx.141 ssh进入机器141
nvidia-smi141机器的gpu已经被占用，若是有需求要使用该机器，需要提前沟通
exit 从141机器退出到137机器上
示例二：
ssh root@xxx.xx.xx.78
nvidia-smi 78机器的gpu没有被使用，可以进行模型训练任务
物料准备工作
所有的物料位置存储在xxx.xx.xx137机器上的/home/disk_sdb0/llama2下。
物料包含原始数据集文件夹raw_data，离线切分完成的数据集文件夹datas、镜像llama2-70b.tar、70B模型权重Llama-2-70b-hf、13B模型权重Llama-2-13b-hf、7B模型权重Llama-2-7b-hf、k8s拉起预训练的配置文件llama2-70b-test.yaml、k8s拉起微调训练的配置文件llama2-70b-finetune.yaml，预训练/微调的训练代码train.py、预训练的配置脚本train.example.sh、微调训练的配置脚本finetune.example.sh、节点ip的信息配置文件hostfile.txt，此外还需要创建一个目录output来存放训练过程中的输出日志及checkpoint信息mkdir output。

训练环境配置准备
/etc/hosts配置
6台机器都需要对vim /etc/hosts文件进行修改，加入对应的 IP 地址和与其对应的主机名或域名的映射关系，以此保证各主机之间通信正常。此外，还需要加入百度镜像仓库地址的ip映射。否则会报socket通信错误。
socket.gaierror: [Errno -2] Name or service not known
xxx.xx.xx.103 xxx.xx.xx..baidu.comxxx.xx.xx137 xxx-xx-xx-xxx-137xxx.xx.xx.78 xxx-xx-xx-78.XXX-stack.comxxx.xx.xx.79 xxx-xx-xx-79.XXX-stack.comxxx.xx.xx.80 xxx-xx-xx-80.XXX-stack.comxxx.xx.xx.141 xxx-xx-xx-141.XXX-stack.comxxx.xx.xx.142 xxx-xx-xx-142.XXX-stack.comxxx.xx.xx.143 xxx-xx-xx-143.XXX-stack.com
k8s命名空间创建
因为未使用百度的测试平台来拉训练，通过k8s来拉起，所以需要自定义一个namespace，并在改ns下启动pod拉起训练任务。该ns需要与llama2-70b-test.yaml中的namespace: llama2-70b-test对应。
kubectl create namespace llama2-70b-test
kubectl get ns
镜像push至仓库
镜像需push到百度指定的仓库中
将镜像load之后，并将镜像push至 xxx.xx.xx…baidu.com 仓库中。
    1. `docker load -i llama2-70b.tar`
    2. `docker images`
    3. `docker tag 9cb8e972304f xxx.xx.xx..baidu.com/public/llama2-70b:v1.0`
    4. `docker login xxx.xx.xx..baidu.com -u admin -p XXXXXXX`
    5. `docker push xxx.xx.xx..baidu.com/public/llama2-70b:v1.0`

hostfile配置
通过ssh登录到5台机器，查看是否有gpu占用，拉起训练时要保证节点的8个gpu都没有被占用。将空闲的机器bond4的ip写入到hostfile中。bond4的ip地址可以通过ifconfig来进行查看，有的机器使用的是以太网eth0的ip配置。
xxx.xx.xx.142xxx.xx.xx.143xxx.xx.xx.78xxx.xx.xx.79xxx.xx.xx.80
训练脚本
修改预训练启动的脚本train.example.sh，网卡的socket通信配置要修改为bond4，并在拉起的训练的节点中选取一个作为matser主节点--master_addr
#!/bin/bash# NCCL IB environment variablesexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1export NCCL_IB_DISABLE=0export NCCL_SOCKET_IFNAME=bond4export NCCL_IB_GID_INDEX=3export NCCL_IB_TIMEOUT=23export NCCL_IB_RETRY_CNT=7export OMP_NUM_THREADS=8PROJECT_NAME=&quot;llama2-70b-pt&quot;PARENT_SAVE_DIR=&quot;/opt/tmp/ColossalAI/output/save/&quot;PARENT_TENSORBOARD_DIR=&quot;/opt/tmp/ColossalAI/output/tensorboard/&quot;PARENT_CONFIG_FILE=&quot;/opt/tmp/ColossalAI/output/config/&quot;PRETRAINED_MODEL_PATH=&quot;/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf&quot;declare -a dataset=(    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009&quot;)TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)FULL_PROJECT_NAME=&quot;$&#123;PROJECT_NAME&#125;-$&#123;TIMESTAMP&#125;&quot;SAVE_DIR=&quot;$&#123;PARENT_SAVE_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;TENSORBOARD_DIR=&quot;$&#123;PARENT_TENSORBOARD_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;CONFIG_FILE=&quot;$&#123;PARENT_CONFIG_FILE&#125;$&#123;FULL_PROJECT_NAME&#125;.json&quot;colossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.78 --master_port 30015 train.py \    --pretrained $PRETRAINED_MODEL_PATH \    --dataset $&#123;dataset[@]&#125; \    --plugin &quot;gemini_auto&quot; \    --save_interval 400 \    --save_dir $SAVE_DIR \    --tensorboard_dir $TENSORBOARD_DIR \    --config_file $CONFIG_FILE \    --num_epochs 1 \    --micro_batch_size 1 \    --lr 1e-4 \    --mixed_precision &quot;fp16&quot; \    --grad_clip 1.0 \    --weight_decay 0.01 \    --warmup_steps 100 \    --use_grad_checkpoint \    --max_length 4096 \    --accumulation_steps 1 \    --pad_token &quot;unk&quot;
启动k8s的yaml
修改k8s拉起的配置文件llama2-70b-test.yaml
apiVersion: apps/v1kind: Deploymentmetadata:  namespace: llama2-70b-test  name: llama2-70b-pt # Deployment 的名称spec:  replicas: 2 # 副本数量  selector:    matchLabels:      app: llama2-70b-pt  template:    metadata:      labels:        app: llama2-70b-pt    spec:      hostNetwork: true      containers:      - name: llama2-70b-container # 容器的名称        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0        imagePullPolicy: IfNotPresent        command: [&quot;/bin/bash&quot;, &quot;-c&quot;]        args: [&quot;cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ &amp;&amp; chmod +x start.sh &amp;&amp; ./start.sh &amp;&amp; chmod +x train.example.sh &amp;&amp; ./train.example.sh&quot;] # 启动容器后执行的命令，切换目录并运行训练脚本        resources:          limits:            nvidia.com/gpu: 8 # 限制使用 8 个 GPU，根据实际情况调整          requests:            nvidia.com/gpu: 8        volumeMounts:        - name: colossalai-orgdataset          mountPath: /opt/tmp/ColossalAI/datas/org_dataset # 将容器内数据目录挂载        - name: colossalai-weight          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载        - name: colossalai-train          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py          subPath: train.py          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它        - name: colossalai-output          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载        - name: host-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile          subPath: hostfile.txt          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它        - name: train-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # 容器内train_example.sh          subPath: train.example.sh        - name: shm-volume          mountPath: /dev/shm      nodeSelector:        model: llama2-70b      volumes:      - name: colossalai-orgdataset        hostPath:          path: /home/disk_sdb0/llama2/datas/org_dataset # 宿主机中 ColossalAI 数据的路径          type: Directory      - name: colossalai-weight        hostPath:          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # 宿主机中 ColossalAI 权重的路径          type: Directory      - name: colossalai-train        hostPath:          path: /home/disk_sdb0/llama2          type: Directory      - name: colossalai-output        hostPath:          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径          type: Directory      - name: host-script        hostPath:          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径          type: Directory      - name: train-script        hostPath:          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径          type: Directory      - name: shm-volume        emptyDir:          medium: Memory          sizeLimit: 8Gi
node节点打标签
给待拉起训练的node打上标签（首先需要确认待拉起训练的node的状态是否Ready）
打标签时，其中的model=llama2-70b对应的是llama2-70b-test.yaml文件中的nodeSelector部分，需保持一致。
kubectl label nodes xxx-xx-xx-141.XXX-stack.com model=llama2-70b
kubectl label nodes xxx-xx-xx-142.XXX-stack.com model=llama2-70b
kubectl label nodes xxx-xx-xx-143.XXX-stack.com model=llama2-70b
kubectl label nodes xxx-xx-xx-78.XXX-stack.com model=llama2-70b
kubectl label nodes xxx-xx-xx-79.XXX-stack.com model=llama2-70b
kubectl label nodes xxx-xx-xx-80.XXX-stack.com model=llama2-70b
训练物料传输与同步
因为多机拉起训练时，每个node节点都需要有一份训练所需的物料（权重、数据集、训练脚本等文件），故需要通过rsync或scp命令将137机器上的物料文件传输至待训练的节点中。(除去启动k8s所需的llama2-70b-test.yaml配置文件要在matser主节点上，其余的所有物料在待训练的node的同路径中/home/disk_sdb0/llama2都需要同步拷贝一份)
137节点传输至79节点的示例：
scp -r /home/disk_sdb0/llama2/Llama-2-70b-hf root@xxx.xx.xx.79:/home/disk_sdb0/llama2/Llama-2-70b-hf
scp -r /home/disk_sdb0/llama2/datas root@xxx.xx.xx.79:/home/disk_sdb0/llama2/datas
scp -r /home/disk_sdb0/llama2/output root@xxx.xx.xx.79:/home/disk_sdb0/llama2/output
scp -r /home/disk_sdb0/llama2/hostfile.txt root@xxx.xx.xx.79:/home/disk_sdb0/llama2
scp -r /home/disk_sdb0/llama2/train.py root@xxx.xx.xx.79:/home/disk_sdb0/llama2
scp -r /home/disk_sdb0/llama2/train.example.sh root@xxx.xx.xx.79:/home/disk_sdb0/llama2
训练拉起
打完标签之后，注意切换至cd /home/disk_sdb0/llama2，apply启动训练脚本。
kubectl apply -f llama2-70b-test.yaml 启动yaml。
kubectl get pod -n llama2-70b-test -owide 查看ns为llama2-70b-test下的pod信息。
kubectl describe pod -n llama2-70b-test xxx 查看每个正在运行的pod信息，用来追踪pod的运行状态，看是否有error报错。
kubectl logs xxx -n llama2-70b-test -f 实时跟踪pod的训练日志（最终的训练日志会在设定的主节点master上打印）。
kubectl delete -f llama2-70b-test.yaml删除yaml后，将终止训练任务。
模型训练测试记录
报错记录
nvidia driver 与 cuda版本问题
（nvidia driver 470 和 cuda11.4版本过低，故一直调不到机器GPU的资源，所以在并行策略zero和gemini时候会去调度CPU的资源，然后CPU撑不起来，直接就down掉了）该错误通过升级nvidia driver至525.125.06，cuda升级为12.0时，问题解决。

docker容器拉起测试问题
尝试通过该镜像run一个容器来测试，由于缺失nvidia-container-toolkit，后续安装完成后，再次run，发现–gpus all 未能调用（nvidia驱动太低导致）。去掉–gpus all之后，容器启动后，识别不到GPU。
dpkg -l | grep nvidia-container-toolkit 查看是否安装了nvidia-container-toolkit
A100需要nvidia-fabricmanager支持
驱动升级完成后，在xxx.xx.xx.78机器上尝试拉起单机8卡的llama2-7B模型的预训练，报了错误经查阅资料后发现A100需要nvidia-fabricmanager支持，后续安装对应nvidia driver驱动525.125.06一致的nvidia-fabricmanager后成功拉起训练。
错误解决参考博客：https://www.cnblogs.com/huadongw/p/16504137.html

单机8卡的llama2-7B的预训练成功拉起

5机40卡预训练时存在某进程随机占用gpu资源问题
在拉起5机40卡时，一直存在一个ns为single-gpu的pod被调度并占用一个gpu资源（删除之后会随机在6台机器上占用一个gpu）
kubectl describe node xxx-xx-xx-142.XXX-stack.com查看142节点情况
正常情况下，一台机器的8台gpu都处于空闲状态下，nvidia.com/gpu的占用应该是0
解决方法：删除掉启动该pod的对应的label（gpu-a100）并删除掉该pod
kubectl label nodes xxx-xx-xx-141.XXX-stack.com gpu-
kubectl label nodes xxx-xx-xx-142.XXX-stack.com gpu-
kubectl label nodes xxx-xx-xx-143.XXX-stack.com gpu-
kubectl label nodes xxx-xx-xx-78.XXX-stack.com gpu-
kubectl label nodes xxx-xx-xx-79.XXX-stack.com gpu-
kubectl label nodes xxx-xx-xx-80.XXX-stack.com gpu-
训练拉起测试
预训练
llama2-13B 2机16卡

启动k8s的yaml配置文件

apiVersion: apps/v1kind: Deploymentmetadata:  namespace: llama2-70b-test  name: llama2-70b-pt # Deployment 的名称spec:  replicas: 2 # 副本数量  selector:    matchLabels:      app: llama2-70b-pt  template:    metadata:      labels:        app: llama2-70b-pt    spec:      hostNetwork: true      containers:      - name: llama2-70b-container # 容器的名称        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0        imagePullPolicy: IfNotPresent        command: [&quot;/bin/bash&quot;, &quot;-c&quot;]        args: [&quot;cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ &amp;&amp; chmod +x train.example.sh &amp;&amp; ./train.example.sh&quot;] # 启动容器后执行的命令，切换目录并运行训练脚本        resources:          limits:            nvidia.com/gpu: 8 # 限制使用 8 个 GPU，根据实际情况调整          requests:            nvidia.com/gpu: 8        volumeMounts:        - name: colossalai-weight          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载        - name: colossalai-train          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py          subPath: train.py          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它        - name: colossalai-output          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载        - name: host-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile          subPath: hostfile.txt          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它        - name: train-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # 容器内train_example.sh          subPath: train.example.sh        - name: shm-volume          mountPath: /dev/shm      nodeSelector:        model: llama2-70b      volumes:      - name: colossalai-weight        hostPath:          path: /home/disk_sdb0/llama2/Llama-2-13b-hf # 宿主机中 ColossalAI 权重的路径          type: Directory      - name: colossalai-train        hostPath:          path: /home/disk_sdb0/llama2          type: Directory      - name: colossalai-output        hostPath:          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径          type: Directory      - name: host-script        hostPath:          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径          type: Directory      - name: train-script        hostPath:          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径          type: Directory      - name: train-script        hostPath:          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径          type: Directory      - name: shm-volume        emptyDir:          medium: Memory          sizeLimit: 8Gi

训练脚本

#!/bin/bash# NCCL IB environment variablesexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1export NCCL_IB_DISABLE=0export NCCL_SOCKET_IFNAME=bond4export NCCL_IB_GID_INDEX=3export NCCL_IB_TIMEOUT=23export NCCL_IB_RETRY_CNT=7export OMP_NUM_THREADS=8PROJECT_NAME=&quot;llama2-70b-pt&quot;PARENT_SAVE_DIR=&quot;/opt/tmp/ColossalAI/output/save/&quot;PARENT_TENSORBOARD_DIR=&quot;/opt/tmp/ColossalAI/output/tensorboard/&quot;PARENT_CONFIG_FILE=&quot;/opt/tmp/ColossalAI/output/config/&quot;PRETRAINED_MODEL_PATH=&quot;/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf&quot;declare -a dataset=(    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009&quot;)TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)FULL_PROJECT_NAME=&quot;$&#123;PROJECT_NAME&#125;-$&#123;TIMESTAMP&#125;&quot;SAVE_DIR=&quot;$&#123;PARENT_SAVE_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;TENSORBOARD_DIR=&quot;$&#123;PARENT_TENSORBOARD_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;CONFIG_FILE=&quot;$&#123;PARENT_CONFIG_FILE&#125;$&#123;FULL_PROJECT_NAME&#125;.json&quot;colossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.78 --master_port 30015 train.py \    --pretrained $PRETRAINED_MODEL_PATH \    --dataset $&#123;dataset[@]&#125; \    --plugin &quot;gemini_auto&quot; \    --save_interval 400 \    --save_dir $SAVE_DIR \    --tensorboard_dir $TENSORBOARD_DIR \    --config_file $CONFIG_FILE \    --num_epochs 1 \    --micro_batch_size 8 \    --lr 1e-4 \    --mixed_precision &quot;fp16&quot; \    --grad_clip 1.0 \    --weight_decay 0.01 \    --warmup_steps 100 \    --use_grad_checkpoint \    --max_length 512 \    --accumulation_steps 1 \    --pad_token &quot;unk&quot;

hostfile.txt

xxx.xx.xx.78xxx.xx.xx.79

训练拉起截图




78，79显存占用情况



llama2-70B 2机16卡

参数配置




micro_batch_size
max_length
plugin
mixed_precision
tflops




8
512
gemini_auto
fp16
64.4685




启动k8s的yaml配置文件

apiVersion: apps/v1kind: Deploymentmetadata:  namespace: llama2-70b-test  name: llama2-70b-pt # Deployment 的名称spec:  replicas: 2 # 副本数量  selector:    matchLabels:      app: llama2-70b-pt  template:    metadata:      labels:        app: llama2-70b-pt    spec:      hostNetwork: true      containers:      - name: llama2-70b-container # 容器的名称        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0        imagePullPolicy: IfNotPresent        command: [&quot;/bin/bash&quot;, &quot;-c&quot;]        args: [&quot;cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ &amp;&amp; chmod +x train.example.sh &amp;&amp; ./train.example.sh&quot;] # 启动容器后执行的命令，切换目录并运行训练脚本        resources:          limits:            nvidia.com/gpu: 8 # 限制使用 4 个 GPU，根据实际情况调整          requests:            nvidia.com/gpu: 8        volumeMounts:        - name: colossalai-weight          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载        - name: colossalai-train          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py          subPath: train.py          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它        - name: colossalai-output          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载        - name: host-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile          subPath: hostfile.txt          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它        - name: train-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # 容器内train_example.sh          subPath: train.example.sh        - name: profiler-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/performance_evaluator.py  # 容器内train_example.sh          subPath: performance_evaluator.py        - name: shm-volume          mountPath: /dev/shm      nodeSelector:        model: llama2-70b      volumes:      - name: colossalai-weight        hostPath:          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # 宿主机中 ColossalAI 权重的路径          type: Directory      - name: colossalai-train        hostPath:          path: /home/disk_sdb0/llama2          type: Directory      - name: colossalai-output        hostPath:          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径          type: Directory      - name: host-script        hostPath:          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径          type: Directory      - name: train-script        hostPath:          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径          type: Directory      - name: profiler-script        hostPath:            path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径            type: Directory      - name: shm-volume        emptyDir:          medium: Memory          sizeLimit: 8Gi

训练脚本

#!/bin/bash# NCCL IB environment variablesexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1export NCCL_IB_DISABLE=0export NCCL_SOCKET_IFNAME=bond4export NCCL_IB_GID_INDEX=3export NCCL_IB_TIMEOUT=23export NCCL_IB_RETRY_CNT=7export OMP_NUM_THREADS=8PROJECT_NAME=&quot;llama2-70b-pt&quot;PARENT_SAVE_DIR=&quot;/opt/tmp/ColossalAI/output/save/&quot;PARENT_TENSORBOARD_DIR=&quot;/opt/tmp/ColossalAI/output/tensorboard/&quot;PARENT_CONFIG_FILE=&quot;/opt/tmp/ColossalAI/output/config/&quot;PRETRAINED_MODEL_PATH=&quot;/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf&quot;declare -a dataset=(    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009&quot;)TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)FULL_PROJECT_NAME=&quot;$&#123;PROJECT_NAME&#125;-$&#123;TIMESTAMP&#125;&quot;SAVE_DIR=&quot;$&#123;PARENT_SAVE_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;TENSORBOARD_DIR=&quot;$&#123;PARENT_TENSORBOARD_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;CONFIG_FILE=&quot;$&#123;PARENT_CONFIG_FILE&#125;$&#123;FULL_PROJECT_NAME&#125;.json&quot;colossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.78 --master_port 30015 train.py \    --pretrained $PRETRAINED_MODEL_PATH \    --dataset $&#123;dataset[@]&#125; \    --plugin &quot;gemini_auto&quot; \    --save_interval 400 \    --save_dir $SAVE_DIR \    --tensorboard_dir $TENSORBOARD_DIR \    --config_file $CONFIG_FILE \    --num_epochs 1 \    --micro_batch_size 8 \    --lr 1e-4 \    --mixed_precision &quot;fp16&quot; \    --grad_clip 1.0 \    --weight_decay 0.01 \    --warmup_steps 100 \    --use_grad_checkpoint \    --max_length 512 \    --accumulation_steps 1 \    --pad_token &quot;unk&quot;

hostfile.txt

xxx.xx.xx.78xxx.xx.xx.79

训练拉起截图




78，79显存占用情况



llama2-70B 5机40卡

预训练数据集


由于模型需要对通过脚本对raw原始数据集进行切分，这个步骤会特别耗时，会随着数据集大小的增加而增加切分时长，由于每次拉起训练时都需要切分并消耗很多时间，所以采用了离线切分的方式将数据集离线切分好并直接挂载读取使用。
离线切分好的数据集文件共211G，其中会将原始数据集切分成10份并为arrow格式文件。


启动k8s的yaml配置文件

apiVersion: apps/v1kind: Deploymentmetadata:  namespace: llama2-70b-test  name: llama2-70b-pt # Deployment 的名称spec:  replicas: 5 # 副本数量  selector:    matchLabels:      app: llama2-70b-pt  template:    metadata:      labels:        app: llama2-70b-pt    spec:      hostNetwork: true      containers:      - name: llama2-70b-container # 容器的名称        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0        imagePullPolicy: IfNotPresent        command: [&quot;/bin/bash&quot;, &quot;-c&quot;]        args: [&quot;cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ &amp;&amp; chmod +x train.example.sh &amp;&amp; ./train.example.sh&quot;] # 启动容器后执行的命令，切换目录并运行训练脚本        resources:          limits:            nvidia.com/gpu: 8 # 限制使用 8 个 GPU，根据实际情况调整          requests:            nvidia.com/gpu: 8        volumeMounts:        - name: colossalai-orgdataset          mountPath: /opt/tmp/ColossalAI/datas/org_dataset # 将容器内数据目录挂载        - name: colossalai-predataset          mountPath: /opt/tmp/ColossalAI/datas/pre_dataset2        - name: colossalai-weight          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载        - name: colossalai-train          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py          subPath: train.py          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它        - name: colossalai-output          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载        - name: host-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile          subPath: hostfile.txt          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它        - name: train-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # 容器内train_example.sh          subPath: train.example.sh        - name: shm-volume          mountPath: /dev/shm      nodeSelector:        model: llama2-70b      volumes:      - name: colossalai-orgdataset        hostPath:          path: /home/disk_sdb0/llama2/datas/org_dataset # 宿主机中 ColossalAI 数据的路径          type: Directory      - name: colossalai-predataset        hostPath:          path: /home/disk_sdb0/llama2/datas/pre_dataset2          type: Directory      - name: colossalai-weight        hostPath:          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # 宿主机中 ColossalAI 权重的路径          type: Directory      - name: colossalai-train        hostPath:          path: /home/disk_sdb0/llama2          type: Directory      - name: colossalai-output        hostPath:          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径          type: Directory      - name: host-script        hostPath:          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径          type: Directory      - name: train-script        hostPath:          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径          type: Directory      - name: profiler-script        hostPath:            path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径            type: Directory      - name: shm-volume        emptyDir:          medium: Memory          sizeLimit: 8Gi

训练脚本

#!/bin/bash# NCCL IB environment variablesexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1export NCCL_IB_DISABLE=0export NCCL_SOCKET_IFNAME=bond4export NCCL_IB_GID_INDEX=3export NCCL_IB_TIMEOUT=23export NCCL_IB_RETRY_CNT=7export OMP_NUM_THREADS=8PROJECT_NAME=&quot;llama2-70b-pt&quot;PARENT_SAVE_DIR=&quot;/opt/tmp/ColossalAI/output/save/&quot;PARENT_TENSORBOARD_DIR=&quot;/opt/tmp/ColossalAI/output/tensorboard/&quot;PARENT_CONFIG_FILE=&quot;/opt/tmp/ColossalAI/output/config/&quot;PRETRAINED_MODEL_PATH=&quot;/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf&quot;declare -a dataset=(    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009&quot;)TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)FULL_PROJECT_NAME=&quot;$&#123;PROJECT_NAME&#125;-$&#123;TIMESTAMP&#125;&quot;SAVE_DIR=&quot;$&#123;PARENT_SAVE_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;TENSORBOARD_DIR=&quot;$&#123;PARENT_TENSORBOARD_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;CONFIG_FILE=&quot;$&#123;PARENT_CONFIG_FILE&#125;$&#123;FULL_PROJECT_NAME&#125;.json&quot;colossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.142 --master_port 30015 train.py \    --pretrained $PRETRAINED_MODEL_PATH \    --dataset $&#123;dataset[@]&#125; \    --plugin &quot;gemini_auto&quot; \    --save_interval 1600 \    --save_dir $SAVE_DIR \    --tensorboard_dir $TENSORBOARD_DIR \    --config_file $CONFIG_FILE \    --num_epochs 1 \    --micro_batch_size 16 \    --lr 1e-4 \    --mixed_precision &quot;fp16&quot; \    --grad_clip 1.0 \    --weight_decay 0.01 \    --warmup_steps 100 \    --use_grad_checkpoint \    --max_length 512 \    --accumulation_steps 1 \    --pad_token &quot;unk&quot;

hostfile.txt

xxx.xx.xx.142xxx.xx.xx.143xxx.xx.xx.78xxx.xx.xx.79xxx.xx.xx.80

参数配置调优




micro_batch_size
max_length
plugin
mixed_precision
tflops




1
512
gemini_auto
fp16
10.8231


2
512
gemini_auto
fp16
20.4685


4
512
gemini_auto
fp16
45.6885


8
512
gemini_auto
fp16
78.6885


16
512
gemini_auto
fp16
136.5975


16
512
zero2
fp16
OOM


16
512
3d
fp16
OOM


1
1024
gemini_auto
fp16
OOM


4
2048
gemini_auto
fp16
OOM


16
1024
gemini_auto
fp16
OOM




训练拉起截图


总体step1519


142，143，78，79，80的GPU显存占用







XXX平台监控

6台机器的GPU性能监控
涉及客户信息，图片略去。
单机器141的GPU性能监控
涉及客户信息，图片略去。

断点续训

7.4日晚上由于142、143、78、79、80五台机器的ssh没有设置免密登录，模型预训练过程中在主节点142保存的checkpoint文件不能及时同步给其他143、78、79、80四个节点，故导致了训练中断，现五台机器的ssh已互相免密，checkpoint信息可以通过主节点142来下发共享。
之后设置读取断点400步后保存的checkpoint与权重信息，拉起训练时会从第400步开始继续训练。
加入--load_checkpoint $PARENT_LOAD_DIR \参数，设定加载的checkpoint位置PARENT_LOAD_DIR=&quot;/opt/tmp/ColossalAI/output/save/llama2-70b-pt-2024-07-03-14-11-44/epoch-0_step-400&quot;
#!/bin/bash# NCCL IB environment variablesexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1export NCCL_IB_DISABLE=0export NCCL_SOCKET_IFNAME=bond4export NCCL_IB_GID_INDEX=3export NCCL_IB_TIMEOUT=23export NCCL_IB_RETRY_CNT=7export OMP_NUM_THREADS=8PROJECT_NAME=&quot;llama2-70b-pt&quot;PARENT_SAVE_DIR=&quot;/opt/tmp/ColossalAI/output/save/&quot;PARENT_LOAD_DIR=&quot;/opt/tmp/ColossalAI/output/save/llama2-70b-pt-2024-07-03-14-11-44/epoch-0_step-400&quot;PARENT_TENSORBOARD_DIR=&quot;/opt/tmp/ColossalAI/output/tensorboard/&quot;PARENT_CONFIG_FILE=&quot;/opt/tmp/ColossalAI/output/config/&quot;PRETRAINED_MODEL_PATH=&quot;/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf&quot;declare -a dataset=(    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008&quot;    &quot;/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009&quot;)TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)FULL_PROJECT_NAME=&quot;$&#123;PROJECT_NAME&#125;-$&#123;TIMESTAMP&#125;&quot;SAVE_DIR=&quot;$&#123;PARENT_SAVE_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;TENSORBOARD_DIR=&quot;$&#123;PARENT_TENSORBOARD_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;CONFIG_FILE=&quot;$&#123;PARENT_CONFIG_FILE&#125;$&#123;FULL_PROJECT_NAME&#125;.json&quot;colossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.142 --master_port 30015 train.py \    --pretrained $PRETRAINED_MODEL_PATH \    --dataset $&#123;dataset[@]&#125; \    --plugin &quot;gemini_auto&quot; \    --save_interval 1600 \    --save_dir $SAVE_DIR \    --load_checkpoint $PARENT_LOAD_DIR \    --tensorboard_dir $TENSORBOARD_DIR \    --config_file $CONFIG_FILE \    --num_epochs 1 \    --micro_batch_size 16 \    --lr 1e-4 \    --mixed_precision &quot;fp16&quot; \    --grad_clip 1.0 \    --weight_decay 0.01 \    --warmup_steps 100 \    --use_grad_checkpoint \    --max_length 512 \    --accumulation_steps 1 \    --pad_token &quot;unk&quot;




预训练结束

结束后会在设定的主节点142上生成对应权重文件


微调
llama2-70B 5机40卡

数据集准备

微调数据集参照Colossalai官网要求的数据对话格式进行准备
数据样式

数据集的离线切分，需要通过kubectl进入pod内执行切分脚本prepare_sft_dataset.py进行切分
#!/bin/bashpython3 prepare_sft_dataset.py \--data_input_dirs &quot;/opt/tmp/ColossalAI/applications/Colossal-LLaMA/data&quot; \--tokenizer_dir &quot;/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf&quot; \--data_output_dirs &quot;/opt/tmp/ColossalAI/applications/Colossal-LLaMA/sftdata&quot; \--max_length 4096 \--num_spliced_dataset_bins 10 \--llama_version 2
执行切分脚本后，开始进行切分，会将原始数据集格式jsonl转为arrow格式

tar -cvf sftdata.tar sftdata 压缩切分好的数据集sftdata
kubectl cp llama2-70b-test/llama2-70b-pt-f5877d7c9-n7448:/opt/tmp/ColossalAI/applications/Colossal-LLaMA/sftdata.tar /home/disk_sdb0/llama2/sftdata.tar通过kubectl cp将压缩好的sftdata.tar传输至宿主机137上
tar -xvf sftdata.tar解压
切分好的数据集为52G


启动k8s的yaml的配置文件

apiVersion: apps/v1kind: Deploymentmetadata:  namespace: llama2-70b-test  name: llama2-70b-pt # Deployment 的名称spec:  replicas: 5 # 副本数量  selector:    matchLabels:      app: llama2-70b-pt  template:    metadata:      labels:        app: llama2-70b-pt    spec:      hostNetwork: true      containers:      - name: llama2-70b-container # 容器的名称        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0        imagePullPolicy: IfNotPresent        command: [&quot;/bin/bash&quot;, &quot;-c&quot;]        args: [&quot;cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ &amp;&amp; chmod +x finetune.example.sh &amp;&amp; ./finetune.example.sh&quot;] # 启动容器后执行的命令，切换目录并运行训练脚本        resources:          limits:            nvidia.com/gpu: 8 # 限制使用 8 个 GPU，根据实际情况调整          requests:            nvidia.com/gpu: 8        volumeMounts:        - name: colossalai-sftdataset          mountPath: /opt/tmp/ColossalAI/datas/sftdata        - name: colossalai-weight          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载        - name: colossalai-train          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py          subPath: train.py          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它        - name: colossalai-output          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载        - name: host-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile          subPath: hostfile.txt          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它        - name: finetune-script          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/finetune.example.sh  # 容器内finetune.example.sh          subPath: finetune.example.sh        - name: shm-volume          mountPath: /dev/shm      nodeSelector:        model: llama2-70b      volumes:      - name: colossalai-sftdataset        hostPath:          path: /home/disk_sdb0/llama2/datas/sftdata          type: Directory      - name: colossalai-weight        hostPath:          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # 宿主机中 ColossalAI 权重的路径          type: Directory      - name: colossalai-train        hostPath:          path: /home/disk_sdb0/llama2          type: Directory      - name: colossalai-output        hostPath:          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径          type: Directory      - name: host-script        hostPath:          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径          type: Directory      - name: finetune-script        hostPath:          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径          type: Directory      - name: shm-volume        emptyDir:          medium: Memory          sizeLimit: 8Gi

微调脚本

#!/bin/bash# NCCL IB environment variablesexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1export NCCL_IB_DISABLE=0export NCCL_SOCKET_IFNAME=bond4export NCCL_IB_GID_INDEX=3export NCCL_IB_TIMEOUT=23export NCCL_IB_RETRY_CNT=7export OMP_NUM_THREADS=8#export NCCL_P2P_DISABLE=1export NCCL_DEBUG=INFOPROJECT_NAME=&quot;llama2-70b-ft&quot;PARENT_SAVE_DIR=&quot;/opt/tmp/ColossalAI/output/save/&quot;PARENT_TENSORBOARD_DIR=&quot;/opt/tmp/ColossalAI/output/tensorboard/&quot;PARENT_CONFIG_FILE=&quot;/opt/tmp/ColossalAI/output/config/&quot;PRETRAINED_MODEL_PATH=&quot;/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf&quot;declare -a dataset=(    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00000&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00001&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00002&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00003&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00004&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00005&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00006&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00007&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00008&quot;    &quot;/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00009&quot;)TIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)FULL_PROJECT_NAME=&quot;$&#123;PROJECT_NAME&#125;-$&#123;TIMESTAMP&#125;&quot;SAVE_DIR=&quot;$&#123;PARENT_SAVE_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;TENSORBOARD_DIR=&quot;$&#123;PARENT_TENSORBOARD_DIR&#125;$&#123;FULL_PROJECT_NAME&#125;&quot;CONFIG_FILE=&quot;$&#123;PARENT_CONFIG_FILE&#125;$&#123;FULL_PROJECT_NAME&#125;.json&quot;colossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.142 --master_port 30013 train.py \    --pretrained $PRETRAINED_MODEL_PATH \    --dataset $&#123;dataset[@]&#125; \    --plugin &quot;gemini&quot; \    --save_interval 400 \    --save_dir $SAVE_DIR \    --tensorboard_dir $TENSORBOARD_DIR \    --config_file $CONFIG_FILE \    --num_epochs 1 \    --accumulation_steps 1 \    --micro_batch_size 1 \    --lr 5e-5 \    --mixed_precision &quot;bf16&quot; \    --grad_clip 1.0 \    --weight_decay 0.01 \    --warmup_steps 100 \    --use_grad_checkpoint \    --use_neft \    --pad_token &quot;eos&quot;

hostfile.txt

xxx.xx.xx.142xxx.xx.xx.143xxx.xx.xx.78xxx.xx.xx.79xxx.xx.xx.80

拉起微调训练

#查看pod的详细信息kubectl get pod -n llama2-70b-test -owide#删除启动微调训练的yamlkubectl delete -f llama2-70b-finetune.yaml#启动微调训练的yamlkubectl apply -f llama2-70b-finetune.yaml#查看pod日志kubectl logs -n llama2-70b-test llama2-70b-pt-7b5884d87c-pmd76 -f#查看pod状态信息kubectl describe pod -n llama2-70b-test llama2-70b-pt-6599d788c7-7nngz


参数配置及调优




micro_batch_size
accumulation_steps
plugin
mixed_precision
tflops




1
1
gemini
bf16
38.8488


4
1
gemini
bf16
OOM


8
1
gemini
bf16
OOM


1
1
gemini_auto
bf16
OOM


8
1
gemini_auto
bf16
OOM


16
1
gemini_auto
bf16
OOM


1
1
zero2
bf16
OOM


1
1
3d
bf16
OOM


1
4
gemini
bf16
OOM


1
8
gemini
bf16
OOM




142，143，78，79，80的GPU显存占用







备注

在拉起微调训练时，尝试调整了训练参数micro_batch_size、accumulation_steps、并行策略gemini、gemini_auto、zero2等参数来尽量缩小总的迭代step步数（数据集的大小也会影响step数），但尝试增加了micro_batch_size等参数后，发现训练会拉不起来（out of memory），当前5机40卡可以成功拉起的参数只能将micro_batch_size调整为1，accumulation_steps为1、并行策略为gemini，总的迭代步数为35132步（每步迭代耗时需要3分钟左右，在每一步迭代中，首先进行前向传播来计算预测输出和损失，然后进行反向传播来计算梯度，最后根据梯度更新模型参数。每个批次都会进行一次梯度和损失的计算，直到完成一个epoch的训练。）
类似参照某模型175B（千卡规模）的拉起的训练进度，总的迭代步数为574833。

类似参照某模型57B（千卡规模）的拉起训练进度，总的迭代步数574636。

故完成整套大模型的训练会持续较长的训练周期，并在现有的卡数限制上，在短期内实现所有流程存在难度。
结论与说明
现有环境训练模型参数大小及显存占用
现有的5机40卡预训练/微调llama2-70B，模型参数及显存占用如下表格所示（注：由于xxx.xx.xx.141机器日常有业务在跑，无法空闲出完整的8块gpu资源，故最多以5机40卡来拉起训练。）




模型参数
机器
GPU卡
显存占用
GPU利用率




预训练
70B
142
0
77886MiB/81920MiB
99%





1
77898MiB/81920MiB
99%





2
77898MiB/81920MiB
99%





3
77898MiB/81920MiB
99%





4
79946MiB/81920MiB
99%





5
77898MiB/81920MiB
99%





6
77898MiB/81920MiB
99%





7
77874MiB/81920MiB
100%




143
0
77886MiB/81920MiB
100%





1
77898MiB/81920MiB
100%





2
77898MiB/81920MiB
100%





3
77898MiB/81920MiB
99%





4
77898MiB/81920MiB
99%





5
77886MiB/81920MiB
99%





6
77898MiB/81920MiB
99%





7
77886MiB/81920MiB
99%




78
0
78764MiB/81920MiB
99%





1
78776MiB/81920MiB
99%





2
78776MiB/81920MiB
99%





3
78776MiB/81920MiB
99%





4
78776MiB/81920MiB
100%





5
78776MiB/81920MiB
100%





6
78776MiB/81920MiB
100%





7
78764MiB/81920MiB
99%




79
0
77886MiB/81920MiB
99%





1
77898MiB/81920MiB
99%





2
77886MiB/81920MiB
99%





3
77898MiB/81920MiB
100%





4
77898MiB/81920MiB
99%





5
77898MiB/81920MiB
99%





6
77898MiB/81920MiB
100%





7
77886MiB/81920MiB
100%




80
0
77886MiB/81920MiB
99%





1
77898MiB/81920MiB
99%





2
79948MiB/81920MiB
99%





3
77898MiB/81920MiB
99%





4
78350MiB/81920MiB
99%





5
79386MiB/81920MiB
99%





6
79374MiB/81920MiB
100%





7
78338MiB/81920MiB
99%


微调
70B
142
0
53278MiB/81920MiB
100%





1
52160MiB/81920MiB
100%





2
53660MiB/81920MiB
100%





3
53164MiB/81920MiB
100%





4
52382MiB/81920MiB
100%





5
53260MiB/81920MiB
100%





6
53660MiB/81920MiB
100%





7
54652MiB/81920MiB
100%




143
0
53494MiB/81920MiB
100%





1
53254MiB/81920MiB
100%





2
53950MiB/81920MiB
100%





3
53172MiB/81920MiB
100%





4
67234MiB/81920MiB
100%





5
53212MiB/81920MiB
100%





6
53160MiB/81920MiB
100%





7
53604MiB/81920MiB
100%




78
0
51978MiB/81920MiB
100%





1
51992MiB/81920MiB
100%





2
51410MiB/81920MiB
100%





3
51492MiB/81920MiB
100%





4
51992MiB/81920MiB
100%





5
51488MiB/81920MiB
100%





6
52154MiB/81920MiB
100%





7
47638MiB/81920MiB
100%




79
0
48468MiB/81920MiB
100%





1
49280MiB/81920MiB
100%





2
49182MiB/81920MiB
100%





3
48980MiB/81920MiB
100%





4
49160MiB/81920MiB
100%





5
49966MiB/81920MiB
100%





6
49160MiB/81920MiB
100%





7
49858MiB/81920MiB
100%




80
0
49366MiB/81920MiB
100%





1
48740MiB/81920MiB
100%





2
49400MiB/81920MiB
99%





3
47470MiB/81920MiB
99%





4
36662MiB/81920MiB
100%





5
37262MiB/81920MiB
100%





6
37660MiB/81920MiB
99%





7
37148MiB/81920MiB
100%



AI平台训练可视化分析能力
4.1中描述的关于GPU的显存使用及利用率情况，主要是通过nvidia-smiNVIDIA提供的命令行工具监控分析GPU设备信息，其中的信息包括

GPU 利用率（GPU-Util）：显示 GPU 核心的利用率百分比。
显存使用情况（Memory-Usage）：显示 GPU 当前显存的使用量/总显存量。
功耗（Pwr:Usage/Cap）：显示 GPU 的当前/最大功耗。
温度（GPU Temperature）：显示 GPU 的当前温度。
进程 ID（PID）：显示当前使用 GPU 的进程的进程 ID。
进程名称：显示使用 GPU 的进程的名称。
使用 GPU 的应用程序：显示哪些应用程序正在使用 GPU。
GPU 型号：显示 GPU 的型号和版本信息。
驱动版本：显示当前安装的 NVIDIA 驱动版本。
CUDA 版本：显示当前安装的 CUDA 版本。
ECC 状态：显示错误校正码（ECC）的状态。

对比XXX监控平台的数据，两者在GPU显存使用和GPU利用率上可能会存在差异，由于nvidia-smi是实时监控当前时间下的GPU信息，XXX监控平台的信息与nvidia-smi监控信息存在时间差，故存在某些数据不同步，这也在合理的范围内。
XXX监控平台包含了各机器的单个GPU的显存使用、GPU利用率、温度、功耗等信息，基本覆盖了模型训练监控所需的一些指标。此外，XXX中还显示了GPU的时钟频率，也代表了GPU性能的关键指标。
XXX监控平台可以进一步监控机间网卡的通信带宽、机内单向通信带宽、Leaf层/spine层交换机信息、模型训练参数等等大模型训练的物理层与软件层信息。
用户的易用性,长时间训练,任务断点,监控告警,自动化拉起

本次训练采用k8s的方式来管理和拉起集群pod节点，其中涉及到一些拉起k8s的yaml配置文件、以及集群机器的ssh通信配置、etc/hosts主机ip名映射配置、标签管理等步骤，可能需要用户熟悉并操作。此外，机器的驱动环境可以按期更新迭代，以此适应和匹配最新系列大模型训练所需要的新特性。
本次预训练任务长达70h+，其中存在训练中断问题，当前的方法是通过手动拉起和利用训练脚本中save_interval的间隔步数保存的checkpoint信息来恢复至间隔步数的训练状态。
训练的监控告警可以通过kubectl命令查看podkubectl describe pod、nodekubectl describe node、训练日志kubectl logs里的错误信息进行排错处理。
自动化拉起可能需要EKI（k8s管理）平台与PyTorchJob 支持，EKI可以自动化处理pod节点异常并自动拉起新的节点。

]]></content>
      <categories>
        <category>模型训练调优</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>模型训练调优</tag>
        <tag>Nvidia</tag>
        <tag>Llama2</tag>
        <tag>A100</tag>
        <tag>Colossal-AI</tag>
      </tags>
  </entry>
  <entry>
    <title>1.1-Qwen模型本地部署与单机单卡/多卡训练</title>
    <url>/2025/11/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Qwen/1.1-Qwen%E6%A8%A1%E5%9E%8B%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2%E4%B8%8E%E5%8D%95%E6%9C%BA%E5%8D%95%E5%8D%A1_%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[一、V100环境部署

项目地址： https://github.com/QwenLM/Qwen
下载到本地
git clone  https://github.com/QwenLM/Qwen.git 
基础环境搭建
conda create -n qwen python=3.10
conda activate qwen
安装pytorch
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
安装依赖环境
pip install -r requirements.txt 
7B模型下载
git clone [https://www.modelscope.cn/qwen/Qwen-7B-Chat.git](https://www.modelscope.cn/qwen/Qwen-7B-Chat.git)
在使用sdk的python脚本下载权重时，需要pip安装modelscope
pip install modelscope
使用git clone发现权重未下载成功，使用modelscope官方sdk脚本下载。
将以下代码写入download.py文件中，并执行python download.py

#模型下载from modelscope import snapshot_downloadmodel_dir = snapshot_download(&#x27;qwen/Qwen-7B-Chat&#x27;, cache_dir=&#x27;/opt/tmp/Qwen&#x27;, revision=&#x27;v1.1.9&#x27;)

数据集下载
进入modelscope下载数据集
https://modelscope.cn/datasets/Robin021/DISC-Law-SFT/files
数据格式处理
数据集处理后会生成train_data_law.json文件
head -n 20 train_data_law.json
处理之后的数据格式如下：


import json# 读取以.jsonl结尾的文件json_data = []with open(&#x27;/opt/tmp/Qwen/dataset/DISC-Law-SFT-Triplet-released.jsonl&#x27;, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as file:    for line in file:        data = json.loads(line)        json_data.append(data)# 待填入的模板template = []# 遍历json数据集for idx, data in enumerate(json_data[:]):    conversation = [        &#123;            &quot;from&quot;:&quot;user&quot;,            &quot;value&quot;: data[&quot;input&quot;]        &#125;,        &#123;            &quot;from&quot;: &quot;assistant&quot;,            &quot;value&quot;: data[&quot;output&quot;]        &#125;    ]    template.append(&#123;        &quot;id&quot;: f&quot;identity_&#123;idx&#125;&quot;,        &quot;conversations&quot;: conversation    &#125;)print(len(template))# 输出填充数据后的模板print(json.dumps(template[2], ensure_ascii=False, indent=2))# 将template写入到本地文件output_file_path = &quot;/opt/tmp/Qwen/train_data_law.json&quot;with open(output_file_path, &#x27;w&#x27;, encoding=&#x27;utf-8&#x27;) as f:    json.dump(template, f, ensure_ascii=False, indent=2)print(f&quot;处理好的数据已写入到本地文件: &#123;output_file_path&#125;&quot;)

训练依赖安装
deepspeed安装
pip install &quot;peft&lt;0.8.0&quot; deepspeed
修改模型微调脚本参数
修改MODEL和DATA的路径，及per_device_train_batch_size

#!/bin/bashexport CUDA_DEVICE_MAX_CONNECTIONS=1MODEL=&quot;/opt/tmp/Qwen/Qwen-7B-Chat&quot; # Set the path if you do not want to load from huggingface directly# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.# See the section for finetuning in README for more information.DATA=&quot;/opt/tmp/Qwen/train_data_law.json&quot;function usage() &#123;    echo &#x27;Usage: bash finetune/finetune_lora_single_gpu.sh [-m MODEL_PATH] [-d DATA_PATH]&#x27;&#125;while [[ &quot;$1&quot; != &quot;&quot; ]]; do    case $1 in        -m | --model )            shift            MODEL=$1            ;;        -d | --data )            shift            DATA=$1            ;;        -h | --help )            usage            exit 0            ;;        * )            echo &quot;Unknown argument $&#123;1&#125;&quot;            exit 1            ;;    esac    shiftdoneexport CUDA_VISIBLE_DEVICES=0python finetune.py \  --model_name_or_path $MODEL \  --data_path $DATA \  --bf16 False \  --output_dir output_qwen \  --num_train_epochs 5 \  --per_device_train_batch_size 2 \  --per_device_eval_batch_size 1 \  --gradient_accumulation_steps 8 \  --evaluation_strategy &quot;no&quot; \  --save_strategy &quot;steps&quot; \  --save_steps 100 \  --save_total_limit 10 \  --learning_rate 3e-4 \  --weight_decay 0.1 \  --adam_beta2 0.95 \  --warmup_ratio 0.01 \  --lr_scheduler_type &quot;cosine&quot; \  --logging_steps 1 \  --report_to &quot;none&quot; \  --model_max_length 512 \  --lazy_preprocess True \  --gradient_checkpointing \  --use_lora# If you use fp16 instead of bf16, you should use deepspeed# --fp16 True --deepspeed finetune/ds_config_zero2.json

开启单机单卡训练
bash finetune/finetune_lora_single_gpu.sh

单机多卡训练（在分配的3号机器上执行）

gpu显存利用情况


]]></content>
      <categories>
        <category>模型训练调优</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>模型训练调优</tag>
        <tag>Nvidia</tag>
        <tag>Qwen</tag>
        <tag>Deepspeed</tag>
      </tags>
  </entry>
  <entry>
    <title>1.3-基于docker的Qwen单机单卡_多卡训练</title>
    <url>/2025/11/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Qwen/1.3-%E5%9F%BA%E4%BA%8Edocker%E7%9A%84Qwen%E5%8D%95%E6%9C%BA%E5%8D%95%E5%8D%A1_%E5%A4%9A%E5%8D%A1%E8%AE%AD%E7%BB%83/</url>
    <content><![CDATA[1. 查看容器
docker ps
2. exec进入容器
docker exec -it containerid bash
3. 进入qwen目录并修改finetune_lora_single_gpu.sh参数
#!/bin/bashexport CUDA_DEVICE_MAX_CONNECTIONS=1MODEL=&quot;/qwen/Qwen-7B-Chat&quot; # Set the path if you do not want to load from huggingface directly# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.# See the section for finetuning in README for more information.DATA=&quot;/qwen/train_data_law.json&quot;function usage() &#123;    echo &#x27;Usage: bash finetune/finetune_lora_single_gpu.sh [-m MODEL_PATH] [-d DATA_PATH]&#x27;&#125;while [[ &quot;$1&quot; != &quot;&quot; ]]; do    case $1 in        -m | --model )            shift            MODEL=$1            ;;        -d | --data )            shift            DATA=$1            ;;        -h | --help )            usage            exit 0            ;;        * )            echo &quot;Unknown argument $&#123;1&#125;&quot;            exit 1            ;;    esac    shiftdoneexport CUDA_VISIBLE_DEVICES=0python finetune.py \  --model_name_or_path $MODEL \  --data_path $DATA \  --bf16 True \  --output_dir output_qwen \  --num_train_epochs 5 \  --per_device_train_batch_size 1 \  --per_device_eval_batch_size 1 \  --gradient_accumulation_steps 8 \  --evaluation_strategy &quot;no&quot; \  --save_strategy &quot;steps&quot; \  --save_steps 100 \  --save_total_limit 10 \  --learning_rate 3e-4 \  --weight_decay 0.1 \  --adam_beta2 0.95 \  --warmup_ratio 0.01 \  --lr_scheduler_type &quot;cosine&quot; \  --logging_steps 1 \  --report_to &quot;none&quot; \  --model_max_length 512 \  --lazy_preprocess True \  --gradient_checkpointing \  --use_lora# If you use fp16 instead of bf16, you should use deepspeed# --fp16 True --deepspeed finetune/ds_config_zero2.json
4. 执行finetune_lora_single_gpu.sh单机单卡
5. 修改finetune_lora_ds.sh参数
#!/bin/bashexport CUDA_DEVICE_MAX_CONNECTIONS=1DIR=`pwd`# Guide:# This script supports distributed training on multi-gpu workers (as well as single-worker training).# Please set the options below according to the comments.# For multi-gpu workers training, these options should be manually set for each worker.# After setting the options, please run the script on each worker.# Number of GPUs per GPU workerGPUS_PER_NODE=$(python -c &#x27;import torch; print(torch.cuda.device_count())&#x27;)# Number of GPU workers, for single-worker training, please set to 1NNODES=$&#123;NNODES:-1&#125;# The rank of this worker, should be in &#123;0, ..., WORKER_CNT-1&#125;, for single-worker training, please set to 0NODE_RANK=$&#123;NODE_RANK:-0&#125;# The ip address of the rank-0 worker, for single-worker training, please set to localhostMASTER_ADDR=$&#123;MASTER_ADDR:-localhost&#125;# The port for communicationMASTER_PORT=$&#123;MASTER_PORT:-6001&#125;MODEL=&quot;/qwen/Qwen-7B-Chat&quot; # Set the path if you do not want to load from huggingface directly# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.# See the section for finetuning in README for more information.DATA=&quot;/qwen/train_data_law.json&quot;DS_CONFIG_PATH=&quot;finetune/ds_config_zero2.json&quot;function usage() &#123;    echo &#x27;Usage: bash finetune/finetune_lora_ds.sh [-m MODEL_PATH] [-d DATA_PATH] [--deepspeed DS_CONFIG_PATH]&#x27;&#125;while [[ &quot;$1&quot; != &quot;&quot; ]]; do    case $1 in        -m | --model )            shift            MODEL=$1            ;;        -d | --data )            shift            DATA=$1            ;;        --deepspeed )            shift            DS_CONFIG_PATH=$1            ;;        -h | --help )            usage            exit 0            ;;        * )            echo &quot;Unknown argument $&#123;1&#125;&quot;            exit 1            ;;    esac    shiftdoneDISTRIBUTED_ARGS=&quot;    --nproc_per_node $GPUS_PER_NODE \    --nnodes $NNODES \    --node_rank $NODE_RANK \    --master_addr $MASTER_ADDR \    --master_port $MASTER_PORT&quot;#export CUDA_VISIBLE_DEVICES=2,3torchrun $DISTRIBUTED_ARGS finetune.py \    --model_name_or_path $MODEL \    --data_path $DATA \    --bf16 False \    --output_dir output_qwen \    --num_train_epochs 5 \    --per_device_train_batch_size 2 \    --per_device_eval_batch_size 1 \    --gradient_accumulation_steps 8 \    --evaluation_strategy &quot;no&quot; \    --save_strategy &quot;steps&quot; \    --save_steps 100 \    --save_total_limit 10 \    --learning_rate 3e-4 \    --weight_decay 0.1 \    --adam_beta2 0.95 \    --warmup_ratio 0.01 \    --lr_scheduler_type &quot;cosine&quot; \    --logging_steps 1 \    --report_to &quot;none&quot; \    --model_max_length 512 \    --lazy_preprocess True \    --use_lora \    --gradient_checkpointing \    --deepspeed $&#123;DS_CONFIG_PATH&#125;
6. 修改deepspeed中ds_config_zero2.json配置文件，增加TFlops显示
其中具体增加的参数配置为flops_profiler
&quot;flops_profiler&quot;: &#123;        &quot;enabled&quot;: true,        &quot;profile_step&quot;: 1,        &quot;module_depth&quot;: -1,        &quot;top_modules&quot;: 1,        &quot;detailed&quot;: false,        &quot;output_file&quot;: null    &#125;,
&#123;    &quot;fp16&quot;: &#123;        &quot;enabled&quot;: &quot;auto&quot;,        &quot;loss_scale&quot;: 0,        &quot;loss_scale_window&quot;: 1000,        &quot;initial_scale_power&quot;: 16,        &quot;hysteresis&quot;: 2,        &quot;min_loss_scale&quot;: 1    &#125;,    &quot;bf16&quot;: &#123;        &quot;enabled&quot;: &quot;auto&quot;    &#125;,    &quot;optimizer&quot;: &#123;        &quot;type&quot;: &quot;AdamW&quot;,        &quot;params&quot;: &#123;            &quot;lr&quot;: &quot;auto&quot;,            &quot;betas&quot;: &quot;auto&quot;,            &quot;eps&quot;: &quot;auto&quot;,            &quot;weight_decay&quot;: &quot;auto&quot;        &#125;    &#125;,    &quot;scheduler&quot;: &#123;        &quot;type&quot;: &quot;WarmupLR&quot;,        &quot;params&quot;: &#123;            &quot;warmup_min_lr&quot;: &quot;auto&quot;,            &quot;warmup_max_lr&quot;: &quot;auto&quot;,            &quot;warmup_num_steps&quot;: &quot;auto&quot;        &#125;    &#125;,    &quot;zero_optimization&quot;: &#123;        &quot;stage&quot;: 2,        &quot;offload_optimizer&quot;: &#123;            &quot;device&quot;: &quot;none&quot;,            &quot;pin_memory&quot;: true        &#125;,        &quot;allgather_partitions&quot;: true,        &quot;allgather_bucket_size&quot;: 2e8,        &quot;overlap_comm&quot;: true,        &quot;reduce_scatter&quot;: true,        &quot;reduce_bucket_size&quot;: 2e8,        &quot;contiguous_gradients&quot;: true    &#125;,    &quot;flops_profiler&quot;: &#123;        &quot;enabled&quot;: true,        &quot;profile_step&quot;: 1,        &quot;module_depth&quot;: -1,        &quot;top_modules&quot;: 1,        &quot;detailed&quot;: false,        &quot;output_file&quot;: null    &#125;,    &quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,    &quot;gradient_clipping&quot;: &quot;auto&quot;,    &quot;steps_per_print&quot;: 100,    &quot;train_batch_size&quot;: &quot;auto&quot;,    &quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;,    &quot;wall_clock_breakdown&quot;: false&#125;
7. 执行finetune_lora_ds.sh单机多卡训练
其中报了Error while creating shared memory segment /dev/shm/nccl-KXWrmA (size 9637888)导致在docker中单机多卡拉起失败
问题原因：docker的shm共享内存不足，可以通过命令
df -h | grep shm查看当前容器的shm大小，默认为64M，这是远远不够的，所以要增加该容器的shm共享内存大小。参考博文https://blog.csdn.net/gg864461719/article/details/112466585
解决方法1：创建完容器之后，手动修改shm共享内存大小
a. 首先要关闭docker, 否则下面的操作步骤会无效.
service docker stop
b. 进入宿主机中/docker/containers/容器id 修改该容器的hostconfig.json文件，把其中的ShmSize的大小后面增加22（就变为了6.3G）其默认的是67108864_KB_ 就约等于64M。
c. 重启docker服务
systemctl start docker
d. 解决完之后，重新查看shm的共享内存大小，此时已经变为了6.3G
解决方法2：在通过镜像run容器时，就直接指定–shm-size 6G
docker run -it --name qwen --gpus all --shm-size 6G registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0 bash
重新拉起训练
bash finetune/finetune_lora_ds.sh
其中tflops在9.5~10.5之间



]]></content>
      <categories>
        <category>模型训练调优</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>模型训练调优</tag>
        <tag>Nvidia</tag>
        <tag>Qwen</tag>
        <tag>Deepspeed</tag>
      </tags>
  </entry>
  <entry>
    <title>2.2 Deepseek-7B预训练适配</title>
    <url>/2025/11/13/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.2%20Deepseek-7B%E9%A2%84%E8%AE%AD%E7%BB%83%E9%80%82%E9%85%8D/</url>
    <content><![CDATA[预训练物料及代码准备
Deepseek-7B的Tokenizer下载
位置位于/data/code/temp/deepseek/deepseek-ckpt

数据集准备
位置位于/data/code/temp/deepseek/deepseek-datasets

pretrain_mcore_deepseek.sh预训练脚本
export NCCL_P2P_DISABLE=1export NCCL_DEBUG=INFOexport NCCL_SOCKET_IFNAME=eth0export GLOO_SOCKET_IFNAME=eth0export CUDA_DEVICE_MAX_CONNECTIONS=1export CUDA_VISIBLE_DEVICES=0,1,2,3MEGATRON_PATCH_PATH=/data/code MEGATRON_PATH=$&#123;MEGATRON_PATCH_PATH&#125;/Megatron-LMexport PYTHONPATH=$&#123;MEGATRON_PATH&#125;:$&#123;MEGATRON_PATCH_PATH&#125;:$PYTHONPATHNNODES=2NODE_RANK=0GPUS_PER_NODE=4MASTER_ADDR=192.168.0.22MASTER_PORT=29500DISTRIBUTED_ARGS=&quot;--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT&quot;MODEL_SIZE=7BBATCH_SIZE=1GLOBAL_BATCH_SIZE=8LR=1e-5MIN_LR=1e-6SEQ_LEN=1024PAD_LEN=1024EXTRA_VOCAB_SIZE=0PR=fp16TP=4PP=1AC=selDO=trueFL=falseSP=trueTE=trueMOE=falseSAVE_INTERVAL=1000DATASET_PATH=/data/code/temp/deepseek/deepseek-datasets/mmap_deepseek_datasets_text_document#PRETRAIN_CHECKPOINT_PATH=noneTOKENIZER_PATH=/data/code/temp/deepseek/deepseek-ckptTRAIN_TOKENS=10000000WARMUP_TOKENS=0OUTPUT_BASEPATH=/data/code/temp/output/deepseek/pretrain_outputif [ $MODEL_SIZE = 7B ]; thenNUM_LAYERS=32HIDDEN_SIZE=4096NUM_ATTN_HEADS=32INTERMEDIATE_SIZE=11008MAX_POSITION_EMBEDDINGS=4096gqa_options=&quot;&quot;fiif [ $AC = full ]; then    activation_checkpoint_options=&quot; \		    --recompute-method uniform \		    --recompute-granularity full&quot;elif [ $AC = sel ]; then    activation_checkpoint_options=&quot; \        --recompute-activations&quot;elif [ $AC = none ]; then    activation_checkpoint_options=&quot; \    &quot;fiif [ $PR = fp16 ]; then    pr_options=&quot; \		    --fp16 \            --apply-query-key-layer-scaling&quot;    export NVTE_APPLY_QK_LAYER_SCALING=1elif [ $PR = bf16 ]; then    pr_options=&quot; \        --bf16&quot;elif [ $PR = fp8 ]; then    pr_options=&quot; \        --bf16 \        --fp8-hybrid \        --fp8-amax-compute-algo max \        --fp8-amax-history-len 1024 \        --transformer-impl transformer_engine&quot;fiif [ $DO = true ]; then    do_options=&quot; \		    --use-distributed-optimizer&quot;elif [ $DO = false ]; then    do_options=&quot; \                    &quot;fiif [ $FL = true ]; then    flash_options=&quot; \		    --use-flash-attn&quot;elif [ $FL = false ]; then    flash_options=&quot; \                    &quot;fiif [ $TE = true ]; then    te_options=&quot; \		    --transformer-impl transformer_engine&quot;elif [ $TE = false ]; then    te_options=&quot; \                    &quot;fiif [ $MOE = true ]; then    moe_options=&quot; \		    --moe-router-topk 1 \		    --num-experts 4 \		    --expert-model-parallel-size 2&quot;elif [ $MOE = false ]; then    moe_options=&quot; \                    &quot;fiif [ $SP = true ] &amp;&amp; [ $TP -gt 1 ]; then    sp_options=&quot; \		    --sequence-parallel&quot;elif [ $SP = false ]; then    sp_options=&quot; \                    &quot;fiif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then    load_options=&quot; \            --load $PRETRAIN_CHECKPOINT_PATH&quot;fiTRAIN_ITERS=$(( $&#123;TRAIN_TOKENS&#125; / $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))LR_WARMUP_ITERS=$(( $&#123;WARMUP_TOKENS&#125;  / $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))LR_DECAY_ITERS=$(( $&#123;TRAIN_TOKENS&#125; /  $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))NAME=&quot;$&#123;ENV&#125;-pretrain-mcore-deepseek-$&#123;MODEL_SIZE&#125;-lr-$&#123;LR&#125;-bs-$&#123;BATCH_SIZE&#125;-seqlen-$&#123;SEQ_LEN&#125;-pr-$&#123;PR&#125;-tp-$&#123;TP&#125;-pp-$&#123;PP&#125;-ac-$&#123;AC&#125;-do-$&#123;DO&#125;-sp-$&#123;SP&#125;-moe-$&#123;MOE&#125;-tt-$&#123;TRAIN_TOKENS&#125;-wt-$&#123;WARMUP_TOKENS&#125;&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/log/&quot;current_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)TENSORBOARD_DIR=&quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/$&#123;NAME&#125;_$&#123;current_time&#125;&quot;mkdir -p $&#123;TENSORBOARD_DIR&#125;SAVED_PRETRAIN_CHECKPOINT_PATH=&quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/$&#123;NAME&#125;&quot;megatron_options=&quot;  \        --tokenizer-model $TOKENIZER_PATH \        --save $&#123;SAVED_PRETRAIN_CHECKPOINT_PATH&#125; \        --data-path $&#123;DATASET_PATH&#125; \        --lr $&#123;LR&#125; \        --min-lr $&#123;MIN_LR&#125; \        --lr-decay-style linear \        --adam-beta1 0.9 \        --adam-beta2 0.95 \        --weight-decay 0.1 \        --clip-grad 1.0 \        --init-method-std 0.006 \        --attention-dropout 0.0 \        --hidden-dropout 0.0  \        --lr-decay-iters $&#123;LR_DECAY_ITERS&#125; \        --lr-warmup-iters $&#123;LR_WARMUP_ITERS&#125; \        --train-iters $&#123;TRAIN_ITERS&#125; \        --split 99,1,0 \        --micro-batch-size $&#123;BATCH_SIZE&#125; \        --global-batch-size $&#123;GLOBAL_BATCH_SIZE&#125; \        --num-layers $&#123;NUM_LAYERS&#125; \        --hidden-size $&#123;HIDDEN_SIZE&#125; \        --num-attention-heads $&#123;NUM_ATTN_HEADS&#125; \        --ffn-hidden-size $&#123;INTERMEDIATE_SIZE&#125; \        --seq-length $&#123;SEQ_LEN&#125; \        --max-position-embeddings $&#123;MAX_POSITION_EMBEDDINGS&#125; \        --max-padding-length $&#123;PAD_LEN&#125; \        --log-throughput \        --log-interval 1 \        --eval-interval 10000 \        --eval-iters 10 \        --save-interval $&#123;SAVE_INTERVAL&#125; \        --tensorboard-queue-size 1 \        --tensorboard-dir $&#123;TENSORBOARD_DIR&#125; \        --log-timers-to-tensorboard \        --log-batch-size-to-tensorboard \        --log-validation-ppl-to-tensorboard \        --tensor-model-parallel-size $&#123;TP&#125; \        --pipeline-model-parallel-size $&#123;PP&#125; \        --no-load-optim \        --no-load-rng \        --num-workers 8 \        --seed 1234 \        --extra-vocab-size $&#123;EXTRA_VOCAB_SIZE&#125; \        --patch-tokenizer-type DeepseekTokenizer \        --dataset LLama-Pretrain-Idxmap \        --swiglu \        --normalization RMSNorm \        --norm-epsilon 1e-05 \        --use-rotary-position-embeddings \        --no-rope-fusion \        --position-embedding-type rope \        --untie-embeddings-and-output-weights \        --disable-bias-linear \        --rotary-base 10000 \        --use-mcore-models         &quot;LOG_SAVE=&quot;/data/code/temp/output/deepseek/pretrain_output/log/&quot;$&#123;current_time&#125;-deepseek-$&#123;NODE_RANK&#125;.log&quot;&quot; run_cmd=&quot;torchrun $DISTRIBUTED_ARGS pretrain_mcore_deepseek.py $&#123;megatron_options&#125; $&#123;pr_options&#125; $&#123;load_options&#125; $&#123;te_options&#125; $&#123;activation_checkpoint_options&#125; $&#123;do_options&#125; $&#123;flash_options&#125; $&#123;sp_options&#125; $&#123;gqa_options&#125; $&#123;moe_options&#125; &gt; $&#123;LOG_SAVE&#125; 2&gt;&amp;1&quot;echo $&#123;run_cmd&#125;eval $&#123;run_cmd&#125;set +x
pretrain_mcore_deepseek.py训练代码
# Copyright (c) 2023 Alibaba PAI and Nvidia Megatron-LM Team.## Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);# you may not use this file except in compliance with the License.# You may obtain a copy of the License at##     http://www.apache.org/licenses/LICENSE-2.0## Unless required by applicable law or agreed to in writing, software# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.# See the License for the specific language governing permissions and# limitations under the License.import osimport torchfrom torch import Tensorfrom functools import partialfrom typing import Unionfrom megatron import get_argsfrom megatron import get_timersfrom megatron.core import mpu, tensor_parallelfrom megatron.core.enums import ModelTypeimport megatron.modelfrom megatron.utils import (    get_batch_on_this_tp_rank,    get_batch_on_this_cp_rank,    average_losses_across_data_parallel_group)from megatron.core.datasets.blended_megatron_dataset_builder import BlendedMegatronDatasetBuilderfrom megatron.training import pretrainfrom megatron.core.datasets.gpt_dataset import GPTDatasetConfigfrom megatron.core.datasets.gpt_dataset import GPTDataset# from megatron.core.models.gpt import GPTModelfrom megatron.core.transformer.spec_utils import import_modulefrom megatron.arguments import core_transformer_config_from_argsfrom megatron_patch.model.deepseek_core.model import GPTModelfrom megatron_patch.data import build_pretrain_dataset_from_originalfrom megatron_patch.data.utils import get_batch_on_this_tp_rank_originalfrom megatron_patch.tokenizer import get_tokenizer, build_tokenizerfrom megatron_patch.arguments import get_patch_argsfrom megatron_patch.model.deepseek_core.transformer_config import DeepseekTransformerConfigfrom megatron_patch.model.deepseek_core.layer_specs import get_gpt_layer_with_transformer_engine_specimport torch._dynamotorch._dynamo.config.suppress_errors = Truedef model_provider(    pre_process=True, post_process=True) -&gt; Union[GPTModel, megatron.model.GPTModel]:    &quot;&quot;&quot;Builds the model.    If you set the use_mcore_models to True, it will return the mcore GPT model and if not the legacy GPT model.    Args:        pre_process (bool, optional): Set to true if you need to compute embedings. Defaults to True.        post_process (bool, optional): Set to true if you need to want to compute output logits/loss. Defaults to True.    Returns:        Union[GPTModel, megatron.model.GPTModel]: The returned model    &quot;&quot;&quot;    args = get_args()    build_tokenizer(args)    config = core_transformer_config_from_args(args, DeepseekTransformerConfig)    #config = core_transformer_config_from_args(get_args())    if args.use_mcore_models:        if args.spec is not None:            transformer_layer_spec = import_module(args.spec)        else:            transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.num_experts, args.moe_grouped_gemm)        model = GPTModel(            config=config,            transformer_layer_spec=transformer_layer_spec,            vocab_size=args.padded_vocab_size,            max_sequence_length=args.max_position_embeddings,            pre_process=pre_process,            post_process=post_process,            fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,            parallel_output=True,            share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights,            position_embedding_type=args.position_embedding_type,            rotary_percent=args.rotary_percent,            rotary_base=args.rotary_base,        )    return modeldef get_batch(data_iterator):    &quot;&quot;&quot;Generate a batch.&quot;&quot;&quot;    # TODO: this is pretty hacky, find a better way    if (not mpu.is_pipeline_first_stage()) and (not mpu.is_pipeline_last_stage()):        return None, None, None, None, None    args = get_args()    if &quot;-Raw&quot; in args.dataset:        # get batches based on the TP rank you are on        batch = get_batch_on_this_tp_rank_original(data_iterator)        # slice batch along sequence dimension for context parallelism        batch = get_batch_on_this_cp_rank(batch)    elif &quot;-Idxmap&quot; in args.dataset:        # get batches based on the TP rank you are on        batch = get_batch_on_this_tp_rank(data_iterator)        # slice batch along sequence dimension for context parallelism        batch = get_batch_on_this_cp_rank(batch)    else:        raise ValueError(&quot;please set correct --dataset &quot;)    return batch.values()def loss_func(loss_mask: Tensor, output_tensor: Tensor):    &quot;&quot;&quot;Loss function.    Args:        loss_mask (Tensor): Used to mask out some portions of the loss        output_tensor (Tensor): The tensor with the losses    &quot;&quot;&quot;    args = get_args()    losses = output_tensor.float()    loss_mask = loss_mask.view(-1).float()    if args.context_parallel_size &gt; 1:        loss = torch.cat([torch.sum(losses.view(-1) * loss_mask).view(1), loss_mask.sum().view(1)])        torch.distributed.all_reduce(loss, group=mpu.get_context_parallel_group())        loss = loss[0] / loss[1]    else:        loss = torch.sum(losses.view(-1) * loss_mask) / loss_mask.sum()    # Check individual rank losses are not NaN prior to DP all-reduce.    if args.check_for_nan_in_loss_and_grad:        global_rank = torch.distributed.get_rank()        assert not loss.isnan(), (            f&#x27;Rank &#123;global_rank&#125;: found NaN in local forward loss calculation. &#x27;            f&#x27;Device: &#123;torch.cuda.current_device()&#125;, node: &#123;os.uname()[1]&#125;&#x27;        )    # Reduce loss for logging.    averaged_loss = average_losses_across_data_parallel_group([loss])    return loss * args.context_parallel_size, &#123;&#x27;lm loss&#x27;: averaged_loss[0]&#125;def forward_step(data_iterator, model):    &quot;&quot;&quot;Forward step.&quot;&quot;&quot;    timers = get_timers()    # Get the batch.    timers(&#x27;batch-generator&#x27;, log_level=2).start()    tokens, labels, loss_mask, attention_mask, position_ids = get_batch(        data_iterator)    timers(&#x27;batch-generator&#x27;).stop()    output_tensor = model(tokens, position_ids, attention_mask,                          labels=labels)    return output_tensor, partial(loss_func, loss_mask)def is_dataset_built_on_rank():    return (mpu.is_pipeline_first_stage() or mpu.is_pipeline_last_stage()) and mpu.get_tensor_model_parallel_rank() == 0def core_gpt_dataset_config_from_args(args):    tokenizer = get_tokenizer()    return GPTDatasetConfig(        is_built_on_rank=is_dataset_built_on_rank,        random_seed=args.seed,        sequence_length=args.seq_length,        blend=args.data_path,        split=args.split,        path_to_cache=args.data_cache_path,        reset_attention_mask=args.reset_attention_mask,        eod_mask_loss=args.eod_mask_loss,        eod_id=tokenizer.eod    )def train_valid_test_datasets_provider(train_val_test_num_samples):    &quot;&quot;&quot;Build train, valid, and test datasets.&quot;&quot;&quot;    args = get_args()    if &quot;-Raw&quot; in args.dataset:                train_ds, valid_ds, test_ds = \                                    build_pretrain_dataset_from_original(args.dataset)    else:        train_ds, valid_ds, test_ds = BlendedMegatronDatasetBuilder(            GPTDataset,            train_val_test_num_samples,            core_gpt_dataset_config_from_args(args)        ).build()    return train_ds, valid_ds, test_dsif __name__ == &quot;__main__&quot;:    train_valid_test_datasets_provider.is_distributed = True    pretrain(        train_valid_test_datasets_provider,        model_provider,        ModelType.encoder_or_decoder,        forward_step,        extra_args_provider=get_patch_args,    )
在/data/code/megatron_patch/model在加入deepseek_core的相关配置

DeepseekTokenizer配置
在/data/code/megatron_patch/tokenizer/init.py下加入DeepseekTokenizer配置
elif args.patch_tokenizer_type == &#x27;DeepseekTokenizer&#x27;:        from megatron.core.datasets.megatron_tokenizer import MegatronTokenizer        class _DeepseekTokenizer(MegatronTokenizer):            def __init__(self, tokenizer_path, extra_vocab_size):                super().__init__(tokenizer_path)                self.tokenizer = AutoTokenizer.from_pretrained(                    tokenizer_path,                    padding_side=&quot;right&quot;,                    use_fast=False,                    trust_remote_code=True,                )                self.extra_vocab_size = extra_vocab_size            def __call__(self, text, return_tensors=None,                         padding=None, max_length=None, truncation=None, add_special_tokens=True):                return self.tokenizer(text, return_tensors=return_tensors, padding=padding,                        max_length=max_length, truncation=truncation, add_special_tokens=add_special_tokens)            @property            def vocab_size(self):                return self.tokenizer.vocab_size + self.extra_vocab_size            @property            def vocab(self):                return self.tokenizer.encoder            @property            def inv_vocab(self):                return self.tokenizer.decoder            def tokenize(self, text):                return self.tokenizer.encode(text)            def detokenize(self, token_ids):                return self.tokenizer.decode(token_ids)            @property            def eod(self):                return self.tokenizer.eos_token_id            @property            def eos_token(self):                return self.tokenizer.eos_token            @property            def pad_token_id(self):                return self.tokenizer.pad_token_id            @property            def eos_token_id(self):                return self.tokenizer.eos_token_id        tokenizer = _DeepseekTokenizer(args.tokenizer_model, args.extra_vocab_size)        args.padded_vocab_size = tokenizer.vocab_size
启动预训练k8s的yaml
apiVersion: apps/v1kind: DaemonSetmetadata:  name: deepseek-core  namespace: deepseekspec:  selector:    matchLabels:      app: deepseek-core  template:    metadata:      labels:        app: deepseek-core    spec:      hostNetwork: true      nodeSelector:        deepseek: deepseek-7B        containers:      - name: deepseek        image: registry.paas/cmss/nemo:24.05_v1.1        imagePullPolicy: IfNotPresent        resources:         limits:           nvidia.com/gpu: 4         requests:           nvidia.com/gpu: 4        command:              - &quot;/bin/bash&quot;              - &quot;-c&quot;              - |                cd /data/code/temp/conf/deepseek_core/pretrain &amp;&amp;                bash setRank.sh &amp;&amp;                cd /data/ &amp;&amp;                bash pretrain_mcore_deepseek.sh        securityContext:          privileged: true        volumeMounts:        - mountPath: /data/code          name: code           - mountPath: /etc/localtime          name: localtime        - mountPath: /dev/shm          name: dshm      volumes:      - name: code        hostPath:          path: /mnt/users/lihai/distribute/core/megatron-core      - name: localtime        hostPath:          path: /etc/localtime      - name: dshm        emptyDir:          medium: Memory          sizeLimit: 20G
setRank.sh
#!/bin/bashorigin_shell=&quot;/data/code/examples/deepseek_core/pretrain_mcore_deepseek.sh&quot;conf_dir=&quot;/data/code/temp/conf/&quot;local_dir=&quot;/data/&quot;shell_name=pretrain_mcore_deepseek.sh## 复制脚本到/workspace下cp $origin_shell $local_dir## 读取hostfilereadarray -t ips &lt; &lt;(grep -vE &#x27;^[[:space:]]*$&#x27; &quot;$conf_dir&quot;hostfile-67)## 获取rank0 IPrank0_ip=$(echo &quot;$&#123;ips[0]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)nodes=$&#123;#ips[@]&#125;## 获取hostfile中配置的IP前缀## 使用cut提取IP地址的前三个数字部分ip_prefix=$(echo &quot;$&#123;ips[0]&#125;&quot; | cut -d &#x27;.&#x27; -f 1-3)## 获取本机IPip=$(hostname -I | grep -oE &quot;$ip_prefix\.[0-9]+&quot;)# 初始化ranknode_rank=-1# 遍历数组for i in &quot;$&#123;!ips[@]&#125;&quot;; do    # 使用tr命令去除空白字符    clean_string=$(echo &quot;$&#123;ips[$i]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)    if [[ &quot;$clean_string&quot; == &quot;$ip&quot; ]]; then        node_rank=$i        break    fidoneif [ $node_rank -ne -1 ]; then    ## 修改脚本中MASTER_ADDR    sed -i &quot;s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/&quot; $local_dir$shell_name    ## 修改NNODES    sed -i &quot;s/^NNODES=.*/NNODES=$nodes/&quot; $local_dir$shell_name    ## 修改NODE_RANK    sed -i &quot;s/^NODE_RANK=.*/NODE_RANK=$node_rank/&quot; $local_dir$shell_namefi
多机多卡拉起测试




断点续训测试
loss曲线对比实验
patch框架训练代码准备
patch的预训练yaml
apiVersion: apps/v1kind: DaemonSetmetadata:  name: deepseek-core  namespace: deepseekspec:  selector:    matchLabels:      app: deepseek-core  template:    metadata:      labels:        app: deepseek-core    spec:      hostNetwork: true      nodeSelector:        deepseek: deepseek-7B      containers:        - name: deepseek          image: registry.paas/library/megatron-lm:v1          imagePullPolicy: IfNotPresent          resources:            limits:              nvidia.com/gpu: 4            requests:              nvidia.com/gpu: 4          command:            - &quot;/bin/bash&quot;            - &quot;-c&quot;            - |              cd /data/code/examples/deepseek/conf &amp;&amp;               bash setRank-patch.sh &amp;&amp;              cd /data/ &amp;&amp;              bash pretrain_patch_deepseek.sh          securityContext:            privileged: true          volumeMounts:            - mountPath: /data/code              name: code            - mountPath: /etc/localtime              name: localtime            - mountPath: /dev/shm              name: dshm      volumes:        - name: code          hostPath:            path: /mnt/users/wangxiangbo/Pai-Megatron-Patch        - name: localtime          hostPath:            path: /etc/localtime        - name: dshm          emptyDir:            medium: Memory            sizeLimit: 20G
patch的预训练脚本
#!/bin/bash#sh run_pretrain_megatron_deepseek.sh dsw /workspace/Pai-Megatron-Patch 7B 1 8 1e-5 1e-6 2048 2048 0 bf16 1 1 sel true true true false 100000 /mnt/deepseek-datasets/wudao_llamabpe_text_document /mnt/deepseek-ckpts/Llama-2-7b-hf-to-mg-tp1-pp1/ 10000000000 100000000 /mnt/output_patch_testexport NCCL_P2P_DISABLE=1#export NCCL_DEBUG=INFOexport NCCL_SOCKET_IFNAME=eth0export GLOO_SOCKET_IFNAME=eth0export CUDA_DEVICE_MAX_CONNECTIONS=1export CUDA_VISIBLE_DEVICES=0,1,2,3MEGATRON_PATCH_PATH=/data/code MEGATRON_PATH=$&#123;MEGATRON_PATCH_PATH&#125;/Megatron-LM-231007export PYTHONPATH=$&#123;MEGATRON_PATH&#125;:$&#123;MEGATRON_PATCH_PATH&#125;:$PYTHONPATHNNODES=2NODE_RANK=0GPUS_PER_NODE=4MASTER_ADDR=192.168.0.22MASTER_PORT=8889DISTRIBUTED_ARGS=&quot;--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT&quot;MODEL_SIZE=7BBATCH_SIZE=1GLOBAL_BATCH_SIZE=8LR=1e-5MIN_LR=1e-6SEQ_LEN=1024PAD_LEN=1024EXTRA_VOCAB_SIZE=0PR=fp16TP=4PP=1AC=selDO=trueFL=falseSP=trueTE=falseSAVE_INTERVAL=1000DATASET_PATH=/data/code/examples/deepseek/dataset/wudao_deepseekbpe_text_documentPRETRAIN_CHECKPOINT_PATH=/data/code/examples/deepseek/ckptTRAIN_TOKENS=10000000WARMUP_TOKENS=0OUTPUT_BASEPATH=/data/code/examples/deepseek/output/pretrain_outputif [ $MODEL_SIZE = 7B ]; thenNUM_LAYERS=30HIDDEN_SIZE=4096NUM_ATTN_HEADS=32INTERMEDIATE_SIZE=11008gqa_options=&quot;&quot;fiif [ $AC = full ]; then    activation_checkpoint_options=&quot; \		    --recompute-method uniform \		    --recompute-granularity full&quot;elif [ $AC = sel ]; then    activation_checkpoint_options=&quot; \        --recompute-activations&quot;elif [ $AC = none ]; then    activation_checkpoint_options=&quot; \                    &quot;fiif [ $PR = fp16 ]; then    pr_options=&quot; \		    --fp16&quot;elif [ $PR = bf16 ]; then    pr_options=&quot; \        --bf16&quot;elif [ $PR = fp8 ]; then    pr_options=&quot; \        --bf16        --fp8-hybrid \        --fp8-amax-compute-algo max \        --fp8-amax-history-len 1024 \        --transformer-impl transformer_engine&quot;fiif [ $DO = true ]; then    do_options=&quot; \		    --use-distributed-optimizer&quot;elif [ $DO = false ]; then    do_options=&quot; \                    &quot;fiif [ $FL = true ]; then    flash_options=&quot; \		    --use-flash-attn&quot;elif [ $FL = false ]; then    flash_options=&quot; \                    &quot;fiif [ $TE = true ]; then    te_options=&quot; \		    --transformer-impl transformer_engine&quot;elif [ $TE = false ]; then    te_options=&quot; \                    &quot;fiif [ $SP = true ] &amp;&amp; [ $TP -gt 1 ]; then    sp_options=&quot; \		    --sequence-parallel&quot;elif [ $SP = false ]; then    sp_options=&quot; \                    &quot;fiif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then    load_options=&quot; \            --load $PRETRAIN_CHECKPOINT_PATH&quot;fiTRAIN_ITERS=$(( $&#123;TRAIN_TOKENS&#125; / $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))LR_WARMUP_ITERS=$(( $&#123;WARMUP_TOKENS&#125;  / $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))LR_DECAY_ITERS=$(( $&#123;TRAIN_TOKENS&#125; /  $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))NAME=&quot;$&#123;ENV&#125;-pretrain-patch-deepseek-$&#123;MODEL_SIZE&#125;-lr-$&#123;LR&#125;-bs-$&#123;BATCH_SIZE&#125;-seqlen-$&#123;SEQ_LEN&#125;-pr-$&#123;PR&#125;-tp-$&#123;TP&#125;-pp-$&#123;PP&#125;-ac-$&#123;AC&#125;-do-$&#123;DO&#125;-sp-$&#123;SP&#125;-tt-$&#123;TRAIN_TOKENS&#125;-wt-$&#123;WARMUP_TOKENS&#125;&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/log/&quot;current_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)TENSORBOARD_DIR=&quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/$&#123;NAME&#125;_$&#123;current_time&#125;&quot;mkdir -p $&#123;TENSORBOARD_DIR&#125;SAVED_PRETRAIN_CHECKPOINT_PATH=&quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/$&#123;NAME&#125;&quot;megatron_options=&quot;  \        --save $&#123;SAVED_PRETRAIN_CHECKPOINT_PATH&#125; \        --split 99,1,0 \        --train-data-path $&#123;DATASET_PATH&#125;        --lr $&#123;LR&#125; \        --min-lr $&#123;MIN_LR&#125; \        --lr-decay-style linear \        --adam-beta1 0.9 \        --adam-beta2 0.95 \        --weight-decay 0.1 \        --clip-grad 1.0 \        --init-method-std 0.006 \        --lr-decay-iters $&#123;LR_DECAY_ITERS&#125; \        --lr-warmup-iters $&#123;LR_WARMUP_ITERS&#125; \        --train-iters $&#123;TRAIN_ITERS&#125; \        --micro-batch-size $&#123;BATCH_SIZE&#125; \        --global-batch-size $&#123;GLOBAL_BATCH_SIZE&#125; \        --num-layers $&#123;NUM_LAYERS&#125; \        --hidden-size $&#123;HIDDEN_SIZE&#125; \        --num-attention-heads $&#123;NUM_ATTN_HEADS&#125; \        --ffn-hidden-size $&#123;INTERMEDIATE_SIZE&#125; \        --seq-length $&#123;SEQ_LEN&#125; \        --max-position-embeddings $&#123;SEQ_LEN&#125; \        --log-interval 1 \        --eval-interval 10000 \        --eval-iters 10 \        --save-interval $&#123;SAVE_INTERVAL&#125; \        --tensorboard-queue-size 1 \        --tensorboard-dir $&#123;TENSORBOARD_DIR&#125; \        --log-timers-to-tensorboard \        --log-batch-size-to-tensorboard \        --log-validation-ppl-to-tensorboard \        --tensor-model-parallel-size $&#123;TP&#125; \        --pipeline-model-parallel-size $&#123;PP&#125; \        --dataset LLama-Pretrain-Idxmap \        --no-load-optim \        --no-load-rng \        --num-workers 8 \        --seed 1234 \        --max-padding-length $&#123;PAD_LEN&#125; \        --extra-vocab-size $&#123;EXTRA_VOCAB_SIZE&#125; \        --patch-tokenizer-type LLamaTokenizer \        --swiglu \        --normalization RMSNorm \        --use-llama2-rotary-position-embeddings \        --position-embedding-type rope \        --untie-embeddings-and-output-weights \        --rotary-base 10000 \        --rotary-scale-factor 4 \        --loss-scale 16384 \        --disable-bias-linear        &quot;#--no-query-key-layer-scaling \#/data/code/examples/llama2/pretrain_megatron_llama.pyLOG_SAVE=&quot;/data/code/examples/deepseek/output/pretrain_output/log/&quot;$&#123;current_time&#125;-deepseek-patch-$&#123;NODE_RANK&#125;.log&quot;&quot; run_cmd=&quot;torchrun $DISTRIBUTED_ARGS /data/code/examples/llama2/pretrain_megatron_llama.py $&#123;megatron_options&#125; $&#123;pr_options&#125; $&#123;load_options&#125; $&#123;te_options&#125; $&#123;activation_checkpoint_options&#125; $&#123;do_options&#125; $&#123;flash_options&#125; $&#123;sp_options&#125; $&#123;gqa_options&#125; &gt; $&#123;LOG_SAVE&#125; 2&gt;&amp;1&quot;echo $&#123;run_cmd&#125;eval $&#123;run_cmd&#125;set +x
patch的预训练setrank
#!/bin/bashorigin_shell=&quot;/data/code/examples/deepseek/pretrain_patch_deepseek.sh&quot;conf_dir=&quot;/data/code/examples/deepseek/conf/&quot;local_dir=&quot;/data/&quot;shell_name=pretrain_patch_deepseek.sh## 复制脚本到/workspace下cp $origin_shell $local_dir## 读取hostfilereadarray -t ips &lt; &lt;(grep -vE &#x27;^[[:space:]]*$&#x27; &quot;$conf_dir&quot;hostfile)## 获取rank0 IPrank0_ip=$(echo &quot;$&#123;ips[0]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)nodes=$&#123;#ips[@]&#125;## 获取hostfile中配置的IP前缀## 使用cut提取IP地址的前三个数字部分ip_prefix=$(echo &quot;$&#123;ips[0]&#125;&quot; | cut -d &#x27;.&#x27; -f 1-3)## 获取本机IPip=$(hostname -I | grep -oE &quot;$ip_prefix\.[0-9]+&quot;)# 初始化ranknode_rank=-1# 遍历数组for i in &quot;$&#123;!ips[@]&#125;&quot;; do    # 使用tr命令去除空白字符    clean_string=$(echo &quot;$&#123;ips[$i]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)    if [[ &quot;$clean_string&quot; == &quot;$ip&quot; ]]; then        node_rank=$i        break    fidoneif [ $node_rank -ne -1 ]; then    ## 修改脚本中MASTER_ADDR    sed -i &quot;s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/&quot; $local_dir$shell_name    ## 修改NNODES    sed -i &quot;s/^NNODES=.*/NNODES=$nodes/&quot; $local_dir$shell_name    ## 修改NODE_RANK    sed -i &quot;s/^NODE_RANK=.*/NODE_RANK=$node_rank/&quot; $local_dir$shell_namefi
patch框架预训练测试




]]></content>
      <categories>
        <category>模型适配</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>模型适配</tag>
        <tag>Nvidia</tag>
        <tag>Deepseek</tag>
        <tag>Megatron-LM</tag>
      </tags>
  </entry>
  <entry>
    <title>1.5-Qwen多机多卡调优</title>
    <url>/2025/11/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Qwen/1.5-Qwen%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E8%B0%83%E4%BC%98/</url>
    <content><![CDATA[1. lora
1.1 per_device_train_batch_size测试，最优为16



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
bf16
tflops




2
1
512
8
true
8.37


4
1
512
8
true
9.07


8
1
512
8
true
9.88


16
1
512
8
true
10.32


32
1
512
8
true
OOM



单个GPU批次大小增加时，需要的内存也会增加，GPU内存不足以支持更大的批次，可能会导致溢出或效率降低。
1.2 per_device_eval_batch_size测试，最优为2



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
bf16
tflops




16
1
512
8
true
10.32


16
2
512
8
true
10.33


16
4
512
8
true
10.26



1.3 model_max_length测试，最优为512



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
bf16
flops




16
2
128
8
true
8.95


16
2
256
8
true
9.56


16
2
512
8
true
10.33


16
2
1024
8
true
OOM



1.4 gradient_accumulation_steps测试，最优为16



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
bf16
tflops




16
2
512
1
true
9.07


16
2
512
2
true
9.24


16
2
512
4
true
9.89


16
2
512
8
true
10.33


16
2
512
16
true
11.63



1.5 开启Fp16测试



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
tflops




16
2
512
16
true
67.13



1.6 关闭gradient_checkpointing



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
gradient_checkpointing
tflops




16
2
512
16
true
true
67.13


16
2
512
16
true
false
OOM






参数配置
per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
bf16/fp16
gradient_checkpointing
tflops




默认
2
1
512
8
bf16
true
8.32


调优后
16
2
512
16
fp16
true
67.13



基于2机8卡的V100，Qwen-7B模型的lora微调训练中（采用deepspeed的zero2的内存优化并行方式），Tflops的值最高为67.13**（8.32）
最佳参数配置（per_device_train_batch_size：16，per_device_eval_batch_size：2，model_max_length：512，gradient_accumulation_steps：16，Fp16精度，gradient_checkpointing：True）
2. qlora
qlora使用4比特量化模型以及paged attention等技术实现更小的显存开销
2.1 per_device_train_batch_size测试，最优为32



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
tflops




2
1
512
8
true
31.71


4
1
512
8
true
42.91


8
1
512
8
true
51.14


16
1
512
8
true
55.75


32
1
512
8
true
58.44


64
1
512
8
true
OOM



2.2 per_device_eval_batch_size测试，最优为4



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
tflops




32
1
512
8
true
58.44


32
2
512
8
true
58.84


32
4
512
8
true
59.05


32
8
512
8
true
58.42



2.3 model_max_length测试，最优为512



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
tflops




32
4
128
8
true
48.68


32
4
256
8
true
53.02


32
4
512
8
true
59.05


32
4
1024
8
true
OOM



2.4 gradient_accumulation_steps测试，最优为



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
tflops




32
4
512
1
true
52.66


32
4
512
2
true
52.18


32
4
512
4
true
54.99


32
4
512
8
true
59.05


32
4
512
16
true
64.26


32
4
512
32
true
80.37


32
4
512
64
true
103.89



2.5 开启BF16测试



per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
bf16/fp16
tflops




32
4
512
64
bf16
14.23


32
4
512
64
fp16
103.89



2.6 关闭gradient_checkpointing



gradient_checkpointing
per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
tflops




True
32
4
512
64
true
103.89


False
32
4
512
64
true
OOM



DeepSpeed ZeRO 3 对节点间通信速率的要求远大于 ZeRO 2，在多机微调的情况下会大幅降低训练速度。因此，我们不建议在多机微调的情况下使用 DeepSpeed ZeRO 3 配置。



参数配置
per_device_train_batch_size
per_device_eval_batch_size
model_max_length
gradient_accumulation_steps
fp16
tflops




默认
2
1
512
8
true
31.71


调优后
32
4
512
64
true
103.89



基于2机8卡的V100，Qwen-7B模型的qlora微调训练中（采用deepspeed的zero2的内存优化并行方式），Tflops的值最高为103.89 
默认参数配置
最佳参数配置（gradient_checkpointing：True，per_device_train_batch_size：32，per_device_eval_batch_size：4，model_max_length：512，gradient_accumulation_steps：64，Fp16精度）
qlora在拉起训练时，需要对模型权重进行INT-4量化，会损失掉权重小数部分精度，量化后的权重由于使用更少的位数来表示，存储上比原始权重高效，在拉起模型训练过程过中tflops有明显提升。
]]></content>
      <categories>
        <category>模型训练调优</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>模型训练调优</tag>
        <tag>Nvidia</tag>
        <tag>Qwen</tag>
        <tag>Deepspeed</tag>
      </tags>
  </entry>
  <entry>
    <title>1.2-Qwen模型镜像制作</title>
    <url>/2025/11/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Qwen/1.2-Qwen%E6%A8%A1%E5%9E%8B%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/</url>
    <content><![CDATA[由于新机器挂载文件存储速度非常慢，通过dockerfile文件来直接生成镜像非常慢，所以本镜像在自己的V100云主机中进行打包。
1. 首先docker pull拉取一个ubuntu基础环境
https://hub.docker.com/r/nvidia/cuda/tags?page=11&amp;page_size=&amp;name=&amp;ordering=
docker pull nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04
2. 安装nvidia-container-toolkit，使docker可以调用宿主机gpu资源
2.1 下载nvidia-container-toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) &amp;&amp; \ curl -fsSL [https://nvidia.github.io/libnvidia-container/gpgkey](https://nvidia.github.io/libnvidia-container/gpgkey) | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg &amp;&amp; \ curl -s -L [https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list](https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list) | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
2.2 安装nvidia-container-toolkit
sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit
2.3 添加nvidia-docker源
curl -s -L [https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list](https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list) |   sudo tee /etc/apt/sources.list.d/nvidia-docker.list
2.4 更新并重新执行安装
sudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkit
如果执行过程中报W: GPG error: https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64  InRelease: The following signatures couldn’t be verified because the public key is not available: NO_PUBKEY DDCAE044F796ECB0，则需要确认你的系统是否信任NVIDIA的GPG密钥。如果没有，你需要导入它。可以通过以下命令导入GPG密钥：
curl -s [https://nvidia.github.io/libnvidia-container/gpgkey](https://nvidia.github.io/libnvidia-container/gpgkey) | sudo apt-key add -
2.5 完成 nvidia-container-toolkit 的安装之后，我们继续执行 nvidia-ctk runtime configure 命令，为 Docker 添加 nvidia 这个运行时。完成后，我们的应用就能在容器中使用显卡资源了
sudo nvidia-ctk runtime configure --runtime=docker

2.6 重启docker
sudo systemctl restart docker
2.7 查看是否安装成功
dpkg -l | grep nvidia-container-toolkit

3. 拉取基础镜像成功后，创建一个docker容器
docker images
sudo docker run -it --name qwen --gpus all nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04
exit
4. 退出容器后，将本地跑同的qwen模型代码/权重/数据集/环境cp到创建的qwen镜像中
docker cp /opt/tmp/Qwen/ 02649afd9710:/qwen
5. 重启docker，exec执行
docker ps -a
docker start qwen
docker exec -it 02649afd9710 bash
6. 因为想在容器中执行自身的python环境，不借用宿主机的环境，所以需要单独再安装conda、pytorch等环境。
7. 安装完基础环境后，需要安装qwen模型的依赖
pip install -r requirements.txt
8. 没有vim编辑器还需安装vim
apt-get update
apt-get install -y vim
9. deepspeed安装
pip install &quot;peft&lt;0.8.0&quot; deepspeed
10. 此时在qwen容器中执行训练脚本，拉起训练。
bash finetune/finetune_lora_single_gpu.sh
11. 将此时qwen容器打成镜像
docker commit -a &quot;wangxiangbo&quot; -m &quot;qwen 7B&quot; 02649afd9710 qwen-7b:v1.0

12. 将打好的镜像转成tar包，供新机器解压使用
docker save -o qwen-7b.tar qwen-7b:v1.0
13. 加载tar镜像, 使用load进行从tar文件导出镜像
docker load -i qwen-7b.tar
14. 由于新机器挂载文件存储的原因，镜像解压速度太慢，于是打算将打好的qwen镜像push到阿里云个人仓库中，在新机器中直接pull该镜像。
15. 将镜像推送到Registry
docker login --username=aliyun9599911612 registry.cn-shanghai.aliyuncs.com
docker tag 37c7b97b67f6 registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0
docker push registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0
16. 在3号机器中，拉取该镜像
docker pull registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0

17. 通过该镜像，run一个容器
docker run -it --name qwen --gpus all registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0 bash
]]></content>
      <categories>
        <category>模型训练调优</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>模型训练调优</tag>
        <tag>Nvidia</tag>
        <tag>Qwen</tag>
        <tag>Deepspeed</tag>
      </tags>
  </entry>
  <entry>
    <title>1.4-基于k8s拉起Qwen模型的多机多卡微调</title>
    <url>/2025/11/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Qwen/1.4-%E5%9F%BA%E4%BA%8Ek8s%E6%8B%89%E8%B5%B7Qwen%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A4%9A%E6%9C%BA%E5%A4%9A%E5%8D%A1%E5%BE%AE%E8%B0%83/</url>
    <content><![CDATA[一、镜像准备
1. 查看镜像
docker images

2. 修改镜像标签
docker tag ec99659d9677 registry.paas/library/qwen:v3.0
3. 将镜像推至仓库
docker push registry.paas/library/qwen:v3.0
4. 如果出现签名认证失败，需要修改docker守护进程配置文件
vim /etc/docker/daemon.json
增加如下配置：
&#123;&quot;insecure-registries&quot;:[&quot;registry.paas&quot;]&#125;
重启docker
systemctl daemon-reload &amp;&amp; systemctl restart docker
5. 重新push至registry.paas/library/xxx:tags仓库
二、修改配置文件
1. qwentest.yaml
apiVersion: apps/v1kind: DaemonSetmetadata:  name: qwentestspec:  selector:    matchLabels:      app: qwentest  template:    metadata:      labels:        app: qwentest    spec:      hostNetwork: true      nodeSelector:        model: qwen-7b      containers:      - name: qwentest        image: registry.paas/library/qwen:v3.0        imagePullPolicy: IfNotPresent        resources:         limits:           nvidia.com/gpu: &quot;4&quot;         requests:           nvidia.com/gpu: &quot;4&quot;        command:                                  # training command, which can be modified              - &quot;/bin/bash&quot;              - &quot;-c&quot;                #- sleep 10000              - |                cd /mnt/ &amp;&amp;                cp setRank.sh /qwen/ &amp;&amp;                cd /qwen/ &amp;&amp;                chmod +x setRank.sh &amp;&amp;                bash setRank.sh &amp;&amp;                chmod +x finetune_lora_ds.sh &amp;&amp;                bash finetune_lora_ds.sh        securityContext:          privileged: true        volumeMounts:        - name: processeddata          mountPath: /mnt        - name: dshm          mountPath: /dev/shm        - name: tmp-volume          mountPath: /tmp      volumes:      - name: processeddata        hostPath:          path: /mnt/users/wangxiangbo/runk8s      - name: dshm        emptyDir:          medium: Memory          sizeLimit: 1G      - name: tmp-volume        hostPath:          path: /tmp
2. hostfile
192.168.0.20192.168.0.58
3. setRank.sh
#!/bin/bashshell_name=&quot;finetune_lora_ds.sh&quot;shell_dir=&quot;/mnt/&quot;local_dir=&quot;/qwen/&quot;## 复制脚本到/qwen/下cp $shell_dir$shell_name $local_dir## 读取hostfilereadarray -t ips &lt; &lt;(grep -vE &#x27;^[[:space:]]*$&#x27; &quot;$shell_dir&quot;hostfile)## 获取rank0 IPrank0_ip=$(echo &quot;$&#123;ips[0]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)nodes=$&#123;#ips[@]&#125;## 获取hostfile中配置的IP前缀## 使用cut提取IP地址的前三个数字部分ip_prefix=$(echo &quot;$&#123;ips[0]&#125;&quot; | cut -d &#x27;.&#x27; -f 1-3)## 获取本机IPip=$(hostname -I | grep -oE &quot;$ip_prefix\.[0-9]+&quot;)ls# 初始化ranknode_rank=-1# 遍历数组for i in &quot;$&#123;!ips[@]&#125;&quot;; do    # 使用tr命令去除空白字符    clean_string=$(echo &quot;$&#123;ips[$i]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)    if [[ &quot;$clean_string&quot; == &quot;$ip&quot; ]]; then        node_rank=$i        break    fidoneif [ $node_rank -ne -1 ]; then    ## 修改脚本中MASTER_ADDR    sed -i &quot;s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/&quot; $local_dir$shell_name    ## 修改NNODES    sed -i &quot;s/^NNODES=.*/NNODES=$nodes/&quot; $local_dir$shell_name    ## 修改NODE_RANK    sed -i &quot;s/^NODE_RANK=.*/NODE_RANK=$node_rank/&quot; $local_dir$shell_namefi
4. sh
#!/bin/bash/bin/bash -i &lt;&lt;&#x27;EOF&#x27;export NCCL_IB_DISABLE=1export NCCL_SOCKET_IFNAME=eth0export NCCL_P2P_DISABLE=1export NCCL_DEBUG=INFOsource ~/.bashrc. /opt/miniconda/etc/profile.d/conda.shconda activate qwenexport CUDA_DEVICE_MAX_CONNECTIONS=1export CUDA_VISIBLE_DEVICES=0,1,2,3DIR=`pwd`# Number of GPUs per GPU workerGPUS_PER_NODE=4# Number of GPU workers, for single-worker training, please set to 1NNODES=2# The rank of this worker, should be in &#123;0, ..., WORKER_CNT-1&#125;, for single-worker training, please set to 0NODE_RANK=0# The ip address of the rank-0 worker, for single-worker training, please set to localhostMASTER_ADDR=192.168.0.20# The port for communicationMASTER_PORT=6003MODEL=&quot;/qwen/Qwen-7B-Chat&quot; # Set the path if you do not want to load from huggingface directly# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.# See the section for finetuning in README for more information.DATA=&quot;/qwen/train_data_law.json&quot;DS_CONFIG_PATH=&quot;/qwen/finetune/ds_config_zero2.json&quot;function usage() &#123;    echo &#x27;Usage: bash finetune_lora_ds.sh [-m MODEL_PATH] [-d DATA_PATH] [--deepspeed DS_CONFIG_PATH]&#x27;&#125;while [[ &quot;$1&quot; != &quot;&quot; ]]; do    case $1 in        -m | --model )            shift            MODEL=$1            ;;        -d | --data )            shift            DATA=$1            ;;        --deepspeed )            shift            DS_CONFIG_PATH=$1            ;;        -h | --help )            usage            exit 0            ;;        * )            echo &quot;Unknown argument $&#123;1&#125;&quot;            exit 1            ;;    esac    shiftdoneDISTRIBUTED_ARGS=&quot;    --nproc_per_node $GPUS_PER_NODE \    --nnodes $NNODES \    --node_rank $NODE_RANK \    --master_addr $MASTER_ADDR \    --master_port $MASTER_PORT&quot;torchrun $DISTRIBUTED_ARGS finetune.py \    --model_name_or_path $MODEL \    --data_path $DATA \    --bf16 False \    --output_dir output_qwen \    --num_train_epochs 5 \    --per_device_train_batch_size 8 \    --per_device_eval_batch_size 1 \    --gradient_accumulation_steps 8 \    --evaluation_strategy &quot;no&quot; \    --save_strategy &quot;steps&quot; \    --save_steps 100 \    --save_total_limit 10 \    --learning_rate 3e-4 \    --weight_decay 0.1 \    --adam_beta2 0.95 \    --warmup_ratio 0.01 \    --lr_scheduler_type &quot;cosine&quot; \    --logging_steps 1 \    --report_to &quot;none&quot; \    --model_max_length 512 \    --lazy_preprocess True \    --use_lora \    --gradient_checkpointing \    --ddp_find_unused_parameters False \EOF
三、拉起训练（3号和4号两机8卡）
1. 切换到1号机器master节点上，给带训练得3号和4号机器打上标签
kubectl label nodes ecs-jhjs-1234-003 model=qwen-7b
kubectl label nodes ecs-jhjs-1234-004 model=qwen-7b
2. 准备好启动脚本等文件后，在master节点1号机器上，利用修改好的qwentest.yaml文件拉起训练任务
3. kubectl apply -f qwentest.yaml
4. 通过kubectl查看pod节点启动信息

5. 查看两个节点pod的logs日志
kubectl logs qwentest-czm8n -f

kubectl logs qwentest-qthsf -f

]]></content>
      <categories>
        <category>模型训练调优</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>模型训练调优</tag>
        <tag>Nvidia</tag>
        <tag>Qwen</tag>
        <tag>Deepspeed</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Mindformers的llama3.1模型微调</title>
    <url>/2025/11/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/%E6%98%87%E8%85%BE/Mindformers/llama3.1%E5%BE%AE%E8%B0%83/</url>
    <content><![CDATA[启动容器
docker run -dit --ipc=host --net=host \--name=wxb_mindformers \--device=/dev/davinci0 \--device=/dev/davinci1 \--device=/dev/davinci2 \--device=/dev/davinci3 \--device=/dev/davinci4 \--device=/dev/davinci5 \--device=/dev/davinci6 \--device=/dev/davinci7 \--device=/dev/davinci_manager \--device=/dev/devmm_svm \--device=/dev/hisi_hdc \-v /etc/ascend_install.info:/etc/ascend_install.info \-v /etc/hccn.conf:/etc/hccn.conf \-v /etc/localtime:/etc/localtime \-v /var/log/npu/:/usr/slog \-v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \-v /reason-sharedata/training_inference/wangxiangbo/mindformers/code:/job/code \-v /reason-sharedata/training_inference/wangxiangbo/mindformers/data:/job/data \-v /reason-sharedata/training_inference/wangxiangbo/mindformers/output:/job/output \-v /mnt/weight-1:/job/mnt \swr.cn-central-221.ovaijisuan.com/mindformers/mindformers1.3_mindspore2.4:20241114 \/bin/bash

代码准备
git clone https://gitee.com/mindspore/mindformers.git
数据集
python research/llama3/llama_preprocess.py \--dataset_type qa \--input_glob /job/data/alpaca/alpaca-data-conversation.json \--model_file /job/mnt/huggingface/llama-3-8b-hf \--seq_length 8192 \--output_file /job/data/alpaca
权重转换
python convert_weight.py --model llama --input_path /job/mnt/huggingface/llama-3-8b-hf --output_path /job/data/mindformers/llama3_8b_ckpt
训练脚本
seed: 0output_dir: &#x27;/job/output&#x27; # path to save checkpoint/strategyload_checkpoint: &#x27;/job/data/mindformers/llama3_8b_ckpt/&#x27;src_strategy_path_or_dir: &#x27;&#x27;auto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed modelonly_save_strategy: Falseresume_training: Falserun_mode: &#x27;finetune&#x27; # trainer configtrainer:  type: CausalLanguageModelingTrainer  model_name: &#x27;llama3_8b&#x27; # runner configrunner_config:  epochs: 2  batch_size: 1  sink_mode: True  sink_size: 2 # optimizeroptimizer:  type: FP32StateAdamWeightDecay  beta1: 0.9  beta2: 0.95  eps: 1.e-8 # lr sechdulelr_schedule:  type: CosineWithWarmUpLR  learning_rate: 1.e-5  lr_end: 0.0  warmup_ratio: 0.03  total_steps: -1 # -1 means it will load the total steps of the dataset # datasettrain_dataset: &amp;train_dataset  data_loader:    type: MindDataset    dataset_dir: &quot;/job/data/alpaca/alpaca_llama3_8192/alpaca-fastchat8192.mindrecord&quot;    shuffle: True  input_columns: [&quot;input_ids&quot;,&quot;labels&quot;]  # &quot;input_ids&quot;, &quot;labels&quot; , labels are used in instruction finetune.  num_parallel_workers: 8  python_multiprocessing: False  drop_remainder: True  batch_size: 6  repeat: 1  numa_enable: False  prefetch_size: 1train_dataset_task:  type: CausalLanguageModelDataset  dataset_config: *train_dataset# if True, do evaluate during the training process. if false, do nothing.# note that the task trainer should support _evaluate_in_training function.do_eval: False # eval dataseteval_dataset: &amp;eval_dataset  data_loader:    type: MindDataset    dataset_dir: &quot;&quot;    shuffle: False  input_columns: [&quot;input_ids&quot;]  num_parallel_workers: 8  python_multiprocessing: False  drop_remainder: False  repeat: 1  numa_enable: False  prefetch_size: 1eval_dataset_task:  type: CausalLanguageModelDataset  dataset_config: *eval_dataset use_parallel: True# parallel context configparallel:  parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel  gradients_mean: False  enable_alltoall: False  full_batch: True  search_mode: &quot;sharding_propagation&quot;  enable_parallel_optimizer: True  strategy_ckpt_save_file: &quot;./ckpt_strategy.ckpt&quot;  parallel_optimizer_config:    gradient_accumulation_shard: False    parallel_optimizer_threshold: 64# default parallel of device num = 8 for Atlas 800T A2parallel_config:  data_parallel: 1  model_parallel: 4  pipeline_stage: 2  use_seq_parallel: False  micro_batch_num: 8  vocab_emb_dp: True  gradient_aggregation_group: 4# when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.micro_batch_interleave_num: 1 # recompute configrecompute_config:  recompute: True  select_recompute: False  parallel_optimizer_comm_recompute: True  mp_comm_recompute: True  recompute_slice_activation: True # callbackscallbacks:  - type: MFLossMonitor  - type: CheckpointMointor    prefix: &quot;llama3_8b&quot;    save_checkpoint_steps: 10000    integrated_save: False    async_save: False  - type: ObsMonitor # mindspore context init configcontext:  mode: 0 #0--Graph Mode; 1--Pynative Mode  device_target: &quot;Ascend&quot;  enable_graph_kernel: False  graph_kernel_flags: &quot;--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true&quot;  max_call_depth: 10000  max_device_memory: &quot;26GB&quot;  save_graphs: False  save_graphs_path: &quot;./graph&quot;  device_id: 0  runtime_num_threads: 1 # model configmodel:  model_config:    type: LlamaConfig    batch_size: 1 # add for increase predict    seq_length: 8192    hidden_size: 4096    num_layers: 32    num_heads: 32    n_kv_heads: 8    vocab_size: 128256    intermediate_size: 14336    rms_norm_eps: 1.0e-5    bos_token_id: 128000    eos_token_id: 128001    pad_token_id: 128002    ignore_token_id: -100    compute_dtype: &quot;bfloat16&quot;    layernorm_compute_type: &quot;float32&quot;    softmax_compute_type: &quot;float32&quot;    rotary_dtype: &quot;float32&quot;    param_init_type: &quot;bfloat16&quot;    use_past: False    scaling_factor: 1.0    theta: 500000    extend_method: &quot;None&quot; # support &quot;None&quot;, &quot;PI&quot;, &quot;NTK&quot;    use_flash_attention: True # FA can accelerate training or finetune    offset: 0    fine_grain_interleave: 1    checkpoint_name_or_path: &quot;&quot;    repetition_penalty: 1    max_decode_length: 512    top_k: 3    top_p: 1    do_sample: False  arch:    type: LlamaForCausalLM # metricmetric:  type: PerplexityMetric # wrapper cell configrunner_wrapper:  type: MFTrainOneStepCell  scale_sense: 1.0  use_clip_grad: True eval_callbacks:  - type: ObsMonitor auto_tune: Falsefilepath_prefix: &#x27;./autotune&#x27;autotune_per_step: 10 profile: Falseprofile_start_step: 5profile_stop_step: 7init_start_profile: Trueprofile_communication: Trueprofile_memory: Truelayer_scale: Falselayer_decay: 0.65lr_scale_factor: 256 # aiccremote_save_url: &quot;Please input obs url on AICC platform.&quot;
训练拉起

git
 clone https://gitee.com/mindspore/mindformers.gitgit
 clone https://gitee.com/mindspore/mindformers.git
]]></content>
      <categories>
        <category>模型训练调优</category>
        <category>昇腾</category>
      </categories>
      <tags>
        <tag>模型训练调优</tag>
        <tag>昇腾</tag>
        <tag>910B</tag>
        <tag>Mindformers</tag>
      </tags>
  </entry>
  <entry>
    <title>基于Modellink的llama2-7b和Mistral-7b模型微调</title>
    <url>/2025/11/06/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/%E6%98%87%E8%85%BE/Modellink/llama2-7B+Mistral-7B/</url>
    <content><![CDATA[基于docker的mistral-7B微调
容器挂载
sudo docker run -dit --ipc=host --net=host \--name=modellink_wxb \--device=/dev/davinci0 \--device=/dev/davinci1 \--device=/dev/davinci2 \--device=/dev/davinci3 \--device=/dev/davinci4 \--device=/dev/davinci5 \--device=/dev/davinci6 \--device=/dev/davinci7 \--device=/dev/davinci_manager \--device=/dev/devmm_svm \--device=/dev/hisi_hdc \-v /etc/ascend_install.info:/etc/ascend_install.info \-v /etc/hccn.conf:/etc/hccn.conf \-v /etc/localtime:/etc/localtime \-v /var/log/npu/:/usr/slog \-v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \-v /reason-sharedata/training_inference/wangxiangbo/modellink/code/Modellink:/job/code \-v /reason-sharedata/training_inference/wangxiangbo/modellink/data:/job/data \-v /reason-sharedata/training_inference/wangxiangbo/modellink/output:/job/output \-v /mnt/weight-1:/job/mnt \registry.paas/cmss/modellink-cann8.0-torch2.1-mindspeed-0.7:v1.0 \/bin/bash
权重转换
# 修改 ascend-toolkit 路径source /usr/local/Ascend/ascend-toolkit/set_env.sh
python  tools/checkpoint/convert_ckpt.py \    --model-type GPT \    --loader llama2_hf \    --saver megatron \    --load-dir /job/mnt/huggingface/mistral-7b-hf/ \    --save-dir /job/data/megatron/mistral-7b-tp4-pp2 \    --tokenizer-model /job/mnt/huggingface/mistral-7b-hf/tokenizer.model \    --target-tensor-parallel-size 4 \    --target-pipeline-parallel-size 2

数据集
下载
sudo wget https://hf-mirror.com/datasets/silk-road/alpaca-data-gpt4-chinese/resolve/main/Alpaca_data_gpt4_zh.jsonl
处理
python tools/preprocess_data.py \    --input /job/data/dataset/mistral/Alpaca_data_gpt4_zh.jsonl \    --output-prefix /job/data/dataset/mistral/Alpaca_finetune/ \    --tokenizer-type PretrainedFromHF \    --tokenizer-name-or-path /job/mnt/huggingface/mistral-7b-hf/ \    --append-eod \    --tokenizer-not-use-fast \    --handler-name GeneralInstructionHandler \    --workers 4

处理完成

微调
微调脚本
#!/bin/bashexport CUDA_DEVICE_MAX_CONNECTIONS=1export PYTORCH_NPU_ALLOC_CONF=expandable_segments:TrueGPUS_PER_NODE=8MASTER_ADDR=localhostMASTER_PORT=6006NNODES=1NODE_RANK=0WORLD_SIZE=$(($GPUS_PER_NODE * $NNODES))DISTRIBUTED_ARGS=&quot;    --nproc_per_node $GPUS_PER_NODE \    --nnodes $NNODES \    --node_rank $NODE_RANK \    --master_addr $MASTER_ADDR \    --master_port $MASTER_PORT&quot;echo &quot;NODE_RANK $&#123;NODE_RANK&#125;&quot;DATA_PATH=&quot;/job/data/dataset/mistral/Alpaca_finetune/&quot;TOKENIZER_MODEL=&quot;/job/mnt/huggingface/mistral-7b-hf&quot;CKPT_SAVE_DIR=&quot;/job/output/mistral_ckpt/&quot;CKPT_LOAD_DIR=&quot;/job/data/megatron/mistral-7b-tp4-pp2/&quot;TP=4PP=2NUM_LAYERS=32GPT_ARGS=&quot;    --tensor-model-parallel-size $&#123;TP&#125; \    --pipeline-model-parallel-size $&#123;PP&#125; \    --sequence-parallel \    --sliding-window 4096 \    --num-layers $&#123;NUM_LAYERS&#125; \    --hidden-size 4096 \    --ffn-hidden-size 14336 \    --num-attention-heads 32 \    --group-query-attention \    --num-query-groups 8 \    --tokenizer-type PretrainedFromHF \    --tokenizer-name-or-path $&#123;TOKENIZER_MODEL&#125; \    --seq-length 32768 \    --max-position-embeddings 32768 \    --micro-batch-size 1 \    --global-batch-size 32 \    --make-vocab-size-divisible-by 1 \    --lr 1.25e-6 \    --train-iters 1000 \    --lr-decay-style cosine \    --untie-embeddings-and-output-weights \    --disable-bias-linear \    --attention-dropout 0.0 \    --init-method-std 0.01 \    --hidden-dropout 0.0 \    --position-embedding-type rope \    --normalization RMSNorm \    --use-fused-rmsnorm \    --use-fused-swiglu \    --use-rotary-position-embeddings \    --use-fused-rotary-pos-emb \    --use-mc2 \    --swiglu \    --use-flash-attn \    --no-masked-softmax-fusion \    --attention-softmax-in-fp32 \    --min-lr 1.25e-7 \    --weight-decay 1e-1 \    --lr-warmup-fraction 0.01 \    --clip-grad 1.0 \    --adam-beta1 0.9 \    --initial-loss-scale 65536 \    --adam-beta2 0.95 \    --no-gradient-accumulation-fusion \    --no-load-optim \    --no-load-rng \    --use-distributed-optimizer \    --overlap-grad-reduce \    --load $&#123;CKPT_LOAD_DIR&#125; \    --save $&#123;CKPT_SAVE_DIR&#125; \    --bf16 \    --finetune \    --is-instruction-dataset \    --log-throughput \    --recompute-granularity full \    --recompute-method block \    --recompute-num-layers 32&quot;DATA_ARGS=&quot;    --data-path $DATA_PATH  \    --split 100,0,0 \&quot;OUTPUT_ARGS=&quot;    --log-interval 1 \    --save-interval 1000 \    --eval-interval 1000 \    --eval-iters 0 \&quot;torchrun $DISTRIBUTED_ARGS ../pretrain_gpt.py \  $GPT_ARGS \  $DATA_ARGS \  $OUTPUT_ARGS \  --distributed-backend nccl \  | tee /job/output/logs/train_mistral_7B.log
拉起训练

基于docker的llama2-7B微调
容器挂载
sudo docker run -dit --ipc=host --net=host \--name=modellink_wxb \--device=/dev/davinci0 \--device=/dev/davinci1 \--device=/dev/davinci2 \--device=/dev/davinci3 \--device=/dev/davinci4 \--device=/dev/davinci5 \--device=/dev/davinci6 \--device=/dev/davinci7 \--device=/dev/davinci_manager \--device=/dev/devmm_svm \--device=/dev/hisi_hdc \-v /etc/ascend_install.info:/etc/ascend_install.info \-v /etc/hccn.conf:/etc/hccn.conf \-v /etc/localtime:/etc/localtime \-v /var/log/npu/:/usr/slog \-v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \-v /reason-sharedata/training_inference/wangxiangbo/modellink/code/Modellink:/job/code \-v /reason-sharedata/training_inference/wangxiangbo/modellink/data:/job/data \-v /reason-sharedata/training_inference/wangxiangbo/modellink/output:/job/output \-v /mnt/weight-1:/job/mnt \registry.paas/cmss/modellink-cann8.0-torch2.1-mindspeed-0.7:v1.0 \/bin/bash
权重转换
# 修改 ascend-toolkit 路径source /usr/local/Ascend/ascend-toolkit/set_env.sh
python  tools/checkpoint/convert_ckpt.py \       --model-type GPT \       --loader llama2_hf \       --saver megatron \       --target-tensor-parallel-size 8 \       --target-pipeline-parallel-size 1 \       --load-dir /job/mnt/huggingface/Llama-2-7b-hf// \       --save-dir /job/data/megatron/llama-2-7b-hf-v0.1-tp8-pp1/ \       --tokenizer-model /job/mnt/huggingface/Llama-2-7b-hf/tokenizer.json

数据集
python tools/preprocess_data.py \    --input /job/data/dataset/mistral/Alpaca_data_gpt4_zh.jsonl \    --output-prefix /job/data/dataset/llama2/Alpaca_finetune/ \    --tokenizer-type PretrainedFromHF \    --tokenizer-name-or-path /job/mnt/huggingface/Llama-2-7b-hf/ \    --append-eod \    --tokenizer-not-use-fast \    --handler-name GeneralInstructionHandler \    --workers 4


微调
微调脚本
#!/bin/bashexport CUDA_DEVICE_MAX_CONNECTIONS=1export PYTORCH_NPU_ALLOC_CONF=expandable_segments:TrueGPUS_PER_NODE=8MASTER_ADDR=localhostMASTER_PORT=6006NNODES=1NODE_RANK=0WORLD_SIZE=$(($GPUS_PER_NODE * $NNODES))DISTRIBUTED_ARGS=&quot;    --nproc_per_node $GPUS_PER_NODE \    --nnodes $NNODES \    --node_rank $NODE_RANK \    --master_addr $MASTER_ADDR \    --master_port $MASTER_PORT&quot;echo &quot;NODE_RANK $&#123;NODE_RANK&#125;&quot;DATA_PATH=&quot;/job/data/dataset/llama2/Alpaca_finetune&quot;TOKENIZER_MODEL=&quot;/job/mnt/huggingface/Llama-2-7b-hf/&quot;CKPT_SAVE_DIR=&quot;/job/output/llama2_ckpt/&quot;CKPT_LOAD_DIR=&quot;/job/data/megatron/llama-2-7b-hf-v0.1-tp8-pp1&quot;TP=8PP=1NUM_LAYERS=32GPT_ARGS=&quot;    --tensor-model-parallel-size $&#123;TP&#125; \    --pipeline-model-parallel-size $&#123;PP&#125; \    --sequence-parallel \    --num-layers 32 \    --hidden-size 4096 \    --ffn-hidden-size 11008 \    --num-attention-heads 32 \    --tokenizer-type Llama2Tokenizer \    --tokenizer-model $&#123;TOKENIZER_MODEL&#125; \    --seq-length 4096 \    --max-position-embeddings 4096 \    --micro-batch-size 1 \    --global-batch-size 256 \    --make-vocab-size-divisible-by 1 \    --lr 1.25e-6 \    --train-iters 5000 \    --lr-decay-style cosine \    --untie-embeddings-and-output-weights \    --disable-bias-linear \    --attention-dropout 0.0 \    --init-method-std 0.01 \    --hidden-dropout 0.0 \    --position-embedding-type rope \    --normalization RMSNorm \    --use-fused-rmsnorm \    --swiglu \    --use-flash-attn \    --no-masked-softmax-fusion \    --attention-softmax-in-fp32 \    --min-lr 1.25e-7 \    --weight-decay 1e-1 \    --lr-warmup-fraction 0.01 \    --clip-grad 1.0 \    --adam-beta1 0.9 \    --initial-loss-scale 65536 \    --adam-beta2 0.95 \    --no-gradient-accumulation-fusion \    --no-load-optim \    --no-load-rng \    --use-distributed-optimizer \    --use-fused-swiglu \    --use-fused-rotary-pos-emb \    --overlap-grad-reduce \    --bf16&quot;DATA_ARGS=&quot;    --data-path $DATA_PATH  \    --split 100,0,0 \&quot;OUTPUT_ARGS=&quot;    --log-interval 1 \    --save-interval 1000 \    --eval-interval 1000 \    --eval-iters 0 \&quot;torchrun $DISTRIBUTED_ARGS ../pretrain_gpt.py \  $GPT_ARGS \  $DATA_ARGS \  $OUTPUT_ARGS \  --distributed-backend nccl \  | tee /job/output/logs/train_llama2_7B.log
拉起训练

]]></content>
      <categories>
        <category>模型训练调优</category>
        <category>昇腾</category>
      </categories>
      <tags>
        <tag>模型训练调优</tag>
        <tag>昇腾</tag>
        <tag>910B</tag>
        <tag>Mindformers</tag>
      </tags>
  </entry>
  <entry>
    <title>Tulu3-8B-后训练实践</title>
    <url>/2025/11/12/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Tulu3/Tulu3-8B-%E5%90%8E%E8%AE%AD%E7%BB%83%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[训练环境
镜像
registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:tulu3
镜像中已包含tulu3后训练所需要的所有依赖环境。
代码结构
/mnt/users/wangxiangbo/model/tulu3/open-instruct-main

data
包含了sft、dpo、rlvr所需的数据集

eval相关

scripts/configs
训练脚本


tulu3_sft.sh #tulu3 sft单机脚本

hostfilehostfile-12setRank.shtulu3_dpo.sh #多机dpo脚本tulu3_dpo_single.sh #单机4卡dpo脚本tulu3_dpo.yaml #多机yamltulu3_sft.sh #多机sft脚本

tulu3_rlvr_single.sh #tulu3 rlvr单机脚本

loss_plot.py #loss曲线绘图脚本
open_instruct
sft、dpo、rlvr相关训练代码

sft实践
单机4卡测试
sft数据集
sft数据集采用tulu3官网提供的数据集

sft数据集样例：
&#123;&quot;id&quot;:&quot;oasst1_2644&quot;,&quot;messages&quot;:[&#123;&quot;content&quot;:&quot;有缓解社交恐惧症的实用方法吗？&quot;,&quot;role&quot;:&quot;user&quot;&#125;,&#123;&quot;content&quot;:&quot;以下是一些实用的方法，可能有助于缓解社交恐惧症：\n\n1.  寻求专业帮助：寻求心理医生或心理治疗师的帮助是缓解社交恐惧症的最佳方法之一。专业的治疗师可以帮助你识别和处理恐惧的源头，并提供有效的治疗方案。\n\n2. 练习深呼吸和放松技巧：当你感到焦虑和紧张时，尝试通过深呼吸和放松技巧来帮助你冷静下来。深呼吸可以帮助你减轻身体上的压力和紧张感，从而使你更放松和镇定。\n\n3. 学习积极思考：避免消极的自我对话，例如“我会失败”、“我不行”等等。改变自己的思考方式，尝试以积极、自信的态度面对社交场合。\n\n4.  採取柔性方法：為了對抗焦慮症狀，建議使用一些方法來提醒你的身體是安全的，例如：  \n\n＊ 呼吸訓練（用鼻子深深的吸一口氣再用嘴巴將氣慢慢地吐出，當你專注在吐氣時，你就會被帶出抵抗或逃避的情緒中。） \n\n ＊ 專注於現在（這麼做讓你能夠專注於自己所經歷的每分每秒。試著說出你現在能看見、聞到、碰觸到並聽得到的東西，或是坐在椅子上並感受自己的腳觸碰地面的感覺，或身體正被支撐著的感覺。）\n\n5. 給自己一點從容：在過渡時期練習自我同情，並提醒自己感到不確定與不安是很正常的。\n\n这些方法可以帮助你缓解社交恐惧症，但不同的人可能需要不同的方法和治疗方案。如果你感到严重的焦虑和恐惧，请及时寻求专业帮助。&quot;,&quot;role&quot;:&quot;assistant&quot;&#125;,&#123;&quot;content&quot;:&quot;好吧，可是我要去哪裡尋求專業幫助？&quot;,&quot;role&quot;:&quot;user&quot;&#125;,&#123;&quot;content&quot;:&quot;您可以先參考美國精神疾病診斷標準 **DSM-5** 中的6點標準，或者進行一些專業的心理測試。如果您符合這些標准或測試結果顯示您有社交恐懼症，您應儘早尋求專業的幫助。\n\n尋求專業幫助的途徑有以下幾種：\n\n- **諮詢醫生或心理健康專家**。他們可以為您提供診斷，藥物治療，心理治療等服務。\n- **參加心理諮詢或治療機構**。這些機構通常有專業的心理諮詢師或治療師，可以為您提供個人或團體的心理諮詢或治療服務。\n- **加入互助小組或社區**。這些小組或社區通常由有相同或類似問題的人組成，可以為您提供支持，鼓勵，交流和分享的平台。\n\n以上是我為您搜索到的一些信息，希望對您有所幫助。如果您還有其他問題，歡迎繼續提問。&quot;,&quot;role&quot;:&quot;assistant&quot;&#125;],&quot;source&quot;:&quot;ai2-adapt-dev\/oasst1_converted&quot;&#125;
由于是.parquet格式数据，转成json后一共有93w条数据。

由于机器数量有限，训练其完整数据集需要较长时间，故将.parquet转为json后将数据集缩减至5k条，并再次转回.parquet格式数据。

单机脚本配置
export CUDA_VISIBLE_DEVICES=0,1,2,3export WANDB_MODE=disabledcurrent_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)LOG_SAVE=&quot;/mnt/open-instruct-main/output/sft/$&#123;current_time&#125;-tulu3-sft.log&quot;MODEL_SIZE=8BNUM_GPUS=4BATCH_SIZE_PER_GPU=2TOTAL_BATCH_SIZE=8GRADIENT_ACC_STEPS=$(($TOTAL_BATCH_SIZE/($NUM_GPUS * $BATCH_SIZE_PER_GPU)))echo &quot;Training llama model $&#123;MODEL_SIZE&#125; using $NUM_GPUS GPUs, $BATCH_SIZE_PER_GPU batch size per GPU, $GRADIENT_ACC_STEPS gradient accumulation steps&quot;accelerate launch \    --mixed_precision fp16 \    --num_machines 1 \    --num_processes $NUM_GPUS \    --use_deepspeed \    --deepspeed_config_file /mnt/open-instruct-main/configs/ds_configs/stage3_offloading_accelerate.conf \    /mnt/open-instruct-main/open_instruct/finetune.py \    --model_name_or_path /mnt/LLM-Research/Meta-Llama-31-8B \    --tokenizer_name /mnt/LLM-Research/Meta-Llama-31-8B \    --use_slow_tokenizer \    --train_file /mnt/open-instruct-main/data/sft_data_json/sft_dataset_5k.json \    --max_seq_length 4096 \    --preprocessing_num_workers 4 \    --per_device_train_batch_size $BATCH_SIZE_PER_GPU \    --gradient_accumulation_steps $GRADIENT_ACC_STEPS \    --learning_rate 2e-5 \    --lr_scheduler_type linear \    --warmup_ratio 0.03 \    --weight_decay 0. \    --num_train_epochs 1 \    --output_dir /mnt/open-instruct-main/output/sft \    --gradient_checkpointing true \    --report_to none \    --use_flash_attn false 2&gt;&amp;1 | tee -a &quot;$LOG_SAVE&quot;
为了尽可能地还原tulu3原论文的实验结果，其中部分超参与论文中最优保持一致。
--max_seq_length 4096
--learning_rate 2e-5
--lr_scheduler_type linear
--warmup_ratio 0.03
其中的模型权重利用llama3.1-8B-base模型。
训练测试
训练结束





资源类型
利用率
内存/显存占用量




CPU
53.2%
80.7%


GPU
85.5%
66.4%



loss曲线

eval
llama3.1-8B通过tulu3提供的sft数据集（删减至5k条）后，得到的tulu3_sft权重




类别
Llama-31-8B
tulu3-sft-weight
Llama-31-Tulu-3-8B-SFT




平均准确率
0.6014
0.5729
0.6356


子类别准确率
-
-
-


数学
0.4098
0.3778
0.4192


健康
0.6561
0.6183
0.6720


物理
0.5063
0.4891
0.5328


商业
0.7643
0.7346
0.8101


生物
0.7555
0.6982
0.7775


化学
0.4851
0.4851
0.5186


计算机科学
0.5752
0.5194
0.6092


经济
0.5930
0.5809
0.6213


工程
0.5517
0.5172
0.5517


哲学
0.4881
0.4627
0.5775


其他
0.6524
0.6532⬆
0.7090


历史
0.7452
0.7172
0.7774


地理
0.7323
0.7273
0.7677


政治
0.7546
0.7099
0.7762


心理学
0.7174
0.6845
0.7485


文化
0.8102
0.7440
0.7922


法律
0.4878
0.4589
0.5020


类别准确率
-
-
-


STEM
0.5192
0.4864
0.5394


人文学科
0.5388
0.5116
0.5887


社会科学
0.7062
0.6740
0.7296


其他（商业、健康等）
0.6693
0.6465
0.7039



eval结果分析

SFT后权重的平均准确率: 0.5729
SFT前权重的平均准确率: 0.6014

整体平均准确率下降了 ~2.85%。
原因：本次实验（由于机器环境限制）将原始的tulu3 SFT数据集（93w）删减到5k条，导致模型未能充分学习各个领域的知识和任务特性，泛化能力下降。
dpo实践
单机4卡测试
偏好数据集
偏好数据集采用tulu3官网提供的数据集。

偏好数据集样例：
&#123;&quot;id&quot;:&quot;allenai\/tulu-3-sft-reused-off-policy_0&quot;,&quot;prompt&quot;:&quot;Peter has 5 cages with rabbits (the cages are arranged in a row). It is known that there is at least one rabbit in each cage. We will call two rabbits neighbors if they are either in the same cage or in adjacent cages. It turned out that each rabbit has either 3 or 7 neighbors. How many rabbits are in the central cage?&quot;,&quot;chosen&quot;:[&#123;&quot;content&quot;:&quot;Peter has 5 cages with rabbits (the cages are arranged in a row). It is known that there is at least one rabbit in each cage. We will call two rabbits neighbors if they are either in the same cage or in adjacent cages. It turned out that each rabbit has either 3 or 7 neighbors. How many rabbits are in the central cage?&quot;,&quot;role&quot;:&quot;user&quot;&#125;,&#123;&quot;content&quot;:&quot;Here&#x27;s how to solve this problem:\n\n**Understanding the Information**\n\n* **Each rabbit has 3 or 7 neighbors:** This means there must be a pattern in how the rabbits are distributed within the cages.\n* **At least one rabbit in each cage:**  We can&#x27;t have cages without any rabbits.\n\n**Reasoning**\n\n* **Corner Cages:**  Rabbits in the corner cages (cages 1 and 5) can only have 3 neighbors (two adjacent cages and one within their own cage).\n* **Middle Cages:** Rabbits in the middle cages (cages 2, 3, and 4) must have 7 neighbors to balance out the setup. \n\n**The Solution**\n\nSince rabbits in the middle cages need 7 neighbors, and there&#x27;s at least one rabbit in each cage, the central cage (cage 3) must have **4 rabbits**. \n\n\n\nLet me know if you&#x27;d like to explore other logic puzzles! \n&quot;,&quot;role&quot;:&quot;assistant&quot;&#125;],&quot;rejected&quot;:[&#123;&quot;content&quot;:&quot;Peter has 5 cages with rabbits (the cages are arranged in a row). It is known that there is at least one rabbit in each cage. We will call two rabbits neighbors if they are either in the same cage or in adjacent cages. It turned out that each rabbit has either 3 or 7 neighbors. How many rabbits are in the central cage?&quot;,&quot;role&quot;:&quot;user&quot;&#125;,&#123;&quot;content&quot;:&quot;The central cage has the same number of rabbits as any other cage, so it also has either 3 or 7 neighbors.\nThere are (at most) 7 rabbits in the central cage and (at most) 3 rabbits in any cage on either side. Hence, there are (at most) 7 rabbits in each of those cages. The total number of rabbits in the 5 cages is (at most) 5 * 7 + 1 = 36 + 1 = 37.\nOne cage must have exactly the right number of neighbors, but two cages cannot have the right number.\nIf each cage has exactly 3 neighbors, the total number of rabbits in all 5 cages is 1 + 5 * 3 = 16\nIf each cage has exactly 7 neighbors, the total number of rabbits in all 5 cages is 1 + 5 * 7 = 32\nNow that we know there are at most 36 rabbits, we can safely conclude that there is 1 rabbit in the central cage, that there are 5 cages with 3 rabbits each and one cage with 7 rabbits and that the central cage has 3 neighbors.\nThe central cage has exactly 3 rabbits and 3 neighbors.\nIn the other 4 cages, there are (at most) 5 * 3 = 15 rabbits.\nIn the other 5 cages, there are (at least) 32 – 16 = 16 rabbits.&quot;,&quot;role&quot;:&quot;assistant&quot;&#125;],&quot;source&quot;:&quot;allenai\/tulu-3-sft-reused-off-policy&quot;&#125;
由于是.parquet格式数据，转成json后一共有67w条数据。

由于机器数量有限，训练其完整数据集需要较长时间，故将.parquet转为json后将数据集缩减至5k条，并再次转回.parquet格式数据。

单机脚本配置
单机脚本，部分配置OOM，采用deepseed中zero3以及cpu_offloading，开启梯度累积。
# you need 8 GPUs for full finetuningexport CUDA_VISIBLE_DEVICES=0,1,2,3export WANDB_MODE=disabledNUM_GPUS=4BATCH_SIZE_PER_GPU=1TOTAL_BATCH_SIZE=8GRADIENT_ACC_STEPS=$(($TOTAL_BATCH_SIZE/$NUM_GPUS/$BATCH_SIZE_PER_GPU))current_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)LOG_SAVE=&quot;/mnt/open-instruct-main/output/dpo/$&#123;current_time&#125;-tulu3-dpo.log&quot;echo &quot;Training model using $NUM_GPUS GPUs, $BATCH_SIZE_PER_GPU batch size per GPU, $GRADIENT_ACC_STEPS gradient accumulation steps&quot;#stage3_no_offloading_accelerate.confaccelerate launch \    --mixed_precision fp16 \    --num_machines 1 \    --num_processes $NUM_GPUS \    --use_deepspeed \    --deepspeed_config_file /mnt/open-instruct-main/configs/ds_configs/stage3_offloading_accelerate.conf \    /mnt/open-instruct-main/open_instruct/dpo_tune.py \    --model_name_or_path /mnt/LLM-Research/Llama-31-Tulu-3-8B-SFT \    --use_flash_attn  false\    --gradient_checkpointing \    --tokenizer_name /mnt/LLM-Research/Llama-31-Tulu-3-8B-SFT \    --use_slow_tokenizer \    --dataset_name /mnt/open-instruct-main/data/dpo_data_5k \    --max_seq_length 2048 \    --preprocessing_num_workers 4 \    --per_device_train_batch_size $BATCH_SIZE_PER_GPU \    --gradient_accumulation_steps $GRADIENT_ACC_STEPS \    --learning_rate 5e-7 \    --lr_scheduler_type linear \    --warmup_ratio 0.1 \    --weight_decay 0. \    --num_train_epochs 1 \    --output_dir /mnt/open-instruct-main/output/dpo \    --with_tracking False \    --logging_steps 1 2&gt;&amp;1 | tee -a &quot;$LOG_SAVE&quot;
为了尽可能地还原tulu3原论文的实验结果，其中部分超参与论文中最优保持一致。
--max_seq_length 2048
--learning_rate 5e-7
--lr_scheduler_type linear
--warmup_ratio 0.1
其中的模型权重利用tulu3开源的经过llama3.1-8B sft得到的权重。

训练测试
关闭cpu_offloading
注：关闭cpu_offloading之后，迭代10步左右会OOM




资源类型
利用率
内存/显存占用量




CPU
16.5%
12.1%


GPU
100%
93.6%



开启cpu_offloading




资源类型
利用率
内存/显存占用量




CPU
55.0%
58.2%


GPU
85.8%
68.7%



训练完成

得到的dpo权重

loss曲线

eval
评测数据集采用MMLU，一个包含来自各个知识领域的多项选择题的巨大多任务测试。 测试涵盖了人文学科、社会科学、自然科学以及其他对某些人来说重要的学习领域。 数据集中的问题是由研究生和本科生从在线免费资源中手动收集的。这包括研究生入学考试和美国医学执照考试等考试的练习题。还包括为本科生课程设计的题目，以及为牛津大学出版社书籍读者设计的题目。 一些任务涵盖一个主题，如心理学，但难度级别特定，例如“初阶”、“高中”、“大学”或“专业”。 例如，“专业心理学”任务借鉴了心理学专业实践考试的免费练习题中的问题，而“高中心理学”任务则包含类似高级 Placement 心理学考试中的问题。
:::tips
MMLU数据集样例如下，摘自MMLU论文https://arxiv.org/abs/2009.03300
:::

eg：以下是MMLU上College Biology类问题及答案：
&quot;Based on the characteristic population curves that result from plotting population growth of a species, the most effective means of controlling the mosquito population is to&quot;,maintain the population at a point corresponding to the midpoint of its logistic curve,opt for zero population control once the K value of the curve has been reached,reduce the carrying capacity cif the environment to lower the K value,increase the mortality rate,C
eg：以下是dpo_weight在MMLU上College Biology类问题的回答：
0,1,2,3,4,5,correct,choiceA_probs,choiceB_probs,choiceC_probs,choiceD_probs&quot;Based on the characteristic population curves that result from plotting population growth of a species, the most effective means of controlling the mosquito population is to&quot;,maintain the population at a point corresponding to the midpoint of its logistic curve,opt for zero population control once the K value of the curve has been reached,reduce the carrying capacity cif the environment to lower the K value,increase the mortality rate,C,True,0.07324660569429398,0.0071399579755961895,0.6528399586677551,0.1929791122674942
本次dpo后得到的权重dpo_weight对比tulu3开源的sft权重Llama-31-Tulu-3-8B-SFT。
Llama-31-Tulu-3-8B-SFT在MMLU上的表现：

dpo_weight在MMLU上的表现：




类别
Llama-31-8B
Llama-31-Tulu-3-8B-SFT
dpo_weight
Llama-31-Tulu-3-8B-DPO




平均准确率
0.6014
0.6356
**0.6377**⬆
0.6352


子类别准确率
-
-
-
-


数学
0.4098
0.4192
0.4352⬆
0.4258


健康
0.6561
0.6720
0.6726⬆
0.6915


物理
0.5063
0.5328
0.5500⬆
0.5391


商业
0.7643
0.8101
0.7941
0.7849


生物
0.7555
0.7775
0.7775⬆
0.7731


化学
0.4851
0.5186
0.5182
0.5248


计算机科学
0.5752
0.6092
0.6189⬆
0.6117


经济
0.5930
0.6213
0.6173
0.6429


工程
0.5517
0.5517
0.5655⬆
0.5586


哲学
0.4881
0.5775
0.5626
0.5388


其他
0.6524
0.7090
0.7107⬆
0.7082


历史
0.7452
0.7774
0.7828⬆
0.7720


地理
0.7323
0.7677
0.7778⬆
0.7778


政治
0.7546
0.7762
0.7793⬆
0.7762


心理学
0.7174
0.7485
0.7519⬆
0.7476


文化
0.8102
0.7922
0.7892
0.8042


法律
0.4878
0.5020
0.5133⬆
0.5156


父类别准确率
-
-
-
-


STEM
0.5192
0.5394
0.5507⬆
0.5437


人文学科
0.5388
0.5887
0.5877
0.5762


社会科学
0.7062
0.7296
0.7309⬆
0.7364


其他（商业、健康、杂项）
0.6693
0.7039
0.7027
0.7101



dpo代码实现分析
Length-Normalized DPO 的实现
_get_batch_logps 函数
tulu3论文中说明了，tulu3-8B的dpo使用了长度归一化的DPO（Length-Normalized DPO），以此来消除因回答的序列长度（对比来说很长的话）带来的概率偏好影响。 当 average_log_prob=True 时，会将每个 token 的 log 概率进行平均处理，实现序列长度归一化。 如果average_log_prob=False，则是直接求和。
代码具体实现如下：
def _get_batch_logps(    logits: torch.FloatTensor, labels: torch.LongTensor, average_log_prob: bool = False) -&gt; torch.FloatTensor:    # 确保logits和labels的形状在除了最后一个维度外是相同的。    assert logits.shape[:-1] == labels.shape    # 两行代码分别对labels和logits进行切片操作，去掉了每个序列的第一个和最后一个元素。    labels = labels[:, 1:].clone()    logits = logits[:, :-1, :]    # 这行代码创建了一个掩码loss_mask，用于标识哪些标签不是-100（即不是要被忽略的标签）。    loss_mask = labels != -100    # 这行代码将labels中值为-100的元素替换为0，因为在PyTorch中，0可以作为无效索引。    labels[labels == -100] = 0    # 计算每个token的对数概率。首先对logits应用log_softmax函数，然后在最后一个维度上使用torch.gather根据labels索引来选择对应的对数概率。    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)    # 根据average_log_prob的值决定返回值。如果average_log_prob为True，则返回每个样本的平均对数概率；否则，返回每个样本的对数概率之和    if average_log_prob:        return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)    else:        return (per_token_logps * loss_mask).sum(-1)
:::tips
函数_get_batch_logps用于计算给定标签（labels）在给定的模型输出（logits）下的对数概率（log probabilities）。
参数说明：

logits：模型的输出（未归一化的）。形状为(batch_size, sequence_length, vocab_size)，其中batch_size是批次大小，sequence_length是序列长度，vocab_size是词汇表的大小。
labels：要计算对数概率的标签。值为-100的标签标记将被忽略。形状为(batch_size, sequence_length)。
average_log_prob：一个布尔值，默认为False。如果为True，则返回每个（未被掩码的）token的平均对数概率；如果为False，则返回（未被掩码的）token的对数概率之和。

返回值：

返回一个形状为(batch_size,)的张量，包含给定标签在给定logits下的对数概率的平均值或总和。

该函数是用来计算模型预测的概率，并根据标签来确定哪些预测是有效的（即标签值不为-100）。如果average_log_prob参数为True，则函数返回的是平均对数概率；如果为False，则返回的是总和。
:::
concatenated_forward 函数
在训练中，concatenated_forward 负责对模型的 logps 进行计算：
def concatenated_forward(    model: nn.Module,    batch: Dict[str, Union[List, torch.LongTensor]],    average_log_prob: bool = False,    output_router_logits: bool = False,) -&gt; Tuple[torch.FloatTensor, torch.FloatTensor]:    concatenated_batch = concatenated_inputs(batch)    if output_router_logits:        outputs = model(            input_ids=concatenated_batch[&quot;concatenated_input_ids&quot;],            attention_mask=concatenated_batch[&quot;concatenated_attention_mask&quot;],            output_router_logits=True,        )        logits = outputs.logits.to(torch.float32)        aux_loss = outputs.aux_loss    else:        logits = model(            input_ids=concatenated_batch[&quot;concatenated_input_ids&quot;],            attention_mask=concatenated_batch[&quot;concatenated_attention_mask&quot;],        ).logits.to(torch.float32)        aux_loss = None    all_logps = _get_batch_logps(logits, concatenated_batch[&quot;concatenated_labels&quot;], average_log_prob=average_log_prob)    chosen_logps = all_logps[: batch[&quot;chosen_input_ids&quot;].shape[0]]    rejected_logps = all_logps[batch[&quot;chosen_input_ids&quot;].shape[0] :]    return chosen_logps, rejected_logps, aux_loss
参数 average_log_prob 是从主脚本dpo_tune.py中传递的，当设置为 True 时，启用长度归一化。
dpo_loss 函数
dpo_loss 中计算 logits 的差值：
pi_logratios = policy_chosen_logps - policy_rejected_logpsref_logratios = reference_chosen_logps - reference_rejected_logpslogits = pi_logratios - ref_logratios
如果 average_log_prob=True，则 policy_chosen_logps 和 policy_rejected_logps 都是归一化的值，从而影响最终的 logits 和损失计算。
控制参数
在主脚本dpo_tune.py中，通过以下代码控制是否启用长度归一化：
average_log_prob_loss_types = [&quot;simpo&quot;, &quot;dpo_norm&quot;]average_log_prob = args.dpo_loss_type in average_log_prob_loss_types
当 dpo_loss_type 设置为 &quot;dpo_norm&quot; 时，average_log_prob归一化被启用，进而在 _get_batch_logps 和 concatenated_forward 中触发长度归一化逻辑。
rlvr实践
单机4卡测试
训练数据集

.parquet转为json后的数据集样例：
&#123;&quot;messages&quot;:[&#123;&quot;content&quot;:&quot;Question: Find the domain of the expression $\\frac&#123;\\sqrt&#123;x-2&#125;&#125;&#123;\\sqrt&#123;5-x&#125;&#125;$.&#125;\nAnswer:The expressions inside each square root must be non-negative.\nTherefore, $x-2 \\ge 0$, so $x\\ge2$, and $5 - x \\ge 0$, so $x \\le 5$.\nAlso, the denominator cannot be equal to zero, so $5-x&gt;0$, which gives $x&lt;5$.\nTherefore, the domain of the expression is $\\boxed&#123;[2,5)&#125;$.\n\nQuestion: If $\\det \\mathbf&#123;A&#125; = 2$ and $\\det \\mathbf&#123;B&#125; = 12,$ then find $\\det (\\mathbf&#123;A&#125; \\mathbf&#123;B&#125;).$\nAnswer:We have that $\\det (\\mathbf&#123;A&#125; \\mathbf&#123;B&#125;) = (\\det \\mathbf&#123;A&#125;)(\\det \\mathbf&#123;B&#125;) = (2)(12) = \\boxed&#123;24&#125;.$\n\nQuestion: Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight?\nAnswer:If Terrell lifts two 20-pound weights 12 times, he lifts a total of $2\\cdot 12\\cdot20=480$ pounds of weight.  If he lifts two 15-pound weights instead for $n$ times, he will lift a total of $2\\cdot15\\cdot n=30n$ pounds of weight.  Equating this to 480 pounds, we can solve for $n$: \\begin&#123;align*&#125;\n30n&amp;=480\\\\\n\\Rightarrow\\qquad n&amp;=480\/30=\\boxed&#123;16&#125;\n\\end&#123;align*&#125;\n\nQuestion: If the system of equations\n\n\\begin&#123;align*&#125;\n6x-4y&amp;=a,\\\\\n6y-9x &amp;=b.\n\\end&#123;align*&#125;has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\frac&#123;a&#125;&#123;b&#125;,$ assuming $b$ is nonzero.\nAnswer:If we multiply the first equation by $-\\frac&#123;3&#125;&#123;2&#125;$, we obtain\n\n$$6y-9x=-\\frac&#123;3&#125;&#123;2&#125;a.$$Since we also know that $6y-9x=b$, we have\n\n$$-\\frac&#123;3&#125;&#123;2&#125;a=b\\Rightarrow\\frac&#123;a&#125;&#123;b&#125;=\\boxed&#123;-\\frac&#123;2&#125;&#123;3&#125;&#125;.$$\n\nQuestion: What is the modulo $13$ residue of $247+5 \\cdot 39 + 7 \\cdot 143 +4 \\cdot 15?$&quot;,&quot;role&quot;:&quot;user&quot;&#125;],&quot;ground_truth&quot;:&quot;8&quot;,&quot;dataset&quot;:&quot;MATH&quot;,&quot;constraint_type&quot;:null,&quot;constraint&quot;:null&#125;

验证数据集

&#123;&quot;messages&quot;:[&#123;&quot;content&quot;:&quot;Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\nAnswer:There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\n\nQuestion: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nAnswer:There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\n\nQuestion: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nAnswer:Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\n\nQuestion: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\nAnswer:Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\n\nQuestion: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\nAnswer:Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\n\nQuestion: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\nAnswer:There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\n\nQuestion: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\nAnswer:Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33.\n\nQuestion: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nAnswer:Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is 8.\n\nQuestion: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?&quot;,&quot;role&quot;:&quot;user&quot;&#125;],&quot;ground_truth&quot;:&quot;3&quot;,&quot;dataset&quot;:&quot;gsm8k&quot;&#125;&#123;&quot;messages&quot;:[&#123;&quot;content&quot;:&quot;Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\nAnswer:There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\n\nQuestion: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nAnswer:There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\n\nQuestion: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\nAnswer:Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\n\nQuestion: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\nAnswer:Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\n\nQuestion: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\nAnswer:Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\n\nQuestion: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\nAnswer:There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\n\nQuestion: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\nAnswer:Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33.\n\nQuestion: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\nAnswer:Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is 8.\n\nQuestion: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?&quot;,&quot;role&quot;:&quot;user&quot;&#125;],&quot;ground_truth&quot;:&quot;70000&quot;,&quot;dataset&quot;:&quot;gsm8k&quot;&#125;

单机脚本配置
export WANDB_MODE=disabledexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:Truecurrent_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)LOG_SAVE=&quot;/mnt/open-instruct-main/output/rlvr/$&#123;current_time&#125;-tulu3-rlvr.log&quot;python /mnt/open-instruct-main/open_instruct/ppo_vllm_thread_ray_gtrl.py \    --dataset_mixer &#x27;&#123;&quot;/mnt/open-instruct-main/data/gsm8k_math_ground_truth_mixed&quot;: 1.0&#125;&#x27; \    --dataset_train_splits train \    --dataset_eval_mixer &#x27;&#123;&quot;/mnt/open-instruct-main/data/gsm8k_math_ground_truth&quot;: 1.0&#125;&#x27; \    --dataset_eval_splits test \    --max_token_length 2048 \    --max_prompt_token_length 2048 \    --response_length 2048 \    --model_name_or_path /mnt/LLM-Research/Llama-31-Tulu-3-8B-DPO \    --reward_model_path /mnt/LLM-Research/LLama-31-Tulu3-8B-RM \    --non_stop_penalty \    --stop_token eos \    --temperature 1.0 \    --ground_truths_key ground_truth \    --chat_template tulu \    --sft_messages_key messages \    --learning_rate 3e-7 \    --total_episodes 10000000 \    --penalty_reward_value -10.0 \    --deepspeed_stage 3 \    --per_device_train_batch_size 1 \    --local_rollout_forward_batch_size 1 \    --local_mini_batch_size 16 \    --local_rollout_batch_size 16 \    --actor_num_gpus_per_node 3 \    --vllm_tensor_parallel_size 2 \    --beta 0.05 \    --apply_verifiable_reward true \    --output_dir /mnt/open-instruct-main/output/rlvr \    --seed 3 \    --num_evals 3 \    --save_freq 100 \    --reward_model_multiplier 0.0 \    --gradient_checkpointing \    --with_tracking False 2&gt;&amp;1 | tee -a &quot;$LOG_SAVE&quot;
为了尽可能地还原tulu3原论文的实验结果，其中部分超参与论文中最优保持一致。
--max_token_length 2048
--max_prompt_token_length 2048
--response_length 2048
--learning_rate 3e-7
--penalty_reward_value -10.0
--reward_model_multiplier 0.0
其中的模型权重利用tulu3开源的经过tulu3-sft dpo得到的Llama-31-Tulu-3-8B-DPO权重，奖励模型采用tulu3开源的LLama-31-Tulu3-8B-RM。
训练测试
加载ckpt时一直卡住不动，尚未解决。

]]></content>
      <categories>
        <category>模型训练调优</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>模型训练调优</tag>
        <tag>Nvidia</tag>
        <tag>Tulu3</tag>
      </tags>
  </entry>
  <entry>
    <title>2.4 Deepseek-7B微调适配</title>
    <url>/2025/11/13/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.4%20Deepseek-7B%E5%BE%AE%E8%B0%83%E9%80%82%E9%85%8D/</url>
    <content><![CDATA[微调物料及代码准备
数据集准备
/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-train.json
/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-valid.json

微调脚本
#!/bin/bashexport NCCL_P2P_DISABLE=1#export NCCL_DEBUG=INFOexport NCCL_SOCKET_IFNAME=eth0export GLOO_SOCKET_IFNAME=eth0export CUDA_DEVICE_MAX_CONNECTIONS=1export CUDA_VISIBLE_DEVICES=0,1,2,3MEGATRON_PATCH_PATH=/data/codeMEGATRON_PATH=$&#123;MEGATRON_PATCH_PATH&#125;/Megatron-LMexport PYTHONPATH=$&#123;MEGATRON_PATH&#125;:$&#123;MEGATRON_PATCH_PATH&#125;:$PYTHONPATHNNODES=2NODE_RANK=0GPUS_PER_NODE=4MASTER_ADDR=192.168.0.65MASTER_PORT=8889DISTRIBUTED_ARGS=&quot;--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT&quot;MODEL_SIZE=7BBATCH_SIZE=1GLOBAL_BATCH_SIZE=8 LR=1e-5MIN_LR=1e-6SEQ_LEN=1024PAD_LEN=1024EXTRA_VOCAB_SIZE=2400PR=fp16TP=4PP=1AC=selDO=trueFL=falseSP=trueTE=trueMOE=falseSAVE_INTERVAL=1000DATASET_PATH=/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-train.jsonVALID_DATASET_PATH=/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-valid.jsonPRETRAIN_CHECKPOINT_PATH=/data/code/temp/deepseek/deepseek-ckptTOKENIZER_PATH=/data/code/temp/deepseek/deepseek-ckptTRAIN_TOKENS=100000000   WARMUP_TOKENS=10000OUTPUT_BASEPATH=/data/code/temp/output/deepseek/finetune_outputif [ $MODEL_SIZE = 7B ]; thenNUM_LAYERS=30HIDDEN_SIZE=4096NUM_ATTN_HEADS=32INTERMEDIATE_SIZE=11008MAX_POSITION_EMBEDDINGS=4096gqa_options=&quot;&quot;fiif [ $AC = full ]; then    activation_checkpoint_options=&quot; \		    --recompute-method uniform \		    --recompute-granularity full&quot;elif [ $AC = sel ]; then    activation_checkpoint_options=&quot; \        --recompute-activations&quot;elif [ $AC = none ]; then    activation_checkpoint_options=&quot; \                    &quot;fiif [ $PR = fp16 ]; then    pr_options=&quot; \		    --fp16 \            --apply-query-key-layer-scaling&quot;    export NVTE_APPLY_QK_LAYER_SCALING=1elif [ $PR = bf16 ]; then    pr_options=&quot; \        --bf16&quot;elif [ $PR = fp8 ]; then    pr_options=&quot; \        --bf16 \        --fp8-hybrid \        --fp8-amax-compute-algo max \        --fp8-amax-history-len 1024 \        --transformer-impl transformer_engine&quot;fiif [ $DO = true ]; then    do_options=&quot; \		    --use-distributed-optimizer&quot;elif [ $DO = false ]; then    do_options=&quot; \                    &quot;fiif [ $FL = true ]; then    flash_options=&quot; \		    --use-flash-attn&quot;elif [ $FL = false ]; then    flash_options=&quot; \                    &quot;fiif [ $TE = true ]; then    te_options=&quot; \		    --transformer-impl transformer_engine&quot;elif [ $TE = false ]; then    te_options=&quot; \                    &quot;fiif [ $MOE = true ]; then    moe_options=&quot; \		    --moe-router-topk 1 \		    --num-experts 4 \		    --moe-aux-loss-coeff 1e-2 \		    --expert-model-parallel-size 2&quot;elif [ $MOE = false ]; then    moe_options=&quot; \                    &quot;fiif [ $SP = true ] &amp;&amp; [ $TP -gt 1 ]; then    sp_options=&quot; \		    --sequence-parallel&quot;elif [ $SP = false ]; then    sp_options=&quot; \                    &quot;fiif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then    load_options=&quot; \            --load $PRETRAIN_CHECKPOINT_PATH&quot;fiTRAIN_ITERS=$(( $&#123;TRAIN_TOKENS&#125; / $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))LR_WARMUP_ITERS=$(( $&#123;WARMUP_TOKENS&#125;  / $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))LR_DECAY_ITERS=$(( $&#123;TRAIN_TOKENS&#125; /  $&#123;GLOBAL_BATCH_SIZE&#125; / $&#123;SEQ_LEN&#125; ))# LR_DECAY_ITERS=$(($&#123;TRAIN_ITERS&#125; - $&#123;LR_WARMUP_ITERS&#125;))NAME=&quot;$&#123;ENV&#125;-finetune-mcore-deepseek-$&#123;MODEL_SIZE&#125;-lr-$&#123;LR&#125;-bs-$&#123;BATCH_SIZE&#125;-seqlen-$&#123;SEQ_LEN&#125;-pr-$&#123;PR&#125;-tp-$&#123;TP&#125;-pp-$&#123;PP&#125;-ac-$&#123;AC&#125;-do-$&#123;DO&#125;-sp-$&#123;SP&#125;-tt-$&#123;TRAIN_TOKENS&#125;-wt-$&#123;WARMUP_ITERS&#125;&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/log/&quot;current_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)TENSORBOARD_DIR=&quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/$&#123;NAME&#125;_$&#123;current_time&#125;&quot;mkdir -p $&#123;TENSORBOARD_DIR&#125;SAVED_PRETRAIN_CHECKPOINT_PATH=&quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/$&#123;NAME&#125;&quot;megatron_options=&quot;  \        --tokenizer-model $&#123;TOKENIZER_PATH&#125; \        --save $&#123;SAVED_PRETRAIN_CHECKPOINT_PATH&#125; \        --split 99,1,0 \        --train-data-path $&#123;DATASET_PATH&#125; \        --valid-data-path $&#123;VALID_DATASET_PATH&#125; \        --test-data-path $&#123;VALID_DATASET_PATH&#125; \        --lr $&#123;LR&#125; \        --min-lr $&#123;MIN_LR&#125; \        --lr-decay-style linear \        --adam-beta1 0.9 \        --adam-beta2 0.95 \        --weight-decay 0.1 \        --clip-grad 1.0 \        --init-method-std 0.006 \        --dataloader-type cyclic \        --lr-decay-iters $&#123;LR_DECAY_ITERS&#125; \        --lr-warmup-iters $&#123;LR_WARMUP_ITERS&#125; \        --train-iters $&#123;TRAIN_ITERS&#125; \        --micro-batch-size $&#123;BATCH_SIZE&#125; \        --global-batch-size $&#123;GLOBAL_BATCH_SIZE&#125; \        --num-layers $&#123;NUM_LAYERS&#125; \        --hidden-size $&#123;HIDDEN_SIZE&#125; \        --num-attention-heads $&#123;NUM_ATTN_HEADS&#125; \        --ffn-hidden-size $&#123;INTERMEDIATE_SIZE&#125; \        --seq-length $&#123;SEQ_LEN&#125; \        --max-position-embeddings $&#123;MAX_POSITION_EMBEDDINGS&#125; \        --max-padding-length $&#123;PAD_LEN&#125; \        --log-interval 1 \        --eval-interval 10000 \        --eval-iters 10 \        --save-interval $&#123;SAVE_INTERVAL&#125; \        --tensorboard-queue-size 1 \        --tensorboard-dir $&#123;TENSORBOARD_DIR&#125; \        --log-timers-to-tensorboard \        --log-batch-size-to-tensorboard \        --log-validation-ppl-to-tensorboard \        --tensor-model-parallel-size $&#123;TP&#125; \        --pipeline-model-parallel-size $&#123;PP&#125; \        --no-load-optim \        --no-load-rng \        --num-workers 8 \        --seed 1234 \        --extra-vocab-size $&#123;EXTRA_VOCAB_SIZE&#125; \        --patch-tokenizer-type DeepseekTokenizer \        --dataset LLama-Pretrain-Raw \        --swiglu \        --normalization RMSNorm \        --use-rotary-position-embeddings \        --no-rope-fusion \        --position-embedding-type rope \        --untie-embeddings-and-output-weights \        --disable-bias-linear \        --use-mcore-models \        --rotary-base 10000 \        --attention-dropout 0.0 \        --hidden-dropout 0.0 \        --norm-epsilon 1e-05 \        --eod-mask-loss \        --log-throughput \        --loss-scale 2048              &quot;        # --loss-scale 2048  LOG_SAVE=&quot;/data/code/temp/output/deepseek/finetune_output/log/&quot;$&#123;current_time&#125;-deepseek-finetune-$&#123;NODE_RANK&#125;.log&quot;&quot; run_cmd=&quot;torchrun $DISTRIBUTED_ARGS /data/code/examples/deepseek_core/pretrain_mcore_deepseek.py $&#123;megatron_options&#125; $&#123;pr_options&#125; $&#123;load_options&#125; $&#123;te_options&#125; $&#123;activation_checkpoint_options&#125; $&#123;do_options&#125; $&#123;flash_options&#125; $&#123;sp_options&#125; $&#123;gqa_options&#125; $&#123;moe_options&#125; &gt; $&#123;LOG_SAVE&#125; 2&gt;&amp;1&quot;echo $&#123;run_cmd&#125;eval $&#123;run_cmd&#125;set +x
微调yaml
apiVersion: apps/v1kind: DaemonSetmetadata:  name: deepseek-core  namespace: deepseekspec:  selector:    matchLabels:      app: deepseek-core  template:    metadata:      labels:        app: deepseek-core    spec:      hostNetwork: true      nodeSelector:        deepseek: deepseek-7B      containers:        - name: deepseek          image: registry.paas/library/megatron-lm:v1          imagePullPolicy: IfNotPresent          resources:            limits:              nvidia.com/gpu: 4            requests:              nvidia.com/gpu: 4          command:            - &quot;/bin/bash&quot;            - &quot;-c&quot;            - |              cd /data/code/temp/conf/deepseek_core/finetune &amp;&amp;              bash setRank.sh &amp;&amp;              cd /data/ &amp;&amp;              bash finetune_mcore_deepseek.sh          securityContext:            privileged: true          volumeMounts:            - mountPath: /data/code              name: code            - mountPath: /etc/localtime              name: localtime            - mountPath: /dev/shm              name: dshm            - name: weight              mountPath: /data/code/temp/deepseek/deepseek-ckpt/      volumes:        - name: code          hostPath:            path: /mnt/users/lihai/distribute/core/megatron-core        - name: weight          hostPath:            path: /mnt/users/wangxiangbo/model/deepseek-ai/deepseek-llm-7b-core        - name: localtime          hostPath:            path: /etc/localtime        - name: dshm          emptyDir:            medium: Memory            sizeLimit: 20G
setrank
#!/bin/bashorigin_shell=&quot;/data/code/examples/deepseek_core/finetune_mcore_deepseek.sh&quot;conf_dir=&quot;/data/code/temp/conf/&quot;local_dir=&quot;/data/&quot;shell_name=finetune_mcore_deepseek.sh## 复制脚本到/workspace下cp $origin_shell $local_dir## 读取hostfilereadarray -t ips &lt; &lt;(grep -vE &#x27;^[[:space:]]*$&#x27; &quot;$conf_dir&quot;hostfile-23)## 获取rank0 IPrank0_ip=$(echo &quot;$&#123;ips[0]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)nodes=$&#123;#ips[@]&#125;## 获取hostfile中配置的IP前缀## 使用cut提取IP地址的前三个数字部分ip_prefix=$(echo &quot;$&#123;ips[0]&#125;&quot; | cut -d &#x27;.&#x27; -f 1-3)## 获取本机IPip=$(hostname -I | grep -oE &quot;$ip_prefix\.[0-9]+&quot;)# 初始化ranknode_rank=-1# 遍历数组for i in &quot;$&#123;!ips[@]&#125;&quot;; do    # 使用tr命令去除空白字符    clean_string=$(echo &quot;$&#123;ips[$i]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)    if [[ &quot;$clean_string&quot; == &quot;$ip&quot; ]]; then        node_rank=$i        break    fidoneif [ $node_rank -ne -1 ]; then    ## 修改脚本中MASTER_ADDR    sed -i &quot;s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/&quot; $local_dir$shell_name    ## 修改NNODES    sed -i &quot;s/^NNODES=.*/NNODES=$nodes/&quot; $local_dir$shell_name    ## 修改NODE_RANK    sed -i &quot;s/^NODE_RANK=.*/NODE_RANK=$node_rank/&quot; $local_dir$shell_namefi
多机多卡拉起测试





断点续训测试
loss曲线对比
hugging-patch权重转换
patch转换脚本
#!/bin/bashset -eSTART_TIME=$SECONDSMEGATRON_PATH=/data/code/export PYTHONPATH=$PYTHONPATH:$&#123;MEGATRON_PATH&#125;:$&#123;MEGATRON_PATH&#125;/Megatron-LM-231007SOURCE_CKPT_PATH=/mnt/deepseek-ai/deepseek-llm-7b-baseTARGET_CKPT_PATH=/mnt/deepseek-ai/deepseek-llm-7b-patchTP=4PP=1MN=llama-7b #llama-7b, llama-13b, llama-30b, llama-65b, llama2-7b, llama2-13b, llama2-70bEXTRA_VOCAB_SIZE=2400mg2hf=falseif [ $mg2hf = true ]; then    do_options=&quot;                --convert_checkpoint_from_megatron_to_transformers    &quot;elif [ $mg2hf = false ]; then    do_options=&quot;&quot;fipython hf2megatron.py \    --load_path $&#123;SOURCE_CKPT_PATH&#125; \    --save_path $&#123;TARGET_CKPT_PATH&#125; \    --target_params_dtype fp16 \    --megatron-path $&#123;MEGATRON_PATH&#125; \    --target_tensor_model_parallel_size $&#123;TP&#125; \    --target_pipeline_model_parallel_size $&#123;PP&#125; \    --model_name $&#123;MN&#125; \    --extra_num_vocabs $&#123;EXTRA_VOCAB_SIZE&#125; \$&#123;do_options&#125;ELAPSED_TIME=$(($SECONDS - $START_TIME))echo &quot;$(($ELAPSED_TIME/60)) min $(($ELAPSED_TIME%60)) sec&quot;
patch权重转换过程

得到转成patch的权重

patch框架微调代码准备
patch的微调yaml
apiVersion: apps/v1kind: DaemonSetmetadata:  name: deepseek-core  namespace: deepseekspec:  selector:    matchLabels:      app: deepseek-core  template:    metadata:      labels:        app: deepseek-core    spec:      hostNetwork: true      nodeSelector:        deepseek: deepseek-7B      containers:        - name: deepseek          image: registry.paas/library/megatron-lm:v1          imagePullPolicy: IfNotPresent          resources:            limits:              nvidia.com/gpu: 4            requests:              nvidia.com/gpu: 4          command:            - &quot;/bin/bash&quot;            - &quot;-c&quot;            - |              cd /data/code/examples/deepseek/conf &amp;&amp;              bash setRank.sh &amp;&amp;              cd /data/ &amp;&amp;              bash finetune_patch_deepseek.sh          securityContext:            privileged: true          volumeMounts:            - mountPath: /data/code              name: code            - mountPath: /etc/localtime              name: localtime            - mountPath: /dev/shm              name: dshm            - name: weight              mountPath: /data/code/examples/deepseek/ckpt/      volumes:        - name: code          hostPath:            path: /mnt/users/wangxiangbo/Pai-Megatron-Patch        - name: weight          hostPath:            path: /mnt/users/wangxiangbo/model/deepseek-ai/deepseek-llm-7b-patch        - name: localtime          hostPath:            path: /etc/localtime        - name: dshm          emptyDir:            medium: Memory            sizeLimit: 20G
patch的微调脚本
#!/bin/bash#sh run_pretrain_megatron_deepseek.sh dsw /workspace/Pai-Megatron-Patch 7B 1 8 1e-5 1e-6 2048 2048 0 bf16 1 1 sel true true true false 100000 /mnt/deepseek-datasets/wudao_llamabpe_text_document /mnt/deepseek-ckpts/Llama-2-7b-hf-to-mg-tp1-pp1/ 10000000000 100000000 /mnt/output_patch_testexport NCCL_P2P_DISABLE=1#export NCCL_DEBUG=INFOexport NCCL_SOCKET_IFNAME=eth0export GLOO_SOCKET_IFNAME=eth0export CUDA_DEVICE_MAX_CONNECTIONS=1export CUDA_VISIBLE_DEVICES=0,1,2,3MEGATRON_PATCH_PATH=/data/code MEGATRON_PATH=$&#123;MEGATRON_PATCH_PATH&#125;/Megatron-LM-231007export PYTHONPATH=$&#123;MEGATRON_PATH&#125;:$&#123;MEGATRON_PATCH_PATH&#125;:$PYTHONPATHNNODES=2NODE_RANK=0GPUS_PER_NODE=4MASTER_ADDR=192.168.0.65MASTER_PORT=8889DISTRIBUTED_ARGS=&quot;--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT&quot;MODEL_SIZE=7BBATCH_SIZE=1GLOBAL_BATCH_SIZE=8LR=1e-5MIN_LR=1e-6SEQ_LEN=1024PAD_LEN=1024EXTRA_VOCAB_SIZE=4800PR=fp16TP=4PP=1AC=selDO=trueFL=falseSP=trueTE=falseSAVE_INTERVAL=1000DATASET_PATH=/data/code/examples/deepseek/dataset/alpaca_zh-deepseek-train.jsonVALID_DATASET_PATH=/data/code/examples/deepseek/dataset/alpaca_zh-deepseek-valid.jsonPRETRAIN_CHECKPOINT_PATH=/data/code/examples/deepseek/ckptTRAIN_ITERS=10000LR_WARMUP_ITERS=0OUTPUT_BASEPATH=/data/code/examples/deepseek/output/finetune_outputif [ $MODEL_SIZE = 7B ]; thenNUM_LAYERS=30HIDDEN_SIZE=4096NUM_ATTN_HEADS=32INTERMEDIATE_SIZE=11008gqa_options=&quot;&quot;fiif [ $AC = full ]; then    activation_checkpoint_options=&quot; \		    --recompute-method uniform \		    --recompute-granularity full \            --recompute-num-layers $&#123;NUM_LAYERS&#125;&quot;elif [ $AC = sel ]; then    activation_checkpoint_options=&quot; \        --recompute-activations&quot;elif [ $AC = none ]; then    activation_checkpoint_options=&quot; \                    &quot;fiif [ $PR = fp16 ]; then    pr_options=&quot; \		    --fp16&quot;elif [ $PR = bf16 ]; then    pr_options=&quot; \        --bf16&quot;elif [ $PR = fp8 ]; then    pr_options=&quot; \        --bf16        --fp8-hybrid \        --fp8-amax-compute-algo max \        --fp8-amax-history-len 1024 \        --transformer-impl transformer_engine&quot;fiif [ $DO = true ]; then    do_options=&quot; \		    --use-distributed-optimizer&quot;elif [ $DO = false ]; then    do_options=&quot; \                    &quot;fiif [ $FL = true ]; then    flash_options=&quot; \		    --use-flash-attn&quot;elif [ $FL = false ]; then    flash_options=&quot; \                    &quot;fiif [ $TE = true ]; then    te_options=&quot; \		    --transformer-impl transformer_engine&quot;elif [ $TE = false ]; then    te_options=&quot; \                    &quot;fiif [ $SP = true ] &amp;&amp; [ $TP -gt 1 ]; then    sp_options=&quot; \		    --sequence-parallel&quot;elif [ $SP = false ]; then    sp_options=&quot; \                    &quot;fiif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then    load_options=&quot; \            --load $PRETRAIN_CHECKPOINT_PATH&quot;fiLR_DECAY_ITERS=$(( $&#123;TRAIN_ITERS&#125; - $&#123;LR_WARMUP_ITERS&#125;))NAME=&quot;$&#123;ENV&#125;-finetune-patch-deepseek-$&#123;MODEL_SIZE&#125;-lr-$&#123;LR&#125;-bs-$&#123;BATCH_SIZE&#125;-seqlen-$&#123;SEQ_LEN&#125;-pr-$&#123;PR&#125;-tp-$&#123;TP&#125;-pp-$&#123;PP&#125;-ac-$&#123;AC&#125;-do-$&#123;DO&#125;-sp-$&#123;SP&#125;-tt-$&#123;TRAIN_TOKENS&#125;-wt-$&#123;WARMUP_TOKENS&#125;&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/&quot;mkdir -p &quot;$&#123;OUTPUT_BASEPATH&#125;/log/&quot;current_time=$(date &quot;+%Y.%m.%d-%H.%M.%S&quot;)TENSORBOARD_DIR=&quot;$&#123;OUTPUT_BASEPATH&#125;/tensorboard/$&#123;NAME&#125;_$&#123;current_time&#125;&quot;mkdir -p $&#123;TENSORBOARD_DIR&#125;SAVED_PRETRAIN_CHECKPOINT_PATH=&quot;$&#123;OUTPUT_BASEPATH&#125;/checkpoint/$&#123;NAME&#125;&quot;megatron_options=&quot;  \        --save $&#123;SAVED_PRETRAIN_CHECKPOINT_PATH&#125; \        --split 99,1,0 \        --train-data-path $&#123;DATASET_PATH&#125; \        --valid-data-path $&#123;VALID_DATASET_PATH&#125; \        --test-data-path $&#123;VALID_DATASET_PATH&#125; \        --lr $&#123;LR&#125; \        --min-lr $&#123;MIN_LR&#125; \        --lr-decay-style linear \        --adam-beta1 0.9 \        --adam-beta2 0.95 \        --weight-decay 0.1 \        --clip-grad 1.0 \        --init-method-std 0.006 \        --dataloader-type cyclic \        --lr-decay-iters $&#123;LR_DECAY_ITERS&#125; \        --lr-warmup-iters $&#123;LR_WARMUP_ITERS&#125; \        --train-iters $&#123;TRAIN_ITERS&#125; \        --micro-batch-size $&#123;BATCH_SIZE&#125; \        --global-batch-size $&#123;GLOBAL_BATCH_SIZE&#125; \        --num-layers $&#123;NUM_LAYERS&#125; \        --hidden-size $&#123;HIDDEN_SIZE&#125; \        --num-attention-heads $&#123;NUM_ATTN_HEADS&#125; \        --ffn-hidden-size $&#123;INTERMEDIATE_SIZE&#125; \        --seq-length $&#123;SEQ_LEN&#125; \        --max-position-embeddings $&#123;SEQ_LEN&#125; \        --log-interval 1 \        --eval-interval 10000 \        --eval-iters 10 \        --save-interval $&#123;SAVE_INTERVAL&#125; \        --tensorboard-queue-size 1 \        --tensorboard-dir $&#123;TENSORBOARD_DIR&#125; \        --log-timers-to-tensorboard \        --log-batch-size-to-tensorboard \        --log-validation-ppl-to-tensorboard \        --tensor-model-parallel-size $&#123;TP&#125; \        --pipeline-model-parallel-size $&#123;PP&#125; \        --dataset LLama-Pretrain-Raw \        --no-save-optim \        --no-load-optim \        --no-load-rng \        --num-workers 8 \        --seed 1234 \        --max-padding-length $&#123;PAD_LEN&#125; \        --extra-vocab-size $&#123;EXTRA_VOCAB_SIZE&#125; \        --patch-tokenizer-type LLamaTokenizer \        --swiglu \        --normalization RMSNorm \        --use-llama2-rotary-position-embeddings \        --position-embedding-type rope \        --untie-embeddings-and-output-weights \        --rotary-base 10000 \        --rotary-scale-factor 4 \        --loss-scale 2048 \        --disable-bias-linear        &quot;LOG_SAVE=&quot;/data/code/examples/deepseek/output/finetune_output/log/&quot;$&#123;current_time&#125;-deepseek-patch-$&#123;NODE_RANK&#125;.log&quot;&quot; run_cmd=&quot;torchrun $DISTRIBUTED_ARGS /data/code/examples/llama2/pretrain_megatron_llama.py $&#123;megatron_options&#125; $&#123;pr_options&#125; $&#123;load_options&#125; $&#123;te_options&#125; $&#123;activation_checkpoint_options&#125; $&#123;do_options&#125; $&#123;flash_options&#125; $&#123;sp_options&#125; $&#123;gqa_options&#125; &gt; $&#123;LOG_SAVE&#125; 2&gt;&amp;1&quot;echo $&#123;run_cmd&#125;eval $&#123;run_cmd&#125;set +x
patch的微调setrank
#!/bin/bashorigin_shell=&quot;/data/code/examples/deepseek/finetune_patch_deepseek.sh&quot;conf_dir=&quot;/data/code/examples/deepseek/conf/&quot;local_dir=&quot;/data/&quot;shell_name=finetune_patch_deepseek.sh## 复制脚本到/workspace下cp $origin_shell $local_dir## 读取hostfilereadarray -t ips &lt; &lt;(grep -vE &#x27;^[[:space:]]*$&#x27; &quot;$conf_dir&quot;hostfile)## 获取rank0 IPrank0_ip=$(echo &quot;$&#123;ips[0]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)nodes=$&#123;#ips[@]&#125;## 获取hostfile中配置的IP前缀## 使用cut提取IP地址的前三个数字部分ip_prefix=$(echo &quot;$&#123;ips[0]&#125;&quot; | cut -d &#x27;.&#x27; -f 1-3)## 获取本机IPip=$(hostname -I | grep -oE &quot;$ip_prefix\.[0-9]+&quot;)# 初始化ranknode_rank=-1# 遍历数组for i in &quot;$&#123;!ips[@]&#125;&quot;; do    # 使用tr命令去除空白字符    clean_string=$(echo &quot;$&#123;ips[$i]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)    if [[ &quot;$clean_string&quot; == &quot;$ip&quot; ]]; then        node_rank=$i        break    fidoneif [ $node_rank -ne -1 ]; then    ## 修改脚本中MASTER_ADDR    sed -i &quot;s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/&quot; $local_dir$shell_name    ## 修改NNODES    sed -i &quot;s/^NNODES=.*/NNODES=$nodes/&quot; $local_dir$shell_name    ## 修改NODE_RANK    sed -i &quot;s/^NODE_RANK=.*/NODE_RANK=$node_rank/&quot; $local_dir$shell_namefi
patch框架微调测试


]]></content>
      <categories>
        <category>模型适配</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>模型适配</tag>
        <tag>Nvidia</tag>
        <tag>Deepseek</tag>
        <tag>Megatron-LM</tag>
      </tags>
  </entry>
  <entry>
    <title>2.3 Deepseek-7B权重转换</title>
    <url>/2025/11/13/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.3%20Deepseek-7B%E6%9D%83%E9%87%8D%E8%BD%AC%E6%8D%A2/</url>
    <content><![CDATA[Hg-Core权重转换
huggingface开源权重准备

权重转换脚本
#!/bin/bashexport CUDA_VISIBLE_DEVICES=0export CUDA_DEVICE_MAX_CONNECTIONS=1START_TIME=$SECONDSMASTER_ADDR=localhostMASTER_PORT=6666MODEL_SIZE=7BHG_CKPT_PATH=/mnt/model/deepseek-ai/deepseek-llm-7b-baseMEGATRON_PATCH_PATH=/data/code/MEGATRON_PATH=$&#123;MEGATRON_PATCH_PATH&#125;/Megatron-LMexport PYTHONPATH=$&#123;MEGATRON_PATH&#125;:$&#123;MEGATRON_PATCH_PATH&#125;:$PYTHONPATHSOURCE_CKPT_PATH=/mnt/model/deepseek-ai/deepseek-llm-7b-baseTARGET_CKPT_PATH=/mnt/model/deepseek-ai/deepseek-llm-7b-coreTP=4PP=1EXTRA_VOCAB_SIZE=2400NUM_EXPERTS=0EXPERTS_TOPK=0EP=0NUM_EXPERT_SPLITS=0mg2hf=falseif [ $MODEL_SIZE = 7B ]; thenNUM_LAYERS=30HIDDEN_SIZE=4096NUM_ATTN_HEADS=32INTERMEDIATE_SIZE=11008NUM_KV_HEADS=32VOCAB_SIZE=102400ROPE_THETA=10000RMS_NORM_EPS=1e-6gqa_options=&quot;&quot;fiif [ $NUM_EXPERT_SPLITS -gt 0 ]; thenINTERMEDIATE_SIZE=$(( $&#123;INTERMEDIATE_SIZE&#125; / $&#123;NUM_EXPERT_SPLITS&#125;))fiif [ $NUM_EXPERTS -gt 0 ]; then    expert_options=&quot;                --moe-router-topk $&#123;EXPERTS_TOPK&#125; \                --num-experts $&#123;NUM_EXPERTS&#125; \                --expert-model-parallel-size 1 \                --target_expert_model_parallel_size $&#123;EP&#125; \                --num_expert_split_size $&#123;NUM_EXPERT_SPLITS&#125; \    &quot;fiif [ $mg2hf = true ]; then    convert_options=&quot;                --convert_checkpoint_from_megatron_to_transformers    &quot;elif [ $mg2hf = false ]; then    convert_options=&quot;&quot;fiDISTRIBUTED_ARGS=&quot;--nproc_per_node 1 --nnodes 1 --node_rank 0 --master_addr $MASTER_ADDR --master_port $MASTER_PORT&quot;if [ $MODEL_SIZE != 70B ]; thentorchrun $&#123;DISTRIBUTED_ARGS&#125; hf2mcore.py \    --load_path $&#123;SOURCE_CKPT_PATH&#125; \    --save_path $&#123;TARGET_CKPT_PATH&#125; \    --tokenizer-model $&#123;HG_CKPT_PATH&#125; \    --huggingface_model_path $&#123;HG_CKPT_PATH&#125; \    --megatron-path $&#123;MEGATRON_PATH&#125; \    --target_tensor_model_parallel_size $&#123;TP&#125; \    --target_pipeline_model_parallel_size $&#123;PP&#125; \    --micro-batch-size 1 \    --fp16 \    --swiglu \    --num-layers $&#123;NUM_LAYERS&#125; \    --hidden-size $&#123;HIDDEN_SIZE&#125; \    --ffn-hidden-size $&#123;INTERMEDIATE_SIZE&#125; \    --norm-epsilon $&#123;RMS_NORM_EPS&#125;\    --num-attention-heads $&#123;NUM_ATTN_HEADS&#125; \    --max-position-embeddings 4096 \    --seq-length 1 \    --no-async-tensor-model-parallel-allreduce \    --patch-tokenizer-type DeepseekTokenizer \    --extra-vocab-size $&#123;EXTRA_VOCAB_SIZE&#125; \    --untie-embeddings-and-output-weights \    --no-rope-fusion \    --use-rotary-position-embeddings \    --rotary-base $&#123;ROPE_THETA&#125; \    --transformer-impl transformer_engine \    --disable-bias-linear \    --normalization RMSNorm \    --use-mcore-models \    --attention-dropout 0.0 \    --hidden-dropout 0.0 \    $&#123;expert_options&#125; \    $&#123;convert_options&#125; \    $&#123;gqa_options&#125;elsepython hf2mcore_70b.py \  --load $&#123;HG_CKPT_PATH&#125; \  --megatron-path $&#123;MEGATRON_PATH&#125; \  --load_path $&#123;SOURCE_CKPT_PATH&#125; \  --save_path $&#123;TARGET_CKPT_PATH&#125; \  --target_params_dtype bf16 \  --target_tensor_model_parallel_size $&#123;TP&#125; \  --target_pipeline_model_parallel_size $&#123;PP&#125; \$&#123;convert_options&#125; \fiELAPSED_TIME=$(($SECONDS - $START_TIME))echo &quot;$(($ELAPSED_TIME/60)) min $(($ELAPSED_TIME%60)) sec&quot;
tokenizer配置
在/data/code/megatron_patch/tokenizer/init.py中加入Deepseek配置
elif args.patch_tokenizer_type == &#x27;DeepseekTokenizer&#x27;:        from megatron.core.datasets.megatron_tokenizer import MegatronTokenizer        class _DeepseekTokenizer(MegatronTokenizer):            def __init__(self, tokenizer_path, extra_vocab_size):                super().__init__(tokenizer_path)                self.tokenizer = AutoTokenizer.from_pretrained(                    tokenizer_path,                    padding_side=&quot;right&quot;,                    use_fast=False,                    trust_remote_code=True,                )                self.extra_vocab_size = extra_vocab_size            def __call__(self, text, return_tensors=None,                         padding=None, max_length=None, truncation=None, add_special_tokens=True):                return self.tokenizer(text, return_tensors=return_tensors, padding=padding,                        max_length=max_length, truncation=truncation, add_special_tokens=add_special_tokens)            @property            def vocab_size(self):                return self.tokenizer.vocab_size + self.extra_vocab_size            @property            def vocab(self):                return self.tokenizer.encoder            @property            def inv_vocab(self):                return self.tokenizer.decoder            def tokenize(self, text):                return self.tokenizer.encode(text)            def detokenize(self, token_ids):                return self.tokenizer.decode(token_ids)            @property            def eod(self):                return self.tokenizer.eos_token_id            @property            def eos_token(self):                return self.tokenizer.eos_token            @property            def pad_token_id(self):                return self.tokenizer.pad_token_id            @property            def eos_token_id(self):                return self.tokenizer.eos_token_id        tokenizer = _DeepseekTokenizer(args.tokenizer_model, args.extra_vocab_size)        # tokenizer = _DeepseekTokenizer(args.load, args.extra_vocab_size)        args.padded_vocab_size = tokenizer.vocab_size
hf2mcore.py修改
在/data/code/toolkits/model_checkpoints_convertor/deepseek_core/hf2mcore.py中修改导包位置

执行转换脚本，转换流程


转换结束，得到core权重

core权重的推理结果验证
core权重推理结果

huggingface权重推理结果

]]></content>
      <categories>
        <category>模型适配</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>模型适配</tag>
        <tag>Nvidia</tag>
        <tag>Deepseek</tag>
        <tag>Megatron-LM</tag>
      </tags>
  </entry>
  <entry>
    <title>2.5 pytorchjob+eki断点续训</title>
    <url>/2025/11/13/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.5%20pytorchjob+eki%E6%96%AD%E7%82%B9%E7%BB%AD%E8%AE%AD/</url>
    <content><![CDATA[环境准备
pytorchjob训练环境已由小组同事搭建完成

代码准备
断点续训yaml
需要注意的是，不能设定自定义的namespace，需要在default下启动pod
apiVersion: &quot;ai.cmss.chinamobile.com/v1alpha1&quot;kind: PyTorchJobmetadata:  name: deepseek-core  labels:    service: deepseek-corespec:  nprocPerNode: &quot;4&quot;  pytorchReplicaSpecs:    Master:      replicas: 1      template:        metadata:          labels:            app: ckpt        spec:          nodeSelector:            model-ckpt: deepseek-master          restartPolicy: Always          hostNetwork: true          dnsPolicy: ClusterFirstWithHostNet          affinity:            podAntiAffinity:              requiredDuringSchedulingIgnoredDuringExecution:                - labelSelector:                    matchExpressions:                      - key: app                        operator: In                        values:                          - ckpt                  topologyKey: &quot;kubernetes.io/hostname&quot;          containers:            - name: pytorch              image: registry.paas/library/megatron-lm:v1              imagePullPolicy: IfNotPresent              securityContext:                capabilities:                  add:                    - SYS_ADMIN              resources:                limits:                  nvidia.com/gpu: 4 # Request access to all GPU                requests:                  nvidia.com/gpu: 4              env:                - name: OMP_NUM_THREADS                  value: &quot;4&quot;              command:                - &quot;/bin/bash&quot;                - &quot;-c&quot;                - |                  cd /data/code/temp/conf/deepseek_core/pretrain &amp;&amp;                  bash SetRank.sh &amp;&amp;                  cd /data/ &amp;&amp;                  bash pretrain_ckpt_deepseek.sh              volumeMounts:                - mountPath: /data/code                  readOnly: false                  name: code                - name: shm-volume                  mountPath: /dev/shm                - name: pod-gpu-resources                  mountPath: /tmp                  readOnly: false          volumes:            - name: code              hostPath:                path: /mnt/users/lihai/distribute/core/megatron-core            - name: shm-volume              emptyDir:                medium: Memory                sizeLimit: 1G            - name: pod-gpu-resources              hostPath:                path: /var/lib/kubelet/pod-resources          schedulerName: ai-scheduler    Worker:      replicas: 1      restartPolicy: OnFailure      template:        metadata:          labels:            app: ckpt        spec:          nodeSelector:            model-ckpt: deepseek-worker          restartPolicy: Always          hostNetwork: true          dnsPolicy:            ClusterFirstWithHostNet            #affinty:            #podAntiAffinity:            # requiredDuringSchedulingIgnoredDuringExecution:            # - labelSelector:            #     matchExpressions:            #     - key: app            #       operator: In            #       values:            #       - ckpt            #   topologyKey: &quot;kubernetes.io/hostname&quot;          containers:            - name: pytorch              image: registry.paas/library/megatron-lm:v1              imagePullPolicy: IfNotPresent              # workingDir: /workspace/model/glm3/chatglm3/finetune_demo              securityContext:                capabilities:                  add:                    - SYS_ADMIN              resources:                limits:                  nvidia.com/gpu: 4 # Request access to all GPU                requests:                  nvidia.com/gpu: 4              env:                - name: OMP_NUM_THREADS                  value: &quot;4&quot;              command:                - &quot;/bin/bash&quot;                - &quot;-c&quot;                - |                  cd /data/code/temp/conf/deepseek_core/pretrain &amp;&amp;                  bash SetRank.sh &amp;&amp;                  cd /data/ &amp;&amp;                  bash pretrain_ckpt_deepseek.sh              volumeMounts:                - mountPath: /data/code                  readOnly: false                  name: code                - name: shm-volume                  mountPath: /dev/shm                - name: pod-gpu-resources                  mountPath: /tmp                  readOnly: false          volumes:            - name: code              hostPath:                path: /mnt/users/lihai/distribute/core/megatron-core                type: Directory            - name: shm-volume              emptyDir:                medium: Memory                sizeLimit: 1G            - name: pod-gpu-resources              hostPath:                path: /var/lib/kubelet/pod-resources          schedulerName: ai-scheduler
setrank
#!/bin/bashorigin_shell=&quot;/data/code/examples/deepseek_core/pretrain_ckpt_deepseek.sh&quot;conf_dir=&quot;/data/code/temp/conf/&quot;local_dir=&quot;/data/&quot;shell_name=pretrain_ckpt_deepseek.sh## 复制脚本到/workspace下cp $origin_shell $local_dir## 读取hostfilereadarray -t ips &lt; &lt;(grep -vE &#x27;^[[:space:]]*$&#x27; &quot;$conf_dir&quot;hostfile-236)## 获取rank0 IPrank0_ip=$(echo &quot;$&#123;ips[0]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)nodes=$&#123;#ips[@]&#125;## 获取hostfile中配置的IP前缀## 使用cut提取IP地址的前三个数字部分ip_prefix=$(echo &quot;$&#123;ips[0]&#125;&quot; | cut -d &#x27;.&#x27; -f 1-3)## 获取本机IPip=$(hostname -I | grep -oE &quot;$ip_prefix\.[0-9]+&quot;)# 初始化ranknode_rank=-1# 打印调试信息echo &quot;本机IP: $ip&quot;echo &quot;rank0 IP: $rank0_ip&quot;echo &quot;所有节点IP: $&#123;ips[@]&#125;&quot;# 遍历数组# for i in &quot;$&#123;!ips[@]&#125;&quot;; do#     # 使用tr命令去除空白字符#     clean_string=$(echo &quot;$&#123;ips[$i]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)#     if [[ &quot;$clean_string&quot; == &quot;$ip&quot; ]]; then#         node_rank=$i#         break#     fi# done# TODO 测试断点续训for i in &quot;$&#123;!ips[@]&#125;&quot;; do    clean_string=$(echo &quot;$&#123;ips[$i]&#125;&quot; | tr -d &#x27;[:space:]&#x27;)    if [[ &quot;$clean_string&quot; == &quot;$ip&quot; ]]; then        if [[ &quot;$ip&quot; == &quot;$rank0_ip&quot; ]]; then            node_rank=0        else            node_rank=1        fi        break    fidone# 打印调试信息echo &quot;设置的NODE_RANK: $node_rank&quot;if [ $node_rank -ne -1 ]; then    ## 修改脚本中MASTER_ADDR    sed -i &quot;s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/&quot; $local_dir$shell_name    ## 修改NNODES    # sed -i &quot;s/^NNODES=.*/NNODES=$nodes/&quot; $local_dir$shell_name    ## 修改NODE_RANK    sed -i &quot;s/^NODE_RANK=.*/NODE_RANK=$node_rank/&quot; $local_dir$shell_namefi
hostfile
多机节点故障需要选取3个节点来模拟，其中2台用来拉起训练，之后模拟其中1台发生故障，之后再调度到另一个备用机器上继续训练。
其中将192.168.0.65作为master节点，192.168.0.20和192.168.0.63作为worker节点。
192.168.0.65192.168.0.20192.168.0.63
节点故障实践
训练拉起
kubectl apply -f deepseek-ckpt.yamlkubectl get node -owidekubectl get pod -owide

可以看出训练再在master节点192.168.0.65和worker节点192.168.0.63上拉起。

ckpt保存
持续训练，并在interval50步保存ckpt信息，之后继续训练


模拟节点故障
进入192.168.0.63机器，模拟节点故障，停掉k8s服务
systemctl stop kubelet

停掉后在k8s的master节点查看node状态，是否为NotReady

此时192.168.0.63已经出现故障，再去查看拉起训练的pod信息
由于192.168.0.63出现故障，pytorchjob会根据hostfile的节点信息，再重新尝试调度到另外的机器上192.168.0.20，可以看出在192.168.0.20重新进行了Init、PodInitializing、Running的过程。


查看续训log


至此，节点故障断点续训实践完成。
模拟完成后，重新将192.168.0.63节点恢复
systemctl start kubelet


]]></content>
      <categories>
        <category>模型适配</category>
        <category>NVIDIA</category>
      </categories>
      <tags>
        <tag>V100</tag>
        <tag>模型适配</tag>
        <tag>Nvidia</tag>
        <tag>Deepseek</tag>
        <tag>Megatron-LM</tag>
      </tags>
  </entry>
</search>
