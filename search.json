[{"title":"2.1 数据集处理","url":"/2025/11/13/模型训练调优/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.1 Deepseek-7B数据集处理/","content":"#### 下载初始数据集\n1. <font style=\"color:rgb(31, 35, 40);\">下载WuDaoCorpora2.0开源数据集到/mnt/workspace/llama3-datasets工作目录下</font>\n\n```bash\nwget https://atp-modelzoo.oss-cn-hangzhou.aliyuncs.com/release/datasets/WuDaoCorpus2.0_base_sample.tgz\ntar zxvf WuDaoCorpus2.0_base_sample.tgz \n```\n\n#### <font style=\"color:rgb(31, 35, 40);\">Megatron-LM预训练数据清洗</font>\n<font style=\"color:rgb(31, 35, 40);\">mmap数据是一种预先执行tokenize处理的数据格式，可以极大减少训练微调过程中等待数据读入的时间，当数据量极大时，优势显著。</font>\n\n```bash\n#! /bin/bash\nset -ex\n# 请在此处设置原始数据所在路径\ndata_dir=/mnt/users/wangxiangbo/dataset/WuDaoCorpus2.0_base_sample\n\n#开始数据清洗流程\ndataset_dir=$(dirname $data_dir)\nmkdir -p ${dataset_dir}/cleaned_wudao_dataset\ncd ${dataset_dir}/cleaned_wudao_dataset\nwget https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/llama2-codes/preprocess_wudao2.py\n# 此处与上一节不同，增加了key参数设为text\npython3 preprocess_wudao2.py -i ${data_dir} -o ${dataset_dir}/cleaned_wudao_dataset -k text -p 32\n\n# 合并清洗后的数据\nmkdir ${dataset_dir}/wudao\ncd ${dataset_dir}/wudao\nfind ${dataset_dir}/cleaned_wudao_dataset -name \"*.json\" -exec cat {} + > ${dataset_dir}/wudao/merged_wudao_cleaned.json\nrm -rf ${dataset_dir}/cleaned_wudao_dataset\n```\n\n脚本执行完成后，会得到一个合并的json数据集格式\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image1.png)\n\n#### <font style=\"color:rgb(31, 35, 40);\">执行下方的处理数据脚本</font>\n```bash\n#! /bin/bash\nSTART_TIME=$SECONDS\n\nMEGATRON_PATCH_PATH=/data/code\nMEGATRON_PATH=${MEGATRON_PATCH_PATH}/Megatron-LM\n\nexport PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATCH_PATH}:$PYTHONPATH\n\ninput_data_dir=/mnt/dataset/wudao/merged_wudao_cleaned.json\ntokenizer=DeepseekTokenizer\njson_keys=text\noutput_data_dir=/data/code/temp/deepseek/deepseek-datasets\nload_dir=/data/code/temp/deepseek/deepseek-ckpt\nTOKENIZER_PATH=/data/code/temp/deepseek/deepseek-ckpt\n\nINPUT=\"${input_data_dir}\"\n\nif [ $tokenizer = \"Qwen2Tokenizer\" ]; then\n  python preprocess_data_megatron.py \\\n  --input ${INPUT} \\\n  --output-prefix ${output_data_dir}/mmap_qwen2_datasets \\\n  --patch-tokenizer-type Qwen2Tokenizer \\\n  --json-keys ${json_keys} \\\n  --load ${load_dir} \\\n  --workers 2 \\\n  --partitions 2 \\\n  --keep-sequential-samples \\\n  --append-eod\n\nelif [ $tokenizer = \"DeepSeekV2Tokenizer\" ]; then\n  python preprocess_data_megatron.py \\\n  --input ${INPUT} \\\n  --output-prefix ${output_data_dir}/mmap_deepseekv2_datasets \\\n  --patch-tokenizer-type DeepSeekV2Tokenizer \\\n  --json-keys ${json_keys} \\\n  --load ${load_dir} \\\n  --workers 8 \\\n  --partitions 1 \\\n  --keep-sequential-samples \\\n  --append-eod\n\nelif [ $tokenizer = \"LLamaTokenizer\" ]; then\n  python preprocess_data_megatron.py \\\n  --tokenizer-model $TOKENIZER_PATH \\\n  --input ${INPUT} \\\n  --output-prefix ${output_data_dir}/mmap_llama_datasets \\\n  --patch-tokenizer-type LLamaTokenizer \\\n  --load ${load_dir} \\\n  --workers 16 \\\n  --partitions 1 \\\n  --keep-sequential-samples \\\n  --append-eod\n\nelif [ $tokenizer = \"DeepseekTokenizer\" ]; then\n  python preprocess_data_megatron.py \\\n  --tokenizer-model $TOKENIZER_PATH \\\n  --input ${INPUT} \\\n  --output-prefix ${output_data_dir}/mmap_deepseek_datasets \\\n  --patch-tokenizer-type LLamaTokenizer \\\n  --load ${load_dir} \\\n  --workers 16 \\\n  --partitions 1 \\\n  --keep-sequential-samples \\\n  --append-eod\n\nelif [ $tokenizer = \"LLama2Tokenizer\" ]; then\n  python preprocess_data_megatron.py \\\n  --tokenizer-model $TOKENIZER_PATH \\\n  --extra-vocab-size 0 \\\n  --input ${INPUT} \\\n  --output-prefix ${output_data_dir}/mmap_llama2_datasets \\\n  --patch-tokenizer-type LLama2Tokenizer \\\n  --load ${load_dir} \\\n  --workers 16 \\\n  --partitions 1 \\\n  --keep-sequential-samples \\\n  --append-eod\n\nelif [ $tokenizer = \"LLama3Tokenizer\" ]; then\n  python preprocess_data_megatron.py \\\n  --input ${INPUT} \\\n  --output-prefix ${output_data_dir}/mmap_llama3_datasets \\\n  --patch-tokenizer-type LLama3Tokenizer \\\n  --load ${load_dir} \\\n  --workers 16 \\\n  --partitions 1 \\\n  --keep-sequential-samples \\\n  --append-eod\n\nfi\n\nELAPSED_TIME=$(($SECONDS - $START_TIME))\necho \"$(($ELAPSED_TIME/60)) min $(($ELAPSED_TIME%60)) sec\"\n```\n\n在python preprocess_data_megatron.py中，加入DeepseekTokenizer，此外还需要在/data/code/megatron_patch/tokenizer/__init__.py中加入DeepseekTokenizer配置。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image2.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image3.png)\n\n#### 数据处理完成\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image4.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image5.png)\n\n","tags":["模型适配","Nvidia","Deepseek","V100","Megatron-LM"],"categories":["模型适配","NVIDIA"]},{"title":"2.2 Deepseek-7B预训练适配","url":"/2025/11/13/模型训练调优/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.2 Deepseek-7B预训练适配/","content":"#### 预训练物料及代码准备\n##### Deepseek-7B的Tokenizer下载\n位置位于/data/code/temp/deepseek/deepseek-ckpt\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image6.png)\n\n##### 数据集准备\n位置位于/data/code/temp/deepseek/deepseek-datasets\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image7.png)\n\n##### pretrain_mcore_deepseek.sh预训练脚本\n```bash\n\nexport NCCL_P2P_DISABLE=1\nexport NCCL_DEBUG=INFO\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\n\nMEGATRON_PATCH_PATH=/data/code \nMEGATRON_PATH=${MEGATRON_PATCH_PATH}/Megatron-LM\n\nexport PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATCH_PATH}:$PYTHONPATH\n\nNNODES=2\nNODE_RANK=0\nGPUS_PER_NODE=4\nMASTER_ADDR=192.168.0.22\nMASTER_PORT=29500\n\nDISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT\"\n\nMODEL_SIZE=7B\nBATCH_SIZE=1\nGLOBAL_BATCH_SIZE=8\nLR=1e-5\nMIN_LR=1e-6\nSEQ_LEN=1024\nPAD_LEN=1024\nEXTRA_VOCAB_SIZE=0\nPR=fp16\nTP=4\nPP=1\nAC=sel\nDO=true\nFL=false\nSP=true\nTE=true\nMOE=false\nSAVE_INTERVAL=1000\nDATASET_PATH=/data/code/temp/deepseek/deepseek-datasets/mmap_deepseek_datasets_text_document\n#PRETRAIN_CHECKPOINT_PATH=none\nTOKENIZER_PATH=/data/code/temp/deepseek/deepseek-ckpt\nTRAIN_TOKENS=10000000\nWARMUP_TOKENS=0\nOUTPUT_BASEPATH=/data/code/temp/output/deepseek/pretrain_output\n\nif [ $MODEL_SIZE = 7B ]; then\n\nNUM_LAYERS=32\nHIDDEN_SIZE=4096\nNUM_ATTN_HEADS=32\nINTERMEDIATE_SIZE=11008\nMAX_POSITION_EMBEDDINGS=4096\n\ngqa_options=\"\"\n\n\nfi\n\nif [ $AC = full ]; then\n    activation_checkpoint_options=\" \\\n\t\t    --recompute-method uniform \\\n\t\t    --recompute-granularity full\"\nelif [ $AC = sel ]; then\n    activation_checkpoint_options=\" \\\n        --recompute-activations\"\nelif [ $AC = none ]; then\n    activation_checkpoint_options=\" \\\n    \"\nfi\n\nif [ $PR = fp16 ]; then\n    pr_options=\" \\\n\t\t    --fp16 \\\n            --apply-query-key-layer-scaling\"\n    export NVTE_APPLY_QK_LAYER_SCALING=1\nelif [ $PR = bf16 ]; then\n    pr_options=\" \\\n        --bf16\"\nelif [ $PR = fp8 ]; then\n    pr_options=\" \\\n        --bf16 \\\n        --fp8-hybrid \\\n        --fp8-amax-compute-algo max \\\n        --fp8-amax-history-len 1024 \\\n        --transformer-impl transformer_engine\"\nfi\n\nif [ $DO = true ]; then\n    do_options=\" \\\n\t\t    --use-distributed-optimizer\"\n\nelif [ $DO = false ]; then\n    do_options=\" \\\n                    \"\nfi\n\nif [ $FL = true ]; then\n    flash_options=\" \\\n\t\t    --use-flash-attn\"\n\nelif [ $FL = false ]; then\n    flash_options=\" \\\n                    \"\nfi\n\nif [ $TE = true ]; then\n    te_options=\" \\\n\t\t    --transformer-impl transformer_engine\"\n\nelif [ $TE = false ]; then\n    te_options=\" \\\n                    \"\nfi\n\n\nif [ $MOE = true ]; then\n    moe_options=\" \\\n\t\t    --moe-router-topk 1 \\\n\t\t    --num-experts 4 \\\n\t\t    --expert-model-parallel-size 2\"\n\nelif [ $MOE = false ]; then\n    moe_options=\" \\\n                    \"\nfi\n\nif [ $SP = true ] && [ $TP -gt 1 ]; then\n    sp_options=\" \\\n\t\t    --sequence-parallel\"\n\nelif [ $SP = false ]; then\n    sp_options=\" \\\n                    \"\nfi\n\nif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then\n    load_options=\" \\\n            --load $PRETRAIN_CHECKPOINT_PATH\"\nfi\n\nTRAIN_ITERS=$(( ${TRAIN_TOKENS} / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\nLR_WARMUP_ITERS=$(( ${WARMUP_TOKENS}  / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\nLR_DECAY_ITERS=$(( ${TRAIN_TOKENS} /  ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\n\nNAME=\"${ENV}-pretrain-mcore-deepseek-${MODEL_SIZE}-lr-${LR}-bs-${BATCH_SIZE}-seqlen-${SEQ_LEN}-pr-${PR}-tp-${TP}-pp-${PP}-ac-${AC}-do-${DO}-sp-${SP}-moe-${MOE}-tt-${TRAIN_TOKENS}-wt-${WARMUP_TOKENS}\"\nmkdir -p \"${OUTPUT_BASEPATH}/tensorboard/\"\nmkdir -p \"${OUTPUT_BASEPATH}/checkpoint/\"\nmkdir -p \"${OUTPUT_BASEPATH}/log/\"\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nTENSORBOARD_DIR=\"${OUTPUT_BASEPATH}/tensorboard/${NAME}_${current_time}\"\nmkdir -p ${TENSORBOARD_DIR}\n\nSAVED_PRETRAIN_CHECKPOINT_PATH=\"${OUTPUT_BASEPATH}/checkpoint/${NAME}\"\n\nmegatron_options=\"  \\\n        --tokenizer-model $TOKENIZER_PATH \\\n        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \\\n        --data-path ${DATASET_PATH} \\\n        --lr ${LR} \\\n        --min-lr ${MIN_LR} \\\n        --lr-decay-style linear \\\n        --adam-beta1 0.9 \\\n        --adam-beta2 0.95 \\\n        --weight-decay 0.1 \\\n        --clip-grad 1.0 \\\n        --init-method-std 0.006 \\\n        --attention-dropout 0.0 \\\n        --hidden-dropout 0.0  \\\n        --lr-decay-iters ${LR_DECAY_ITERS} \\\n        --lr-warmup-iters ${LR_WARMUP_ITERS} \\\n        --train-iters ${TRAIN_ITERS} \\\n        --split 99,1,0 \\\n        --micro-batch-size ${BATCH_SIZE} \\\n        --global-batch-size ${GLOBAL_BATCH_SIZE} \\\n        --num-layers ${NUM_LAYERS} \\\n        --hidden-size ${HIDDEN_SIZE} \\\n        --num-attention-heads ${NUM_ATTN_HEADS} \\\n        --ffn-hidden-size ${INTERMEDIATE_SIZE} \\\n        --seq-length ${SEQ_LEN} \\\n        --max-position-embeddings ${MAX_POSITION_EMBEDDINGS} \\\n        --max-padding-length ${PAD_LEN} \\\n        --log-throughput \\\n        --log-interval 1 \\\n        --eval-interval 10000 \\\n        --eval-iters 10 \\\n        --save-interval ${SAVE_INTERVAL} \\\n        --tensorboard-queue-size 1 \\\n        --tensorboard-dir ${TENSORBOARD_DIR} \\\n        --log-timers-to-tensorboard \\\n        --log-batch-size-to-tensorboard \\\n        --log-validation-ppl-to-tensorboard \\\n        --tensor-model-parallel-size ${TP} \\\n        --pipeline-model-parallel-size ${PP} \\\n        --no-load-optim \\\n        --no-load-rng \\\n        --num-workers 8 \\\n        --seed 1234 \\\n        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \\\n        --patch-tokenizer-type DeepseekTokenizer \\\n        --dataset LLama-Pretrain-Idxmap \\\n        --swiglu \\\n        --normalization RMSNorm \\\n        --norm-epsilon 1e-05 \\\n        --use-rotary-position-embeddings \\\n        --no-rope-fusion \\\n        --position-embedding-type rope \\\n        --untie-embeddings-and-output-weights \\\n        --disable-bias-linear \\\n        --rotary-base 10000 \\\n        --use-mcore-models \n        \"\nLOG_SAVE=\"/data/code/temp/output/deepseek/pretrain_output/log/\"${current_time}-deepseek-${NODE_RANK}.log\"\" \nrun_cmd=\"torchrun $DISTRIBUTED_ARGS pretrain_mcore_deepseek.py\n ${megatron_options} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} ${do_options} ${flash_options} ${sp_options} ${gqa_options} ${moe_options} > ${LOG_SAVE} 2>&1\"\n\necho ${run_cmd}\neval ${run_cmd}\nset +x\n```\n\n##### pretrain_mcore_deepseek.py训练代码\n```python\n# Copyright (c) 2023 Alibaba PAI and Nvidia Megatron-LM Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport torch\nfrom torch import Tensor\nfrom functools import partial\nfrom typing import Union\n\nfrom megatron import get_args\nfrom megatron import get_timers\nfrom megatron.core import mpu, tensor_parallel\nfrom megatron.core.enums import ModelType\nimport megatron.model\nfrom megatron.utils import (\n    get_batch_on_this_tp_rank,\n    get_batch_on_this_cp_rank,\n    average_losses_across_data_parallel_group\n)\nfrom megatron.core.datasets.blended_megatron_dataset_builder import BlendedMegatronDatasetBuilder\nfrom megatron.training import pretrain\nfrom megatron.core.datasets.gpt_dataset import GPTDatasetConfig\nfrom megatron.core.datasets.gpt_dataset import GPTDataset\n# from megatron.core.models.gpt import GPTModel\nfrom megatron.core.transformer.spec_utils import import_module\nfrom megatron.arguments import core_transformer_config_from_args\n\nfrom megatron_patch.model.deepseek_core.model import GPTModel\nfrom megatron_patch.data import build_pretrain_dataset_from_original\nfrom megatron_patch.data.utils import get_batch_on_this_tp_rank_original\nfrom megatron_patch.tokenizer import get_tokenizer, build_tokenizer\nfrom megatron_patch.arguments import get_patch_args\nfrom megatron_patch.model.deepseek_core.transformer_config import DeepseekTransformerConfig\nfrom megatron_patch.model.deepseek_core.layer_specs import get_gpt_layer_with_transformer_engine_spec\n\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True\n\ndef model_provider(\n    pre_process=True, post_process=True\n) -> Union[GPTModel, megatron.model.GPTModel]:\n    \"\"\"Builds the model.\n\n    If you set the use_mcore_models to True, it will return the mcore GPT model and if not the legacy GPT model.\n\n    Args:\n        pre_process (bool, optional): Set to true if you need to compute embedings. Defaults to True.\n        post_process (bool, optional): Set to true if you need to want to compute output logits/loss. Defaults to True.\n\n\n    Returns:\n        Union[GPTModel, megatron.model.GPTModel]: The returned model\n    \"\"\"\n    args = get_args()\n    build_tokenizer(args)\n    config = core_transformer_config_from_args(args, DeepseekTransformerConfig)\n    #config = core_transformer_config_from_args(get_args())\n    if args.use_mcore_models:\n        if args.spec is not None:\n            transformer_layer_spec = import_module(args.spec)\n        else:\n            transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.num_experts, args.moe_grouped_gemm)\n        model = GPTModel(\n            config=config,\n            transformer_layer_spec=transformer_layer_spec,\n            vocab_size=args.padded_vocab_size,\n            max_sequence_length=args.max_position_embeddings,\n            pre_process=pre_process,\n            post_process=post_process,\n            fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,\n            parallel_output=True,\n            share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights,\n            position_embedding_type=args.position_embedding_type,\n            rotary_percent=args.rotary_percent,\n            rotary_base=args.rotary_base,\n        )\n    return model\n\n\ndef get_batch(data_iterator):\n    \"\"\"Generate a batch.\"\"\"\n\n    # TODO: this is pretty hacky, find a better way\n    if (not mpu.is_pipeline_first_stage()) and (not mpu.is_pipeline_last_stage()):\n        return None, None, None, None, None\n\n    args = get_args()\n\n    if \"-Raw\" in args.dataset:\n        # get batches based on the TP rank you are on\n        batch = get_batch_on_this_tp_rank_original(data_iterator)\n        # slice batch along sequence dimension for context parallelism\n        batch = get_batch_on_this_cp_rank(batch)\n\n    elif \"-Idxmap\" in args.dataset:\n        # get batches based on the TP rank you are on\n        batch = get_batch_on_this_tp_rank(data_iterator)\n        # slice batch along sequence dimension for context parallelism\n        batch = get_batch_on_this_cp_rank(batch)\n\n    else:\n        raise ValueError(\"please set correct --dataset \")\n\n    return batch.values()\n\n\ndef loss_func(loss_mask: Tensor, output_tensor: Tensor):\n    \"\"\"Loss function.\n\n    Args:\n        loss_mask (Tensor): Used to mask out some portions of the loss\n        output_tensor (Tensor): The tensor with the losses\n    \"\"\"\n    args = get_args()\n\n    losses = output_tensor.float()\n    loss_mask = loss_mask.view(-1).float()\n    if args.context_parallel_size > 1:\n        loss = torch.cat([torch.sum(losses.view(-1) * loss_mask).view(1), loss_mask.sum().view(1)])\n        torch.distributed.all_reduce(loss, group=mpu.get_context_parallel_group())\n        loss = loss[0] / loss[1]\n    else:\n        loss = torch.sum(losses.view(-1) * loss_mask) / loss_mask.sum()\n\n    # Check individual rank losses are not NaN prior to DP all-reduce.\n    if args.check_for_nan_in_loss_and_grad:\n        global_rank = torch.distributed.get_rank()\n        assert not loss.isnan(), (\n            f'Rank {global_rank}: found NaN in local forward loss calculation. '\n            f'Device: {torch.cuda.current_device()}, node: {os.uname()[1]}'\n        )\n\n    # Reduce loss for logging.\n    averaged_loss = average_losses_across_data_parallel_group([loss])\n\n    return loss * args.context_parallel_size, {'lm loss': averaged_loss[0]}\n\n\ndef forward_step(data_iterator, model):\n    \"\"\"Forward step.\"\"\"\n    timers = get_timers()\n\n    # Get the batch.\n    timers('batch-generator', log_level=2).start()\n    tokens, labels, loss_mask, attention_mask, position_ids = get_batch(\n        data_iterator)\n    timers('batch-generator').stop()\n\n    output_tensor = model(tokens, position_ids, attention_mask,\n                          labels=labels)\n\n    return output_tensor, partial(loss_func, loss_mask)\n\n\n\ndef is_dataset_built_on_rank():\n    return (mpu.is_pipeline_first_stage() or mpu.is_pipeline_last_stage()) and mpu.get_tensor_model_parallel_rank() == 0\n\ndef core_gpt_dataset_config_from_args(args):\n    tokenizer = get_tokenizer()\n    return GPTDatasetConfig(\n        is_built_on_rank=is_dataset_built_on_rank,\n        random_seed=args.seed,\n        sequence_length=args.seq_length,\n        blend=args.data_path,\n        split=args.split,\n        path_to_cache=args.data_cache_path,\n        reset_attention_mask=args.reset_attention_mask,\n        eod_mask_loss=args.eod_mask_loss,\n        eod_id=tokenizer.eod\n    )\n\ndef train_valid_test_datasets_provider(train_val_test_num_samples):\n    \"\"\"Build train, valid, and test datasets.\"\"\"\n    args = get_args()\n    if \"-Raw\" in args.dataset:\n                train_ds, valid_ds, test_ds = \\\n                                    build_pretrain_dataset_from_original(args.dataset)\n    else:\n        train_ds, valid_ds, test_ds = BlendedMegatronDatasetBuilder(\n            GPTDataset,\n            train_val_test_num_samples,\n            core_gpt_dataset_config_from_args(args)\n        ).build()\n\n    return train_ds, valid_ds, test_ds\n\n\n\nif __name__ == \"__main__\":\n    train_valid_test_datasets_provider.is_distributed = True\n    pretrain(\n        train_valid_test_datasets_provider,\n        model_provider,\n        ModelType.encoder_or_decoder,\n        forward_step,\n        extra_args_provider=get_patch_args,\n    )\n```\n\n##### 在/data/code/megatron_patch/model在加入deepseek_core的相关配置\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image8.png)\n\n##### DeepseekTokenizer配置\n在/data/code/megatron_patch/tokenizer/__init__.py下加入DeepseekTokenizer配置\n\n```python\nelif args.patch_tokenizer_type == 'DeepseekTokenizer':\n        from megatron.core.datasets.megatron_tokenizer import MegatronTokenizer\n        class _DeepseekTokenizer(MegatronTokenizer):\n            def __init__(self, tokenizer_path, extra_vocab_size):\n                super().__init__(tokenizer_path)\n                self.tokenizer = AutoTokenizer.from_pretrained(\n                    tokenizer_path,\n                    padding_side=\"right\",\n                    use_fast=False,\n                    trust_remote_code=True,\n                )\n                self.extra_vocab_size = extra_vocab_size\n\n            def __call__(self, text, return_tensors=None,\n                         padding=None, max_length=None, truncation=None, add_special_tokens=True):\n\n                return self.tokenizer(text, return_tensors=return_tensors, padding=padding,\n                        max_length=max_length, truncation=truncation, add_special_tokens=add_special_tokens)\n\n            @property\n            def vocab_size(self):\n                return self.tokenizer.vocab_size + self.extra_vocab_size\n\n            @property\n            def vocab(self):\n                return self.tokenizer.encoder\n\n            @property\n            def inv_vocab(self):\n                return self.tokenizer.decoder\n\n            def tokenize(self, text):\n                return self.tokenizer.encode(text)\n\n            def detokenize(self, token_ids):\n                return self.tokenizer.decode(token_ids)\n\n            @property\n            def eod(self):\n                return self.tokenizer.eos_token_id\n\n            @property\n            def eos_token(self):\n                return self.tokenizer.eos_token\n\n            @property\n            def pad_token_id(self):\n                return self.tokenizer.pad_token_id\n\n            @property\n            def eos_token_id(self):\n                return self.tokenizer.eos_token_id\n\n        tokenizer = _DeepseekTokenizer(args.tokenizer_model, args.extra_vocab_size)\n        args.padded_vocab_size = tokenizer.vocab_size\n```\n\n##### 启动预训练k8s的yaml\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: deepseek-core\n  namespace: deepseek\nspec:\n  selector:\n    matchLabels:\n      app: deepseek-core\n  template:\n    metadata:\n      labels:\n        app: deepseek-core\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        deepseek: deepseek-7B  \n      containers:\n      - name: deepseek\n        image: registry.paas/cmss/nemo:24.05_v1.1\n        imagePullPolicy: IfNotPresent\n        resources:\n         limits:\n           nvidia.com/gpu: 4\n         requests:\n           nvidia.com/gpu: 4\n        command:\n              - \"/bin/bash\"\n              - \"-c\"\n              - |\n                cd /data/code/temp/conf/deepseek_core/pretrain &&\n                bash setRank.sh &&\n                cd /data/ &&\n                bash pretrain_mcore_deepseek.sh\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /data/code\n          name: code   \n        - mountPath: /etc/localtime\n          name: localtime\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: code\n        hostPath:\n          path: /mnt/users/lihai/distribute/core/megatron-core\n      - name: localtime\n        hostPath:\n          path: /etc/localtime\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 20G\n\n```\n\n##### setRank.sh\n```bash\n#!/bin/bash\n\norigin_shell=\"/data/code/examples/deepseek_core/pretrain_mcore_deepseek.sh\"\n\nconf_dir=\"/data/code/temp/conf/\"\n\nlocal_dir=\"/data/\"\n\nshell_name=pretrain_mcore_deepseek.sh\n\n## 复制脚本到/workspace下\ncp $origin_shell $local_dir\n\n\n## 读取hostfile\nreadarray -t ips < <(grep -vE '^[[:space:]]*$' \"$conf_dir\"hostfile-67)\n\n## 获取rank0 IP\nrank0_ip=$(echo \"${ips[0]}\" | tr -d '[:space:]')\n\nnodes=${#ips[@]}\n\n## 获取hostfile中配置的IP前缀\n## 使用cut提取IP地址的前三个数字部分\nip_prefix=$(echo \"${ips[0]}\" | cut -d '.' -f 1-3)\n\n## 获取本机IP\nip=$(hostname -I | grep -oE \"$ip_prefix\\.[0-9]+\")\n\n\n# 初始化rank\nnode_rank=-1\n\n# 遍历数组\nfor i in \"${!ips[@]}\"; do\n    # 使用tr命令去除空白字符\n    clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n    if [[ \"$clean_string\" == \"$ip\" ]]; then\n        node_rank=$i\n        break\n    fi\ndone\n\nif [ $node_rank -ne -1 ]; then\n    ## 修改脚本中MASTER_ADDR\n    sed -i \"s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/\" $local_dir$shell_name\n\n    ## 修改NNODES\n    sed -i \"s/^NNODES=.*/NNODES=$nodes/\" $local_dir$shell_name\n\n    ## 修改NODE_RANK\n    sed -i \"s/^NODE_RANK=.*/NODE_RANK=$node_rank/\" $local_dir$shell_name\nfi\n\n```\n\n#### 多机多卡拉起测试\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image9.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image10.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image11.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image12.png)\n\n#### 断点续训测试\n#### loss曲线对比实验\n##### patch框架训练代码准备\n###### patch的预训练yaml\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: deepseek-core\n  namespace: deepseek\nspec:\n  selector:\n    matchLabels:\n      app: deepseek-core\n  template:\n    metadata:\n      labels:\n        app: deepseek-core\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        deepseek: deepseek-7B\n      containers:\n        - name: deepseek\n          image: registry.paas/library/megatron-lm:v1\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              nvidia.com/gpu: 4\n            requests:\n              nvidia.com/gpu: 4\n          command:\n            - \"/bin/bash\"\n            - \"-c\"\n            - |\n              cd /data/code/examples/deepseek/conf && \n              bash setRank-patch.sh &&\n              cd /data/ &&\n              bash pretrain_patch_deepseek.sh\n          securityContext:\n            privileged: true\n          volumeMounts:\n            - mountPath: /data/code\n              name: code\n            - mountPath: /etc/localtime\n              name: localtime\n            - mountPath: /dev/shm\n              name: dshm\n      volumes:\n        - name: code\n          hostPath:\n            path: /mnt/users/wangxiangbo/Pai-Megatron-Patch\n        - name: localtime\n          hostPath:\n            path: /etc/localtime\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 20G\n```\n\n###### patch的预训练脚本\n```shell\n#!/bin/bash\n#sh run_pretrain_megatron_deepseek.sh dsw /workspace/Pai-Megatron-Patch 7B 1 8 1e-5 1e-6 2048 2048 0 bf16 1 1 sel true true true false 100000 /mnt/deepseek-datasets/wudao_llamabpe_text_document /mnt/deepseek-ckpts/Llama-2-7b-hf-to-mg-tp1-pp1/ 10000000000 100000000 /mnt/output_patch_test\nexport NCCL_P2P_DISABLE=1\n#export NCCL_DEBUG=INFO\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\n\nMEGATRON_PATCH_PATH=/data/code \nMEGATRON_PATH=${MEGATRON_PATCH_PATH}/Megatron-LM-231007\nexport PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATCH_PATH}:$PYTHONPATH\n\nNNODES=2\nNODE_RANK=0\nGPUS_PER_NODE=4\nMASTER_ADDR=192.168.0.22\nMASTER_PORT=8889\n\n\n\nDISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT\"\n\nMODEL_SIZE=7B\nBATCH_SIZE=1\nGLOBAL_BATCH_SIZE=8\nLR=1e-5\nMIN_LR=1e-6\nSEQ_LEN=1024\nPAD_LEN=1024\nEXTRA_VOCAB_SIZE=0\nPR=fp16\nTP=4\nPP=1\nAC=sel\nDO=true\nFL=false\nSP=true\nTE=false\nSAVE_INTERVAL=1000\nDATASET_PATH=/data/code/examples/deepseek/dataset/wudao_deepseekbpe_text_document\nPRETRAIN_CHECKPOINT_PATH=/data/code/examples/deepseek/ckpt\nTRAIN_TOKENS=10000000\nWARMUP_TOKENS=0\nOUTPUT_BASEPATH=/data/code/examples/deepseek/output/pretrain_output\n\n\nif [ $MODEL_SIZE = 7B ]; then\n\nNUM_LAYERS=30\nHIDDEN_SIZE=4096\nNUM_ATTN_HEADS=32\nINTERMEDIATE_SIZE=11008\n\ngqa_options=\"\"\n\n\nfi\n\nif [ $AC = full ]; then\n    activation_checkpoint_options=\" \\\n\t\t    --recompute-method uniform \\\n\t\t    --recompute-granularity full\"\nelif [ $AC = sel ]; then\n    activation_checkpoint_options=\" \\\n        --recompute-activations\"\nelif [ $AC = none ]; then\n    activation_checkpoint_options=\" \\\n                    \"\nfi\n\nif [ $PR = fp16 ]; then\n    pr_options=\" \\\n\t\t    --fp16\"\nelif [ $PR = bf16 ]; then\n    pr_options=\" \\\n        --bf16\"\nelif [ $PR = fp8 ]; then\n    pr_options=\" \\\n        --bf16\n        --fp8-hybrid \\\n        --fp8-amax-compute-algo max \\\n        --fp8-amax-history-len 1024 \\\n        --transformer-impl transformer_engine\"\nfi\n\nif [ $DO = true ]; then\n    do_options=\" \\\n\t\t    --use-distributed-optimizer\"\n\nelif [ $DO = false ]; then\n    do_options=\" \\\n                    \"\nfi\n\nif [ $FL = true ]; then\n    flash_options=\" \\\n\t\t    --use-flash-attn\"\n\nelif [ $FL = false ]; then\n    flash_options=\" \\\n                    \"\nfi\n\nif [ $TE = true ]; then\n    te_options=\" \\\n\t\t    --transformer-impl transformer_engine\"\n\nelif [ $TE = false ]; then\n    te_options=\" \\\n                    \"\nfi\n\nif [ $SP = true ] && [ $TP -gt 1 ]; then\n    sp_options=\" \\\n\t\t    --sequence-parallel\"\n\nelif [ $SP = false ]; then\n    sp_options=\" \\\n                    \"\nfi\n\nif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then\n    load_options=\" \\\n            --load $PRETRAIN_CHECKPOINT_PATH\"\nfi\n\nTRAIN_ITERS=$(( ${TRAIN_TOKENS} / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\nLR_WARMUP_ITERS=$(( ${WARMUP_TOKENS}  / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\nLR_DECAY_ITERS=$(( ${TRAIN_TOKENS} /  ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\n\nNAME=\"${ENV}-pretrain-patch-deepseek-${MODEL_SIZE}-lr-${LR}-bs-${BATCH_SIZE}-seqlen-${SEQ_LEN}-pr-${PR}-tp-${TP}-pp-${PP}-ac-${AC}-do-${DO}-sp-${SP}-tt-${TRAIN_TOKENS}-wt-${WARMUP_TOKENS}\"\nmkdir -p \"${OUTPUT_BASEPATH}/tensorboard/\"\nmkdir -p \"${OUTPUT_BASEPATH}/checkpoint/\"\nmkdir -p \"${OUTPUT_BASEPATH}/log/\"\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nTENSORBOARD_DIR=\"${OUTPUT_BASEPATH}/tensorboard/${NAME}_${current_time}\"\nmkdir -p ${TENSORBOARD_DIR}\n\nSAVED_PRETRAIN_CHECKPOINT_PATH=\"${OUTPUT_BASEPATH}/checkpoint/${NAME}\"\n\nmegatron_options=\"  \\\n        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \\\n        --split 99,1,0 \\\n        --train-data-path ${DATASET_PATH}\n        --lr ${LR} \\\n        --min-lr ${MIN_LR} \\\n        --lr-decay-style linear \\\n        --adam-beta1 0.9 \\\n        --adam-beta2 0.95 \\\n        --weight-decay 0.1 \\\n        --clip-grad 1.0 \\\n        --init-method-std 0.006 \\\n        --lr-decay-iters ${LR_DECAY_ITERS} \\\n        --lr-warmup-iters ${LR_WARMUP_ITERS} \\\n        --train-iters ${TRAIN_ITERS} \\\n        --micro-batch-size ${BATCH_SIZE} \\\n        --global-batch-size ${GLOBAL_BATCH_SIZE} \\\n        --num-layers ${NUM_LAYERS} \\\n        --hidden-size ${HIDDEN_SIZE} \\\n        --num-attention-heads ${NUM_ATTN_HEADS} \\\n        --ffn-hidden-size ${INTERMEDIATE_SIZE} \\\n        --seq-length ${SEQ_LEN} \\\n        --max-position-embeddings ${SEQ_LEN} \\\n        --log-interval 1 \\\n        --eval-interval 10000 \\\n        --eval-iters 10 \\\n        --save-interval ${SAVE_INTERVAL} \\\n        --tensorboard-queue-size 1 \\\n        --tensorboard-dir ${TENSORBOARD_DIR} \\\n        --log-timers-to-tensorboard \\\n        --log-batch-size-to-tensorboard \\\n        --log-validation-ppl-to-tensorboard \\\n        --tensor-model-parallel-size ${TP} \\\n        --pipeline-model-parallel-size ${PP} \\\n        --dataset LLama-Pretrain-Idxmap \\\n        --no-load-optim \\\n        --no-load-rng \\\n        --num-workers 8 \\\n        --seed 1234 \\\n        --max-padding-length ${PAD_LEN} \\\n        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \\\n        --patch-tokenizer-type LLamaTokenizer \\\n        --swiglu \\\n        --normalization RMSNorm \\\n        --use-llama2-rotary-position-embeddings \\\n        --position-embedding-type rope \\\n        --untie-embeddings-and-output-weights \\\n        --rotary-base 10000 \\\n        --rotary-scale-factor 4 \\\n        --loss-scale 16384 \\\n        --disable-bias-linear\n        \"\n#--no-query-key-layer-scaling \\\n#/data/code/examples/llama2/pretrain_megatron_llama.py\nLOG_SAVE=\"/data/code/examples/deepseek/output/pretrain_output/log/\"${current_time}-deepseek-patch-${NODE_RANK}.log\"\" \nrun_cmd=\"torchrun $DISTRIBUTED_ARGS /data/code/examples/llama2/pretrain_megatron_llama.py\n ${megatron_options} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} ${do_options} ${flash_options} ${sp_options} ${gqa_options} > ${LOG_SAVE} 2>&1\"\n\necho ${run_cmd}\neval ${run_cmd}\nset +x\n\n```\n\n###### patch的预训练setrank\n```shell\n#!/bin/bash\n\norigin_shell=\"/data/code/examples/deepseek/pretrain_patch_deepseek.sh\"\n\nconf_dir=\"/data/code/examples/deepseek/conf/\"\n\nlocal_dir=\"/data/\"\n\nshell_name=pretrain_patch_deepseek.sh\n\n## 复制脚本到/workspace下\ncp $origin_shell $local_dir\n\n\n## 读取hostfile\nreadarray -t ips < <(grep -vE '^[[:space:]]*$' \"$conf_dir\"hostfile)\n\n## 获取rank0 IP\nrank0_ip=$(echo \"${ips[0]}\" | tr -d '[:space:]')\n\nnodes=${#ips[@]}\n\n## 获取hostfile中配置的IP前缀\n## 使用cut提取IP地址的前三个数字部分\nip_prefix=$(echo \"${ips[0]}\" | cut -d '.' -f 1-3)\n\n## 获取本机IP\nip=$(hostname -I | grep -oE \"$ip_prefix\\.[0-9]+\")\n\n\n# 初始化rank\nnode_rank=-1\n\n# 遍历数组\nfor i in \"${!ips[@]}\"; do\n    # 使用tr命令去除空白字符\n    clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n    if [[ \"$clean_string\" == \"$ip\" ]]; then\n        node_rank=$i\n        break\n    fi\ndone\n\nif [ $node_rank -ne -1 ]; then\n    ## 修改脚本中MASTER_ADDR\n    sed -i \"s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/\" $local_dir$shell_name\n\n    ## 修改NNODES\n    sed -i \"s/^NNODES=.*/NNODES=$nodes/\" $local_dir$shell_name\n\n    ## 修改NODE_RANK\n    sed -i \"s/^NODE_RANK=.*/NODE_RANK=$node_rank/\" $local_dir$shell_name\nfi\n```\n\n##### patch框架预训练测试\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image13.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image14.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image15.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image16.png)\n\n","tags":["模型适配","Nvidia","Deepseek","V100","Megatron-LM"],"categories":["模型适配","NVIDIA"]},{"title":"2.3 Deepseek-7B权重转换","url":"/2025/11/13/模型训练调优/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.3 Deepseek-7B权重转换/","content":"#### Hg-Core权重转换\n##### huggingface开源权重准备\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image17.png)\n\n##### 权重转换脚本\n```bash\n#!/bin/bash\n\nexport CUDA_VISIBLE_DEVICES=0\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nSTART_TIME=$SECONDS\nMASTER_ADDR=localhost\nMASTER_PORT=6666\n\nMODEL_SIZE=7B\nHG_CKPT_PATH=/mnt/model/deepseek-ai/deepseek-llm-7b-base\nMEGATRON_PATCH_PATH=/data/code/\nMEGATRON_PATH=${MEGATRON_PATCH_PATH}/Megatron-LM\nexport PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATCH_PATH}:$PYTHONPATH\nSOURCE_CKPT_PATH=/mnt/model/deepseek-ai/deepseek-llm-7b-base\nTARGET_CKPT_PATH=/mnt/model/deepseek-ai/deepseek-llm-7b-core\nTP=4\nPP=1\nEXTRA_VOCAB_SIZE=2400\nNUM_EXPERTS=0\nEXPERTS_TOPK=0\nEP=0\nNUM_EXPERT_SPLITS=0\nmg2hf=false\n\nif [ $MODEL_SIZE = 7B ]; then\n\nNUM_LAYERS=30\nHIDDEN_SIZE=4096\nNUM_ATTN_HEADS=32\nINTERMEDIATE_SIZE=11008\nNUM_KV_HEADS=32\nVOCAB_SIZE=102400\nROPE_THETA=10000\nRMS_NORM_EPS=1e-6\ngqa_options=\"\"\n\nfi\n\nif [ $NUM_EXPERT_SPLITS -gt 0 ]; then\n\nINTERMEDIATE_SIZE=$(( ${INTERMEDIATE_SIZE} / ${NUM_EXPERT_SPLITS}))\n\nfi\n\nif [ $NUM_EXPERTS -gt 0 ]; then\n    expert_options=\"\n                --moe-router-topk ${EXPERTS_TOPK} \\\n                --num-experts ${NUM_EXPERTS} \\\n                --expert-model-parallel-size 1 \\\n                --target_expert_model_parallel_size ${EP} \\\n                --num_expert_split_size ${NUM_EXPERT_SPLITS} \\\n    \"\nfi\n\nif [ $mg2hf = true ]; then\n    convert_options=\"\n                --convert_checkpoint_from_megatron_to_transformers\n    \"\nelif [ $mg2hf = false ]; then\n    convert_options=\"\"\nfi\n\n\n\nDISTRIBUTED_ARGS=\"--nproc_per_node 1 --nnodes 1 --node_rank 0 --master_addr $MASTER_ADDR --master_port $MASTER_PORT\"\n\nif [ $MODEL_SIZE != 70B ]; then\n\ntorchrun ${DISTRIBUTED_ARGS} hf2mcore.py \\\n    --load_path ${SOURCE_CKPT_PATH} \\\n    --save_path ${TARGET_CKPT_PATH} \\\n    --tokenizer-model ${HG_CKPT_PATH} \\\n    --huggingface_model_path ${HG_CKPT_PATH} \\\n    --megatron-path ${MEGATRON_PATH} \\\n    --target_tensor_model_parallel_size ${TP} \\\n    --target_pipeline_model_parallel_size ${PP} \\\n    --micro-batch-size 1 \\\n    --fp16 \\\n    --swiglu \\\n    --num-layers ${NUM_LAYERS} \\\n    --hidden-size ${HIDDEN_SIZE} \\\n    --ffn-hidden-size ${INTERMEDIATE_SIZE} \\\n    --norm-epsilon ${RMS_NORM_EPS}\\\n    --num-attention-heads ${NUM_ATTN_HEADS} \\\n    --max-position-embeddings 4096 \\\n    --seq-length 1 \\\n    --no-async-tensor-model-parallel-allreduce \\\n    --patch-tokenizer-type DeepseekTokenizer \\\n    --extra-vocab-size ${EXTRA_VOCAB_SIZE} \\\n    --untie-embeddings-and-output-weights \\\n    --no-rope-fusion \\\n    --use-rotary-position-embeddings \\\n    --rotary-base ${ROPE_THETA} \\\n    --transformer-impl transformer_engine \\\n    --disable-bias-linear \\\n    --normalization RMSNorm \\\n    --use-mcore-models \\\n    --attention-dropout 0.0 \\\n    --hidden-dropout 0.0 \\\n    ${expert_options} \\\n    ${convert_options} \\\n    ${gqa_options}\n\nelse\npython hf2mcore_70b.py \\\n  --load ${HG_CKPT_PATH} \\\n  --megatron-path ${MEGATRON_PATH} \\\n  --load_path ${SOURCE_CKPT_PATH} \\\n  --save_path ${TARGET_CKPT_PATH} \\\n  --target_params_dtype bf16 \\\n  --target_tensor_model_parallel_size ${TP} \\\n  --target_pipeline_model_parallel_size ${PP} \\\n${convert_options} \\\n\nfi\n\nELAPSED_TIME=$(($SECONDS - $START_TIME))\necho \"$(($ELAPSED_TIME/60)) min $(($ELAPSED_TIME%60)) sec\"\n```\n\n##### tokenizer配置\n在/data/code/megatron_patch/tokenizer/__init__.py中加入Deepseek配置\n\n```python\nelif args.patch_tokenizer_type == 'DeepseekTokenizer':\n        from megatron.core.datasets.megatron_tokenizer import MegatronTokenizer\n        class _DeepseekTokenizer(MegatronTokenizer):\n            def __init__(self, tokenizer_path, extra_vocab_size):\n                super().__init__(tokenizer_path)\n                self.tokenizer = AutoTokenizer.from_pretrained(\n                    tokenizer_path,\n                    padding_side=\"right\",\n                    use_fast=False,\n                    trust_remote_code=True,\n                )\n                self.extra_vocab_size = extra_vocab_size\n\n            def __call__(self, text, return_tensors=None,\n                         padding=None, max_length=None, truncation=None, add_special_tokens=True):\n\n                return self.tokenizer(text, return_tensors=return_tensors, padding=padding,\n                        max_length=max_length, truncation=truncation, add_special_tokens=add_special_tokens)\n\n            @property\n            def vocab_size(self):\n                return self.tokenizer.vocab_size + self.extra_vocab_size\n\n            @property\n            def vocab(self):\n                return self.tokenizer.encoder\n\n            @property\n            def inv_vocab(self):\n                return self.tokenizer.decoder\n\n            def tokenize(self, text):\n                return self.tokenizer.encode(text)\n\n            def detokenize(self, token_ids):\n                return self.tokenizer.decode(token_ids)\n\n            @property\n            def eod(self):\n                return self.tokenizer.eos_token_id\n\n            @property\n            def eos_token(self):\n                return self.tokenizer.eos_token\n\n            @property\n            def pad_token_id(self):\n                return self.tokenizer.pad_token_id\n\n            @property\n            def eos_token_id(self):\n                return self.tokenizer.eos_token_id\n\n        tokenizer = _DeepseekTokenizer(args.tokenizer_model, args.extra_vocab_size)\n        # tokenizer = _DeepseekTokenizer(args.load, args.extra_vocab_size)\n        args.padded_vocab_size = tokenizer.vocab_size\n```\n\n##### hf2mcore.py修改\n在/data/code/toolkits/model_checkpoints_convertor/deepseek_core/hf2mcore.py中修改导包位置\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image18.png)\n\n##### 执行转换脚本，转换流程\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image19.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image20.png)\n\n##### 转换结束，得到core权重\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image21.png)\n\n#### core权重的推理结果验证\n##### core权重推理结果\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image22.png)\n\n##### huggingface权重推理结果\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image23.png)\n\n\n\n","tags":["模型适配","Nvidia","Deepseek","V100","Megatron-LM"],"categories":["模型适配","NVIDIA"]},{"title":"2.5 pytorchjob+eki断点续训","url":"/2025/11/13/模型训练调优/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.5 pytorchjob+eki断点续训/","content":"#### 环境准备\npytorchjob训练环境已由小组同事搭建完成\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image34.png)\n\n#### 代码准备\n##### 断点续训yaml\n需要注意的是，不能设定自定义的namespace，需要在default下启动pod\n\n```yaml\napiVersion: \"ai.cmss.chinamobile.com/v1alpha1\"\nkind: PyTorchJob\nmetadata:\n  name: deepseek-core\n  labels:\n    service: deepseek-core\nspec:\n  nprocPerNode: \"4\"\n  pytorchReplicaSpecs:\n    Master:\n      replicas: 1\n      template:\n        metadata:\n          labels:\n            app: ckpt\n        spec:\n          nodeSelector:\n            model-ckpt: deepseek-master\n          restartPolicy: Always\n          hostNetwork: true\n          dnsPolicy: ClusterFirstWithHostNet\n          affinity:\n            podAntiAffinity:\n              requiredDuringSchedulingIgnoredDuringExecution:\n                - labelSelector:\n                    matchExpressions:\n                      - key: app\n                        operator: In\n                        values:\n                          - ckpt\n                  topologyKey: \"kubernetes.io/hostname\"\n          containers:\n            - name: pytorch\n              image: registry.paas/library/megatron-lm:v1\n              imagePullPolicy: IfNotPresent\n              securityContext:\n                capabilities:\n                  add:\n                    - SYS_ADMIN\n              resources:\n                limits:\n                  nvidia.com/gpu: 4 # Request access to all GPU\n                requests:\n                  nvidia.com/gpu: 4\n              env:\n                - name: OMP_NUM_THREADS\n                  value: \"4\"\n              command:\n                - \"/bin/bash\"\n                - \"-c\"\n                - |\n                  cd /data/code/temp/conf/deepseek_core/pretrain &&\n                  bash SetRank.sh &&\n                  cd /data/ &&\n                  bash pretrain_ckpt_deepseek.sh\n              volumeMounts:\n                - mountPath: /data/code\n                  readOnly: false\n                  name: code\n                - name: shm-volume\n                  mountPath: /dev/shm\n                - name: pod-gpu-resources\n                  mountPath: /tmp\n                  readOnly: false\n          volumes:\n            - name: code\n              hostPath:\n                path: /mnt/users/lihai/distribute/core/megatron-core\n            - name: shm-volume\n              emptyDir:\n                medium: Memory\n                sizeLimit: 1G\n            - name: pod-gpu-resources\n              hostPath:\n                path: /var/lib/kubelet/pod-resources\n          schedulerName: ai-scheduler\n\n    Worker:\n      replicas: 1\n      restartPolicy: OnFailure\n      template:\n        metadata:\n          labels:\n            app: ckpt\n        spec:\n          nodeSelector:\n            model-ckpt: deepseek-worker\n          restartPolicy: Always\n          hostNetwork: true\n          dnsPolicy:\n            ClusterFirstWithHostNet\n            #affinty:\n            #podAntiAffinity:\n            # requiredDuringSchedulingIgnoredDuringExecution:\n            # - labelSelector:\n            #     matchExpressions:\n            #     - key: app\n            #       operator: In\n            #       values:\n            #       - ckpt\n            #   topologyKey: \"kubernetes.io/hostname\"\n          containers:\n            - name: pytorch\n              image: registry.paas/library/megatron-lm:v1\n              imagePullPolicy: IfNotPresent\n              # workingDir: /workspace/model/glm3/chatglm3/finetune_demo\n              securityContext:\n                capabilities:\n                  add:\n                    - SYS_ADMIN\n              resources:\n                limits:\n                  nvidia.com/gpu: 4 # Request access to all GPU\n                requests:\n                  nvidia.com/gpu: 4\n              env:\n                - name: OMP_NUM_THREADS\n                  value: \"4\"\n              command:\n                - \"/bin/bash\"\n                - \"-c\"\n                - |\n                  cd /data/code/temp/conf/deepseek_core/pretrain &&\n                  bash SetRank.sh &&\n                  cd /data/ &&\n                  bash pretrain_ckpt_deepseek.sh\n              volumeMounts:\n                - mountPath: /data/code\n                  readOnly: false\n                  name: code\n                - name: shm-volume\n                  mountPath: /dev/shm\n                - name: pod-gpu-resources\n                  mountPath: /tmp\n                  readOnly: false\n          volumes:\n            - name: code\n              hostPath:\n                path: /mnt/users/lihai/distribute/core/megatron-core\n                type: Directory\n            - name: shm-volume\n              emptyDir:\n                medium: Memory\n                sizeLimit: 1G\n            - name: pod-gpu-resources\n              hostPath:\n                path: /var/lib/kubelet/pod-resources\n          schedulerName: ai-scheduler\n\n```\n\n##### setrank\n```shell\n#!/bin/bash\n\norigin_shell=\"/data/code/examples/deepseek_core/pretrain_ckpt_deepseek.sh\"\n\nconf_dir=\"/data/code/temp/conf/\"\n\nlocal_dir=\"/data/\"\n\nshell_name=pretrain_ckpt_deepseek.sh\n\n## 复制脚本到/workspace下\ncp $origin_shell $local_dir\n\n\n## 读取hostfile\nreadarray -t ips < <(grep -vE '^[[:space:]]*$' \"$conf_dir\"hostfile-236)\n\n## 获取rank0 IP\nrank0_ip=$(echo \"${ips[0]}\" | tr -d '[:space:]')\n\nnodes=${#ips[@]}\n\n## 获取hostfile中配置的IP前缀\n## 使用cut提取IP地址的前三个数字部分\nip_prefix=$(echo \"${ips[0]}\" | cut -d '.' -f 1-3)\n\n## 获取本机IP\nip=$(hostname -I | grep -oE \"$ip_prefix\\.[0-9]+\")\n\n\n# 初始化rank\nnode_rank=-1\n\n# 打印调试信息\necho \"本机IP: $ip\"\necho \"rank0 IP: $rank0_ip\"\necho \"所有节点IP: ${ips[@]}\"\n\n# 遍历数组\n# for i in \"${!ips[@]}\"; do\n#     # 使用tr命令去除空白字符\n#     clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n#     if [[ \"$clean_string\" == \"$ip\" ]]; then\n#         node_rank=$i\n#         break\n#     fi\n# done\n\n# TODO 测试断点续训\nfor i in \"${!ips[@]}\"; do\n    clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n    if [[ \"$clean_string\" == \"$ip\" ]]; then\n        if [[ \"$ip\" == \"$rank0_ip\" ]]; then\n            node_rank=0\n        else\n            node_rank=1\n        fi\n        break\n    fi\ndone\n\n# 打印调试信息\necho \"设置的NODE_RANK: $node_rank\"\n\nif [ $node_rank -ne -1 ]; then\n    ## 修改脚本中MASTER_ADDR\n    sed -i \"s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/\" $local_dir$shell_name\n\n    ## 修改NNODES\n    # sed -i \"s/^NNODES=.*/NNODES=$nodes/\" $local_dir$shell_name\n\n    ## 修改NODE_RANK\n    sed -i \"s/^NODE_RANK=.*/NODE_RANK=$node_rank/\" $local_dir$shell_name\nfi\n\n```\n\n##### hostfile\n多机节点故障需要选取3个节点来模拟，其中2台用来拉起训练，之后模拟其中1台发生故障，之后再调度到另一个备用机器上继续训练。\n\n其中将192.168.0.65作为master节点，192.168.0.20和192.168.0.63作为worker节点。\n\n```plain\n192.168.0.65\n192.168.0.20\n192.168.0.63\n```\n\n#### 节点故障实践\n##### 训练拉起\n```bash\nkubectl apply -f deepseek-ckpt.yaml\nkubectl get node -owide\nkubectl get pod -owide\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image35.png)\n\n可以看出训练再在master节点192.168.0.65和worker节点192.168.0.63上拉起。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image36.png)\n\n##### ckpt保存\n持续训练，并在interval50步保存ckpt信息，之后继续训练\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image37.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image38.png)\n\n##### 模拟节点故障\n进入192.168.0.63机器，模拟节点故障，停掉k8s服务\n\n```bash\nsystemctl stop kubelet\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image39.png)\n\n停掉后在k8s的master节点查看node状态，是否为NotReady\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image40.png)\n\n此时192.168.0.63已经出现故障，再去查看拉起训练的pod信息\n\n由于192.168.0.63出现故障，pytorchjob会根据hostfile的节点信息，再重新尝试调度到另外的机器上192.168.0.20，可以看出在192.168.0.20重新进行了Init、PodInitializing、Running的过程。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image41.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image42.png)\n\n##### 查看续训log\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image43.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image44.png)\n\n至此，节点故障断点续训实践完成。\n\n模拟完成后，重新将192.168.0.63节点恢复\n\n```bash\nsystemctl start kubelet\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image45.png)\n\n\n\n#### \n\n\n","tags":["模型适配","Nvidia","Deepseek","V100","Megatron-LM"],"categories":["模型适配","NVIDIA"]},{"title":"2.4 Deepseek-7B微调适配","url":"/2025/11/13/模型训练调优/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.4 Deepseek-7B微调适配/","content":"#### 微调物料及代码准备\n##### 数据集准备\n/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-train.json\n\n/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-valid.json\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image24.png)\n\n##### 微调脚本\n```shell\n#!/bin/bash\nexport NCCL_P2P_DISABLE=1\n#export NCCL_DEBUG=INFO\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\n\nMEGATRON_PATCH_PATH=/data/code\nMEGATRON_PATH=${MEGATRON_PATCH_PATH}/Megatron-LM\n\nexport PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATCH_PATH}:$PYTHONPATH\n\nNNODES=2\nNODE_RANK=0\nGPUS_PER_NODE=4\nMASTER_ADDR=192.168.0.65\nMASTER_PORT=8889\n\nDISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT\"\n\n\nMODEL_SIZE=7B\nBATCH_SIZE=1\nGLOBAL_BATCH_SIZE=8 \nLR=1e-5\nMIN_LR=1e-6\nSEQ_LEN=1024\nPAD_LEN=1024\nEXTRA_VOCAB_SIZE=2400\nPR=fp16\nTP=4\nPP=1\nAC=sel\nDO=true\nFL=false\nSP=true\nTE=true\nMOE=false\nSAVE_INTERVAL=1000\nDATASET_PATH=/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-train.json\nVALID_DATASET_PATH=/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-valid.json\nPRETRAIN_CHECKPOINT_PATH=/data/code/temp/deepseek/deepseek-ckpt\nTOKENIZER_PATH=/data/code/temp/deepseek/deepseek-ckpt\nTRAIN_TOKENS=100000000   \nWARMUP_TOKENS=10000\nOUTPUT_BASEPATH=/data/code/temp/output/deepseek/finetune_output\n\n\nif [ $MODEL_SIZE = 7B ]; then\n\nNUM_LAYERS=30\nHIDDEN_SIZE=4096\nNUM_ATTN_HEADS=32\nINTERMEDIATE_SIZE=11008\nMAX_POSITION_EMBEDDINGS=4096\n\ngqa_options=\"\"\n\nfi\n\nif [ $AC = full ]; then\n    activation_checkpoint_options=\" \\\n\t\t    --recompute-method uniform \\\n\t\t    --recompute-granularity full\"\nelif [ $AC = sel ]; then\n    activation_checkpoint_options=\" \\\n        --recompute-activations\"\nelif [ $AC = none ]; then\n    activation_checkpoint_options=\" \\\n                    \"\nfi\n\nif [ $PR = fp16 ]; then\n    pr_options=\" \\\n\t\t    --fp16 \\\n            --apply-query-key-layer-scaling\"\n    export NVTE_APPLY_QK_LAYER_SCALING=1\nelif [ $PR = bf16 ]; then\n    pr_options=\" \\\n        --bf16\"\nelif [ $PR = fp8 ]; then\n    pr_options=\" \\\n        --bf16 \\\n        --fp8-hybrid \\\n        --fp8-amax-compute-algo max \\\n        --fp8-amax-history-len 1024 \\\n        --transformer-impl transformer_engine\"\nfi\n\nif [ $DO = true ]; then\n    do_options=\" \\\n\t\t    --use-distributed-optimizer\"\n\nelif [ $DO = false ]; then\n    do_options=\" \\\n                    \"\nfi\n\nif [ $FL = true ]; then\n    flash_options=\" \\\n\t\t    --use-flash-attn\"\n\nelif [ $FL = false ]; then\n    flash_options=\" \\\n                    \"\nfi\n\nif [ $TE = true ]; then\n    te_options=\" \\\n\t\t    --transformer-impl transformer_engine\"\n\nelif [ $TE = false ]; then\n    te_options=\" \\\n                    \"\nfi\n\nif [ $MOE = true ]; then\n    moe_options=\" \\\n\t\t    --moe-router-topk 1 \\\n\t\t    --num-experts 4 \\\n\t\t    --moe-aux-loss-coeff 1e-2 \\\n\t\t    --expert-model-parallel-size 2\"\n\nelif [ $MOE = false ]; then\n    moe_options=\" \\\n                    \"\nfi\n\nif [ $SP = true ] && [ $TP -gt 1 ]; then\n    sp_options=\" \\\n\t\t    --sequence-parallel\"\n\nelif [ $SP = false ]; then\n    sp_options=\" \\\n                    \"\nfi\n\nif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then\n    load_options=\" \\\n            --load $PRETRAIN_CHECKPOINT_PATH\"\nfi\n\nTRAIN_ITERS=$(( ${TRAIN_TOKENS} / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\nLR_WARMUP_ITERS=$(( ${WARMUP_TOKENS}  / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\nLR_DECAY_ITERS=$(( ${TRAIN_TOKENS} /  ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\n# LR_DECAY_ITERS=$((${TRAIN_ITERS} - ${LR_WARMUP_ITERS}))\n\nNAME=\"${ENV}-finetune-mcore-deepseek-${MODEL_SIZE}-lr-${LR}-bs-${BATCH_SIZE}-seqlen-${SEQ_LEN}-pr-${PR}-tp-${TP}-pp-${PP}-ac-${AC}-do-${DO}-sp-${SP}-tt-${TRAIN_TOKENS}-wt-${WARMUP_ITERS}\"\nmkdir -p \"${OUTPUT_BASEPATH}/tensorboard/\"\nmkdir -p \"${OUTPUT_BASEPATH}/checkpoint/\"\nmkdir -p \"${OUTPUT_BASEPATH}/log/\"\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nTENSORBOARD_DIR=\"${OUTPUT_BASEPATH}/tensorboard/${NAME}_${current_time}\"\nmkdir -p ${TENSORBOARD_DIR}\n\nSAVED_PRETRAIN_CHECKPOINT_PATH=\"${OUTPUT_BASEPATH}/checkpoint/${NAME}\"\n\nmegatron_options=\"  \\\n        --tokenizer-model ${TOKENIZER_PATH} \\\n        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \\\n        --split 99,1,0 \\\n        --train-data-path ${DATASET_PATH} \\\n        --valid-data-path ${VALID_DATASET_PATH} \\\n        --test-data-path ${VALID_DATASET_PATH} \\\n        --lr ${LR} \\\n        --min-lr ${MIN_LR} \\\n        --lr-decay-style linear \\\n        --adam-beta1 0.9 \\\n        --adam-beta2 0.95 \\\n        --weight-decay 0.1 \\\n        --clip-grad 1.0 \\\n        --init-method-std 0.006 \\\n        --dataloader-type cyclic \\\n        --lr-decay-iters ${LR_DECAY_ITERS} \\\n        --lr-warmup-iters ${LR_WARMUP_ITERS} \\\n        --train-iters ${TRAIN_ITERS} \\\n        --micro-batch-size ${BATCH_SIZE} \\\n        --global-batch-size ${GLOBAL_BATCH_SIZE} \\\n        --num-layers ${NUM_LAYERS} \\\n        --hidden-size ${HIDDEN_SIZE} \\\n        --num-attention-heads ${NUM_ATTN_HEADS} \\\n        --ffn-hidden-size ${INTERMEDIATE_SIZE} \\\n        --seq-length ${SEQ_LEN} \\\n        --max-position-embeddings ${MAX_POSITION_EMBEDDINGS} \\\n        --max-padding-length ${PAD_LEN} \\\n        --log-interval 1 \\\n        --eval-interval 10000 \\\n        --eval-iters 10 \\\n        --save-interval ${SAVE_INTERVAL} \\\n        --tensorboard-queue-size 1 \\\n        --tensorboard-dir ${TENSORBOARD_DIR} \\\n        --log-timers-to-tensorboard \\\n        --log-batch-size-to-tensorboard \\\n        --log-validation-ppl-to-tensorboard \\\n        --tensor-model-parallel-size ${TP} \\\n        --pipeline-model-parallel-size ${PP} \\\n        --no-load-optim \\\n        --no-load-rng \\\n        --num-workers 8 \\\n        --seed 1234 \\\n        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \\\n        --patch-tokenizer-type DeepseekTokenizer \\\n        --dataset LLama-Pretrain-Raw \\\n        --swiglu \\\n        --normalization RMSNorm \\\n        --use-rotary-position-embeddings \\\n        --no-rope-fusion \\\n        --position-embedding-type rope \\\n        --untie-embeddings-and-output-weights \\\n        --disable-bias-linear \\\n        --use-mcore-models \\\n        --rotary-base 10000 \\\n        --attention-dropout 0.0 \\\n        --hidden-dropout 0.0 \\\n        --norm-epsilon 1e-05 \\\n        --eod-mask-loss \\\n        --log-throughput \\\n        --loss-scale 2048      \n        \"\n        # --loss-scale 2048  \n\nLOG_SAVE=\"/data/code/temp/output/deepseek/finetune_output/log/\"${current_time}-deepseek-finetune-${NODE_RANK}.log\"\" \nrun_cmd=\"torchrun $DISTRIBUTED_ARGS /data/code/examples/deepseek_core/pretrain_mcore_deepseek.py\n ${megatron_options} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} ${do_options} ${flash_options} ${sp_options} ${gqa_options} ${moe_options} > ${LOG_SAVE} 2>&1\"\n\necho ${run_cmd}\neval ${run_cmd}\nset +x\n```\n\n##### 微调yaml\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: deepseek-core\n  namespace: deepseek\nspec:\n  selector:\n    matchLabels:\n      app: deepseek-core\n  template:\n    metadata:\n      labels:\n        app: deepseek-core\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        deepseek: deepseek-7B\n      containers:\n        - name: deepseek\n          image: registry.paas/library/megatron-lm:v1\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              nvidia.com/gpu: 4\n            requests:\n              nvidia.com/gpu: 4\n          command:\n            - \"/bin/bash\"\n            - \"-c\"\n            - |\n              cd /data/code/temp/conf/deepseek_core/finetune &&\n              bash setRank.sh &&\n              cd /data/ &&\n              bash finetune_mcore_deepseek.sh\n          securityContext:\n            privileged: true\n          volumeMounts:\n            - mountPath: /data/code\n              name: code\n            - mountPath: /etc/localtime\n              name: localtime\n            - mountPath: /dev/shm\n              name: dshm\n            - name: weight\n              mountPath: /data/code/temp/deepseek/deepseek-ckpt/\n      volumes:\n        - name: code\n          hostPath:\n            path: /mnt/users/lihai/distribute/core/megatron-core\n        - name: weight\n          hostPath:\n            path: /mnt/users/wangxiangbo/model/deepseek-ai/deepseek-llm-7b-core\n        - name: localtime\n          hostPath:\n            path: /etc/localtime\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 20G\n\n```\n\n##### setrank\n```shell\n#!/bin/bash\n\norigin_shell=\"/data/code/examples/deepseek_core/finetune_mcore_deepseek.sh\"\n\nconf_dir=\"/data/code/temp/conf/\"\n\nlocal_dir=\"/data/\"\n\nshell_name=finetune_mcore_deepseek.sh\n\n## 复制脚本到/workspace下\ncp $origin_shell $local_dir\n\n\n## 读取hostfile\nreadarray -t ips < <(grep -vE '^[[:space:]]*$' \"$conf_dir\"hostfile-23)\n\n## 获取rank0 IP\nrank0_ip=$(echo \"${ips[0]}\" | tr -d '[:space:]')\n\nnodes=${#ips[@]}\n\n## 获取hostfile中配置的IP前缀\n## 使用cut提取IP地址的前三个数字部分\nip_prefix=$(echo \"${ips[0]}\" | cut -d '.' -f 1-3)\n\n## 获取本机IP\nip=$(hostname -I | grep -oE \"$ip_prefix\\.[0-9]+\")\n\n\n# 初始化rank\nnode_rank=-1\n\n# 遍历数组\nfor i in \"${!ips[@]}\"; do\n    # 使用tr命令去除空白字符\n    clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n    if [[ \"$clean_string\" == \"$ip\" ]]; then\n        node_rank=$i\n        break\n    fi\ndone\n\nif [ $node_rank -ne -1 ]; then\n    ## 修改脚本中MASTER_ADDR\n    sed -i \"s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/\" $local_dir$shell_name\n\n    ## 修改NNODES\n    sed -i \"s/^NNODES=.*/NNODES=$nodes/\" $local_dir$shell_name\n\n    ## 修改NODE_RANK\n    sed -i \"s/^NODE_RANK=.*/NODE_RANK=$node_rank/\" $local_dir$shell_name\nfi\n\n\n```\n\n#### 多机多卡拉起测试\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image25.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image26.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image27.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image28.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image29.png)\n\n#### 断点续训测试\n#### loss曲线对比\n##### hugging-patch权重转换\n###### patch转换脚本\n```bash\n#!/bin/bash\n\nset -e\nSTART_TIME=$SECONDS\n\nMEGATRON_PATH=/data/code/\nexport PYTHONPATH=$PYTHONPATH:${MEGATRON_PATH}:${MEGATRON_PATH}/Megatron-LM-231007\nSOURCE_CKPT_PATH=/mnt/deepseek-ai/deepseek-llm-7b-base\nTARGET_CKPT_PATH=/mnt/deepseek-ai/deepseek-llm-7b-patch\nTP=4\nPP=1\nMN=llama-7b #llama-7b, llama-13b, llama-30b, llama-65b, llama2-7b, llama2-13b, llama2-70b\nEXTRA_VOCAB_SIZE=2400\nmg2hf=false\n\nif [ $mg2hf = true ]; then\n    do_options=\"\n                --convert_checkpoint_from_megatron_to_transformers\n    \"\nelif [ $mg2hf = false ]; then\n    do_options=\"\"\nfi\n\n\npython hf2megatron.py \\\n    --load_path ${SOURCE_CKPT_PATH} \\\n    --save_path ${TARGET_CKPT_PATH} \\\n    --target_params_dtype fp16 \\\n    --megatron-path ${MEGATRON_PATH} \\\n    --target_tensor_model_parallel_size ${TP} \\\n    --target_pipeline_model_parallel_size ${PP} \\\n    --model_name ${MN} \\\n    --extra_num_vocabs ${EXTRA_VOCAB_SIZE} \\\n${do_options}\n\nELAPSED_TIME=$(($SECONDS - $START_TIME))\necho \"$(($ELAPSED_TIME/60)) min $(($ELAPSED_TIME%60)) sec\"\n\n```\n\n###### patch权重转换过程\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image30.png)\n\n###### 得到转成patch的权重\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image31.png)\n\n##### patch框架微调代码准备\n###### patch的微调yaml\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: deepseek-core\n  namespace: deepseek\nspec:\n  selector:\n    matchLabels:\n      app: deepseek-core\n  template:\n    metadata:\n      labels:\n        app: deepseek-core\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        deepseek: deepseek-7B\n      containers:\n        - name: deepseek\n          image: registry.paas/library/megatron-lm:v1\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              nvidia.com/gpu: 4\n            requests:\n              nvidia.com/gpu: 4\n          command:\n            - \"/bin/bash\"\n            - \"-c\"\n            - |\n              cd /data/code/examples/deepseek/conf &&\n              bash setRank.sh &&\n              cd /data/ &&\n              bash finetune_patch_deepseek.sh\n          securityContext:\n            privileged: true\n          volumeMounts:\n            - mountPath: /data/code\n              name: code\n            - mountPath: /etc/localtime\n              name: localtime\n            - mountPath: /dev/shm\n              name: dshm\n            - name: weight\n              mountPath: /data/code/examples/deepseek/ckpt/\n      volumes:\n        - name: code\n          hostPath:\n            path: /mnt/users/wangxiangbo/Pai-Megatron-Patch\n        - name: weight\n          hostPath:\n            path: /mnt/users/wangxiangbo/model/deepseek-ai/deepseek-llm-7b-patch\n        - name: localtime\n          hostPath:\n            path: /etc/localtime\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 20G\n\n```\n\n###### patch的微调脚本\n```python\n#!/bin/bash\n#sh run_pretrain_megatron_deepseek.sh dsw /workspace/Pai-Megatron-Patch 7B 1 8 1e-5 1e-6 2048 2048 0 bf16 1 1 sel true true true false 100000 /mnt/deepseek-datasets/wudao_llamabpe_text_document /mnt/deepseek-ckpts/Llama-2-7b-hf-to-mg-tp1-pp1/ 10000000000 100000000 /mnt/output_patch_test\nexport NCCL_P2P_DISABLE=1\n#export NCCL_DEBUG=INFO\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\n\nMEGATRON_PATCH_PATH=/data/code \nMEGATRON_PATH=${MEGATRON_PATCH_PATH}/Megatron-LM-231007\nexport PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATCH_PATH}:$PYTHONPATH\n\nNNODES=2\nNODE_RANK=0\nGPUS_PER_NODE=4\nMASTER_ADDR=192.168.0.65\nMASTER_PORT=8889\n\n\n\nDISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT\"\n\nMODEL_SIZE=7B\nBATCH_SIZE=1\nGLOBAL_BATCH_SIZE=8\nLR=1e-5\nMIN_LR=1e-6\nSEQ_LEN=1024\nPAD_LEN=1024\nEXTRA_VOCAB_SIZE=4800\nPR=fp16\nTP=4\nPP=1\nAC=sel\nDO=true\nFL=false\nSP=true\nTE=false\nSAVE_INTERVAL=1000\nDATASET_PATH=/data/code/examples/deepseek/dataset/alpaca_zh-deepseek-train.json\nVALID_DATASET_PATH=/data/code/examples/deepseek/dataset/alpaca_zh-deepseek-valid.json\nPRETRAIN_CHECKPOINT_PATH=/data/code/examples/deepseek/ckpt\nTRAIN_ITERS=10000\nLR_WARMUP_ITERS=0\nOUTPUT_BASEPATH=/data/code/examples/deepseek/output/finetune_output\n\n\nif [ $MODEL_SIZE = 7B ]; then\n\nNUM_LAYERS=30\nHIDDEN_SIZE=4096\nNUM_ATTN_HEADS=32\nINTERMEDIATE_SIZE=11008\n\ngqa_options=\"\"\n\nfi\n\nif [ $AC = full ]; then\n    activation_checkpoint_options=\" \\\n\t\t    --recompute-method uniform \\\n\t\t    --recompute-granularity full \\\n            --recompute-num-layers ${NUM_LAYERS}\"\nelif [ $AC = sel ]; then\n    activation_checkpoint_options=\" \\\n        --recompute-activations\"\nelif [ $AC = none ]; then\n    activation_checkpoint_options=\" \\\n                    \"\nfi\n\nif [ $PR = fp16 ]; then\n    pr_options=\" \\\n\t\t    --fp16\"\nelif [ $PR = bf16 ]; then\n    pr_options=\" \\\n        --bf16\"\nelif [ $PR = fp8 ]; then\n    pr_options=\" \\\n        --bf16\n        --fp8-hybrid \\\n        --fp8-amax-compute-algo max \\\n        --fp8-amax-history-len 1024 \\\n        --transformer-impl transformer_engine\"\nfi\n\nif [ $DO = true ]; then\n    do_options=\" \\\n\t\t    --use-distributed-optimizer\"\n\nelif [ $DO = false ]; then\n    do_options=\" \\\n                    \"\nfi\n\nif [ $FL = true ]; then\n    flash_options=\" \\\n\t\t    --use-flash-attn\"\n\nelif [ $FL = false ]; then\n    flash_options=\" \\\n                    \"\nfi\n\nif [ $TE = true ]; then\n    te_options=\" \\\n\t\t    --transformer-impl transformer_engine\"\n\nelif [ $TE = false ]; then\n    te_options=\" \\\n                    \"\nfi\n\nif [ $SP = true ] && [ $TP -gt 1 ]; then\n    sp_options=\" \\\n\t\t    --sequence-parallel\"\n\nelif [ $SP = false ]; then\n    sp_options=\" \\\n                    \"\nfi\n\nif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then\n    load_options=\" \\\n            --load $PRETRAIN_CHECKPOINT_PATH\"\nfi\n\nLR_DECAY_ITERS=$(( ${TRAIN_ITERS} - ${LR_WARMUP_ITERS}))\n\nNAME=\"${ENV}-finetune-patch-deepseek-${MODEL_SIZE}-lr-${LR}-bs-${BATCH_SIZE}-seqlen-${SEQ_LEN}-pr-${PR}-tp-${TP}-pp-${PP}-ac-${AC}-do-${DO}-sp-${SP}-tt-${TRAIN_TOKENS}-wt-${WARMUP_TOKENS}\"\nmkdir -p \"${OUTPUT_BASEPATH}/tensorboard/\"\nmkdir -p \"${OUTPUT_BASEPATH}/checkpoint/\"\nmkdir -p \"${OUTPUT_BASEPATH}/log/\"\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nTENSORBOARD_DIR=\"${OUTPUT_BASEPATH}/tensorboard/${NAME}_${current_time}\"\nmkdir -p ${TENSORBOARD_DIR}\n\nSAVED_PRETRAIN_CHECKPOINT_PATH=\"${OUTPUT_BASEPATH}/checkpoint/${NAME}\"\n\nmegatron_options=\"  \\\n        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \\\n        --split 99,1,0 \\\n        --train-data-path ${DATASET_PATH} \\\n        --valid-data-path ${VALID_DATASET_PATH} \\\n        --test-data-path ${VALID_DATASET_PATH} \\\n        --lr ${LR} \\\n        --min-lr ${MIN_LR} \\\n        --lr-decay-style linear \\\n        --adam-beta1 0.9 \\\n        --adam-beta2 0.95 \\\n        --weight-decay 0.1 \\\n        --clip-grad 1.0 \\\n        --init-method-std 0.006 \\\n        --dataloader-type cyclic \\\n        --lr-decay-iters ${LR_DECAY_ITERS} \\\n        --lr-warmup-iters ${LR_WARMUP_ITERS} \\\n        --train-iters ${TRAIN_ITERS} \\\n        --micro-batch-size ${BATCH_SIZE} \\\n        --global-batch-size ${GLOBAL_BATCH_SIZE} \\\n        --num-layers ${NUM_LAYERS} \\\n        --hidden-size ${HIDDEN_SIZE} \\\n        --num-attention-heads ${NUM_ATTN_HEADS} \\\n        --ffn-hidden-size ${INTERMEDIATE_SIZE} \\\n        --seq-length ${SEQ_LEN} \\\n        --max-position-embeddings ${SEQ_LEN} \\\n        --log-interval 1 \\\n        --eval-interval 10000 \\\n        --eval-iters 10 \\\n        --save-interval ${SAVE_INTERVAL} \\\n        --tensorboard-queue-size 1 \\\n        --tensorboard-dir ${TENSORBOARD_DIR} \\\n        --log-timers-to-tensorboard \\\n        --log-batch-size-to-tensorboard \\\n        --log-validation-ppl-to-tensorboard \\\n        --tensor-model-parallel-size ${TP} \\\n        --pipeline-model-parallel-size ${PP} \\\n        --dataset LLama-Pretrain-Raw \\\n        --no-save-optim \\\n        --no-load-optim \\\n        --no-load-rng \\\n        --num-workers 8 \\\n        --seed 1234 \\\n        --max-padding-length ${PAD_LEN} \\\n        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \\\n        --patch-tokenizer-type LLamaTokenizer \\\n        --swiglu \\\n        --normalization RMSNorm \\\n        --use-llama2-rotary-position-embeddings \\\n        --position-embedding-type rope \\\n        --untie-embeddings-and-output-weights \\\n        --rotary-base 10000 \\\n        --rotary-scale-factor 4 \\\n        --loss-scale 2048 \\\n        --disable-bias-linear\n        \"\nLOG_SAVE=\"/data/code/examples/deepseek/output/finetune_output/log/\"${current_time}-deepseek-patch-${NODE_RANK}.log\"\" \nrun_cmd=\"torchrun $DISTRIBUTED_ARGS /data/code/examples/llama2/pretrain_megatron_llama.py\n ${megatron_options} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} ${do_options} ${flash_options} ${sp_options} ${gqa_options} > ${LOG_SAVE} 2>&1\"\n\necho ${run_cmd}\neval ${run_cmd}\nset +x\n```\n\n###### patch的微调setrank\n```shell\n#!/bin/bash\n\norigin_shell=\"/data/code/examples/deepseek/finetune_patch_deepseek.sh\"\n\nconf_dir=\"/data/code/examples/deepseek/conf/\"\n\nlocal_dir=\"/data/\"\n\nshell_name=finetune_patch_deepseek.sh\n\n## 复制脚本到/workspace下\ncp $origin_shell $local_dir\n\n\n## 读取hostfile\nreadarray -t ips < <(grep -vE '^[[:space:]]*$' \"$conf_dir\"hostfile)\n\n## 获取rank0 IP\nrank0_ip=$(echo \"${ips[0]}\" | tr -d '[:space:]')\n\nnodes=${#ips[@]}\n\n## 获取hostfile中配置的IP前缀\n## 使用cut提取IP地址的前三个数字部分\nip_prefix=$(echo \"${ips[0]}\" | cut -d '.' -f 1-3)\n\n## 获取本机IP\nip=$(hostname -I | grep -oE \"$ip_prefix\\.[0-9]+\")\n\n\n# 初始化rank\nnode_rank=-1\n\n# 遍历数组\nfor i in \"${!ips[@]}\"; do\n    # 使用tr命令去除空白字符\n    clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n    if [[ \"$clean_string\" == \"$ip\" ]]; then\n        node_rank=$i\n        break\n    fi\ndone\n\nif [ $node_rank -ne -1 ]; then\n    ## 修改脚本中MASTER_ADDR\n    sed -i \"s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/\" $local_dir$shell_name\n\n    ## 修改NNODES\n    sed -i \"s/^NNODES=.*/NNODES=$nodes/\" $local_dir$shell_name\n\n    ## 修改NODE_RANK\n    sed -i \"s/^NODE_RANK=.*/NODE_RANK=$node_rank/\" $local_dir$shell_name\nfi\n\n```\n\n##### patch框架微调测试\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image32.png)\n\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image33.png)\n\n","tags":["模型适配","Nvidia","Deepseek","V100","Megatron-LM"],"categories":["模型适配","NVIDIA"]},{"title":"A100集群6机48卡-基于k8s的Colossal-AI llama2 70B训练操作文档","url":"/2025/11/13/模型训练调优/NVIDIA/Llama2/A100集群6机48卡-基于k8s的Colossal-AI llama2 70B训练操作文档/","content":"<font style=\"color:#DF2A3F;background-color:#C1E77E;font-weight: bold;\">注：该项目为本人支撑客户项目，文档内涉及到的客户、设备信息等已脱敏处理。</font>\n### 基础环境说明\n#### 概述\n模型名称: Colossal-AI llama2 70B  \n模型参数: 70B参数  \n硬件需求: 6台服务器，每台服务器配备8张NVIDIA A100 GPU\n\n#### 硬件配置\nGPU型号: NVIDIA A100  \nGPU数量: 6台服务器 x 8张/台 = 48张GPU  \n内存容量: 每张A100 GPU拥有80GB显存  \nCPU: Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz，x86_64  \n存储: 高速SSD存储，用于数据读写  \n网络: 高速网络连接，支持多机训练\n\n#### GPU环境说明\n驱动版本: 470.141.03  \nCUDA版本: CUDA 11.4  \nNvidia-smi版本：470.141.03  \nNCCL: 用于多GPU和多节点通信\n持久性模式: Persistence-Ml\n功率使用上限: 功率上限为400W\n显存：显存总量为81920MiB (80GB)\n\n#### 软件环境\n操作系统: Linux (CentOS Linux 7 (Core))  \nPython版本: Python 3.8 或更高版本  \n依赖库:  \nPyTorch 2.1  \nColossal-AI: 支持70B参数模型版本  \n其他依赖: NumPy, SciPy, Pandas等\n\n#### 模型参数大小\n参数总数: 70B  \n模型占用内存: 由于GPU显存占用会根据模型的参数量、模型的内存占用、训练时的batch size、并行优化技术等不同而发生变化，</font>现参数未定，暂不能量化GPU占用情况。\n\n### 模型训练流程\n<font style=\"background-color:#C1E77E;\">注：由于xxx.xx.xx.141机器日常有业务在跑，无法空闲出完整的8块gpu资源，故以下最多以</font>**<font style=\"color:#DF2A3F;background-color:#C1E77E;\">5机40卡</font>**<font style=\"background-color:#C1E77E;\">来拉起训练。</font>\n\n#### 查看A100机器的ip信息\n`cat /root/a100.txt`\n\n#### 查看待训练节点的GPU占用情况\n通过137机器，分别ssh进入以上6台机器（已设置免密登录），查看对应机器的gpu使用情况`nvidia-smi`、网卡配置信息`ifconfig`等。\n\n示例一：\n\n`ssh root@xxx.xx.xx.141` ssh进入机器141\n\n`nvidia-smi`141机器的gpu已经被占用，若是有需求要使用该机器，需要提前沟通\n\n`exit` 从141机器退出到137机器上\n\n示例二：\n\n`ssh root@xxx.xx.xx.78`\n\n`nvidia-smi` 78机器的gpu没有被使用，可以进行模型训练任务\n\n#### 物料准备工作\n<font style=\"background-color:#C1E77E;\">所有的物料位置存储在xxx.xx.xx137机器上的</font>`/home/disk_sdb0/llama2`<font style=\"background-color:#C1E77E;\">下。</font>\n\n物料包含原始数据集文件夹`raw_data`，离线切分完成的数据集文件夹`datas`、镜像`llama2-70b.tar`、70B模型权重`Llama-2-70b-hf`、13B模型权重`Llama-2-13b-hf`、7B模型权重`Llama-2-7b-hf`、k8s拉起预训练的配置文件`llama2-70b-test.yaml`、k8s拉起微调训练的配置文件`llama2-70b-finetune.yaml`，预训练/微调的训练代码`train.py`、预训练的配置脚本`train.example.sh`、微调训练的配置脚本`finetune.example.sh`、节点ip的信息配置文件`hostfile.txt`，此外还需要创建一个目录`output`来存放训练过程中的输出日志及checkpoint信息`mkdir output`。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image1.png)\n\n#### 训练环境配置准备\n##### /etc/hosts配置\n6台机器都需要对`vim /etc/hosts`文件进行修改，加入对应的<font style=\"color:rgba(0, 0, 0, 0.85);\"> IP 地址和与其对应的主机名或域名的映射关系，以此保证各主机之间通信正常。此外，还需要加入百度镜像仓库地址的ip映射。否则会报socket通信错误。</font>\n\n<font style=\"color:#DF2A3F;background-color:#C1E77E;\">socket.gaierror: [Errno -2] Name or service not known</font>\n\n```plain\nxxx.xx.xx.103 xxx.xx.xx..baidu.com\nxxx.xx.xx137 xxx-xx-xx-xxx-137\nxxx.xx.xx.78 xxx-xx-xx-78.XXX-stack.com\nxxx.xx.xx.79 xxx-xx-xx-79.XXX-stack.com\nxxx.xx.xx.80 xxx-xx-xx-80.XXX-stack.com\nxxx.xx.xx.141 xxx-xx-xx-141.XXX-stack.com\nxxx.xx.xx.142 xxx-xx-xx-142.XXX-stack.com\nxxx.xx.xx.143 xxx-xx-xx-143.XXX-stack.com\n```\n\n##### k8s命名空间创建\n因为未使用百度的测试平台来拉训练，通过k8s来拉起，所以需要自定义一个`namespace`，并在改ns下启动pod拉起训练任务。该ns需要与`llama2-70b-test.yaml`中的`namespace: llama2-70b-test`对应。\n\n`kubectl create namespace llama2-70b-test`\n\n`kubectl get ns`\n\n##### 镜像push至仓库\n镜像需push到百度指定的仓库中\n\n将镜像load之后，并将镜像push至 <font style=\"background-color:#C1E77E;\">xxx.xx.xx..baidu.com </font>仓库中。\n\n        1. `docker load -i llama2-70b.tar`\n        2. `docker images`\n        3. `docker tag 9cb8e972304f xxx.xx.xx..baidu.com/public/llama2-70b:v1.0`\n        4. `docker login xxx.xx.xx..baidu.com -u admin -p XXXXXXX`\n        5. `docker push xxx.xx.xx..baidu.com/public/llama2-70b:v1.0`\n\n#####   hostfile配置\n通过ssh登录到5台机器，查看是否有gpu占用，拉起训练时要保证节点的8个gpu都没有被占用。将空闲的机器bond4的ip写入到hostfile中。bond4的ip地址可以通过`ifconfig`来进行查看，有的机器使用的是以太网eth0的ip配置。\n\n```plain\nxxx.xx.xx.142\nxxx.xx.xx.143\nxxx.xx.xx.78\nxxx.xx.xx.79\nxxx.xx.xx.80\n```\n\n#####  训练脚本\n修改预训练启动的脚本`train.example.sh`，网卡的socket通信配置要修改为bond4，并在拉起的训练的节点中选取一个作为matser主节点`--master_addr`\n\n```bash\n#!/bin/bash\n\n# NCCL IB environment variables\nexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1\nexport NCCL_IB_DISABLE=0\nexport NCCL_SOCKET_IFNAME=bond4\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_IB_TIMEOUT=23\nexport NCCL_IB_RETRY_CNT=7\nexport OMP_NUM_THREADS=8\n\nPROJECT_NAME=\"llama2-70b-pt\"\nPARENT_SAVE_DIR=\"/opt/tmp/ColossalAI/output/save/\"\nPARENT_TENSORBOARD_DIR=\"/opt/tmp/ColossalAI/output/tensorboard/\"\nPARENT_CONFIG_FILE=\"/opt/tmp/ColossalAI/output/config/\"\nPRETRAINED_MODEL_PATH=\"/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf\"\ndeclare -a dataset=(\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009\"\n)\n\nTIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)\nFULL_PROJECT_NAME=\"${PROJECT_NAME}-${TIMESTAMP}\"\nSAVE_DIR=\"${PARENT_SAVE_DIR}${FULL_PROJECT_NAME}\"\nTENSORBOARD_DIR=\"${PARENT_TENSORBOARD_DIR}${FULL_PROJECT_NAME}\"\nCONFIG_FILE=\"${PARENT_CONFIG_FILE}${FULL_PROJECT_NAME}.json\"\n\ncolossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.78 --master_port 30015 train.py \\\n    --pretrained $PRETRAINED_MODEL_PATH \\\n    --dataset ${dataset[@]} \\\n    --plugin \"gemini_auto\" \\\n    --save_interval 400 \\\n    --save_dir $SAVE_DIR \\\n    --tensorboard_dir $TENSORBOARD_DIR \\\n    --config_file $CONFIG_FILE \\\n    --num_epochs 1 \\\n    --micro_batch_size 1 \\\n    --lr 1e-4 \\\n    --mixed_precision \"fp16\" \\\n    --grad_clip 1.0 \\\n    --weight_decay 0.01 \\\n    --warmup_steps 100 \\\n    --use_grad_checkpoint \\\n    --max_length 4096 \\\n    --accumulation_steps 1 \\\n    --pad_token \"unk\"\n```\n\n#####  启动k8s的yaml\n修改k8s拉起的配置文件`llama2-70b-test.yaml`\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: llama2-70b-test\n  name: llama2-70b-pt # Deployment 的名称\nspec:\n  replicas: 2 # 副本数量\n  selector:\n    matchLabels:\n      app: llama2-70b-pt\n  template:\n    metadata:\n      labels:\n        app: llama2-70b-pt\n    spec:\n      hostNetwork: true\n      containers:\n      - name: llama2-70b-container # 容器的名称\n        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/bash\", \"-c\"]\n        args: [\"cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ && chmod +x start.sh && ./start.sh && chmod +x train.example.sh && ./train.example.sh\"] # 启动容器后执行的命令，切换目录并运行训练脚本\n        resources:\n          limits:\n            nvidia.com/gpu: 8 # 限制使用 8 个 GPU，根据实际情况调整\n          requests:\n            nvidia.com/gpu: 8\n        volumeMounts:\n        - name: colossalai-orgdataset\n          mountPath: /opt/tmp/ColossalAI/datas/org_dataset # 将容器内数据目录挂载\n        - name: colossalai-weight\n          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载\n        - name: colossalai-train\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py\n          subPath: train.py\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: colossalai-output\n          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载\n        - name: host-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile\n          subPath: hostfile.txt\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: train-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # 容器内train_example.sh\n          subPath: train.example.sh\n        - name: shm-volume\n          mountPath: /dev/shm\n      nodeSelector:\n        model: llama2-70b\n      volumes:\n      - name: colossalai-orgdataset\n        hostPath:\n          path: /home/disk_sdb0/llama2/datas/org_dataset # 宿主机中 ColossalAI 数据的路径\n          type: Directory\n      - name: colossalai-weight\n        hostPath:\n          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # 宿主机中 ColossalAI 权重的路径\n          type: Directory\n      - name: colossalai-train\n        hostPath:\n          path: /home/disk_sdb0/llama2\n          type: Directory\n      - name: colossalai-output\n        hostPath:\n          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径\n          type: Directory\n      - name: host-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: train-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 8Gi\n```\n\n#####  node节点打标签\n给待拉起训练的node打上标签（首先需要确认待拉起训练的node的状态是否Ready）\n\n打标签时，其中的model=llama2-70b对应的是llama2-70b-test.yaml文件中的nodeSelector部分，需保持一致。\n\n`kubectl label nodes xxx-xx-xx-141.XXX-stack.com model=llama2-70b`\n\n`kubectl label nodes xxx-xx-xx-142.XXX-stack.com model=llama2-70b`\n\n`kubectl label nodes xxx-xx-xx-143.XXX-stack.com model=llama2-70b`\n\n`kubectl label nodes xxx-xx-xx-78.XXX-stack.com model=llama2-70b`\n\n`kubectl label nodes xxx-xx-xx-79.XXX-stack.com model=llama2-70b`\n\n`kubectl label nodes xxx-xx-xx-80.XXX-stack.com model=llama2-70b`\n\n#####  训练物料传输与同步\n因为多机拉起训练时，每个node节点都需要有一份训练所需的物料（权重、数据集、训练脚本等文件），故需要通过`rsync`或`scp`命令将137机器上的物料文件传输至待训练的节点中。(除去启动k8s所需的`llama2-70b-test.yaml`配置文件要在matser主节点上，其余的所有物料在待训练的node的同路径中`/home/disk_sdb0/llama2`都需要同步拷贝一份)\n\n137节点传输至79节点的示例：\n\n`scp -r /home/disk_sdb0/llama2/Llama-2-70b-hf root@xxx.xx.xx.79:/home/disk_sdb0/llama2/Llama-2-70b-hf`\n\n`scp -r /home/disk_sdb0/llama2/datas root@xxx.xx.xx.79:/home/disk_sdb0/llama2/datas`\n\n`scp -r /home/disk_sdb0/llama2/output root@xxx.xx.xx.79:/home/disk_sdb0/llama2/output`\n\n`scp -r /home/disk_sdb0/llama2/hostfile.txt root@xxx.xx.xx.79:/home/disk_sdb0/llama2`\n\n`scp -r /home/disk_sdb0/llama2/train.py root@xxx.xx.xx.79:/home/disk_sdb0/llama2`\n\n`scp -r /home/disk_sdb0/llama2/train.example.sh root@xxx.xx.xx.79:/home/disk_sdb0/llama2`\n\n#### 训练拉起\n打完标签之后，注意切换至`cd /home/disk_sdb0/llama2`，apply启动训练脚本。\n\n`kubectl apply -f llama2-70b-test.yaml` 启动yaml。\n\n`kubectl get pod -n llama2-70b-test -owide` 查看ns为llama2-70b-test下的pod信息。\n\n`kubectl describe pod -n llama2-70b-test xxx` 查看每个正在运行的pod信息，用来追踪pod的运行状态，看是否有error报错。\n\n`kubectl logs xxx -n llama2-70b-test -f` 实时跟踪pod的训练日志（最终的训练日志会在设定的主节点master上打印）。\n\n`kubectl delete -f llama2-70b-test.yaml`删除yaml后，将终止训练任务。\n\n### 模型训练测试记录\n#### 报错记录\n##### nvidia driver 与 cuda版本问题\n（nvidia driver 470 和 cuda11.4版本过低，故一直调不到机器GPU的资源，所以在并行策略zero和gemini时候会去调度CPU的资源，然后CPU撑不起来，直接就down掉了）<font style=\"background-color:#C1E77E;\">该错误通过升级nvidia driver至525.125.06，cuda升级为12.0时，问题解决。</font>\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image2.png)\n\n##### docker容器拉起测试问题\n尝试通过该镜像run一个容器来测试，由于缺失<font style=\"color:#000000;\">nvidia-container-toolkit，后续安装完成后，再次run，发现--gpus all 未能调用（nvidia驱动太低导致）。去掉--gpus all之后，容器启动后，识别不到GPU。</font>\n\n`dpkg -l | grep nvidia-container-toolkit` 查看是否安装了<font style=\"color:#000000;\">nvidia-container-toolkit</font>\n\n##### A100需要nvidia-fabricmanager支持\n驱动升级完成后，在xxx.xx.xx.78机器上尝试拉起单机8卡的llama2-7B模型的预训练，报了错误经查阅资料后发现<font style=\"background-color:#C1E77E;\">A100需要nvidia-fabricmanager支持，后续安装对应nvidia driver驱动525.125.06一致的nvidia-fabricmanager后成功拉起训练。</font>\n\n错误解决参考博客：[https://www.cnblogs.com/huadongw/p/16504137.html](https://www.cnblogs.com/huadongw/p/16504137.html)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image3.png)\n\n单机8卡的llama2-7B的预训练成功拉起\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image4.png)\n\n##### 5机40卡预训练时存在某进程随机占用gpu资源问题\n在拉起5机40卡时，一直存在一个ns为single-gpu的pod被调度并占用一个gpu资源（删除之后会随机在6台机器上占用一个gpu）\n\n`kubectl describe node xxx-xx-xx-142.XXX-stack.com`查看142节点情况\n\n正常情况下，一台机器的8台gpu都处于空闲状态下，nvidia.com/gpu的占用应该是0\n\n<font style=\"background-color:#C1E77E;\">解决方法：删除掉启动该pod的对应的label（gpu-a100）并删除掉该pod</font>\n\n`kubectl label nodes xxx-xx-xx-141.XXX-stack.com gpu-`\n\n`kubectl label nodes xxx-xx-xx-142.XXX-stack.com gpu-`\n\n`kubectl label nodes xxx-xx-xx-143.XXX-stack.com gpu-`\n\n`kubectl label nodes xxx-xx-xx-78.XXX-stack.com gpu-`\n\n`kubectl label nodes xxx-xx-xx-79.XXX-stack.com gpu-`\n\n`kubectl label nodes xxx-xx-xx-80.XXX-stack.com gpu-`\n\n#### 训练拉起测试\n##### 预训练\n###### llama2-13B 2机16卡\n1. 启动k8s的yaml配置文件\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: llama2-70b-test\n  name: llama2-70b-pt # Deployment 的名称\nspec:\n  replicas: 2 # 副本数量\n  selector:\n    matchLabels:\n      app: llama2-70b-pt\n  template:\n    metadata:\n      labels:\n        app: llama2-70b-pt\n    spec:\n      hostNetwork: true\n      containers:\n      - name: llama2-70b-container # 容器的名称\n        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/bash\", \"-c\"]\n        args: [\"cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ && chmod +x train.example.sh && ./train.example.sh\"] # 启动容器后执行的命令，切换目录并运行训练脚本\n        resources:\n          limits:\n            nvidia.com/gpu: 8 # 限制使用 8 个 GPU，根据实际情况调整\n          requests:\n            nvidia.com/gpu: 8\n        volumeMounts:\n        - name: colossalai-weight\n          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载\n        - name: colossalai-train\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py\n          subPath: train.py\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: colossalai-output\n          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载\n        - name: host-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile\n          subPath: hostfile.txt\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: train-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # 容器内train_example.sh\n          subPath: train.example.sh\n        - name: shm-volume\n          mountPath: /dev/shm\n      nodeSelector:\n        model: llama2-70b\n      volumes:\n      - name: colossalai-weight\n        hostPath:\n          path: /home/disk_sdb0/llama2/Llama-2-13b-hf # 宿主机中 ColossalAI 权重的路径\n          type: Directory\n      - name: colossalai-train\n        hostPath:\n          path: /home/disk_sdb0/llama2\n          type: Directory\n      - name: colossalai-output\n        hostPath:\n          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径\n          type: Directory\n      - name: host-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: train-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: train-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 8Gi\n```\n\n2. 训练脚本\n\n```bash\n#!/bin/bash\n\n# NCCL IB environment variables\nexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1\nexport NCCL_IB_DISABLE=0\nexport NCCL_SOCKET_IFNAME=bond4\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_IB_TIMEOUT=23\nexport NCCL_IB_RETRY_CNT=7\nexport OMP_NUM_THREADS=8\n\nPROJECT_NAME=\"llama2-70b-pt\"\nPARENT_SAVE_DIR=\"/opt/tmp/ColossalAI/output/save/\"\nPARENT_TENSORBOARD_DIR=\"/opt/tmp/ColossalAI/output/tensorboard/\"\nPARENT_CONFIG_FILE=\"/opt/tmp/ColossalAI/output/config/\"\nPRETRAINED_MODEL_PATH=\"/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf\"\ndeclare -a dataset=(\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009\"\n)\n\nTIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)\nFULL_PROJECT_NAME=\"${PROJECT_NAME}-${TIMESTAMP}\"\nSAVE_DIR=\"${PARENT_SAVE_DIR}${FULL_PROJECT_NAME}\"\nTENSORBOARD_DIR=\"${PARENT_TENSORBOARD_DIR}${FULL_PROJECT_NAME}\"\nCONFIG_FILE=\"${PARENT_CONFIG_FILE}${FULL_PROJECT_NAME}.json\"\n\ncolossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.78 --master_port 30015 train.py \\\n    --pretrained $PRETRAINED_MODEL_PATH \\\n    --dataset ${dataset[@]} \\\n    --plugin \"gemini_auto\" \\\n    --save_interval 400 \\\n    --save_dir $SAVE_DIR \\\n    --tensorboard_dir $TENSORBOARD_DIR \\\n    --config_file $CONFIG_FILE \\\n    --num_epochs 1 \\\n    --micro_batch_size 8 \\\n    --lr 1e-4 \\\n    --mixed_precision \"fp16\" \\\n    --grad_clip 1.0 \\\n    --weight_decay 0.01 \\\n    --warmup_steps 100 \\\n    --use_grad_checkpoint \\\n    --max_length 512 \\\n    --accumulation_steps 1 \\\n    --pad_token \"unk\"\n```\n\n3. hostfile.txt\n\n```plain\nxxx.xx.xx.78\nxxx.xx.xx.79\n```\n\n4. 训练拉起截图\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image5.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image6.png)\n\n\n5. 78，79显存占用情况\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image7.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image8.png)\n\n###### llama2-70B 2机16卡\n1. 参数配置\n\n| micro_batch_size | max_length | plugin | mixed_precision | tflops |\n| --- | --- | --- | --- | --- |\n| 8 | 512 | gemini_auto | fp16 | 64.4685 |\n\n\n2. 启动k8s的yaml配置文件\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: llama2-70b-test\n  name: llama2-70b-pt # Deployment 的名称\nspec:\n  replicas: 2 # 副本数量\n  selector:\n    matchLabels:\n      app: llama2-70b-pt\n  template:\n    metadata:\n      labels:\n        app: llama2-70b-pt\n    spec:\n      hostNetwork: true\n      containers:\n      - name: llama2-70b-container # 容器的名称\n        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/bash\", \"-c\"]\n        args: [\"cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ && chmod +x train.example.sh && ./train.example.sh\"] # 启动容器后执行的命令，切换目录并运行训练脚本\n        resources:\n          limits:\n            nvidia.com/gpu: 8 # 限制使用 4 个 GPU，根据实际情况调整\n          requests:\n            nvidia.com/gpu: 8\n        volumeMounts:\n        - name: colossalai-weight\n          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载\n        - name: colossalai-train\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py\n          subPath: train.py\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: colossalai-output\n          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载\n        - name: host-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile\n          subPath: hostfile.txt\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: train-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # 容器内train_example.sh\n          subPath: train.example.sh\n        - name: profiler-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/performance_evaluator.py  # 容器内train_example.sh\n          subPath: performance_evaluator.py\n        - name: shm-volume\n          mountPath: /dev/shm\n      nodeSelector:\n        model: llama2-70b\n      volumes:\n      - name: colossalai-weight\n        hostPath:\n          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # 宿主机中 ColossalAI 权重的路径\n          type: Directory\n      - name: colossalai-train\n        hostPath:\n          path: /home/disk_sdb0/llama2\n          type: Directory\n      - name: colossalai-output\n        hostPath:\n          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径\n          type: Directory\n      - name: host-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: train-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: profiler-script\n        hostPath:\n            path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n            type: Directory\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 8Gi\n```\n\n3. 训练脚本\n\n```bash\n#!/bin/bash\n\n# NCCL IB environment variables\nexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1\nexport NCCL_IB_DISABLE=0\nexport NCCL_SOCKET_IFNAME=bond4\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_IB_TIMEOUT=23\nexport NCCL_IB_RETRY_CNT=7\nexport OMP_NUM_THREADS=8\n\nPROJECT_NAME=\"llama2-70b-pt\"\nPARENT_SAVE_DIR=\"/opt/tmp/ColossalAI/output/save/\"\nPARENT_TENSORBOARD_DIR=\"/opt/tmp/ColossalAI/output/tensorboard/\"\nPARENT_CONFIG_FILE=\"/opt/tmp/ColossalAI/output/config/\"\nPRETRAINED_MODEL_PATH=\"/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf\"\ndeclare -a dataset=(\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009\"\n)\n\nTIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)\nFULL_PROJECT_NAME=\"${PROJECT_NAME}-${TIMESTAMP}\"\nSAVE_DIR=\"${PARENT_SAVE_DIR}${FULL_PROJECT_NAME}\"\nTENSORBOARD_DIR=\"${PARENT_TENSORBOARD_DIR}${FULL_PROJECT_NAME}\"\nCONFIG_FILE=\"${PARENT_CONFIG_FILE}${FULL_PROJECT_NAME}.json\"\n\ncolossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.78 --master_port 30015 train.py \\\n    --pretrained $PRETRAINED_MODEL_PATH \\\n    --dataset ${dataset[@]} \\\n    --plugin \"gemini_auto\" \\\n    --save_interval 400 \\\n    --save_dir $SAVE_DIR \\\n    --tensorboard_dir $TENSORBOARD_DIR \\\n    --config_file $CONFIG_FILE \\\n    --num_epochs 1 \\\n    --micro_batch_size 8 \\\n    --lr 1e-4 \\\n    --mixed_precision \"fp16\" \\\n    --grad_clip 1.0 \\\n    --weight_decay 0.01 \\\n    --warmup_steps 100 \\\n    --use_grad_checkpoint \\\n    --max_length 512 \\\n    --accumulation_steps 1 \\\n    --pad_token \"unk\"\n```\n\n4. hostfile.txt\n\n```plain\nxxx.xx.xx.78\nxxx.xx.xx.79\n```\n\n5. 训练拉起截图\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image9.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image10.png)\n\n6. 78，79显存占用情况\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image11.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image12.png)\n\n###### llama2-70B 5机40卡\n1. 预训练数据集\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image13.png)\n\n由于模型需要对通过脚本对raw原始数据集进行切分，这个步骤会特别耗时，会随着数据集大小的增加而增加切分时长，由于每次拉起训练时都需要切分并消耗很多时间，所以采用了<font style=\"background-color:#C1E77E;\">离线切分的方式将数据集离线切分好并直接挂载读取使用</font>。\n\n<font style=\"background-color:#C1E77E;\">离线切分好的数据集文件共211G，其中会将原始数据集切分成10份并为arrow格式文件</font>。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image14.png)\n\n2. 启动k8s的yaml配置文件\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: llama2-70b-test\n  name: llama2-70b-pt # Deployment 的名称\nspec:\n  replicas: 5 # 副本数量\n  selector:\n    matchLabels:\n      app: llama2-70b-pt\n  template:\n    metadata:\n      labels:\n        app: llama2-70b-pt\n    spec:\n      hostNetwork: true\n      containers:\n      - name: llama2-70b-container # 容器的名称\n        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/bash\", \"-c\"]\n        args: [\"cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ && chmod +x train.example.sh && ./train.example.sh\"] # 启动容器后执行的命令，切换目录并运行训练脚本\n        resources:\n          limits:\n            nvidia.com/gpu: 8 # 限制使用 8 个 GPU，根据实际情况调整\n          requests:\n            nvidia.com/gpu: 8\n        volumeMounts:\n        - name: colossalai-orgdataset\n          mountPath: /opt/tmp/ColossalAI/datas/org_dataset # 将容器内数据目录挂载\n        - name: colossalai-predataset\n          mountPath: /opt/tmp/ColossalAI/datas/pre_dataset2\n        - name: colossalai-weight\n          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载\n        - name: colossalai-train\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py\n          subPath: train.py\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: colossalai-output\n          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载\n        - name: host-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile\n          subPath: hostfile.txt\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: train-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # 容器内train_example.sh\n          subPath: train.example.sh\n        - name: shm-volume\n          mountPath: /dev/shm\n      nodeSelector:\n        model: llama2-70b\n      volumes:\n      - name: colossalai-orgdataset\n        hostPath:\n          path: /home/disk_sdb0/llama2/datas/org_dataset # 宿主机中 ColossalAI 数据的路径\n          type: Directory\n      - name: colossalai-predataset\n        hostPath:\n          path: /home/disk_sdb0/llama2/datas/pre_dataset2\n          type: Directory\n      - name: colossalai-weight\n        hostPath:\n          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # 宿主机中 ColossalAI 权重的路径\n          type: Directory\n      - name: colossalai-train\n        hostPath:\n          path: /home/disk_sdb0/llama2\n          type: Directory\n      - name: colossalai-output\n        hostPath:\n          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径\n          type: Directory\n      - name: host-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: train-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: profiler-script\n        hostPath:\n            path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n            type: Directory\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 8Gi\n```\n\n3. 训练脚本\n\n```bash\n#!/bin/bash\n\n# NCCL IB environment variables\nexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1\nexport NCCL_IB_DISABLE=0\nexport NCCL_SOCKET_IFNAME=bond4\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_IB_TIMEOUT=23\nexport NCCL_IB_RETRY_CNT=7\nexport OMP_NUM_THREADS=8\n\nPROJECT_NAME=\"llama2-70b-pt\"\nPARENT_SAVE_DIR=\"/opt/tmp/ColossalAI/output/save/\"\nPARENT_TENSORBOARD_DIR=\"/opt/tmp/ColossalAI/output/tensorboard/\"\nPARENT_CONFIG_FILE=\"/opt/tmp/ColossalAI/output/config/\"\nPRETRAINED_MODEL_PATH=\"/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf\"\ndeclare -a dataset=(\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009\"\n)\n\nTIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)\nFULL_PROJECT_NAME=\"${PROJECT_NAME}-${TIMESTAMP}\"\nSAVE_DIR=\"${PARENT_SAVE_DIR}${FULL_PROJECT_NAME}\"\nTENSORBOARD_DIR=\"${PARENT_TENSORBOARD_DIR}${FULL_PROJECT_NAME}\"\nCONFIG_FILE=\"${PARENT_CONFIG_FILE}${FULL_PROJECT_NAME}.json\"\n\ncolossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.142 --master_port 30015 train.py \\\n    --pretrained $PRETRAINED_MODEL_PATH \\\n    --dataset ${dataset[@]} \\\n    --plugin \"gemini_auto\" \\\n    --save_interval 1600 \\\n    --save_dir $SAVE_DIR \\\n    --tensorboard_dir $TENSORBOARD_DIR \\\n    --config_file $CONFIG_FILE \\\n    --num_epochs 1 \\\n    --micro_batch_size 16 \\\n    --lr 1e-4 \\\n    --mixed_precision \"fp16\" \\\n    --grad_clip 1.0 \\\n    --weight_decay 0.01 \\\n    --warmup_steps 100 \\\n    --use_grad_checkpoint \\\n    --max_length 512 \\\n    --accumulation_steps 1 \\\n    --pad_token \"unk\"\n```\n\n4. hostfile.txt\n\n```plain\nxxx.xx.xx.142\nxxx.xx.xx.143\nxxx.xx.xx.78\nxxx.xx.xx.79\nxxx.xx.xx.80\n```\n\n5. 参数配置调优\n\n| micro_batch_size | max_length | plugin | mixed_precision | tflops |\n| --- | --- | --- | --- | --- |\n| 1 | 512 | gemini_auto | fp16 | 10.8231 |\n| 2 | 512 | gemini_auto | fp16 | 20.4685 |\n| 4 | 512 | gemini_auto | fp16 | 45.6885 |\n| 8 | 512 | gemini_auto | fp16 | 78.6885 |\n| **16** | **512** | **gemini_auto** | **fp16** | **136.5975** |\n| 16 | 512  | zero2 | fp16 | OOM |\n| 16 | 512 | 3d | fp16 | OOM |\n| 1 | 1024 | gemini_auto | fp16 | OOM |\n| 4 | 2048 | gemini_auto | fp16 | OOM |\n| 16 | 1024 | gemini_auto | fp16 | OOM |\n\n\n6. 训练拉起截图\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image15.png)\n\n总体step1519\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image16.png)\n\n7. 142，143，78，79，80的GPU显存占用\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image17.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image18.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image19.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image20.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image21.png)\n\n8. XXX平台监控\n\n6台机器的GPU性能监控\n涉及客户信息，图片略去。\n单机器141的GPU性能监控\n涉及客户信息，图片略去。\n\n9. 断点续训\n\n7.4日晚上由于142、143、78、79、80五台机器的ssh没有设置免密登录，模型预训练过程中在主节点142保存的checkpoint文件不能及时同步给其他143、78、79、80四个节点，故导致了训练中断，现五台机器的ssh已互相免密，checkpoint信息可以通过主节点142来下发共享。\n\n之后设置读取断点400步后保存的checkpoint与权重信息，拉起训练时会从第400步开始继续训练。\n\n加入`--load_checkpoint $PARENT_LOAD_DIR \\`参数，设定加载的checkpoint位置`PARENT_LOAD_DIR=\"/opt/tmp/ColossalAI/output/save/llama2-70b-pt-2024-07-03-14-11-44/epoch-0_step-400\"`\n\n```bash\n#!/bin/bash\n\n# NCCL IB environment variables\nexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1\nexport NCCL_IB_DISABLE=0\nexport NCCL_SOCKET_IFNAME=bond4\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_IB_TIMEOUT=23\nexport NCCL_IB_RETRY_CNT=7\nexport OMP_NUM_THREADS=8\n\nPROJECT_NAME=\"llama2-70b-pt\"\nPARENT_SAVE_DIR=\"/opt/tmp/ColossalAI/output/save/\"\nPARENT_LOAD_DIR=\"/opt/tmp/ColossalAI/output/save/llama2-70b-pt-2024-07-03-14-11-44/epoch-0_step-400\"\nPARENT_TENSORBOARD_DIR=\"/opt/tmp/ColossalAI/output/tensorboard/\"\nPARENT_CONFIG_FILE=\"/opt/tmp/ColossalAI/output/config/\"\nPRETRAINED_MODEL_PATH=\"/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf\"\ndeclare -a dataset=(\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009\"\n)\n\nTIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)\nFULL_PROJECT_NAME=\"${PROJECT_NAME}-${TIMESTAMP}\"\nSAVE_DIR=\"${PARENT_SAVE_DIR}${FULL_PROJECT_NAME}\"\nTENSORBOARD_DIR=\"${PARENT_TENSORBOARD_DIR}${FULL_PROJECT_NAME}\"\nCONFIG_FILE=\"${PARENT_CONFIG_FILE}${FULL_PROJECT_NAME}.json\"\n\ncolossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.142 --master_port 30015 train.py \\\n    --pretrained $PRETRAINED_MODEL_PATH \\\n    --dataset ${dataset[@]} \\\n    --plugin \"gemini_auto\" \\\n    --save_interval 1600 \\\n    --save_dir $SAVE_DIR \\\n    --load_checkpoint $PARENT_LOAD_DIR \\\n    --tensorboard_dir $TENSORBOARD_DIR \\\n    --config_file $CONFIG_FILE \\\n    --num_epochs 1 \\\n    --micro_batch_size 16 \\\n    --lr 1e-4 \\\n    --mixed_precision \"fp16\" \\\n    --grad_clip 1.0 \\\n    --weight_decay 0.01 \\\n    --warmup_steps 100 \\\n    --use_grad_checkpoint \\\n    --max_length 512 \\\n    --accumulation_steps 1 \\\n    --pad_token \"unk\"\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image22.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image23.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image24.png)\n\n8. 预训练结束\n\n结束后会在设定的主节点142上生成对应权重文件\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image25.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image26.png)\n\n\n#####  微调\n###### llama2-70B 5机40卡\n1. 数据集准备\n\n微调数据集参照Colossalai官网要求的数据对话格式进行准备\n\n数据样式\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image27.png)\n\n数据集的离线切分，需要通过kubectl进入pod内执行切分脚本`prepare_sft_dataset.py`进行切分\n\n```bash\n#!/bin/bash\npython3 prepare_sft_dataset.py \\\n--data_input_dirs \"/opt/tmp/ColossalAI/applications/Colossal-LLaMA/data\" \\\n--tokenizer_dir \"/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf\" \\\n--data_output_dirs \"/opt/tmp/ColossalAI/applications/Colossal-LLaMA/sftdata\" \\\n--max_length 4096 \\\n--num_spliced_dataset_bins 10 \\\n--llama_version 2\n```\n\n执行切分脚本后，开始进行切分，会将原始数据集格式jsonl转为arrow格式\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image28.png)\n\n`tar -cvf sftdata.tar sftdata` 压缩切分好的数据集sftdata\n\n`kubectl cp llama2-70b-test/llama2-70b-pt-f5877d7c9-n7448:/opt/tmp/ColossalAI/applications/Colossal-LLaMA/sftdata.tar /home/disk_sdb0/llama2/sftdata.tar`通过`kubectl cp`将压缩好的sftdata.tar传输至宿主机137上\n\n`tar -xvf sftdata.tar`解压\n\n切分好的数据集为52G\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image29.png)\n\n2. 启动k8s的yaml的配置文件\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: llama2-70b-test\n  name: llama2-70b-pt # Deployment 的名称\nspec:\n  replicas: 5 # 副本数量\n  selector:\n    matchLabels:\n      app: llama2-70b-pt\n  template:\n    metadata:\n      labels:\n        app: llama2-70b-pt\n    spec:\n      hostNetwork: true\n      containers:\n      - name: llama2-70b-container # 容器的名称\n        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/bash\", \"-c\"]\n        args: [\"cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ && chmod +x finetune.example.sh && ./finetune.example.sh\"] # 启动容器后执行的命令，切换目录并运行训练脚本\n        resources:\n          limits:\n            nvidia.com/gpu: 8 # 限制使用 8 个 GPU，根据实际情况调整\n          requests:\n            nvidia.com/gpu: 8\n        volumeMounts:\n        - name: colossalai-sftdataset\n          mountPath: /opt/tmp/ColossalAI/datas/sftdata\n        - name: colossalai-weight\n          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载\n        - name: colossalai-train\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py\n          subPath: train.py\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: colossalai-output\n          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载\n        - name: host-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile\n          subPath: hostfile.txt\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: finetune-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/finetune.example.sh  # 容器内finetune.example.sh\n          subPath: finetune.example.sh\n        - name: shm-volume\n          mountPath: /dev/shm\n      nodeSelector:\n        model: llama2-70b\n      volumes:\n      - name: colossalai-sftdataset\n        hostPath:\n          path: /home/disk_sdb0/llama2/datas/sftdata\n          type: Directory\n      - name: colossalai-weight\n        hostPath:\n          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # 宿主机中 ColossalAI 权重的路径\n          type: Directory\n      - name: colossalai-train\n        hostPath:\n          path: /home/disk_sdb0/llama2\n          type: Directory\n      - name: colossalai-output\n        hostPath:\n          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径\n          type: Directory\n      - name: host-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: finetune-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 8Gi\n```\n\n3. 微调脚本\n\n```bash\n#!/bin/bash\n\n# NCCL IB environment variables\nexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1\nexport NCCL_IB_DISABLE=0\nexport NCCL_SOCKET_IFNAME=bond4\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_IB_TIMEOUT=23\nexport NCCL_IB_RETRY_CNT=7\nexport OMP_NUM_THREADS=8\n#export NCCL_P2P_DISABLE=1\nexport NCCL_DEBUG=INFO\n\nPROJECT_NAME=\"llama2-70b-ft\"\nPARENT_SAVE_DIR=\"/opt/tmp/ColossalAI/output/save/\"\nPARENT_TENSORBOARD_DIR=\"/opt/tmp/ColossalAI/output/tensorboard/\"\nPARENT_CONFIG_FILE=\"/opt/tmp/ColossalAI/output/config/\"\nPRETRAINED_MODEL_PATH=\"/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf\"\ndeclare -a dataset=(\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00000\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00001\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00002\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00003\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00004\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00005\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00006\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00007\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00008\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00009\"\n)\n\nTIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)\nFULL_PROJECT_NAME=\"${PROJECT_NAME}-${TIMESTAMP}\"\nSAVE_DIR=\"${PARENT_SAVE_DIR}${FULL_PROJECT_NAME}\"\nTENSORBOARD_DIR=\"${PARENT_TENSORBOARD_DIR}${FULL_PROJECT_NAME}\"\nCONFIG_FILE=\"${PARENT_CONFIG_FILE}${FULL_PROJECT_NAME}.json\"\n\ncolossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.142 --master_port 30013 train.py \\\n    --pretrained $PRETRAINED_MODEL_PATH \\\n    --dataset ${dataset[@]} \\\n    --plugin \"gemini\" \\\n    --save_interval 400 \\\n    --save_dir $SAVE_DIR \\\n    --tensorboard_dir $TENSORBOARD_DIR \\\n    --config_file $CONFIG_FILE \\\n    --num_epochs 1 \\\n    --accumulation_steps 1 \\\n    --micro_batch_size 1 \\\n    --lr 5e-5 \\\n    --mixed_precision \"bf16\" \\\n    --grad_clip 1.0 \\\n    --weight_decay 0.01 \\\n    --warmup_steps 100 \\\n    --use_grad_checkpoint \\\n    --use_neft \\\n    --pad_token \"eos\"\n```\n\n4. hostfile.txt\n\n```plain\nxxx.xx.xx.142\nxxx.xx.xx.143\nxxx.xx.xx.78\nxxx.xx.xx.79\nxxx.xx.xx.80\n```\n\n5. 拉起微调训练\n\n```bash\n#查看pod的详细信息\nkubectl get pod -n llama2-70b-test -owide\n#删除启动微调训练的yaml\nkubectl delete -f llama2-70b-finetune.yaml\n#启动微调训练的yaml\nkubectl apply -f llama2-70b-finetune.yaml\n#查看pod日志\nkubectl logs -n llama2-70b-test llama2-70b-pt-7b5884d87c-pmd76 -f\n#查看pod状态信息\nkubectl describe pod -n llama2-70b-test llama2-70b-pt-6599d788c7-7nngz\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image30.png)\n\n6. 参数配置及调优\n\n| micro_batch_size | accumulation_steps | plugin | mixed_precision | tflops |\n| --- | --- | --- | --- | --- |\n| 1 | 1 | gemini | bf16 | 38.8488 |\n| 4 | 1 | gemini | bf16 | OOM |\n| 8 | 1 | gemini | bf16 | OOM |\n| 1 | 1 | gemini_auto | bf16 | OOM |\n| 8 | 1 | gemini_auto | bf16 | OOM |\n| 16 | 1 | gemini_auto | bf16 | OOM |\n| 1 | 1 | zero2 | bf16 | OOM |\n| 1 | 1 | 3d | bf16 | OOM |\n| 1 | 4 | gemini | bf16 | OOM |\n| 1 | 8 | gemini | bf16 | OOM |\n\n\n7. 142，143，78，79，80的GPU显存占用\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image31.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image32.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image33.png)  \n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image34.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image35.png)\n\n8. 备注\n\n在拉起微调训练时，尝试调整了训练参数micro_batch_size、accumulation_steps、并行策略gemini、gemini_auto、zero2等参数来尽量缩小总的迭代step步数（数据集的大小也会影响step数），但尝试增加了micro_batch_size等参数后，发现训练会拉不起来（out of memory），当前5机40卡可以成功拉起的参数只能将micro_batch_size调整为1，accumulation_steps为1、并行策略为gemini，总的迭代步数为<font style=\"color:#DF2A3F;\">35132</font>步（<font style=\"background-color:#C1E77E;\">每步迭代耗时需要3分钟左右，</font><font style=\"color:rgb(6, 6, 7);background-color:#C1E77E;\">在每一步迭代中，首先进行前向传播来计算预测输出和损失，然后进行反向传播来计算梯度，最后根据梯度更新模型参数。每个批次都会进行一次梯度和损失的计算，直到完成一个epoch的训练。</font>）\n\n类似参照某模型175B（千卡规模）的拉起的训练进度，总的迭代步数为<font style=\"color:#DF2A3F;\">574833。</font>\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image36.png)\n\n类似参照某模型57B（千卡规模）的拉起训练进度，总的迭代步数<font style=\"color:#DF2A3F;\">574636。</font>\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image37.png)\n\n<font style=\"background-color:#C1E77E;\">故完成整套大模型的训练会持续较长的训练周期</font>，并在现有的卡数限制上，在短期内实现所有流程存在难度。\n\n### 结论与说明\n#### 现有环境训练模型参数大小及显存占用\n现有的5机40卡预训练/微调llama2-70B，模型参数及显存占用如下表格所示（<font style=\"background-color:#C1E77E;\">注：由于xxx.xx.xx.141机器日常有业务在跑，无法空闲出完整的8块gpu资源，故最多以</font>**<font style=\"color:#DF2A3F;background-color:#C1E77E;\">5机40卡</font>**<font style=\"background-color:#C1E77E;\">来拉起训练。</font>）\n\n| | 模型参数 | 机器 | GPU卡 | 显存占用 | GPU利用率 |\n| --- | --- | --- | --- | --- | --- |\n| 预训练 | 70B | 142 | 0 | 77886MiB/81920MiB | 99% |\n| | | | 1 | 77898MiB/81920MiB | 99% |\n| | | | 2 | 77898MiB/81920MiB | 99% |\n| | | | 3 | 77898MiB/81920MiB | 99% |\n| | | | 4 | 79946MiB/81920MiB | 99% |\n| | | | 5 | 77898MiB/81920MiB | 99% |\n| | | | 6 | 77898MiB/81920MiB | 99% |\n| | | | 7 | 77874MiB/81920MiB | 100% |\n| | | 143 | 0 | 77886MiB/81920MiB | 100% |\n| | | | 1 | 77898MiB/81920MiB | 100% |\n| | | | 2 | 77898MiB/81920MiB | 100% |\n| | | | 3 | 77898MiB/81920MiB | 99% |\n| | | | 4 | 77898MiB/81920MiB | 99% |\n| | | | 5 | 77886MiB/81920MiB | 99% |\n| | | | 6 | 77898MiB/81920MiB | 99% |\n| | | | 7 | 77886MiB/81920MiB | 99% |\n| | | 78 | 0 | 78764MiB/81920MiB | 99% |\n| | | | 1 | 78776MiB/81920MiB | 99% |\n| | | | 2 | 78776MiB/81920MiB | 99% |\n| | | | 3 | 78776MiB/81920MiB | 99% |\n| | | | 4 | 78776MiB/81920MiB | 100% |\n| | | | 5 | 78776MiB/81920MiB | 100% |\n| | | | 6 | 78776MiB/81920MiB | 100% |\n| | | | 7 | 78764MiB/81920MiB | 99% |\n| | | 79 | 0 | 77886MiB/81920MiB | 99% |\n| | | | 1 | 77898MiB/81920MiB | 99% |\n| | | | 2 | 77886MiB/81920MiB | 99% |\n| | | | 3 | 77898MiB/81920MiB | 100% |\n| | | | 4 | 77898MiB/81920MiB | 99% |\n| | | | 5 | 77898MiB/81920MiB | 99% |\n| | | | 6 | 77898MiB/81920MiB | 100% |\n| | | | 7 | 77886MiB/81920MiB | 100% |\n| | | 80 | 0 | 77886MiB/81920MiB | 99% |\n| | | | 1 | 77898MiB/81920MiB | 99% |\n| | | | 2 | 79948MiB/81920MiB | 99% |\n| | | | 3 | 77898MiB/81920MiB | 99% |\n| | | | 4 | 78350MiB/81920MiB | 99% |\n| | | | 5 | 79386MiB/81920MiB | 99% |\n| | | | 6 | 79374MiB/81920MiB | 100% |\n| | | | 7 | 78338MiB/81920MiB | 99% |\n| 微调 | 70B | 142 | 0 | 53278MiB/81920MiB | 100% |\n| | | | 1 | 52160MiB/81920MiB | 100% |\n| | | | 2 | 53660MiB/81920MiB | 100% |\n| | | | 3 | 53164MiB/81920MiB | 100% |\n| | | | 4 | 52382MiB/81920MiB | 100% |\n| | | | 5 | 53260MiB/81920MiB | 100% |\n| | | | 6 | 53660MiB/81920MiB | 100% |\n| | | | 7 | 54652MiB/81920MiB | 100% |\n| | | 143 | 0 | 53494MiB/81920MiB | 100% |\n| | | | 1 | 53254MiB/81920MiB | 100% |\n| | | | 2 | 53950MiB/81920MiB | 100% |\n| | | | 3 | 53172MiB/81920MiB | 100% |\n| | | | 4 | 67234MiB/81920MiB | 100% |\n| | | | 5 | 53212MiB/81920MiB | 100% |\n| | | | 6 | 53160MiB/81920MiB | 100% |\n| | | | 7 | 53604MiB/81920MiB | 100% |\n| | | 78 | 0 | 51978MiB/81920MiB | 100% |\n| | | | 1 | 51992MiB/81920MiB | 100% |\n| | | | 2 | 51410MiB/81920MiB | 100% |\n| | | | 3 | 51492MiB/81920MiB | 100% |\n| | | | 4 | 51992MiB/81920MiB | 100% |\n| | | | 5 | 51488MiB/81920MiB | 100% |\n| | | | 6 | 52154MiB/81920MiB | 100% |\n| | | | 7 | 47638MiB/81920MiB | 100% |\n| | | 79 | 0 | 48468MiB/81920MiB | 100% |\n| | | | 1 | 49280MiB/81920MiB | 100% |\n| | | | 2 | 49182MiB/81920MiB | 100% |\n| | | | 3 | 48980MiB/81920MiB | 100% |\n| | | | 4 | 49160MiB/81920MiB | 100% |\n| | | | 5 | 49966MiB/81920MiB | 100% |\n| | | | 6 | 49160MiB/81920MiB | 100% |\n| | | | 7 | 49858MiB/81920MiB | 100% |\n| | | 80 | 0 | 49366MiB/81920MiB | 100% |\n| | | | 1 | 48740MiB/81920MiB | 100% |\n| | | | 2 | 49400MiB/81920MiB | 99% |\n| | | | 3 | 47470MiB/81920MiB | 99% |\n| | | | 4 | 36662MiB/81920MiB | 100% |\n| | | | 5 | 37262MiB/81920MiB | 100% |\n| | | | 6 | 37660MiB/81920MiB | 99% |\n| | | | 7 | 37148MiB/81920MiB | 100% |\n\n\n#### AI平台训练可视化分析能力\n4.1中描述的关于GPU的显存使用及利用率情况，主要是通过`nvidia-smi`NVIDIA提供的命令行工具监控分析GPU设备信息，其中的信息包括\n\n1. GPU 利用率（GPU-Util）：显示 GPU 核心的利用率百分比。\n2. 显存使用情况（Memory-Usage）：显示 GPU 当前显存的使用量/总显存量。\n3. 功耗（Pwr:Usage/Cap）：显示 GPU 的当前/最大功耗。\n4. 温度（GPU Temperature）：显示 GPU 的当前温度。\n5. 进程 ID（PID）：显示当前使用 GPU 的进程的进程 ID。\n6. 进程名称：显示使用 GPU 的进程的名称。\n7. 使用 GPU 的应用程序：显示哪些应用程序正在使用 GPU。\n8. GPU 型号：显示 GPU 的型号和版本信息。\n9. 驱动版本：显示当前安装的 NVIDIA 驱动版本。\n10. CUDA 版本：显示当前安装的 CUDA 版本。\n11. ECC 状态：显示错误校正码（ECC）的状态。\n\n对比XXX监控平台的数据，两者在GPU显存使用和GPU利用率上可能会存在差异，由于`nvidia-smi`是实时监控当前时间下的GPU信息，XXX监控平台的信息与`nvidia-smi`监控信息存在时间差，故存在某些数据不同步，这也在合理的范围内。\n\nXXX监控平台包含了各机器的单个GPU的显存使用、GPU利用率、温度、功耗等信息，基本覆盖了模型训练监控所需的一些指标。此外，XXX中还显示了GPU的时钟频率，也代表了GPU性能的关键指标。\n\nXXX监控平台可以进一步监控机间网卡的通信带宽、机内单向通信带宽、Leaf层/spine层交换机信息、模型训练参数等等大模型训练的物理层与软件层信息。\n\n#### 用户的易用性,长时间训练,任务断点,监控告警,自动化拉起\n1. 本次训练采用k8s的方式来管理和拉起集群pod节点，其中涉及到一些拉起k8s的yaml配置文件、以及集群机器的ssh通信配置、etc/hosts主机ip名映射配置、标签管理等步骤，可能需要用户熟悉并操作。此外，机器的驱动环境可以按期更新迭代，以此适应和匹配最新系列大模型训练所需要的新特性。\n2. 本次预训练任务长达70h+，其中存在训练中断问题，当前的方法是通过手动拉起和利用训练脚本中save_interval的间隔步数保存的checkpoint信息来恢复至间隔步数的训练状态。\n3. 训练的监控告警可以通过kubectl命令查看pod`kubectl describe pod`、node`kubectl describe node`、训练日志`kubectl logs`里的错误信息进行排错处理。\n4. 自动化拉起可能需要EKI（k8s管理）平台与PyTorchJob 支持，EKI可以自动化处理pod节点异常并自动拉起新的节点。\n\n","tags":["Nvidia","模型训练调优","Llama2","A100","Colossal-AI"],"categories":["模型训练调优","NVIDIA"]},{"title":"Tulu3-8B-后训练实践","url":"/2025/11/12/模型训练调优/NVIDIA/Tulu3/Tulu3-8B-后训练实践/","content":"# 训练环境\n## 镜像\n```bash\nregistry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:tulu3\n```\n\n镜像中已包含tulu3后训练所需要的所有依赖环境。\n\n## 代码结构\n```bash\n/mnt/users/wangxiangbo/model/tulu3/open-instruct-main\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image1.png)\n\n### data\n包含了`sft`、`dpo`、`rlvr`所需的数据集\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image2.png)\n\n`eval`相关\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image3.png)\n\n### scripts/configs\n训练脚本\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image4.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image5.png)\n\n```bash\ntulu3_sft.sh #tulu3 sft单机脚本\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image6.png)\n\n```bash\nhostfile\nhostfile-12\nsetRank.sh\ntulu3_dpo.sh #多机dpo脚本\ntulu3_dpo_single.sh #单机4卡dpo脚本\ntulu3_dpo.yaml #多机yaml\ntulu3_sft.sh #多机sft脚本\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image7.png)\n\n```bash\ntulu3_rlvr_single.sh #tulu3 rlvr单机脚本\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image8.png)\n\n```bash\nloss_plot.py #loss曲线绘图脚本\n```\n\n### open_instruct\nsft、dpo、rlvr相关训练代码\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image9.png)\n\n\n\n## sft实践\n### 单机4卡测试\n#### sft数据集\nsft数据集采用tulu3官网提供的数据集\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image10.png)\n\nsft数据集样例：\n\n```json\n{\"id\":\"oasst1_2644\",\"messages\":[{\"content\":\"有缓解社交恐惧症的实用方法吗？\",\"role\":\"user\"},{\"content\":\"以下是一些实用的方法，可能有助于缓解社交恐惧症：\\n\\n1.  寻求专业帮助：寻求心理医生或心理治疗师的帮助是缓解社交恐惧症的最佳方法之一。专业的治疗师可以帮助你识别和处理恐惧的源头，并提供有效的治疗方案。\\n\\n2. 练习深呼吸和放松技巧：当你感到焦虑和紧张时，尝试通过深呼吸和放松技巧来帮助你冷静下来。深呼吸可以帮助你减轻身体上的压力和紧张感，从而使你更放松和镇定。\\n\\n3. 学习积极思考：避免消极的自我对话，例如“我会失败”、“我不行”等等。改变自己的思考方式，尝试以积极、自信的态度面对社交场合。\\n\\n4.  採取柔性方法：為了對抗焦慮症狀，建議使用一些方法來提醒你的身體是安全的，例如：  \\n\\n＊ 呼吸訓練（用鼻子深深的吸一口氣再用嘴巴將氣慢慢地吐出，當你專注在吐氣時，你就會被帶出抵抗或逃避的情緒中。） \\n\\n ＊ 專注於現在（這麼做讓你能夠專注於自己所經歷的每分每秒。試著說出你現在能看見、聞到、碰觸到並聽得到的東西，或是坐在椅子上並感受自己的腳觸碰地面的感覺，或身體正被支撐著的感覺。）\\n\\n5. 給自己一點從容：在過渡時期練習自我同情，並提醒自己感到不確定與不安是很正常的。\\n\\n这些方法可以帮助你缓解社交恐惧症，但不同的人可能需要不同的方法和治疗方案。如果你感到严重的焦虑和恐惧，请及时寻求专业帮助。\",\"role\":\"assistant\"},{\"content\":\"好吧，可是我要去哪裡尋求專業幫助？\",\"role\":\"user\"},{\"content\":\"您可以先參考美國精神疾病診斷標準 **DSM-5** 中的6點標準，或者進行一些專業的心理測試。如果您符合這些標准或測試結果顯示您有社交恐懼症，您應儘早尋求專業的幫助。\\n\\n尋求專業幫助的途徑有以下幾種：\\n\\n- **諮詢醫生或心理健康專家**。他們可以為您提供診斷，藥物治療，心理治療等服務。\\n- **參加心理諮詢或治療機構**。這些機構通常有專業的心理諮詢師或治療師，可以為您提供個人或團體的心理諮詢或治療服務。\\n- **加入互助小組或社區**。這些小組或社區通常由有相同或類似問題的人組成，可以為您提供支持，鼓勵，交流和分享的平台。\\n\\n以上是我為您搜索到的一些信息，希望對您有所幫助。如果您還有其他問題，歡迎繼續提問。\",\"role\":\"assistant\"}],\"source\":\"ai2-adapt-dev\\/oasst1_converted\"}\n```\n\n由于是.parquet格式数据，转成json后一共有93w条数据。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image11.png)\n\n由于机器数量有限，训练其完整数据集需要较长时间，故将.parquet转为json后将数据集缩减至5k条，并再次转回.parquet格式数据。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image12.png)\n\n#### 单机脚本配置\n```bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\nexport WANDB_MODE=disabled\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nLOG_SAVE=\"/mnt/open-instruct-main/output/sft/${current_time}-tulu3-sft.log\"\nMODEL_SIZE=8B\nNUM_GPUS=4\nBATCH_SIZE_PER_GPU=2\nTOTAL_BATCH_SIZE=8\nGRADIENT_ACC_STEPS=$(($TOTAL_BATCH_SIZE/($NUM_GPUS * $BATCH_SIZE_PER_GPU)))\necho \"Training llama model ${MODEL_SIZE} using $NUM_GPUS GPUs, $BATCH_SIZE_PER_GPU batch size per GPU, $GRADIENT_ACC_STEPS gradient accumulation steps\"\n\naccelerate launch \\\n    --mixed_precision fp16 \\\n    --num_machines 1 \\\n    --num_processes $NUM_GPUS \\\n    --use_deepspeed \\\n    --deepspeed_config_file /mnt/open-instruct-main/configs/ds_configs/stage3_offloading_accelerate.conf \\\n    /mnt/open-instruct-main/open_instruct/finetune.py \\\n    --model_name_or_path /mnt/LLM-Research/Meta-Llama-31-8B \\\n    --tokenizer_name /mnt/LLM-Research/Meta-Llama-31-8B \\\n    --use_slow_tokenizer \\\n    --train_file /mnt/open-instruct-main/data/sft_data_json/sft_dataset_5k.json \\\n    --max_seq_length 4096 \\\n    --preprocessing_num_workers 4 \\\n    --per_device_train_batch_size $BATCH_SIZE_PER_GPU \\\n    --gradient_accumulation_steps $GRADIENT_ACC_STEPS \\\n    --learning_rate 2e-5 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.03 \\\n    --weight_decay 0. \\\n    --num_train_epochs 1 \\\n    --output_dir /mnt/open-instruct-main/output/sft \\\n    --gradient_checkpointing true \\\n    --report_to none \\\n    --use_flash_attn false 2>&1 | tee -a \"$LOG_SAVE\"\n```\n\n为了尽可能地还原tulu3原论文的实验结果，其中部分超参与论文中最优保持一致。\n\n`--max_seq_length 4096`\n\n`--learning_rate 2e-5`\n\n`--lr_scheduler_type linear`\n\n`--warmup_ratio 0.03`\n\n其中的模型权重利用llama3.1-8B-base模型。\n\n#### 训练测试\n##### 训练结束\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image13.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image14.png)\n\n| 资源类型 | 利用率 | 内存/显存占用量 |\n| :---: | :---: | :---: |\n| CPU | 53.2% | 80.7% |\n| GPU | 85.5% | 66.4% |\n\n\n##### loss曲线\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image15.png)\n\n##### eval\nllama3.1-8B通过tulu3提供的sft数据集（删减至5k条）后，得到的tulu3_sft权重\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image16.png)\n\n| 类别 | Llama-31-8B | tulu3-sft-weight | Llama-31-Tulu-3-8B-SFT |\n| :---: | :---: | :---: | :---: |\n| 平均准确率 | 0.6014 | 0.5729 | **<font style=\"color:#DF2A3F;\">0.6356</font>** |\n| 子类别准确率 | - | - | **<font style=\"color:#000000;\">-</font>** |\n| 数学 | 0.4098 | 0.3778 | **0.4192** |\n| 健康 | 0.6561 | 0.6183 | **0.6720** |\n| 物理 | 0.5063 | 0.4891 | **0.5328** |\n| 商业 | 0.7643 | 0.7346 | **0.8101** |\n| 生物 | 0.7555 | 0.6982 | **0.7775** |\n| 化学 | 0.4851 | 0.4851 | **0.5186** |\n| 计算机科学 | 0.5752 | 0.5194 | **0.6092** |\n| 经济 | 0.5930 | 0.5809 | **0.6213** |\n| 工程 | 0.5517 | 0.5172 | **0.5517** |\n| 哲学 | 0.4881 | 0.4627 | **0.5775** |\n| 其他 | 0.6524 | 0.6532<font style=\"color:#000000;\">⬆</font> | **0.7090** |\n| 历史 | 0.7452 | 0.7172 | **0.7774** |\n| 地理 | 0.7323 | 0.7273 | **0.7677** |\n| 政治 | 0.7546 | 0.7099 | **0.7762** |\n| 心理学 | 0.7174 | 0.6845 | **0.7485** |\n| 文化 | **0.8102** | 0.7440 | 0.7922 |\n| 法律 | 0.4878 | 0.4589 | **0.5020** |\n| 类别准确率 | - | - | - |\n| STEM | 0.5192 | 0.4864 | **0.5394** |\n| 人文学科 | 0.5388 | 0.5116 | **0.5887** |\n| 社会科学 | 0.7062 | 0.6740 | **0.7296** |\n| 其他（商业、健康等） | 0.6693 | 0.6465 | **0.7039** |\n\n\n##### eval结果分析\n+ SFT后权重的平均准确率: 0.5729\n+ SFT前权重的平均准确率: 0.6014\n\n整体平均准确率下降了 ~2.85%。\n\n原因：本次实验（由于机器环境限制）将原始的tulu3 SFT数据集（93w）删减到5k条，导致模型未能充分学习各个领域的知识和任务特性，泛化能力下降。\n\n## dpo实践\n### 单机4卡测试\n#### 偏好数据集\n偏好数据集采用tulu3官网提供的数据集。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image17.png)\n\n偏好数据集样例：\n\n```json\n{\"id\":\"allenai\\/tulu-3-sft-reused-off-policy_0\",\"prompt\":\"Peter has 5 cages with rabbits (the cages are arranged in a row). It is known that there is at least one rabbit in each cage. We will call two rabbits neighbors if they are either in the same cage or in adjacent cages. It turned out that each rabbit has either 3 or 7 neighbors. How many rabbits are in the central cage?\",\"chosen\":[{\"content\":\"Peter has 5 cages with rabbits (the cages are arranged in a row). It is known that there is at least one rabbit in each cage. We will call two rabbits neighbors if they are either in the same cage or in adjacent cages. It turned out that each rabbit has either 3 or 7 neighbors. How many rabbits are in the central cage?\",\"role\":\"user\"},{\"content\":\"Here's how to solve this problem:\\n\\n**Understanding the Information**\\n\\n* **Each rabbit has 3 or 7 neighbors:** This means there must be a pattern in how the rabbits are distributed within the cages.\\n* **At least one rabbit in each cage:**  We can't have cages without any rabbits.\\n\\n**Reasoning**\\n\\n* **Corner Cages:**  Rabbits in the corner cages (cages 1 and 5) can only have 3 neighbors (two adjacent cages and one within their own cage).\\n* **Middle Cages:** Rabbits in the middle cages (cages 2, 3, and 4) must have 7 neighbors to balance out the setup. \\n\\n**The Solution**\\n\\nSince rabbits in the middle cages need 7 neighbors, and there's at least one rabbit in each cage, the central cage (cage 3) must have **4 rabbits**. \\n\\n\\n\\nLet me know if you'd like to explore other logic puzzles! \\n\",\"role\":\"assistant\"}],\"rejected\":[{\"content\":\"Peter has 5 cages with rabbits (the cages are arranged in a row). It is known that there is at least one rabbit in each cage. We will call two rabbits neighbors if they are either in the same cage or in adjacent cages. It turned out that each rabbit has either 3 or 7 neighbors. How many rabbits are in the central cage?\",\"role\":\"user\"},{\"content\":\"The central cage has the same number of rabbits as any other cage, so it also has either 3 or 7 neighbors.\\nThere are (at most) 7 rabbits in the central cage and (at most) 3 rabbits in any cage on either side. Hence, there are (at most) 7 rabbits in each of those cages. The total number of rabbits in the 5 cages is (at most) 5 * 7 + 1 = 36 + 1 = 37.\\nOne cage must have exactly the right number of neighbors, but two cages cannot have the right number.\\nIf each cage has exactly 3 neighbors, the total number of rabbits in all 5 cages is 1 + 5 * 3 = 16\\nIf each cage has exactly 7 neighbors, the total number of rabbits in all 5 cages is 1 + 5 * 7 = 32\\nNow that we know there are at most 36 rabbits, we can safely conclude that there is 1 rabbit in the central cage, that there are 5 cages with 3 rabbits each and one cage with 7 rabbits and that the central cage has 3 neighbors.\\nThe central cage has exactly 3 rabbits and 3 neighbors.\\nIn the other 4 cages, there are (at most) 5 * 3 = 15 rabbits.\\nIn the other 5 cages, there are (at least) 32 – 16 = 16 rabbits.\",\"role\":\"assistant\"}],\"source\":\"allenai\\/tulu-3-sft-reused-off-policy\"}\n```\n\n由于是.parquet格式数据，转成json后一共有67w条数据。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image18.png)\n\n由于机器数量有限，训练其完整数据集需要较长时间，故将.parquet转为json后将数据集缩减至5k条，并再次转回.parquet格式数据。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image19.png)\n\n#### 单机脚本配置\n单机脚本，部分配置OOM，采用deepseed中zero3以及cpu_offloading，开启梯度累积。\n\n```bash\n# you need 8 GPUs for full finetuning\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\nexport WANDB_MODE=disabled\n\nNUM_GPUS=4\nBATCH_SIZE_PER_GPU=1\nTOTAL_BATCH_SIZE=8\nGRADIENT_ACC_STEPS=$(($TOTAL_BATCH_SIZE/$NUM_GPUS/$BATCH_SIZE_PER_GPU))\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nLOG_SAVE=\"/mnt/open-instruct-main/output/dpo/${current_time}-tulu3-dpo.log\"\necho \"Training model using $NUM_GPUS GPUs, $BATCH_SIZE_PER_GPU batch size per GPU, $GRADIENT_ACC_STEPS gradient accumulation steps\"\n\n#stage3_no_offloading_accelerate.conf\naccelerate launch \\\n    --mixed_precision fp16 \\\n    --num_machines 1 \\\n    --num_processes $NUM_GPUS \\\n    --use_deepspeed \\\n    --deepspeed_config_file /mnt/open-instruct-main/configs/ds_configs/stage3_offloading_accelerate.conf \\\n    /mnt/open-instruct-main/open_instruct/dpo_tune.py \\\n    --model_name_or_path /mnt/LLM-Research/Llama-31-Tulu-3-8B-SFT \\\n    --use_flash_attn  false\\\n    --gradient_checkpointing \\\n    --tokenizer_name /mnt/LLM-Research/Llama-31-Tulu-3-8B-SFT \\\n    --use_slow_tokenizer \\\n    --dataset_name /mnt/open-instruct-main/data/dpo_data_5k \\\n    --max_seq_length 2048 \\\n    --preprocessing_num_workers 4 \\\n    --per_device_train_batch_size $BATCH_SIZE_PER_GPU \\\n    --gradient_accumulation_steps $GRADIENT_ACC_STEPS \\\n    --learning_rate 5e-7 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.1 \\\n    --weight_decay 0. \\\n    --num_train_epochs 1 \\\n    --output_dir /mnt/open-instruct-main/output/dpo \\\n    --with_tracking False \\\n    --logging_steps 1 2>&1 | tee -a \"$LOG_SAVE\"\n```\n\n为了尽可能地还原tulu3原论文的实验结果，其中部分超参与论文中最优保持一致。\n\n`--max_seq_length 2048`\n\n`--learning_rate 5e-7`\n\n`--lr_scheduler_type linear`\n\n`--warmup_ratio 0.1`\n\n其中的模型权重利用tulu3开源的经过llama3.1-8B sft得到的权重。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image20.png)\n\n#### 训练测试\n##### 关闭cpu_offloading\n注：关闭cpu_offloading之后，迭代10步左右会OOM\n\n\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image21.png)\n\n| 资源类型 | 利用率 | 内存/显存占用量 |\n| :---: | :---: | :---: |\n| CPU | 16.5% | 12.1% |\n| GPU | 100% | 93.6% |\n\n\n##### 开启cpu_offloading\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image22.png)\n\n| 资源类型 | 利用率 | 内存/显存占用量 |\n| :---: | :---: | :---: |\n| CPU | 55.0% | 58.2% |\n| GPU | 85.8% | 68.7% |\n\n\n##### 训练完成\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image23.png)\n\n得到的dpo权重\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image24.png)\n\n##### loss曲线\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image25.png)\n\n##### eval\n评测数据集采用MMLU，一个包含来自各个知识领域的多项选择题的巨大多任务测试。 测试涵盖了人文学科、社会科学、自然科学以及其他对某些人来说重要的学习领域。 数据集中的问题是由研究生和本科生从在线免费资源中手动收集的。这包括研究生入学考试和美国医学执照考试等考试的练习题。还包括为本科生课程设计的题目，以及为牛津大学出版社书籍读者设计的题目。 一些任务涵盖一个主题，如心理学，但难度级别特定，例如“初阶”、“高中”、“大学”或“专业”。 例如，“专业心理学”任务借鉴了心理学专业实践考试的免费练习题中的问题，而“高中心理学”任务则包含类似高级 Placement 心理学考试中的问题。\n\n:::tips\nMMLU数据集样例如下，摘自MMLU论文[https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300)\n\n:::\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image26.png)\n\neg：以下是MMLU上<font style=\"color:rgb(0,0,0);\">College Biology类问题及答案</font>：\n\n```plain\n\"Based on the characteristic population curves that result from plotting population growth of a species, the most effective means of controlling the mosquito population is to\",\nmaintain the population at a point corresponding to the midpoint of its logistic curve,\nopt for zero population control once the K value of the curve has been reached,\nreduce the carrying capacity cif the environment to lower the K value,\nincrease the mortality rate,\nC\n```\n\neg：以下是`dpo_weight`在MMLU上<font style=\"color:rgb(0,0,0);\">College Biology类问题的回答</font>：\n\n```plain\n0,1,2,3,4,5,correct,choiceA_probs,choiceB_probs,choiceC_probs,choiceD_probs\n\"Based on the characteristic population curves that result from plotting population growth of a species, the most effective means of controlling the mosquito population is to\",\nmaintain the population at a point corresponding to the midpoint of its logistic curve,\nopt for zero population control once the K value of the curve has been reached,\nreduce the carrying capacity cif the environment to lower the K value,\nincrease the mortality rate,\nC,\nTrue,\n0.07324660569429398,\n0.0071399579755961895,\n0.6528399586677551,\n0.1929791122674942\n```\n\n\n\n本次dpo后得到的权重`dpo_weight`对比tulu3开源的sft权重`Llama-31-Tulu-3-8B-SFT`。\n\n`Llama-31-Tulu-3-8B-SFT`在MMLU上的表现：\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image27.png)\n\n`dpo_weight`在MMLU上的表现：\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image28.png)\n\n| 类别 | Llama-31-8B | Llama-31-Tulu-3-8B-SFT | dpo_weight | Llama-31-Tulu-3-8B-DPO |\n| :---: | :---: | :---: | :---: | :---: |\n| 平均准确率 | 0.6014 | 0.6356 | **<font style=\"color:#DF2A3F;\">0.6377</font>**⬆ | 0.6352 |\n| 子类别准确率 | - | **<font style=\"color:#000000;\">-</font>** | **<font style=\"color:#000000;\">-</font>** | - |\n| 数学 | 0.4098 | 0.4192 | **0.4352**⬆ | 0.4258 |\n| 健康 | 0.6561 | 0.6720 | 0.6726⬆ | **0.6915** |\n| 物理 | 0.5063 | 0.5328 | **0.5500**⬆ | 0.5391 |\n| 商业 | 0.7643 | **0.8101** | 0.7941 | 0.7849 |\n| 生物 | 0.7555 | 0.7775 | **0.7775**⬆ | 0.7731 |\n| 化学 | 0.4851 | 0.5186 | 0.5182 | **0.5248** |\n| 计算机科学 | 0.5752 | 0.6092 | **0.6189**⬆ | 0.6117 |\n| 经济 | 0.5930 | 0.6213 | 0.6173 | **0.6429** |\n| 工程 | 0.5517 | 0.5517 | **0.5655**⬆ | 0.5586 |\n| 哲学 | 0.4881 | **0.5775** | 0.5626 | 0.5388 |\n| 其他 | 0.6524 | 0.7090 | **0.7107**⬆ | 0.7082 |\n| 历史 | 0.7452 | 0.7774 | **0.7828**⬆ | 0.7720 |\n| 地理 | 0.7323 | 0.7677 | 0.7778⬆ | **0.7778** |\n| 政治 | 0.7546 | 0.7762 | **0.7793**⬆ | 0.7762 |\n| 心理学 | 0.7174 | 0.7485 | **0.7519**⬆ | 0.7476 |\n| 文化 | **0.8102** | 0.7922 | 0.7892 | 0.8042 |\n| 法律 | 0.4878 | 0.5020 | **0.5133**⬆ | 0.5156 |\n| 父类别准确率 | - | - | **-** | - |\n| STEM | 0.5192 | 0.5394 | **0.5507**⬆ | 0.5437 |\n| 人文学科 | 0.5388 | **0.5887** | 0.5877 | 0.5762 |\n| 社会科学 | 0.7062 | 0.7296 | 0.7309⬆ | **0.7364** |\n| 其他（商业、健康、杂项） | 0.6693 | 0.7039 | 0.7027 | **0.7101** |\n\n\n### dpo代码实现分析\n#### Length-Normalized DPO 的实现\n##### `_get_batch_logps` 函数  \ntulu3论文中说明了，tulu3-8B的dpo使用了长度归一化的DPO（Length-Normalized DPO），以此来消除<font style=\"color:#000000;background-color:#F1A2AB;\">因回答的序列长度（对比来说很长的话）带来的概率偏好影响。</font> 当 `average_log_prob=True` 时，会将每个 token 的 log 概率进行平均处理，实现序列长度归一化。 如果`average_log_prob=False`，则是直接求和。\n\n代码具体实现如下：\n\n```python\ndef _get_batch_logps(\n    logits: torch.FloatTensor, labels: torch.LongTensor, average_log_prob: bool = False\n) -> torch.FloatTensor:\n    # 确保logits和labels的形状在除了最后一个维度外是相同的。\n    assert logits.shape[:-1] == labels.shape\n    # 两行代码分别对labels和logits进行切片操作，去掉了每个序列的第一个和最后一个元素。\n    labels = labels[:, 1:].clone()\n    logits = logits[:, :-1, :]\n    # 这行代码创建了一个掩码loss_mask，用于标识哪些标签不是-100（即不是要被忽略的标签）。\n    loss_mask = labels != -100\n\n    # 这行代码将labels中值为-100的元素替换为0，因为在PyTorch中，0可以作为无效索引。\n    labels[labels == -100] = 0\n\n    # 计算每个token的对数概率。首先对logits应用log_softmax函数，然后在最后一个维度上使用torch.gather根据labels索引来选择对应的对数概率。\n    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n\n    # 根据average_log_prob的值决定返回值。如果average_log_prob为True，则返回每个样本的平均对数概率；否则，返回每个样本的对数概率之和\n    if average_log_prob:\n        return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n    else:\n        return (per_token_logps * loss_mask).sum(-1)\n```\n\n:::tips\n<font style=\"color:rgba(0, 0, 0, 0.85);\">函数_get_batch_logps用于计算给定标签（labels）在给定的模型输出（logits）下的对数概率（log probabilities）。</font>\n\n<font style=\"color:rgba(0, 0, 0, 0.85);\">参数说明：</font>\n\n+ <font style=\"color:rgba(0, 0, 0, 0.85);\">logits</font><font style=\"color:rgba(0, 0, 0, 0.85);\">：模型的输出（未归一化的）。形状为</font><font style=\"color:rgba(0, 0, 0, 0.85);\">(batch_size, sequence_length, vocab_size)</font><font style=\"color:rgba(0, 0, 0, 0.85);\">，其中</font><font style=\"color:rgba(0, 0, 0, 0.85);\">batch_size</font><font style=\"color:rgba(0, 0, 0, 0.85);\">是批次大小，</font><font style=\"color:rgba(0, 0, 0, 0.85);\">sequence_length</font><font style=\"color:rgba(0, 0, 0, 0.85);\">是序列长度，</font><font style=\"color:rgba(0, 0, 0, 0.85);\">vocab_size</font><font style=\"color:rgba(0, 0, 0, 0.85);\">是词汇表的大小。</font>\n+ <font style=\"color:rgba(0, 0, 0, 0.85);\">labels</font><font style=\"color:rgba(0, 0, 0, 0.85);\">：要计算对数概率的标签。值为-100的标签标记将被忽略。形状为</font><font style=\"color:rgba(0, 0, 0, 0.85);\">(batch_size, sequence_length)</font><font style=\"color:rgba(0, 0, 0, 0.85);\">。</font>\n+ <font style=\"color:rgba(0, 0, 0, 0.85);\">average_log_prob</font><font style=\"color:rgba(0, 0, 0, 0.85);\">：一个布尔值，默认为</font><font style=\"color:rgba(0, 0, 0, 0.85);\">False</font><font style=\"color:rgba(0, 0, 0, 0.85);\">。如果为</font><font style=\"color:rgba(0, 0, 0, 0.85);\">True</font><font style=\"color:rgba(0, 0, 0, 0.85);\">，则返回每个（未被掩码的）token的平均对数概率；如果为</font><font style=\"color:rgba(0, 0, 0, 0.85);\">False</font><font style=\"color:rgba(0, 0, 0, 0.85);\">，则返回（未被掩码的）token的对数概率之和。</font>\n\n<font style=\"color:rgba(0, 0, 0, 0.85);\">返回值：</font>\n\n+ <font style=\"color:rgba(0, 0, 0, 0.85);\">返回一个形状为</font><font style=\"color:rgba(0, 0, 0, 0.85);\">(batch_size,)</font><font style=\"color:rgba(0, 0, 0, 0.85);\">的张量，包含给定标签在给定logits下的对数概率的平均值或总和。</font>\n\n<font style=\"color:rgba(0, 0, 0, 0.85);\">该函数是用来计算模型预测的概率，并根据标签来确定哪些预测是有效的（即标签值不为-100）。如果average_log_prob参数为True，则函数返回的是平均对数概率；如果为False，则返回的是总和。</font>\n\n:::\n\n##### `concatenated_forward` 函数  \n 在训练中，`concatenated_forward` 负责对模型的 `logps` 进行计算：  \n\n```python\ndef concatenated_forward(\n    model: nn.Module,\n    batch: Dict[str, Union[List, torch.LongTensor]],\n    average_log_prob: bool = False,\n    output_router_logits: bool = False,\n) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n    concatenated_batch = concatenated_inputs(batch)\n    if output_router_logits:\n        outputs = model(\n            input_ids=concatenated_batch[\"concatenated_input_ids\"],\n            attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n            output_router_logits=True,\n        )\n        logits = outputs.logits.to(torch.float32)\n        aux_loss = outputs.aux_loss\n    else:\n        logits = model(\n            input_ids=concatenated_batch[\"concatenated_input_ids\"],\n            attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n        ).logits.to(torch.float32)\n        aux_loss = None\n    all_logps = _get_batch_logps(logits, concatenated_batch[\"concatenated_labels\"], average_log_prob=average_log_prob)\n    chosen_logps = all_logps[: batch[\"chosen_input_ids\"].shape[0]]\n    rejected_logps = all_logps[batch[\"chosen_input_ids\"].shape[0] :]\n    return chosen_logps, rejected_logps, aux_loss\n```\n\n 参数 `average_log_prob` 是从主脚本`dpo_tune.py`中传递的，当设置为 `True` 时，启用长度归一化。  \n\n##### `dpo_loss` 函数  \n`dpo_loss` 中计算 logits 的差值：  \n\n```python\npi_logratios = policy_chosen_logps - policy_rejected_logps\nref_logratios = reference_chosen_logps - reference_rejected_logps\nlogits = pi_logratios - ref_logratios\n```\n\n 如果 `average_log_prob=True`，则 `policy_chosen_logps` 和 `policy_rejected_logps` 都是归一化的值，从而影响最终的 logits 和损失计算。  \n\n##### 控制参数  \n 在主脚本dpo_tune.py中，通过以下代码控制是否启用长度归一化：  \n\n```python\naverage_log_prob_loss_types = [\"simpo\", \"dpo_norm\"]\naverage_log_prob = args.dpo_loss_type in average_log_prob_loss_types\n```\n\n 当 `dpo_loss_type` 设置为 `\"dpo_norm\"` 时，`average_log_prob`归一化被启用，进而在 `_get_batch_logps` 和 `concatenated_forward` 中触发长度归一化逻辑。  \n\n## rlvr实践\n### 单机4卡测试\n####  训练数据集\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image29.png)\n\n.parquet转为json后的数据集样例：\n\n```json\n{\"messages\":[{\"content\":\"Question: Find the domain of the expression $\\\\frac{\\\\sqrt{x-2}}{\\\\sqrt{5-x}}$.}\\nAnswer:The expressions inside each square root must be non-negative.\\nTherefore, $x-2 \\\\ge 0$, so $x\\\\ge2$, and $5 - x \\\\ge 0$, so $x \\\\le 5$.\\nAlso, the denominator cannot be equal to zero, so $5-x>0$, which gives $x<5$.\\nTherefore, the domain of the expression is $\\\\boxed{[2,5)}$.\\n\\nQuestion: If $\\\\det \\\\mathbf{A} = 2$ and $\\\\det \\\\mathbf{B} = 12,$ then find $\\\\det (\\\\mathbf{A} \\\\mathbf{B}).$\\nAnswer:We have that $\\\\det (\\\\mathbf{A} \\\\mathbf{B}) = (\\\\det \\\\mathbf{A})(\\\\det \\\\mathbf{B}) = (2)(12) = \\\\boxed{24}.$\\n\\nQuestion: Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight?\\nAnswer:If Terrell lifts two 20-pound weights 12 times, he lifts a total of $2\\\\cdot 12\\\\cdot20=480$ pounds of weight.  If he lifts two 15-pound weights instead for $n$ times, he will lift a total of $2\\\\cdot15\\\\cdot n=30n$ pounds of weight.  Equating this to 480 pounds, we can solve for $n$: \\\\begin{align*}\\n30n&=480\\\\\\\\\\n\\\\Rightarrow\\\\qquad n&=480\\/30=\\\\boxed{16}\\n\\\\end{align*}\\n\\nQuestion: If the system of equations\\n\\n\\\\begin{align*}\\n6x-4y&=a,\\\\\\\\\\n6y-9x &=b.\\n\\\\end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\\\frac{a}{b},$ assuming $b$ is nonzero.\\nAnswer:If we multiply the first equation by $-\\\\frac{3}{2}$, we obtain\\n\\n$$6y-9x=-\\\\frac{3}{2}a.$$Since we also know that $6y-9x=b$, we have\\n\\n$$-\\\\frac{3}{2}a=b\\\\Rightarrow\\\\frac{a}{b}=\\\\boxed{-\\\\frac{2}{3}}.$$\\n\\nQuestion: What is the modulo $13$ residue of $247+5 \\\\cdot 39 + 7 \\\\cdot 143 +4 \\\\cdot 15?$\",\"role\":\"user\"}],\"ground_truth\":\"8\",\"dataset\":\"MATH\",\"constraint_type\":null,\"constraint\":null}\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image30.png)\n\n#### 验证数据集\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image31.png)\n\n```json\n{\"messages\":[{\"content\":\"Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nAnswer:There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\\n\\nQuestion: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nAnswer:There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\\n\\nQuestion: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\nAnswer:Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\\n\\nQuestion: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\nAnswer:Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\\n\\nQuestion: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\nAnswer:Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\\n\\nQuestion: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\\nAnswer:There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\\n\\nQuestion: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\nAnswer:Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33.\\n\\nQuestion: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\nAnswer:Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is 8.\\n\\nQuestion: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\",\"role\":\"user\"}],\"ground_truth\":\"3\",\"dataset\":\"gsm8k\"}\n{\"messages\":[{\"content\":\"Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nAnswer:There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\\n\\nQuestion: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nAnswer:There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\\n\\nQuestion: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\nAnswer:Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\\n\\nQuestion: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\nAnswer:Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\\n\\nQuestion: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\nAnswer:Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\\n\\nQuestion: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\\nAnswer:There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\\n\\nQuestion: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\nAnswer:Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33.\\n\\nQuestion: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\nAnswer:Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is 8.\\n\\nQuestion: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?\",\"role\":\"user\"}],\"ground_truth\":\"70000\",\"dataset\":\"gsm8k\"}\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image32.png)\n\n#### 单机脚本配置\n```bash\nexport WANDB_MODE=disabled\nexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nLOG_SAVE=\"/mnt/open-instruct-main/output/rlvr/${current_time}-tulu3-rlvr.log\"\n\npython /mnt/open-instruct-main/open_instruct/ppo_vllm_thread_ray_gtrl.py \\\n    --dataset_mixer '{\"/mnt/open-instruct-main/data/gsm8k_math_ground_truth_mixed\": 1.0}' \\\n    --dataset_train_splits train \\\n    --dataset_eval_mixer '{\"/mnt/open-instruct-main/data/gsm8k_math_ground_truth\": 1.0}' \\\n    --dataset_eval_splits test \\\n    --max_token_length 2048 \\\n    --max_prompt_token_length 2048 \\\n    --response_length 2048 \\\n    --model_name_or_path /mnt/LLM-Research/Llama-31-Tulu-3-8B-DPO \\\n    --reward_model_path /mnt/LLM-Research/LLama-31-Tulu3-8B-RM \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --temperature 1.0 \\\n    --ground_truths_key ground_truth \\\n    --chat_template tulu \\\n    --sft_messages_key messages \\\n    --learning_rate 3e-7 \\\n    --total_episodes 10000000 \\\n    --penalty_reward_value -10.0 \\\n    --deepspeed_stage 3 \\\n    --per_device_train_batch_size 1 \\\n    --local_rollout_forward_batch_size 1 \\\n    --local_mini_batch_size 16 \\\n    --local_rollout_batch_size 16 \\\n    --actor_num_gpus_per_node 3 \\\n    --vllm_tensor_parallel_size 2 \\\n    --beta 0.05 \\\n    --apply_verifiable_reward true \\\n    --output_dir /mnt/open-instruct-main/output/rlvr \\\n    --seed 3 \\\n    --num_evals 3 \\\n    --save_freq 100 \\\n    --reward_model_multiplier 0.0 \\\n    --gradient_checkpointing \\\n    --with_tracking False 2>&1 | tee -a \"$LOG_SAVE\"\n```\n\n为了尽可能地还原tulu3原论文的实验结果，其中部分超参与论文中最优保持一致。\n\n`--max_token_length 2048`\n\n`--max_prompt_token_length 2048` \n\n`--response_length 2048`\n\n`--learning_rate 3e-7`\n\n`--penalty_reward_value -10.0`\n\n`--reward_model_multiplier 0.0`\n\n其中的模型权重利用tulu3开源的经过tulu3-sft dpo得到的Llama-31-Tulu-3-8B-DPO权重，奖励模型采用tulu3开源的LLama-31-Tulu3-8B-RM。\n\n#### 训练测试\n加载ckpt时一直卡住不动，尚未解决。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image33.png)\n\n","tags":["Nvidia","V100","模型训练调优","Tulu3"],"categories":["模型训练调优","NVIDIA"]},{"title":"前端1_调用api接口的写法","url":"/2025/11/06/前端/前端1_调用api接口的写法/","content":"\n### Get请求写法\n#### 1. 看Parameters中是否有需要带参数，其中`Authorization`为授权认证的token可以不用考虑。\n1.1 若是没有其余的参数限定，那么SyncRequestFuncType<请求参数类型,响应返回参数类型>，第一个参数就为void或者undefined。返回类型可以使用any，等获取到响应参数之后，在对其通过Interface/type进行明确。\n\n```javascript\nexport const getApi: SyncRequestFuncType<\nvoid,\nresponseType\n> = () => {\nreturn javaAxios({\n method: \"get\",\n url: \"xxx/xxxx\",\n});\n};\n```\n\n1.2 若是有其余参数限定，这时需要箭头函数中需要带着params来进行请求，且reuturn中也需要将params带着。\n\n\t1.2.1 `单个参数`，例如id，这时在请求参数类型中，需要明确出请求参数的类型<{id:string},responseType>，这时请求参数的类型为对象{}的形式，可以直接在尖括号中写出来，也可以通过引入定义的Interface/type类型来写<IdType,responseType>。\n\n```javascript\nexport const getApi: SyncRequestFuncType<\n {id:string},\n void\n> = (params) => {\nreturn javaAxios({\n method: \"get\",\n url: \"xxx/xxxx\",\n params,\n});\n};\n```\n\n\t1.2.2 `多个参数`，例如email，type...，因为参数比较多，最好通过Interface/type的方式SendEmailCaptchaForUserInfoModifyType来明确请求参数类型。\n\n```javascript\nexport type SendEmailCaptchaForUserInfoModifyType = {\ntype: ModifyType;\nemail?: string;\n};\n```\n\n```javascript\nexport const getApi: SyncRequestFuncType<\nSendEmailCaptchaForUserInfoModifyType,\nvoid\n> = (params) => {\nreturn javaAxios({\n method: \"get\",\n url: \"xxx/xxx\",\n params,\n});\n};\n```\n\n### Post请求写法\n#### 1. 看看Parameters中是否有需要带参数，其中`Authorization`为授权认证的token可以不用考虑。\n1.1 若是没有其余的参数限定，那么SyncRequestFuncType<请求参数类型,响应返回参数类型>，第一个参数就为void或者undefined。返回类型可以使用any，等获取到响应参数之后，在对其通过Interface/type进行明确。\n\n```javascript\nexport const clearVideoRecycle: SyncRequestFuncType<void, void> = () => {\nreturn javaAxios({\n method: \"post\",\n url: \"xxx/xxx\",\n});\n};\n```\n\n1.2 若是有其余参数限定，这时需要箭头函数中需要带着params来进行请求，且reuturn中也需要将params带着。\n\n\t1.2.1 单个参数时\n\n```javascript\nexport const cancelSubscribeVideo: SyncRequestFuncType<\n{ themeId: string },\nany\n> = (params) => {\nreturn javaAxios({\n method: \"post\",\n url: \"xxx/xxx\",\n params,\n});\n};\n```\n\n\t1.2.2 多个参数时\n\n```javascript\nexport type SendEmailCaptchaForUserInfoModifyType = {\ntype: ModifyType;\nemail?: string;\n};\n```\n\n```javascript\nexport const sendEmailCaptchaForUserInfoModify: SyncRequestFuncType<\nSendEmailCaptchaForUserInfoModifyType,\nvoid\n> = (params) => {\nreturn javaAxios({\n method: \"get\",\n url: \"xxx/xxx\",\n params,\n});\n};\n```\n\n#### 2. 当出现请求体`Request body`时，这时就需要在return的javaAxios中添加键值对`data: params`，来将参数添加到请求体中传递过去。\n2.1 Request body为：\n\n```javascript\n[\n\"string\"\n]\n```\n\n```javascript\nexport const batchDeleteVideo: SyncRequestFuncType<string[], void> = (\nparams\n) => {\nreturn javaAxios({\n method: \"post\",\n url: \"videos/themes/batchDel\",\n data: params,\n});\n};\n```\n\n2.2 Request body为：这时候需要在data中进一步在约束一下，使其对应api的请求体的格式。\n\n```javascript\n{\n\"themes\": [\n \"string\"\n]\n}\n```\n\n```javascript\nexport const sortVideos: SyncRequestFuncType<string[], void> = (params) => {\nreturn javaAxios({\n method: \"post\",\n url: \"videos/themes/changeVideoThemesSort\",\n data: { themes: params },\n});\n};\n```\n\n","tags":["前端","Javascript"],"categories":["前端"]},{"title":"基于Modellink的llama2-7b和Mistral-7b模型微调","url":"/2025/11/06/模型训练调优/昇腾/Modellink/llama2-7B+Mistral-7B/","content":"## 基于docker的mistral-7B微调\n### 容器挂载\n```bash\nsudo docker run -dit --ipc=host --net=host \\\n--name=modellink_wxb \\\n--device=/dev/davinci0 \\\n--device=/dev/davinci1 \\\n--device=/dev/davinci2 \\\n--device=/dev/davinci3 \\\n--device=/dev/davinci4 \\\n--device=/dev/davinci5 \\\n--device=/dev/davinci6 \\\n--device=/dev/davinci7 \\\n--device=/dev/davinci_manager \\\n--device=/dev/devmm_svm \\\n--device=/dev/hisi_hdc \\\n-v /etc/ascend_install.info:/etc/ascend_install.info \\\n-v /etc/hccn.conf:/etc/hccn.conf \\\n-v /etc/localtime:/etc/localtime \\\n-v /var/log/npu/:/usr/slog \\\n-v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \\\n-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n-v /reason-sharedata/training_inference/wangxiangbo/modellink/code/Modellink:/job/code \\\n-v /reason-sharedata/training_inference/wangxiangbo/modellink/data:/job/data \\\n-v /reason-sharedata/training_inference/wangxiangbo/modellink/output:/job/output \\\n-v /mnt/weight-1:/job/mnt \\\nregistry.paas/cmss/modellink-cann8.0-torch2.1-mindspeed-0.7:v1.0 \\\n/bin/bash\n```\n\n### 权重转换\n```bash\n# 修改 ascend-toolkit 路径\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n```\n\n```bash\npython  tools/checkpoint/convert_ckpt.py \\\n    --model-type GPT \\\n    --loader llama2_hf \\\n    --saver megatron \\\n    --load-dir /job/mnt/huggingface/mistral-7b-hf/ \\\n    --save-dir /job/data/megatron/mistral-7b-tp4-pp2 \\\n    --tokenizer-model /job/mnt/huggingface/mistral-7b-hf/tokenizer.model \\\n    --target-tensor-parallel-size 4 \\\n    --target-pipeline-parallel-size 2\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image1.png)\n\n### 数据集\n#### 下载\n```bash\nsudo wget https://hf-mirror.com/datasets/silk-road/alpaca-data-gpt4-chinese/resolve/main/Alpaca_data_gpt4_zh.jsonl\n```\n\n#### 处理\n```bash\npython tools/preprocess_data.py \\\n    --input /job/data/dataset/mistral/Alpaca_data_gpt4_zh.jsonl \\\n    --output-prefix /job/data/dataset/mistral/Alpaca_finetune/ \\\n    --tokenizer-type PretrainedFromHF \\\n    --tokenizer-name-or-path /job/mnt/huggingface/mistral-7b-hf/ \\\n    --append-eod \\\n    --tokenizer-not-use-fast \\\n    --handler-name GeneralInstructionHandler \\\n    --workers 4\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image2.png)\n\n处理完成\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image3.png)\n\n### 微调\n#### 微调脚本\n```bash\n#!/bin/bash\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport PYTORCH_NPU_ALLOC_CONF=expandable_segments:True\n\nGPUS_PER_NODE=8\nMASTER_ADDR=localhost\nMASTER_PORT=6006\nNNODES=1\nNODE_RANK=0\nWORLD_SIZE=$(($GPUS_PER_NODE * $NNODES))\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\n\necho \"NODE_RANK ${NODE_RANK}\"\n\nDATA_PATH=\"/job/data/dataset/mistral/Alpaca_finetune/\"\nTOKENIZER_MODEL=\"/job/mnt/huggingface/mistral-7b-hf\"\nCKPT_SAVE_DIR=\"/job/output/mistral_ckpt/\"\nCKPT_LOAD_DIR=\"/job/data/megatron/mistral-7b-tp4-pp2/\"\n\n\nTP=4\nPP=2\nNUM_LAYERS=32\n\nGPT_ARGS=\"\n    --tensor-model-parallel-size ${TP} \\\n    --pipeline-model-parallel-size ${PP} \\\n    --sequence-parallel \\\n    --sliding-window 4096 \\\n    --num-layers ${NUM_LAYERS} \\\n    --hidden-size 4096 \\\n    --ffn-hidden-size 14336 \\\n    --num-attention-heads 32 \\\n    --group-query-attention \\\n    --num-query-groups 8 \\\n    --tokenizer-type PretrainedFromHF \\\n    --tokenizer-name-or-path ${TOKENIZER_MODEL} \\\n    --seq-length 32768 \\\n    --max-position-embeddings 32768 \\\n    --micro-batch-size 1 \\\n    --global-batch-size 32 \\\n    --make-vocab-size-divisible-by 1 \\\n    --lr 1.25e-6 \\\n    --train-iters 1000 \\\n    --lr-decay-style cosine \\\n    --untie-embeddings-and-output-weights \\\n    --disable-bias-linear \\\n    --attention-dropout 0.0 \\\n    --init-method-std 0.01 \\\n    --hidden-dropout 0.0 \\\n    --position-embedding-type rope \\\n    --normalization RMSNorm \\\n    --use-fused-rmsnorm \\\n    --use-fused-swiglu \\\n    --use-rotary-position-embeddings \\\n    --use-fused-rotary-pos-emb \\\n    --use-mc2 \\\n    --swiglu \\\n    --use-flash-attn \\\n    --no-masked-softmax-fusion \\\n    --attention-softmax-in-fp32 \\\n    --min-lr 1.25e-7 \\\n    --weight-decay 1e-1 \\\n    --lr-warmup-fraction 0.01 \\\n    --clip-grad 1.0 \\\n    --adam-beta1 0.9 \\\n    --initial-loss-scale 65536 \\\n    --adam-beta2 0.95 \\\n    --no-gradient-accumulation-fusion \\\n    --no-load-optim \\\n    --no-load-rng \\\n    --use-distributed-optimizer \\\n    --overlap-grad-reduce \\\n    --load ${CKPT_LOAD_DIR} \\\n    --save ${CKPT_SAVE_DIR} \\\n    --bf16 \\\n    --finetune \\\n    --is-instruction-dataset \\\n    --log-throughput \\\n    --recompute-granularity full \\\n    --recompute-method block \\\n    --recompute-num-layers 32\n\"\n\nDATA_ARGS=\"\n    --data-path $DATA_PATH  \\\n    --split 100,0,0 \\\n\"\n\nOUTPUT_ARGS=\"\n    --log-interval 1 \\\n    --save-interval 1000 \\\n    --eval-interval 1000 \\\n    --eval-iters 0 \\\n\"\n\ntorchrun $DISTRIBUTED_ARGS ../pretrain_gpt.py \\\n  $GPT_ARGS \\\n  $DATA_ARGS \\\n  $OUTPUT_ARGS \\\n  --distributed-backend nccl \\\n  | tee /job/output/logs/train_mistral_7B.log\n```\n\n#### 拉起训练\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image4.png)\n\n\n\n## 基于docker的llama2-7B微调\n### 容器挂载\n```bash\nsudo docker run -dit --ipc=host --net=host \\\n--name=modellink_wxb \\\n--device=/dev/davinci0 \\\n--device=/dev/davinci1 \\\n--device=/dev/davinci2 \\\n--device=/dev/davinci3 \\\n--device=/dev/davinci4 \\\n--device=/dev/davinci5 \\\n--device=/dev/davinci6 \\\n--device=/dev/davinci7 \\\n--device=/dev/davinci_manager \\\n--device=/dev/devmm_svm \\\n--device=/dev/hisi_hdc \\\n-v /etc/ascend_install.info:/etc/ascend_install.info \\\n-v /etc/hccn.conf:/etc/hccn.conf \\\n-v /etc/localtime:/etc/localtime \\\n-v /var/log/npu/:/usr/slog \\\n-v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \\\n-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n-v /reason-sharedata/training_inference/wangxiangbo/modellink/code/Modellink:/job/code \\\n-v /reason-sharedata/training_inference/wangxiangbo/modellink/data:/job/data \\\n-v /reason-sharedata/training_inference/wangxiangbo/modellink/output:/job/output \\\n-v /mnt/weight-1:/job/mnt \\\nregistry.paas/cmss/modellink-cann8.0-torch2.1-mindspeed-0.7:v1.0 \\\n/bin/bash\n```\n\n### 权重转换\n```bash\n# 修改 ascend-toolkit 路径\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n```\n\n```bash\npython  tools/checkpoint/convert_ckpt.py \\\n       --model-type GPT \\\n       --loader llama2_hf \\\n       --saver megatron \\\n       --target-tensor-parallel-size 8 \\\n       --target-pipeline-parallel-size 1 \\\n       --load-dir /job/mnt/huggingface/Llama-2-7b-hf// \\\n       --save-dir /job/data/megatron/llama-2-7b-hf-v0.1-tp8-pp1/ \\\n       --tokenizer-model /job/mnt/huggingface/Llama-2-7b-hf/tokenizer.json\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image5.png)\n\n### 数据集\n```bash\npython tools/preprocess_data.py \\\n    --input /job/data/dataset/mistral/Alpaca_data_gpt4_zh.jsonl \\\n    --output-prefix /job/data/dataset/llama2/Alpaca_finetune/ \\\n    --tokenizer-type PretrainedFromHF \\\n    --tokenizer-name-or-path /job/mnt/huggingface/Llama-2-7b-hf/ \\\n    --append-eod \\\n    --tokenizer-not-use-fast \\\n    --handler-name GeneralInstructionHandler \\\n    --workers 4\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image6.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image7.png)\n\n### 微调\n#### 微调脚本\n```bash\n\n#!/bin/bash\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport PYTORCH_NPU_ALLOC_CONF=expandable_segments:True\n\nGPUS_PER_NODE=8\nMASTER_ADDR=localhost\nMASTER_PORT=6006\nNNODES=1\nNODE_RANK=0\nWORLD_SIZE=$(($GPUS_PER_NODE * $NNODES))\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\n\necho \"NODE_RANK ${NODE_RANK}\"\n\nDATA_PATH=\"/job/data/dataset/llama2/Alpaca_finetune\"\nTOKENIZER_MODEL=\"/job/mnt/huggingface/Llama-2-7b-hf/\"\nCKPT_SAVE_DIR=\"/job/output/llama2_ckpt/\"\nCKPT_LOAD_DIR=\"/job/data/megatron/llama-2-7b-hf-v0.1-tp8-pp1\"\n\n\nTP=8\nPP=1\nNUM_LAYERS=32\n\nGPT_ARGS=\"\n    --tensor-model-parallel-size ${TP} \\\n    --pipeline-model-parallel-size ${PP} \\\n    --sequence-parallel \\\n    --num-layers 32 \\\n    --hidden-size 4096 \\\n    --ffn-hidden-size 11008 \\\n    --num-attention-heads 32 \\\n    --tokenizer-type Llama2Tokenizer \\\n    --tokenizer-model ${TOKENIZER_MODEL} \\\n    --seq-length 4096 \\\n    --max-position-embeddings 4096 \\\n    --micro-batch-size 1 \\\n    --global-batch-size 256 \\\n    --make-vocab-size-divisible-by 1 \\\n    --lr 1.25e-6 \\\n    --train-iters 5000 \\\n    --lr-decay-style cosine \\\n    --untie-embeddings-and-output-weights \\\n    --disable-bias-linear \\\n    --attention-dropout 0.0 \\\n    --init-method-std 0.01 \\\n    --hidden-dropout 0.0 \\\n    --position-embedding-type rope \\\n    --normalization RMSNorm \\\n    --use-fused-rmsnorm \\\n    --swiglu \\\n    --use-flash-attn \\\n    --no-masked-softmax-fusion \\\n    --attention-softmax-in-fp32 \\\n    --min-lr 1.25e-7 \\\n    --weight-decay 1e-1 \\\n    --lr-warmup-fraction 0.01 \\\n    --clip-grad 1.0 \\\n    --adam-beta1 0.9 \\\n    --initial-loss-scale 65536 \\\n    --adam-beta2 0.95 \\\n    --no-gradient-accumulation-fusion \\\n    --no-load-optim \\\n    --no-load-rng \\\n    --use-distributed-optimizer \\\n    --use-fused-swiglu \\\n    --use-fused-rotary-pos-emb \\\n    --overlap-grad-reduce \\\n    --bf16\n\"\n\nDATA_ARGS=\"\n    --data-path $DATA_PATH  \\\n    --split 100,0,0 \\\n\"\n\nOUTPUT_ARGS=\"\n    --log-interval 1 \\\n    --save-interval 1000 \\\n    --eval-interval 1000 \\\n    --eval-iters 0 \\\n\"\n\ntorchrun $DISTRIBUTED_ARGS ../pretrain_gpt.py \\\n  $GPT_ARGS \\\n  $DATA_ARGS \\\n  $OUTPUT_ARGS \\\n  --distributed-backend nccl \\\n  | tee /job/output/logs/train_llama2_7B.log\n\n```\n\n#### 拉起训练\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image8.png)\n\n\n","tags":["模型训练调优","昇腾","910B","Mindformers"],"categories":["模型训练调优","昇腾"]},{"title":"基于Mindformers的llama3.1模型微调","url":"/2025/11/06/模型训练调优/昇腾/Mindformers/llama3.1微调/","content":"## 启动容器\n```bash\ndocker run -dit --ipc=host --net=host \\\n--name=wxb_mindformers \\\n--device=/dev/davinci0 \\\n--device=/dev/davinci1 \\\n--device=/dev/davinci2 \\\n--device=/dev/davinci3 \\\n--device=/dev/davinci4 \\\n--device=/dev/davinci5 \\\n--device=/dev/davinci6 \\\n--device=/dev/davinci7 \\\n--device=/dev/davinci_manager \\\n--device=/dev/devmm_svm \\\n--device=/dev/hisi_hdc \\\n-v /etc/ascend_install.info:/etc/ascend_install.info \\\n-v /etc/hccn.conf:/etc/hccn.conf \\\n-v /etc/localtime:/etc/localtime \\\n-v /var/log/npu/:/usr/slog \\\n-v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \\\n-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n-v /reason-sharedata/training_inference/wangxiangbo/mindformers/code:/job/code \\\n-v /reason-sharedata/training_inference/wangxiangbo/mindformers/data:/job/data \\\n-v /reason-sharedata/training_inference/wangxiangbo/mindformers/output:/job/output \\\n-v /mnt/weight-1:/job/mnt \\\nswr.cn-central-221.ovaijisuan.com/mindformers/mindformers1.3_mindspore2.4:20241114 \\\n/bin/bash\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Mindformers/image1.png)\n\n## 代码准备\n```bash\ngit clone https://gitee.com/mindspore/mindformers.git\n```\n\n## 数据集\n```bash\npython research/llama3/llama_preprocess.py \\\n--dataset_type qa \\\n--input_glob /job/data/alpaca/alpaca-data-conversation.json \\\n--model_file /job/mnt/huggingface/llama-3-8b-hf \\\n--seq_length 8192 \\\n--output_file /job/data/alpaca\n```\n\n## 权重转换\n```bash\npython convert_weight.py --model llama --input_path /job/mnt/huggingface/llama-3-8b-hf --output_path /job/data/mindformers/llama3_8b_ckpt\n```\n\n## 训练脚本\n```bash\nseed: 0\noutput_dir: '/job/output' # path to save checkpoint/strategy\nload_checkpoint: '/job/data/mindformers/llama3_8b_ckpt/'\nsrc_strategy_path_or_dir: ''\nauto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model\nonly_save_strategy: False\nresume_training: False\nrun_mode: 'finetune'\n \n# trainer config\ntrainer:\n  type: CausalLanguageModelingTrainer\n  model_name: 'llama3_8b'\n \n# runner config\nrunner_config:\n  epochs: 2\n  batch_size: 1\n  sink_mode: True\n  sink_size: 2\n \n# optimizer\noptimizer:\n  type: FP32StateAdamWeightDecay\n  beta1: 0.9\n  beta2: 0.95\n  eps: 1.e-8\n \n# lr sechdule\nlr_schedule:\n  type: CosineWithWarmUpLR\n  learning_rate: 1.e-5\n  lr_end: 0.0\n  warmup_ratio: 0.03\n  total_steps: -1 # -1 means it will load the total steps of the dataset\n \n# dataset\ntrain_dataset: &train_dataset\n  data_loader:\n    type: MindDataset\n    dataset_dir: \"/job/data/alpaca/alpaca_llama3_8192/alpaca-fastchat8192.mindrecord\"\n    shuffle: True\n  input_columns: [\"input_ids\",\"labels\"]  # \"input_ids\", \"labels\" , labels are used in instruction finetune.\n  num_parallel_workers: 8\n  python_multiprocessing: False\n  drop_remainder: True\n  batch_size: 6\n  repeat: 1\n  numa_enable: False\n  prefetch_size: 1\ntrain_dataset_task:\n  type: CausalLanguageModelDataset\n  dataset_config: *train_dataset\n# if True, do evaluate during the training process. if false, do nothing.\n# note that the task trainer should support _evaluate_in_training function.\ndo_eval: False\n \n# eval dataset\neval_dataset: &eval_dataset\n  data_loader:\n    type: MindDataset\n    dataset_dir: \"\"\n    shuffle: False\n  input_columns: [\"input_ids\"]\n  num_parallel_workers: 8\n  python_multiprocessing: False\n  drop_remainder: False\n  repeat: 1\n  numa_enable: False\n  prefetch_size: 1\neval_dataset_task:\n  type: CausalLanguageModelDataset\n  dataset_config: *eval_dataset\n \nuse_parallel: True\n# parallel context config\nparallel:\n  parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel\n  gradients_mean: False\n  enable_alltoall: False\n  full_batch: True\n  search_mode: \"sharding_propagation\"\n  enable_parallel_optimizer: True\n  strategy_ckpt_save_file: \"./ckpt_strategy.ckpt\"\n  parallel_optimizer_config:\n    gradient_accumulation_shard: False\n    parallel_optimizer_threshold: 64\n# default parallel of device num = 8 for Atlas 800T A2\nparallel_config:\n  data_parallel: 1\n  model_parallel: 4\n  pipeline_stage: 2\n  use_seq_parallel: False\n  micro_batch_num: 8\n  vocab_emb_dp: True\n  gradient_aggregation_group: 4\n# when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.\nmicro_batch_interleave_num: 1\n \n# recompute config\nrecompute_config:\n  recompute: True\n  select_recompute: False\n  parallel_optimizer_comm_recompute: True\n  mp_comm_recompute: True\n  recompute_slice_activation: True\n \n# callbacks\ncallbacks:\n  - type: MFLossMonitor\n  - type: CheckpointMointor\n    prefix: \"llama3_8b\"\n    save_checkpoint_steps: 10000\n    integrated_save: False\n    async_save: False\n  - type: ObsMonitor\n \n# mindspore context init config\ncontext:\n  mode: 0 #0--Graph Mode; 1--Pynative Mode\n  device_target: \"Ascend\"\n  enable_graph_kernel: False\n  graph_kernel_flags: \"--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true\"\n  max_call_depth: 10000\n  max_device_memory: \"26GB\"\n  save_graphs: False\n  save_graphs_path: \"./graph\"\n  device_id: 0\n  runtime_num_threads: 1\n \n# model config\nmodel:\n  model_config:\n    type: LlamaConfig\n    batch_size: 1 # add for increase predict\n    seq_length: 8192\n    hidden_size: 4096\n    num_layers: 32\n    num_heads: 32\n    n_kv_heads: 8\n    vocab_size: 128256\n    intermediate_size: 14336\n    rms_norm_eps: 1.0e-5\n    bos_token_id: 128000\n    eos_token_id: 128001\n    pad_token_id: 128002\n    ignore_token_id: -100\n    compute_dtype: \"bfloat16\"\n    layernorm_compute_type: \"float32\"\n    softmax_compute_type: \"float32\"\n    rotary_dtype: \"float32\"\n    param_init_type: \"bfloat16\"\n    use_past: False\n    scaling_factor: 1.0\n    theta: 500000\n    extend_method: \"None\" # support \"None\", \"PI\", \"NTK\"\n    use_flash_attention: True # FA can accelerate training or finetune\n    offset: 0\n    fine_grain_interleave: 1\n    checkpoint_name_or_path: \"\"\n    repetition_penalty: 1\n    max_decode_length: 512\n    top_k: 3\n    top_p: 1\n    do_sample: False\n  arch:\n    type: LlamaForCausalLM\n \n# metric\nmetric:\n  type: PerplexityMetric\n \n# wrapper cell config\nrunner_wrapper:\n  type: MFTrainOneStepCell\n  scale_sense: 1.0\n  use_clip_grad: True\n \neval_callbacks:\n  - type: ObsMonitor\n \nauto_tune: False\nfilepath_prefix: './autotune'\nautotune_per_step: 10\n \nprofile: False\nprofile_start_step: 5\nprofile_stop_step: 7\ninit_start_profile: True\nprofile_communication: True\nprofile_memory: True\nlayer_scale: False\nlayer_decay: 0.65\nlr_scale_factor: 256\n \n# aicc\nremote_save_url: \"Please input obs url on AICC platform.\"\n```\n\n## 训练拉起\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Mindformers/image2.png)\n\n<font style=\"color:transparent;\">git</font>\n\n<font style=\"color:transparent;\"> clone https://gitee.com/mindspore/mindformers.gitgit</font>\n\n<font style=\"color:transparent;\"> clone https://gitee.com/mindspore/mindformers.git</font>\n\n","tags":["模型训练调优","昇腾","910B","Mindformers"],"categories":["模型训练调优","昇腾"]},{"title":"1.1-Qwen模型本地部署与单机单卡/多卡训练","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/1.1-Qwen模型本地部署与单机单卡_多卡训练/","content":"一、V100环境部署\n1. 项目地址： [https://github.com/QwenLM/Qwen](https://github.com/QwenLM/Qwen)\n2. 下载到本地\n`git clone  https://github.com/QwenLM/Qwen.git `\n3. 基础环境搭建\n`conda create -n qwen python=3.10`\n`conda activate qwen`\n4. 安装pytorch\n`conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia`\n5. 安装依赖环境\n`pip install -r requirements.txt `\n7. 7B模型下载\n`git clone [https://www.modelscope.cn/qwen/Qwen-7B-Chat.git](https://www.modelscope.cn/qwen/Qwen-7B-Chat.git)`\n在使用sdk的python脚本下载权重时，需要pip安装modelscope\n`pip install modelscope`\n使用git clone发现权重未下载成功，使用modelscope官方sdk脚本下载。\n将以下代码写入download.py文件中，并执行`python download.py`\n```python\n#模型下载\nfrom modelscope import snapshot_download\nmodel_dir = snapshot_download('qwen/Qwen-7B-Chat', cache_dir='/opt/tmp/Qwen', revision='v1.1.9')\n```\n8. 数据集下载\n进入modelscope下载数据集\n`https://modelscope.cn/datasets/Robin021/DISC-Law-SFT/files`\n9. 数据格式处理\n数据集处理后会生成train_data_law.json文件\n`head -n 20 train_data_law.json`\n处理之后的数据格式如下：\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen1.png)\n```python\nimport json\n# 读取以.jsonl结尾的文件\njson_data = []\nwith open('/opt/tmp/Qwen/dataset/DISC-Law-SFT-Triplet-released.jsonl', 'r', encoding='utf-8') as file:\n    for line in file:\n        data = json.loads(line)\n        json_data.append(data)\n# 待填入的模板\ntemplate = []\n\n# 遍历json数据集\nfor idx, data in enumerate(json_data[:]):\n    conversation = [\n        {\n            \"from\":\"user\",\n            \"value\": data[\"input\"]\n        },\n        {\n            \"from\": \"assistant\",\n            \"value\": data[\"output\"]\n        }\n    ]\n    template.append({\n        \"id\": f\"identity_{idx}\",\n        \"conversations\": conversation\n    })\nprint(len(template))\n# 输出填充数据后的模板\nprint(json.dumps(template[2], ensure_ascii=False, indent=2))\n# 将template写入到本地文件\noutput_file_path = \"/opt/tmp/Qwen/train_data_law.json\"\nwith open(output_file_path, 'w', encoding='utf-8') as f:\n    json.dump(template, f, ensure_ascii=False, indent=2)\nprint(f\"处理好的数据已写入到本地文件: {output_file_path}\")\n```\n10. 训练依赖安装\ndeepspeed安装\n`pip install \"peft<0.8.0\" deepspeed`\n11. 修改模型微调脚本参数\n修改MODEL和DATA的路径，及per_device_train_batch_size\n```bash\n#!/bin/bash\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\nMODEL=\"/opt/tmp/Qwen/Qwen-7B-Chat\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"/opt/tmp/Qwen/train_data_law.json\"\n\nfunction usage() {\n    echo '\nUsage: bash finetune/finetune_lora_single_gpu.sh [-m MODEL_PATH] [-d DATA_PATH]\n'\n}\n\nwhile [[ \"$1\" != \"\" ]]; do\n    case $1 in\n        -m | --model )\n            shift\n            MODEL=$1\n            ;;\n        -d | --data )\n            shift\n            DATA=$1\n            ;;\n        -h | --help )\n            usage\n            exit 0\n            ;;\n        * )\n            echo \"Unknown argument ${1}\"\n            exit 1\n            ;;\n    esac\n    shift\ndone\n\nexport CUDA_VISIBLE_DEVICES=0\n\npython finetune.py \\\n  --model_name_or_path $MODEL \\\n  --data_path $DATA \\\n  --bf16 False \\\n  --output_dir output_qwen \\\n  --num_train_epochs 5 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 1 \\\n  --gradient_accumulation_steps 8 \\\n  --evaluation_strategy \"no\" \\\n  --save_strategy \"steps\" \\\n  --save_steps 100 \\\n  --save_total_limit 10 \\\n  --learning_rate 3e-4 \\\n  --weight_decay 0.1 \\\n  --adam_beta2 0.95 \\\n  --warmup_ratio 0.01 \\\n  --lr_scheduler_type \"cosine\" \\\n  --logging_steps 1 \\\n  --report_to \"none\" \\\n  --model_max_length 512 \\\n  --lazy_preprocess True \\\n  --gradient_checkpointing \\\n  --use_lora\n\n# If you use fp16 instead of bf16, you should use deepspeed\n# --fp16 True --deepspeed finetune/ds_config_zero2.json\n```\n\n12. 开启单机单卡训练\n`bash finetune/finetune_lora_single_gpu.sh`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen2.png)\n13. 单机多卡训练（在分配的3号机器上执行）\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen3.png)\ngpu显存利用情况\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen4.png)\n\n\n\n","tags":["Nvidia","V100","模型训练调优","Qwen","Deepspeed"],"categories":["模型训练调优","NVIDIA"]},{"title":"1.3-基于docker的Qwen单机单卡_多卡训练","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/1.3-基于docker的Qwen单机单卡_多卡训练/","content":"### 1. 查看容器\n`docker ps`\n### 2. exec进入容器\n`docker exec -it containerid bash`\n### 3. 进入qwen目录并修改finetune_lora_single_gpu.sh参数\n```bash\n#!/bin/bash\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\nMODEL=\"/qwen/Qwen-7B-Chat\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"/qwen/train_data_law.json\"\n\nfunction usage() {\n    echo '\nUsage: bash finetune/finetune_lora_single_gpu.sh [-m MODEL_PATH] [-d DATA_PATH]\n'\n}\n\nwhile [[ \"$1\" != \"\" ]]; do\n    case $1 in\n        -m | --model )\n            shift\n            MODEL=$1\n            ;;\n        -d | --data )\n            shift\n            DATA=$1\n            ;;\n        -h | --help )\n            usage\n            exit 0\n            ;;\n        * )\n            echo \"Unknown argument ${1}\"\n            exit 1\n            ;;\n    esac\n    shift\ndone\n\nexport CUDA_VISIBLE_DEVICES=0\n\npython finetune.py \\\n  --model_name_or_path $MODEL \\\n  --data_path $DATA \\\n  --bf16 True \\\n  --output_dir output_qwen \\\n  --num_train_epochs 5 \\\n  --per_device_train_batch_size 1 \\\n  --per_device_eval_batch_size 1 \\\n  --gradient_accumulation_steps 8 \\\n  --evaluation_strategy \"no\" \\\n  --save_strategy \"steps\" \\\n  --save_steps 100 \\\n  --save_total_limit 10 \\\n  --learning_rate 3e-4 \\\n  --weight_decay 0.1 \\\n  --adam_beta2 0.95 \\\n  --warmup_ratio 0.01 \\\n  --lr_scheduler_type \"cosine\" \\\n  --logging_steps 1 \\\n  --report_to \"none\" \\\n  --model_max_length 512 \\\n  --lazy_preprocess True \\\n  --gradient_checkpointing \\\n  --use_lora\n\n# If you use fp16 instead of bf16, you should use deepspeed\n# --fp16 True --deepspeed finetune/ds_config_zero2.json\n```\n### 4. 执行finetune_lora_single_gpu.sh单机单卡\n### 5. 修改finetune_lora_ds.sh参数\n```bash\n#!/bin/bash\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nDIR=`pwd`\n\n# Guide:\n# This script supports distributed training on multi-gpu workers (as well as single-worker training).\n# Please set the options below according to the comments.\n# For multi-gpu workers training, these options should be manually set for each worker.\n# After setting the options, please run the script on each worker.\n\n# Number of GPUs per GPU worker\nGPUS_PER_NODE=$(python -c 'import torch; print(torch.cuda.device_count())')\n\n# Number of GPU workers, for single-worker training, please set to 1\nNNODES=${NNODES:-1}\n\n# The rank of this worker, should be in {0, ..., WORKER_CNT-1}, for single-worker training, please set to 0\nNODE_RANK=${NODE_RANK:-0}\n\n# The ip address of the rank-0 worker, for single-worker training, please set to localhost\nMASTER_ADDR=${MASTER_ADDR:-localhost}\n\n# The port for communication\nMASTER_PORT=${MASTER_PORT:-6001}\n\nMODEL=\"/qwen/Qwen-7B-Chat\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"/qwen/train_data_law.json\"\nDS_CONFIG_PATH=\"finetune/ds_config_zero2.json\"\n\nfunction usage() {\n    echo '\nUsage: bash finetune/finetune_lora_ds.sh [-m MODEL_PATH] [-d DATA_PATH] [--deepspeed DS_CONFIG_PATH]\n'\n}\n\nwhile [[ \"$1\" != \"\" ]]; do\n    case $1 in\n        -m | --model )\n            shift\n            MODEL=$1\n            ;;\n        -d | --data )\n            shift\n            DATA=$1\n            ;;\n        --deepspeed )\n            shift\n            DS_CONFIG_PATH=$1\n            ;;\n        -h | --help )\n            usage\n            exit 0\n            ;;\n        * )\n            echo \"Unknown argument ${1}\"\n            exit 1\n            ;;\n    esac\n    shift\ndone\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\n#export CUDA_VISIBLE_DEVICES=2,3\n\ntorchrun $DISTRIBUTED_ARGS finetune.py \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 False \\\n    --output_dir output_qwen \\\n    --num_train_epochs 5 \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 8 \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 100 \\\n    --save_total_limit 10 \\\n    --learning_rate 3e-4 \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length 512 \\\n    --lazy_preprocess True \\\n    --use_lora \\\n    --gradient_checkpointing \\\n    --deepspeed ${DS_CONFIG_PATH}\n\n```\n### 6. 修改deepspeed中ds_config_zero2.json配置文件，增加TFlops显示\n其中具体增加的参数配置为flops_profiler\n```json\n\"flops_profiler\": {\n        \"enabled\": true,\n        \"profile_step\": 1,\n        \"module_depth\": -1,\n        \"top_modules\": 1,\n        \"detailed\": false,\n        \"output_file\": null\n    },\n```\n```json\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \"bf16\": {\n        \"enabled\": \"auto\"\n    },\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"none\",\n            \"pin_memory\": true\n        },\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": true\n    },\n\n    \"flops_profiler\": {\n        \"enabled\": true,\n        \"profile_step\": 1,\n        \"module_depth\": -1,\n        \"top_modules\": 1,\n        \"detailed\": false,\n        \"output_file\": null\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 100,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}\n\n```\n### 7. 执行finetune_lora_ds.sh单机多卡训练\n其中报了Error while creating shared memory segment /dev/shm/nccl-KXWrmA (size 9637888)导致在docker中单机多卡拉起失败\n问题原因：docker的shm共享内存不足，可以通过命令\n`df -h | grep shm`查看当前容器的shm大小，默认为64M，这是远远不够的，所以要增加该容器的shm共享内存大小。参考博文[https://blog.csdn.net/gg864461719/article/details/112466585](https://blog.csdn.net/gg864461719/article/details/112466585)\n#### 解决方法1：创建完容器之后，手动修改shm共享内存大小\n##### a. 首先要关闭docker, 否则下面的操作步骤会无效.\n`service docker stop`\n##### b. 进入宿主机中/docker/containers/容器id 修改该容器的hostconfig.json文件，把其中的ShmSize的大小后面增加22（就变为了6.3G）其默认的是67108864_KB_ 就约等于64M。\n##### c. 重启docker服务\n`systemctl start docker`\n##### d. 解决完之后，重新查看shm的共享内存大小，此时已经变为了6.3G\n#### 解决方法2：在通过镜像run容器时，就直接指定--shm-size 6G\n`docker run -it --name qwen --gpus all --shm-size 6G registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0 bash`\n### 重新拉起训练\n`bash finetune/finetune_lora_ds.sh`\n其中tflops在9.5~10.5之间\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen9.png)\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen10.png)\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen11.png)\n\n","tags":["Nvidia","V100","模型训练调优","Qwen","Deepspeed"],"categories":["模型训练调优","NVIDIA"]},{"title":"1.4-基于k8s拉起Qwen模型的多机多卡微调","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/1.4-基于k8s拉起Qwen模型的多机多卡微调/","content":"一、镜像准备\n### 1. 查看镜像\n`docker images`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen12.png)\n### 2. 修改镜像标签\n`docker tag ec99659d9677 registry.paas/library/qwen:v3.0`\n### 3. 将镜像推至仓库\n`docker push registry.paas/library/qwen:v3.0`\n### 4. 如果出现签名认证失败，需要修改docker守护进程配置文件\n`vim /etc/docker/daemon.json`\n增加如下配置：\n```json\n{\n\"insecure-registries\":[\"registry.paas\"]\n}\n```\n重启docker\n`systemctl daemon-reload && systemctl restart docker`\n### 5. 重新push至registry.paas/library/xxx:tags仓库\n二、修改配置文件\n### 1. qwentest.yaml\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: qwentest\nspec:\n  selector:\n    matchLabels:\n      app: qwentest\n  template:\n    metadata:\n      labels:\n        app: qwentest\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        model: qwen-7b\n      containers:\n      - name: qwentest\n        image: registry.paas/library/qwen:v3.0\n        imagePullPolicy: IfNotPresent\n        resources:\n         limits:\n           nvidia.com/gpu: \"4\"\n         requests:\n           nvidia.com/gpu: \"4\"\n        command:                                  # training command, which can be modified\n              - \"/bin/bash\"\n              - \"-c\"\n                #- sleep 10000\n              - |\n                cd /mnt/ &&\n                cp setRank.sh /qwen/ &&\n                cd /qwen/ &&\n                chmod +x setRank.sh &&\n                bash setRank.sh &&\n                chmod +x finetune_lora_ds.sh &&\n                bash finetune_lora_ds.sh\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: processeddata\n          mountPath: /mnt\n        - name: dshm\n          mountPath: /dev/shm\n        - name: tmp-volume\n          mountPath: /tmp\n      volumes:\n      - name: processeddata\n        hostPath:\n          path: /mnt/users/wangxiangbo/runk8s\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1G\n      - name: tmp-volume\n        hostPath:\n          path: /tmp\n```\n### 2. hostfile\n```shell\n192.168.0.20\n192.168.0.58\n```\n### 3. setRank.sh\n```shell\n#!/bin/bash\n\nshell_name=\"finetune_lora_ds.sh\"\n\nshell_dir=\"/mnt/\"\n\nlocal_dir=\"/qwen/\"\n\n## 复制脚本到/qwen/下\ncp $shell_dir$shell_name $local_dir\n\n\n## 读取hostfile\nreadarray -t ips < <(grep -vE '^[[:space:]]*$' \"$shell_dir\"hostfile)\n\n## 获取rank0 IP\nrank0_ip=$(echo \"${ips[0]}\" | tr -d '[:space:]')\n\nnodes=${#ips[@]}\n\n## 获取hostfile中配置的IP前缀\n## 使用cut提取IP地址的前三个数字部分\nip_prefix=$(echo \"${ips[0]}\" | cut -d '.' -f 1-3)\n\n## 获取本机IP\nip=$(hostname -I | grep -oE \"$ip_prefix\\.[0-9]+\")\n\nls\n# 初始化rank\nnode_rank=-1\n\n# 遍历数组\nfor i in \"${!ips[@]}\"; do\n    # 使用tr命令去除空白字符\n    clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n    if [[ \"$clean_string\" == \"$ip\" ]]; then\n        node_rank=$i\n        break\n    fi\ndone\n\nif [ $node_rank -ne -1 ]; then\n    ## 修改脚本中MASTER_ADDR\n    sed -i \"s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/\" $local_dir$shell_name\n\n    ## 修改NNODES\n    sed -i \"s/^NNODES=.*/NNODES=$nodes/\" $local_dir$shell_name\n\n    ## 修改NODE_RANK\n    sed -i \"s/^NODE_RANK=.*/NODE_RANK=$node_rank/\" $local_dir$shell_name\nfi\n\n```\n\n### 4. sh\n```shell\n#!/bin/bash\n/bin/bash -i <<'EOF'\nexport NCCL_IB_DISABLE=1\nexport NCCL_SOCKET_IFNAME=eth0\nexport NCCL_P2P_DISABLE=1\nexport NCCL_DEBUG=INFO\nsource ~/.bashrc\n\n. /opt/miniconda/etc/profile.d/conda.sh\nconda activate qwen\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\nDIR=`pwd`\n\n# Number of GPUs per GPU worker\nGPUS_PER_NODE=4\n# Number of GPU workers, for single-worker training, please set to 1\nNNODES=2\n# The rank of this worker, should be in {0, ..., WORKER_CNT-1}, for single-worker training, please set to 0\nNODE_RANK=0\n# The ip address of the rank-0 worker, for single-worker training, please set to localhost\nMASTER_ADDR=192.168.0.20\n# The port for communication\nMASTER_PORT=6003\n\nMODEL=\"/qwen/Qwen-7B-Chat\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"/qwen/train_data_law.json\"\nDS_CONFIG_PATH=\"/qwen/finetune/ds_config_zero2.json\"\n\nfunction usage() {\n    echo '\nUsage: bash finetune_lora_ds.sh [-m MODEL_PATH] [-d DATA_PATH] [--deepspeed DS_CONFIG_PATH]\n'\n}\n\nwhile [[ \"$1\" != \"\" ]]; do\n    case $1 in\n        -m | --model )\n            shift\n            MODEL=$1\n            ;;\n        -d | --data )\n            shift\n            DATA=$1\n            ;;\n        --deepspeed )\n            shift\n            DS_CONFIG_PATH=$1\n            ;;\n        -h | --help )\n            usage\n            exit 0\n            ;;\n        * )\n            echo \"Unknown argument ${1}\"\n            exit 1\n            ;;\n    esac\n    shift\ndone\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\n\ntorchrun $DISTRIBUTED_ARGS finetune.py \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 False \\\n    --output_dir output_qwen \\\n    --num_train_epochs 5 \\\n    --per_device_train_batch_size 8 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 8 \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 100 \\\n    --save_total_limit 10 \\\n    --learning_rate 3e-4 \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length 512 \\\n    --lazy_preprocess True \\\n    --use_lora \\\n    --gradient_checkpointing \\\n    --ddp_find_unused_parameters False \\\nEOF\n\n```\n三、拉起训练（3号和4号两机8卡）\n### 1. 切换到1号机器master节点上，给带训练得3号和4号机器打上标签\n`kubectl label nodes ecs-jhjs-1234-003 model=qwen-7b`\n`kubectl label nodes ecs-jhjs-1234-004 model=qwen-7b`\n### 2. 准备好启动脚本等文件后，在master节点1号机器上，利用修改好的qwentest.yaml文件拉起训练任务\n### 3. `kubectl apply -f qwentest.yaml`\n### 4. 通过kubectl查看pod节点启动信息\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen13.png)\n### 5. 查看两个节点pod的logs日志\n`kubectl logs qwentest-czm8n -f`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen14.png)\n`kubectl logs qwentest-qthsf -f`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen15.png)\n\n","tags":["Nvidia","V100","模型训练调优","Qwen","Deepspeed"],"categories":["模型训练调优","NVIDIA"]},{"title":"1.2-Qwen模型镜像制作","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/1.2-Qwen模型镜像制作/","content":"由于新机器挂载文件存储速度非常慢，通过dockerfile文件来直接生成镜像非常慢，所以本镜像在自己的V100云主机中进行打包。\n### 1. 首先docker pull拉取一个ubuntu基础环境\n[https://hub.docker.com/r/nvidia/cuda/tags?page=11&page_size=&name=&ordering=](https://hub.docker.com/r/nvidia/cuda/tags?page=11&page_size=&name=&ordering=)\n`docker pull nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04`\n### 2. 安装nvidia-container-toolkit，使docker可以调用宿主机gpu资源\n  #### 2.1 下载nvidia-container-toolkit\n `distribution=$(. /etc/os-release;echo $ID$VERSION_ID) && \\ curl -fsSL [https://nvidia.github.io/libnvidia-container/gpgkey](https://nvidia.github.io/libnvidia-container/gpgkey) | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg && \\ curl -s -L [https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list](https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list) | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list`\n  #### 2.2 安装nvidia-container-toolkit\n  `sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit`\n  #### 2.3 添加nvidia-docker源\n  `curl -s -L [https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list](https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list) |   sudo tee /etc/apt/sources.list.d/nvidia-docker.list`\n  #### 2.4 更新并重新执行安装\n  `sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit`\n如果执行过程中报W: GPG error: [https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64](https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64)  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY DDCAE044F796ECB0，则需要确认你的系统是否信任NVIDIA的GPG密钥。如果没有，你需要导入它。可以通过以下命令导入GPG密钥：\n`curl -s [https://nvidia.github.io/libnvidia-container/gpgkey](https://nvidia.github.io/libnvidia-container/gpgkey) | sudo apt-key add -`\n  #### 2.5 完成 nvidia-container-toolkit 的安装之后，我们继续执行 nvidia-ctk runtime configure 命令，为 Docker 添加 nvidia 这个运行时。完成后，我们的应用就能在容器中使用显卡资源了\n  `sudo nvidia-ctk runtime configure --runtime=docker`\n\n  ![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen5.png)\n  #### 2.6 重启docker\n  `sudo systemctl restart docker`\n  #### 2.7 查看是否安装成功  \n  `dpkg -l | grep nvidia-container-toolkit`\n\n  ![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen6.png)\n\n### 3. 拉取基础镜像成功后，创建一个docker容器\n`docker images`\n`sudo docker run -it --name qwen --gpus all nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04`\n`exit`\n#### 4. 退出容器后，将本地跑同的qwen模型代码/权重/数据集/环境cp到创建的qwen镜像中\n`docker cp /opt/tmp/Qwen/ 02649afd9710:/qwen`\n#### 5. 重启docker，exec执行\n`docker ps -a`\n`docker start qwen`\n`docker exec -it 02649afd9710 bash`\n#### 6. 因为想在容器中执行自身的python环境，不借用宿主机的环境，所以需要单独再安装conda、pytorch等环境。\n#### 7. 安装完基础环境后，需要安装qwen模型的依赖\n`pip install -r requirements.txt`\n#### 8. 没有vim编辑器还需安装vim\n`apt-get update`\n`apt-get install -y vim`\n#### 9. deepspeed安装\n`pip install \"peft<0.8.0\" deepspeed`\n#### 10. 此时在qwen容器中执行训练脚本，拉起训练。\n`bash finetune/finetune_lora_single_gpu.sh`\n#### 11. 将此时qwen容器打成镜像\n`docker commit -a \"wangxiangbo\" -m \"qwen 7B\" 02649afd9710 qwen-7b:v1.0`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen7.png)\n#### 12. 将打好的镜像转成tar包，供新机器解压使用\n`docker save -o qwen-7b.tar qwen-7b:v1.0`\n#### 13. 加载tar镜像, 使用load进行从tar文件导出镜像\n`docker load -i qwen-7b.tar`\n#### 14. 由于新机器挂载文件存储的原因，镜像解压速度太慢，于是打算将打好的qwen镜像push到阿里云个人仓库中，在新机器中直接pull该镜像。\n#### 15. 将镜像推送到Registry\n`docker login --username=aliyun9599911612 registry.cn-shanghai.aliyuncs.com`\n`docker tag 37c7b97b67f6 registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0`\n`docker push registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0`\n#### 16. 在3号机器中，拉取该镜像\n`docker pull registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen8.png)\n#### 17. 通过该镜像，run一个容器\n`docker run -it --name qwen --gpus all registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0 bash`\n\n","tags":["Nvidia","V100","模型训练调优","Qwen","Deepspeed"],"categories":["模型训练调优","NVIDIA"]},{"title":"1.5-Qwen多机多卡调优","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/1.5-Qwen多机多卡调优/","content":"### 1. lora\n#### 1.1 per_device_train_batch_size测试，最优为16\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 2 | 1 | 512 | 8 | true | 8.37 |\n| 4 | 1 | 512 | 8 | true | 9.07 |\n| 8 | 1 | 512 | 8 | true | 9.88 |\n| **16** | **1** | **512** | **8** | **true** | **10.32** |\n| 32 | 1 | 512 | 8 | true | OOM |\n\n\n单个GPU批次大小增加时，需要的内存也会增加，GPU内存不足以支持更大的批次，可能会导致溢出或效率降低。\n\n#### 1.2 per_device_eval_batch_size测试，最优为2\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 16 | 1 | 512 | 8 | true | 10.32 |\n| **16** | **2** | **512** | **8** | **true** | **10.33** |\n| 16 | 4 | 512 | 8 | true | 10.26 |\n\n\n#### 1.3 model_max_length测试，最优为512\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16 | flops |\n| --- | --- | --- | --- | --- | --- |\n| 16 | 2 | 128 | 8 | true | 8.95 |\n| 16 | 2 | 256 | 8 | true | 9.56 |\n| **16** | **2** | **512** | **8** | **true** | **10.33** |\n| 16 | 2 | 1024 | 8 | true | OOM |\n\n\n#### 1.4 gradient_accumulation_steps测试，最优为16\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 16 | 2 | 512 | 1 | true | 9.07 |\n| 16 | 2 | 512 | 2 | true | 9.24 |\n| 16 | 2 | 512 | 4 | true | 9.89 |\n| 16 | 2 | 512 | 8 | true | 10.33 |\n| **16** | **2** | **512** | **16** | **true** | **11.63** |\n\n\n#### 1.5 开启Fp16测试\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| **16** | **2** | **512** | **16** | **true** | **67.13** |\n\n\n#### 1.6 关闭gradient_checkpointing\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | gradient_checkpointing | tflops |\n| --- | --- | --- | --- | --- | --- | --- |\n| **16** | **2** | **512** | **16** | **true** | **true** | **67.13** |\n| 16 | 2 | 512 | 16 | true | false | OOM |\n\n\n| 参数配置 | per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16/fp16 | gradient_checkpointing | tflops |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| 默认 | 2 | 1 | 512 | 8 | bf16 | **true** | 8.32 |\n| 调优后 | **<font style=\"color:#DF2A3F;\">16</font>** | **<font style=\"color:#DF2A3F;\">2</font>** | **<font style=\"color:#DF2A3F;\">512</font>** | **<font style=\"color:#DF2A3F;\">16</font>** | **<font style=\"color:#DF2A3F;\">fp16</font>** | **<font style=\"color:#DF2A3F;\">true</font>** | **<font style=\"color:#DF2A3F;\">67.13</font>** |\n\n\n基于2机8卡的V100，Qwen-7B模型的lora微调训练中（采用deepspeed的zero2的内存优化并行方式），Tflops的值最高为<font style=\"color:#DF2A3F;\">67.13</font>**（8.32）\n\n最佳参数配置（per_device_train_batch_size：16，per_device_eval_batch_size：2，model_max_length：512，gradient_accumulation_steps：16，Fp16精度，gradient_checkpointing：True）\n\n### 2. qlora\n\nqlora使用4比特量化模型以及paged attention等技术实现更小的显存开销\n\n#### 2.1 per_device_train_batch_size测试，最优为32\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 2 | 1 | 512 | 8 | true | 31.71 |\n| 4 | 1 | 512 | 8 | true | 42.91 |\n| 8 | 1 | 512 | 8 | true | 51.14 |\n| 16 | 1 | 512 | 8 | true | 55.75 |\n| **32** | **1** | **512** | **8** | **true** | **58.44** |\n| 64 | 1 | 512 | 8 | true | OOM |\n\n\n#### 2.2 per_device_eval_batch_size测试，最优为4\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 32 | 1 | 512 | 8 | true | 58.44 |\n| 32 | 2 | 512 | 8 | true | 58.84 |\n| **32** | **4** | **512** | **8** | **true** | **59.05** |\n| 32 | 8 | 512 | 8 | true | 58.42 |\n\n\n#### 2.3 model_max_length测试，最优为512\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 32 | 4 | 128 | 8 | true | 48.68 |\n| 32 | 4 | 256 | 8 | true | 53.02 |\n| **32** | **4** | **512** | **8** | **true** | **59.05** |\n| 32 | 4 | 1024 | 8 | true | OOM |\n\n\n#### 2.4 gradient_accumulation_steps测试，最优为\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 32 | 4 | 512 | 1 | true | 52.66 |\n| 32 | 4 | 512 | 2 | true | 52.18 |\n| 32 | 4 | 512 | 4 | true | 54.99 |\n| 32 | 4 | 512 | 8 | true | 59.05 |\n| 32 | 4 | 512 | 16 | true | 64.26 |\n| 32 | 4 | 512 | 32 | true | 80.37 |\n| **32** | **4** | **512** | **64** | **true** | **103.89** |\n\n\n#### 2.5 开启BF16测试\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16/fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 32 | 4 | 512 | 64 | bf16 | 14.23 |\n| **32** | **4** | **512** | **64** | **fp16** | **103.89** |\n\n\n#### 2.6 关闭gradient_checkpointing\n\n| gradient_checkpointing | per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- | --- |\n| **<font style=\"color:#000000;\">True</font>** | **<font style=\"color:#000000;\">32</font>** | **<font style=\"color:#000000;\">4</font>** | **<font style=\"color:#000000;\">512</font>** | **<font style=\"color:#000000;\">64</font>** | **<font style=\"color:#000000;\">true</font>** | **<font style=\"color:#000000;\">103.89</font>** |\n| False | 32 | 4 | 512 | 64 | true | OOM |\n\n\nDeepSpeed ZeRO 3 对节点间通信速率的要求远大于 ZeRO 2，在多机微调的情况下会大幅降低训练速度。因此，我们不建议在多机微调的情况下使用 DeepSpeed ZeRO 3 配置。\n\n| 参数配置 | per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- | --- |\n| 默认 | 2 | 1 | 512 | 8 | true | 31.71 |\n| 调优后 | **<font style=\"color:#DF2A3F;\">32</font>** | **<font style=\"color:#DF2A3F;\">4</font>** | **<font style=\"color:#DF2A3F;\">512</font>** | **<font style=\"color:#DF2A3F;\">64</font>** | **<font style=\"color:#DF2A3F;\">true</font>** | **<font style=\"color:#DF2A3F;\">103.89</font>** |\n\n\n基于2机8卡的V100，Qwen-7B模型的qlora微调训练中（采用deepspeed的zero2的内存优化并行方式），Tflops的值最高为<font style=\"color:#DF2A3F;\">103.89 </font>\n\n默认参数配置\n\n最佳参数配置（gradient_checkpointing：True，per_device_train_batch_size：32，per_device_eval_batch_size：4，model_max_length：512，gradient_accumulation_steps：64，Fp16精度）\n\nqlora在拉起训练时，需要对模型权重进行INT-4量化，会损失掉权重小数部分精度，量化后的权重由于使用更少的位数来表示，存储上比原始权重高效，在拉起模型训练过程过中tflops有明显提升。\n\n","tags":["Nvidia","V100","模型训练调优","Qwen","Deepspeed"],"categories":["模型训练调优","NVIDIA"]},{"title":"Git1_git命令","url":"/2025/11/04/Git/Git1_git 命令/","content":"**<font style=\"background-color:#FBDE28;\">git commmit -v和git push origin develop之后，我怎么撤销本次commit提交的代码，将origin develop的仓库返回值提交之前的状态？</font>**\n\n在Git中，如果你想要撤销最近的一次提交并将仓库状态回退到提交之前，你可以使用`git reset`命令。这里有几种方法可以撤销提交：\n\n1. **使用**`git reset --soft`：\n    - 这个命令会将HEAD指针移动到指定的提交，但是不会改变工作目录和索引的状态。这意味着你的更改仍然在暂存区（staged），你可以重新提交。\n    - 命令：`git reset --soft HEAD~1`（将HEAD指针回退到上一个提交）\n2. **使用**`git reset --mixed`（默认选项）：\n    - 这个命令会将HEAD指针移动到指定的提交，并且将索引（staged changes）重置到HEAD的状态，但是不会改变工作目录。\n    - 命令：`git reset --mixed HEAD~1`（将HEAD和索引回退到上一个提交，但工作目录不变）\n3. **使用**`git reset --hard`：\n    - 这个命令会将HEAD指针、索引和工作目录都重置到指定的提交状态。这意味着所有未提交的更改都会丢失。\n    - 命令：`git reset --hard HEAD~1`（将HEAD、索引和工作目录都回退到上一个提交）\n\n如果你已经将代码推送到了远程仓库（比如`origin develop`），并且想要撤销这次推送，你可以使用`git push`命令配合`--force`选项来强制推送本地的更改到远程仓库。但是请注意，这会覆盖远程仓库的历史，所以只有在你确定远程仓库没有其他人的提交或者你不介意丢失这些提交时才应该使用。\n\n命令如下：\n\n```bash\ngit push origin develop --force\n```\n\n或者，如果你使用的是Git 2.0或更高版本，可以使用更安全的`--force-with-lease`选项，它会在远程分支没有新的提交时才允许强制推送。\n\n命令如下：\n\n```bash\ngit push origin develop --force-with-lease\n```\n\n**警告**：使用`--force`选项会覆盖远程仓库的历史，这是一个破坏性的操作。在团队协作的环境中，这可能会导致其他协作者的工作丢失，所以请谨慎使用。\n\n","tags":["开发","git"],"categories":["开发","git"]},{"title":"Linux1_kimi问题","url":"/2025/11/04/Linux/Linux1_kimi问题/","content":"1. 权重转换代码换行符\nKimi: 出现这些错误通常是因为脚本文件在不同的操作系统环境下被编辑或创建，导致文件的换行符不一致。Unix/Linux系统使用LF（Line Feed，`\\n`）作为换行符，而Windows系统使用CR+LF（Carriage Return + Line Feed，`\\r\\n`）作为换行符。\n错误信息中的 `$'\\r': command not found` 表示脚本尝试执行一个以 `\\r` 开头的命令，这通常是因为脚本文件中的换行符是 `\\r\\n` 而不是 `\\n`。\n此外，`syntax error near unexpected token 'elif'` 错误表明脚本解析器在遇到 `elif` 时遇到了语法错误，这可能是因为 `elif` 之前的行（可能是第59行）以 `\\r\\n` 结尾，导致 `elif` 行实际上是从一个新的行开始，而不是在预期的位置。\n要解决这个问题，你可以使用 `dos2unix` 工具将脚本文件从Windows格式转换为Unix/Linux格式。如果你没有 `dos2unix` 工具，可以使用 `sed` 命令来移除 `\\r` 字符：\n```bash\nsed -i 's/\\r$//' hf2mcore_convertor.sh\n```\n这条命令会查找文件中的所有行，并移除每行末尾的 `\\r` 字符。\n如果你有 `dos2unix` 工具，可以使用以下命令：\n```bash\ndos2unix hf2mcore_convertor.sh\n```\n这将把文件从Windows格式转换为Unix/Linux格式。\n在运行这些命令之后，再次执行脚本应该就不会出现这些错误了。\n2. 修改文件遇到Read-only file system\nKimi: 当你遇到 \"Read-only file system\" 错误时，通常意味着你的文件系统出于某种原因被挂载为只读模式。这可能是由于文件系统损坏、磁盘硬件问题、或者挂载时指定了错误的参数。以下是一些解决步骤：\n1. **检查文件系统挂载状态**：  \n使用 `mount` 命令查看所有挂载的文件系统及其状态。你可以使用如下命令：\n```bash\nmount | grep /path/to/mount/point\n```\n查看挂载点是否为只读（ro）。\n2. **重新挂载文件系统为读写模式**：  \n如果文件系统被挂载为只读，你可以尝试重新挂载它为读写模式。使用以下命令：\n```bash\nsudo mount -o remount,rw /path/to/mount/point\n```\n请将 `/path/to/mount/point` 替换为实际的挂载点路径。\n3. **检查和修复文件系统**：  \n如果重新挂载不起作用，可能是文件系统损坏。你可以使用 `fsck` 命令来检查和修复文件系统。例如，对于 ext4 文件系统，可以使用：\n```bash\nsudo fsck -f /dev/sda1\n```\n请确保替换 `/dev/sda1` 为你的实际分区。\n\n","tags":["运维","Linux"],"categories":["Linux"]},{"title":"Git2_gerrit仓库代码提交流程","url":"/2025/11/04/Git/Git2_gerrit仓库代码提交流程/","content":"1. 生成个人密钥\n```bash\nssh-keygen -C wangxiangbo_JTAGI@cmss.chinamobile.com\n```\n2. 修改config文件，增加gerrit仓库配置，增加刚生成的个人密钥`id_ed25519`\n```bash\nhost gerrit\nport 29418\nhostname gerrit.cmss.com\nuser wangxiangbo_JTAGI\nIdentityFile ~/.ssh/id_ed25519\n```\n3. 在gerrit中setting设置中，增加ssh的公钥，即`id_ed25519.pub`\n4. 设置邮箱，邮箱名`wangxiangbo@cmss.chinamobile.com`为不带JTAGI后缀的邮箱。\n5. 用ssh clone代码仓库\n```bash\ngit clone \"ssh://wangxiangbo_JTAGI@gerrit.cmss.com:29418/AGI/CM_OPTIMUS\" && scp -p -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg \"CM_OPTIMUS/.git/hooks/\"\n```\n6. 查看分支\n```bash\ngit branch -a\n```\n7. 查看远端分支\n```bash\ngit branch -r\n```\n8. 通过远端origin/develop仓库，创建一个本地develop开发分支\n```bash\ngit checkout -b develop origin/develop\n```\n9. 修改代码后，add之后并commit提交\n```bash\ngit commit -v\n```\n10. 推送远端origin/develop仓库\n```bash\ngit push origin HEAD:refs/for/develop\n```\n11. 在推送过程中会出现缺失 `Change-Id `的错误\n```bash\ngitdir=$(git rev-parse --git-dir); scp -p -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg ${gitdir}/hooks/\n```\n执行完毕后如果出现subsystem request failed on channel 0，则将-p修改为-O\n```bash\ngitdir=$(git rev-parse --git-dir); scp -O -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg ${gitdir}/hooks/\n```\n将本次commit提交的末尾加上Change-Id \n```bash\ngit commit --amend --no-edit\n```\n再次push\n```bash\ngit push origin HEAD:refs/for/develop\n```\n12. 提交完成之后，打开gerrit，找到develop分支的gitweb\n13. 选择review\n14. 找到刚刚提交的代码\n15. 点击add reviewer，评审人要最少要两个以上\n16. 第一次之后提交代码步骤\n```bash\ngit pull origin\ngit add xxx\ngit commit -v \ngit push origin HEAD:refs/for/develop\n```\n\n","tags":["开发","gerrit"],"categories":["开发","git"]},{"title":"运维1_docker命令","url":"/2025/11/04/服务器运维/运维1_docker命令/","content":"1. 启动一个容器\n```bash\nsudo docker run -it --name qwen --gpus all nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04\n```\n启动一个容器后，又通过exit关闭。发现docker ps没有了，该如何重新启动这个qwen容器？\n```bash\ndocker ps -a\n\ndocker start qwen\n\ndocker exec -it 容器id bash\n```\n\n2. 拉取一个新镜像后，通过这个镜像创建一个容器。\n```bash\ndocker run -it --name qwen --gpus all 镜像名:镜像tags bash\n```\n\n3. 将此时qwen容器打成镜像\n```bash\ndocker commit -a \"wangxiangbo\" -m \"qwen 7B\" 02649afd9710 qwen-7b:v1.0\n```\n\n\n\n\n","tags":["运维","docker"],"categories":["运维","docker"]},{"title":"Linux2_Linux命令","url":"/2025/11/04/Linux/Linux2_linux命令/","content":"1. 当执行训练任务时，手动关闭训练，此时gup资源仍然占用，需要手动kill掉进程\n查询正在运行的进程并gerp finetune_lora_single_gpu.sh\n`ps aux | grep finetune_lora_single_gpu.sh`\n`ps aux | grep finetune_lora_ds.sh`\n`kill -9 372813` 根据上一步查出来的进程号，用kill -9强制删除\n`nvidia-smi `查看是否gpu显存已经清空\n2. 查看某个文件的前20行\n`head -n 20 train_data_law.json`\n3. 查看当前文件夹的大小\n`du -sh .`\n4. 查看当前文件夹挂载的是哪一个盘\n`df -h .`\n从大到小查看当前文件夹的下的文件大小\n`du -sh * | sort -rh`\n4. 查看当前文件下所有文件的大小\n`ls -lh`\n5. <font style=\"color:#000000;\">把一个名为Yuan2的文件夹的所有内容（包括该文件夹名），cp到/mnt/users/wangxiangbo/nemo/model目录下，使最后的目录为 /mnt/users/wangxiangbo/nemo/model/Yuan2</font>\n`cp -a Yuan2 /mnt/users/wangxiangbo/nemo/model/`\n6. 通过pid查看某个进程的详细信息\n`<font style=\"color:rgb(56, 58, 66);background-color:rgb(250, 250, 250);\">ps -fp 167891</font>`\n7. 查看当前目录下所有文件的磁盘占用情况\n`du -ah | sort -hr | head -n 20`\n8.  使用以下命令将`megatron-core`文件夹压缩成一个名为`megatron-core.zip`的zip文件\n \t` zip -r megatron-core.zip megatron-core  `\n9.  使用以下命令将`megatron-core.zip`解压缩  \n` unzip megatron-core.zip  `\n11. hg上面下载模型\n```bash\npip install -U huggingface_hub\nhuggingface-cli download bigscience/bloom-560m --local-dir bloom-560m\nhuggingface-cli download Qwen/Qwen2-7B-Instruct --local-dir Qwen2-7B-Instruct\nhuggingface-cli download Qwen/Qwen2.5-7B-Instruct --local-dir Qwen2.5-7B-Instruct\nhuggingface-cli download BAAI/IndustryCorpus_computer --repo-type dataset --local-dir IndustryCorpus_computer\nhuggingface-cli download BAAI/IndustryCorpus2_current_affairs_government_administration --repo-type dataset --local-dir government_administration\nhuggingface-cli download ShengbinYue/DISC-Law-SFT --repo-type dataset --local-dir DISC-Law-SFT\nhuggingface-cli download TigerResearch/sft_zh\n```\n12. 归档压缩文件\n```bash\n#-c：创建一个新的压缩文件。\n#-z：通过 gzip 压缩文件。\n#-v：显示详细的压缩过程。\n#-f：指定压缩后的文件名，这里是 colossalai.tar.gz\ntar -czvf colossalai.tar.gz colossalai/  \n\n#-x：表示解压。\n#-z：表示解压 .gz 格式的文件。\n#-v：显示解压过程。\n#-f：指定解压的文件\ntar -xzvf file.tar.gz\n```\n\n\n\n","tags":["运维","Linux"],"categories":["Linux"]},{"title":"Git3_gitlab仓库代码提交流程","url":"/2025/11/04/Git/Git3_gitlab仓库代码提交流程/","content":"1. gitlab上fork主仓库，生成个人的远端仓库origin/develop\n2. 修改git提交用户配置为九天账号\n```bash\ngit config --global --list\ngit config --global user.name wangxiangbo_JTAGI\ngit config --global user.email wangxiangbo_JTAGI@cmss.chinamobile.com\n```\n3. 拉取个人的远端仓库\n```bash\ngit clone http://gitlab.cmss.com/wangxiangbo/CM_OPTIMUS.git \n```\n4. 查看分支\n```bash\ngit branch -a\n```\n5. 查看远程仓库分支\n```bash\ngit branch -r\n```\n6. 通过个人的远端origin/develop仓库，创建一个本地develop开发分支\n```bash\ngit checkout -b develop origin/develop\n```\n7. <font style=\"color:rgba(0, 0, 0, 0.85);\">列出所有的远程仓库以及对应的 URL</font>\n```bash\ngit remote -v\n```\n8. <font style=\"color:rgba(0, 0, 0, 0.85);\">将远程仓库地址添加到本地Git仓库的远程仓库列表中，本地的upstream/develop仓库会和gitlab远程仓库关联起来</font>\n```bash\ngit remote add upstream http://gitlab.cmss.com/AGI/CM_OPTIMUS.git\n```\n8. upstream/develop远端仓库拉取最新的代码\n```bash\ngit fetch upstream\n```\n9. 将upstream/develop远端仓库的最新代码合并到本地的develop分支中\n```bash\ngit merge upstream/develop\n```\n10. 将本地develop最新的代码推到个人的远端仓库origin/develop中\n```bash\ngit push origin develop\n```\n11. <font style=\"background-color:#FBDE28;\">提交develop到origin/develop代码之前，保证自己的本地个人的远端仓库origin/develop和本地gitlab的远端仓库upstream/develop保持一致再提交</font>，防止后续origin/develop合并打upstream/develop上出现冲突。\n```bash\n#本地代码修改后拉取最新代码\n#1.拉取源代码\ngit fetch upstream \n#2.暂存本地修改的代码\ngit stash\n#3.合并源代码到当前的develop仓库\ngit merge upstream/develop\n#4.将本地修改的推送到远程的origin分支，使origin/develop和upstream/develop保持一致\ngit push origin develop\n#5.将暂存的代码弹出来\ngit stash pop\n```\n12. souretree上add需要提交的代码\n13. 提交add之后的代码\n```bash\ngit commit -v\n```\n```bash\n#修改下方的提交信息，第一行为提交描述信息\ndeepseek-7B预训练\n\nCode Source From: Self Code\nDescription: deepseek-7B预训练\nJira: #CMOPTIMUS-1089\n市场项目编号（名称）：CM_OPTIMUS\n```\n14. <font style=\"color:rgb(55, 53, 47);\">将提交完之后的代码合并到origin/develop上</font>\n```bash\ngit push origin develop\n```\n15. <font style=\"color:rgb(55, 53, 47);\">gitlab中找的到自己仓库下CM_OPTIMUS</font>\n16. <font style=\"color:rgb(55, 53, 47);\">找到Merge requests，然后点击开始一个合并</font>\n17. <font style=\"color:rgb(55, 53, 47);\">然后选择自己的分支为develop，还有旁边的分支develop，然后点击最下方的merge即可。该操作是将自己个人远端的orgin/develop仓库提交修改后的代码合并到gitlab远端的主仓库中（upstream/develop已和其关联，并通过git fetch保持最新）</font>\n选中自己刚刚提交到origin/develop上的代码，合并到AGI/CM_OPTIMUS的develop分支上\n在Write里添加合并人信息\n\n\n\n","tags":["开发","gitlab"],"categories":["开发","git"]},{"title":"运维2_k8s命令","url":"/2025/11/04/服务器运维/运维2_k8s命令/","content":"1. 获取特定命名空间的详细信息\n```bash\nkubectl get ns namespace-name -o wide\n```\n2. kubectl通过ns获取所有的pods的详细信息\n```bash\nkubectl get pods -n <ns_name> -o wide\n```\n3. 删除启动失败的pod，可以直接删除启动时的yaml，否则pod会自动重启\n```bash\nkubectl delete -f qwentest.yaml\n```\n4. 查看所有的node的lable标签信息\n```bash\nkubectl get node --show-labels\n\nkubectl get node xxx --show-labels\n\nkubectl get node --show-labels | grep model\n```\n5. 打标签\n```bash\nkubectl label nodes ecs-jhjs-1234-003 key=vaule\n```\n6. 删除某节点的lable标签\n```bash\nkubectl label nodes cce100-64-29-79.cce-stack.com model-\n```\n6. 创建一个命名空间namespace\n```bash\nkubectl create namespace xxx\n```\n7. 查看所有节点的标签\n```bash\nkubectl get nodes --show-labels\nkubectl get nodes --show-labels | grep model=llama2-70b\n```\n","tags":["运维","k8s"],"categories":["运维","k8s"]},{"title":"测试 Hexo 标签分类2","url":"/2025/10/31/页面测试/test copy/","content":"这是文章正文。","tags":["测试2","hexo"],"categories":["测试"]}]