[{"title":"前端1_调用api接口的写法","url":"/2025/11/06/前端/前端1_调用api接口的写法/","content":"\n### Get请求写法\n#### 1. 看Parameters中是否有需要带参数，其中`Authorization`为授权认证的token可以不用考虑。\n1.1 若是没有其余的参数限定，那么SyncRequestFuncType<请求参数类型,响应返回参数类型>，第一个参数就为void或者undefined。返回类型可以使用any，等获取到响应参数之后，在对其通过Interface/type进行明确。\n\n```javascript\nexport const getApi: SyncRequestFuncType<\nvoid,\nresponseType\n> = () => {\nreturn javaAxios({\n method: \"get\",\n url: \"xxx/xxxx\",\n});\n};\n```\n\n1.2 若是有其余参数限定，这时需要箭头函数中需要带着params来进行请求，且reuturn中也需要将params带着。\n\n\t1.2.1 `单个参数`，例如id，这时在请求参数类型中，需要明确出请求参数的类型<{id:string},responseType>，这时请求参数的类型为对象{}的形式，可以直接在尖括号中写出来，也可以通过引入定义的Interface/type类型来写<IdType,responseType>。\n\n```javascript\nexport const getApi: SyncRequestFuncType<\n {id:string},\n void\n> = (params) => {\nreturn javaAxios({\n method: \"get\",\n url: \"xxx/xxxx\",\n params,\n});\n};\n```\n\n\t1.2.2 `多个参数`，例如email，type...，因为参数比较多，最好通过Interface/type的方式SendEmailCaptchaForUserInfoModifyType来明确请求参数类型。\n\n```javascript\nexport type SendEmailCaptchaForUserInfoModifyType = {\ntype: ModifyType;\nemail?: string;\n};\n```\n\n```javascript\nexport const getApi: SyncRequestFuncType<\nSendEmailCaptchaForUserInfoModifyType,\nvoid\n> = (params) => {\nreturn javaAxios({\n method: \"get\",\n url: \"xxx/xxx\",\n params,\n});\n};\n```\n\n### Post请求写法\n#### 1. 看看Parameters中是否有需要带参数，其中`Authorization`为授权认证的token可以不用考虑。\n1.1 若是没有其余的参数限定，那么SyncRequestFuncType<请求参数类型,响应返回参数类型>，第一个参数就为void或者undefined。返回类型可以使用any，等获取到响应参数之后，在对其通过Interface/type进行明确。\n\n```javascript\nexport const clearVideoRecycle: SyncRequestFuncType<void, void> = () => {\nreturn javaAxios({\n method: \"post\",\n url: \"xxx/xxx\",\n});\n};\n```\n\n1.2 若是有其余参数限定，这时需要箭头函数中需要带着params来进行请求，且reuturn中也需要将params带着。\n\n\t1.2.1 单个参数时\n\n```javascript\nexport const cancelSubscribeVideo: SyncRequestFuncType<\n{ themeId: string },\nany\n> = (params) => {\nreturn javaAxios({\n method: \"post\",\n url: \"xxx/xxx\",\n params,\n});\n};\n```\n\n\t1.2.2 多个参数时\n\n```javascript\nexport type SendEmailCaptchaForUserInfoModifyType = {\ntype: ModifyType;\nemail?: string;\n};\n```\n\n```javascript\nexport const sendEmailCaptchaForUserInfoModify: SyncRequestFuncType<\nSendEmailCaptchaForUserInfoModifyType,\nvoid\n> = (params) => {\nreturn javaAxios({\n method: \"get\",\n url: \"xxx/xxx\",\n params,\n});\n};\n```\n\n#### 2. 当出现请求体`Request body`时，这时就需要在return的javaAxios中添加键值对`data: params`，来将参数添加到请求体中传递过去。\n2.1 Request body为：\n\n```javascript\n[\n\"string\"\n]\n```\n\n```javascript\nexport const batchDeleteVideo: SyncRequestFuncType<string[], void> = (\nparams\n) => {\nreturn javaAxios({\n method: \"post\",\n url: \"videos/themes/batchDel\",\n data: params,\n});\n};\n```\n\n2.2 Request body为：这时候需要在data中进一步在约束一下，使其对应api的请求体的格式。\n\n```javascript\n{\n\"themes\": [\n \"string\"\n]\n}\n```\n\n```javascript\nexport const sortVideos: SyncRequestFuncType<string[], void> = (params) => {\nreturn javaAxios({\n method: \"post\",\n url: \"videos/themes/changeVideoThemesSort\",\n data: { themes: params },\n});\n};\n```\n\n","tags":["前端","Javascript"],"categories":["前端"]},{"title":"1-Qwen模型本地部署与单机单卡/多卡训练","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/1-Qwen模型本地部署与单机单卡_多卡训练/","content":"一、V100环境部署\n1. 项目地址： [https://github.com/QwenLM/Qwen](https://github.com/QwenLM/Qwen)\n2. 下载到本地\n`git clone  https://github.com/QwenLM/Qwen.git `\n3. 基础环境搭建\n`conda create -n qwen python=3.10`\n`conda activate qwen`\n4. 安装pytorch\n`conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia`\n5. 安装依赖环境\n`pip install -r requirements.txt `\n7. 7B模型下载\n`git clone [https://www.modelscope.cn/qwen/Qwen-7B-Chat.git](https://www.modelscope.cn/qwen/Qwen-7B-Chat.git)`\n在使用sdk的python脚本下载权重时，需要pip安装modelscope\n`pip install modelscope`\n使用git clone发现权重未下载成功，使用modelscope官方sdk脚本下载。\n将以下代码写入download.py文件中，并执行`python download.py`\n```python\n#模型下载\nfrom modelscope import snapshot_download\nmodel_dir = snapshot_download('qwen/Qwen-7B-Chat', cache_dir='/opt/tmp/Qwen', revision='v1.1.9')\n```\n8. 数据集下载\n进入modelscope下载数据集\n`https://modelscope.cn/datasets/Robin021/DISC-Law-SFT/files`\n9. 数据格式处理\n数据集处理后会生成train_data_law.json文件\n`head -n 20 train_data_law.json`\n处理之后的数据格式如下：\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen1.png)\n```python\nimport json\n# 读取以.jsonl结尾的文件\njson_data = []\nwith open('/opt/tmp/Qwen/dataset/DISC-Law-SFT-Triplet-released.jsonl', 'r', encoding='utf-8') as file:\n    for line in file:\n        data = json.loads(line)\n        json_data.append(data)\n# 待填入的模板\ntemplate = []\n\n# 遍历json数据集\nfor idx, data in enumerate(json_data[:]):\n    conversation = [\n        {\n            \"from\":\"user\",\n            \"value\": data[\"input\"]\n        },\n        {\n            \"from\": \"assistant\",\n            \"value\": data[\"output\"]\n        }\n    ]\n    template.append({\n        \"id\": f\"identity_{idx}\",\n        \"conversations\": conversation\n    })\nprint(len(template))\n# 输出填充数据后的模板\nprint(json.dumps(template[2], ensure_ascii=False, indent=2))\n# 将template写入到本地文件\noutput_file_path = \"/opt/tmp/Qwen/train_data_law.json\"\nwith open(output_file_path, 'w', encoding='utf-8') as f:\n    json.dump(template, f, ensure_ascii=False, indent=2)\nprint(f\"处理好的数据已写入到本地文件: {output_file_path}\")\n```\n10. 训练依赖安装\ndeepspeed安装\n`pip install \"peft<0.8.0\" deepspeed`\n11. 修改模型微调脚本参数\n修改MODEL和DATA的路径，及per_device_train_batch_size\n```bash\n#!/bin/bash\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\nMODEL=\"/opt/tmp/Qwen/Qwen-7B-Chat\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"/opt/tmp/Qwen/train_data_law.json\"\n\nfunction usage() {\n    echo '\nUsage: bash finetune/finetune_lora_single_gpu.sh [-m MODEL_PATH] [-d DATA_PATH]\n'\n}\n\nwhile [[ \"$1\" != \"\" ]]; do\n    case $1 in\n        -m | --model )\n            shift\n            MODEL=$1\n            ;;\n        -d | --data )\n            shift\n            DATA=$1\n            ;;\n        -h | --help )\n            usage\n            exit 0\n            ;;\n        * )\n            echo \"Unknown argument ${1}\"\n            exit 1\n            ;;\n    esac\n    shift\ndone\n\nexport CUDA_VISIBLE_DEVICES=0\n\npython finetune.py \\\n  --model_name_or_path $MODEL \\\n  --data_path $DATA \\\n  --bf16 False \\\n  --output_dir output_qwen \\\n  --num_train_epochs 5 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 1 \\\n  --gradient_accumulation_steps 8 \\\n  --evaluation_strategy \"no\" \\\n  --save_strategy \"steps\" \\\n  --save_steps 100 \\\n  --save_total_limit 10 \\\n  --learning_rate 3e-4 \\\n  --weight_decay 0.1 \\\n  --adam_beta2 0.95 \\\n  --warmup_ratio 0.01 \\\n  --lr_scheduler_type \"cosine\" \\\n  --logging_steps 1 \\\n  --report_to \"none\" \\\n  --model_max_length 512 \\\n  --lazy_preprocess True \\\n  --gradient_checkpointing \\\n  --use_lora\n\n# If you use fp16 instead of bf16, you should use deepspeed\n# --fp16 True --deepspeed finetune/ds_config_zero2.json\n```\n\n12. 开启单机单卡训练\n`bash finetune/finetune_lora_single_gpu.sh`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen2.png)\n13. 单机多卡训练（在分配的3号机器上执行）\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen3.png)\ngpu显存利用情况\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen4.png)\n\n\n\n","tags":["模型训练调优","Nvidia","Qwen"],"categories":["模型训练调优 -NVIDIA"]},{"title":"2-Qwen模型镜像制作","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/2-Qwen模型镜像制作/","content":"由于新机器挂载文件存储速度非常慢，通过dockerfile文件来直接生成镜像非常慢，所以本镜像在自己的V100云主机中进行打包。\n### 1. 首先docker pull拉取一个ubuntu基础环境\n[https://hub.docker.com/r/nvidia/cuda/tags?page=11&page_size=&name=&ordering=](https://hub.docker.com/r/nvidia/cuda/tags?page=11&page_size=&name=&ordering=)\n`docker pull nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04`\n### 2. 安装nvidia-container-toolkit，使docker可以调用宿主机gpu资源\n  #### 2.1 下载nvidia-container-toolkit\n `distribution=$(. /etc/os-release;echo $ID$VERSION_ID) && \\ curl -fsSL [https://nvidia.github.io/libnvidia-container/gpgkey](https://nvidia.github.io/libnvidia-container/gpgkey) | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg && \\ curl -s -L [https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list](https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list) | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list`\n  #### 2.2 安装nvidia-container-toolkit\n  `sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit`\n  #### 2.3 添加nvidia-docker源\n  `curl -s -L [https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list](https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list) |   sudo tee /etc/apt/sources.list.d/nvidia-docker.list`\n  #### 2.4 更新并重新执行安装\n  `sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit`\n如果执行过程中报W: GPG error: [https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64](https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64)  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY DDCAE044F796ECB0，则需要确认你的系统是否信任NVIDIA的GPG密钥。如果没有，你需要导入它。可以通过以下命令导入GPG密钥：\n`curl -s [https://nvidia.github.io/libnvidia-container/gpgkey](https://nvidia.github.io/libnvidia-container/gpgkey) | sudo apt-key add -`\n  #### 2.5 完成 nvidia-container-toolkit 的安装之后，我们继续执行 nvidia-ctk runtime configure 命令，为 Docker 添加 nvidia 这个运行时。完成后，我们的应用就能在容器中使用显卡资源了\n  `sudo nvidia-ctk runtime configure --runtime=docker`\n\n  ![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen5.png)\n  #### 2.6 重启docker\n  `sudo systemctl restart docker`\n  #### 2.7 查看是否安装成功  \n  `dpkg -l | grep nvidia-container-toolkit`\n\n  ![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen6.png)\n\n### 3. 拉取基础镜像成功后，创建一个docker容器\n`docker images`\n`sudo docker run -it --name qwen --gpus all nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04`\n`exit`\n#### 4. 退出容器后，将本地跑同的qwen模型代码/权重/数据集/环境cp到创建的qwen镜像中\n`docker cp /opt/tmp/Qwen/ 02649afd9710:/qwen`\n#### 5. 重启docker，exec执行\n`docker ps -a`\n`docker start qwen`\n`docker exec -it 02649afd9710 bash`\n#### 6. 因为想在容器中执行自身的python环境，不借用宿主机的环境，所以需要单独再安装conda、pytorch等环境。\n#### 7. 安装完基础环境后，需要安装qwen模型的依赖\n`pip install -r requirements.txt`\n#### 8. 没有vim编辑器还需安装vim\n`apt-get update`\n`apt-get install -y vim`\n#### 9. deepspeed安装\n`pip install \"peft<0.8.0\" deepspeed`\n#### 10. 此时在qwen容器中执行训练脚本，拉起训练。\n`bash finetune/finetune_lora_single_gpu.sh`\n#### 11. 将此时qwen容器打成镜像\n`docker commit -a \"wangxiangbo\" -m \"qwen 7B\" 02649afd9710 qwen-7b:v1.0`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen7.png)\n#### 12. 将打好的镜像转成tar包，供新机器解压使用\n`docker save -o qwen-7b.tar qwen-7b:v1.0`\n#### 13. 加载tar镜像, 使用load进行从tar文件导出镜像\n`docker load -i qwen-7b.tar`\n#### 14. 由于新机器挂载文件存储的原因，镜像解压速度太慢，于是打算将打好的qwen镜像push到阿里云个人仓库中，在新机器中直接pull该镜像。\n#### 15. 将镜像推送到Registry\n`docker login --username=aliyun9599911612 registry.cn-shanghai.aliyuncs.com`\n`docker tag 37c7b97b67f6 registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0`\n`docker push registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0`\n#### 16. 在3号机器中，拉取该镜像\n`docker pull registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen8.png)\n#### 17. 通过该镜像，run一个容器\n`docker run -it --name qwen --gpus all registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0 bash`\n\n","tags":["模型训练调优","Nvidia","Qwen"],"categories":["模型训练调优 -NVIDIA"]},{"title":"4-基于k8s拉起Qwen模型的多机多卡微调","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/4-基于k8s拉起Qwen模型的多机多卡微调/","content":"一、镜像准备\n### 1. 查看镜像\n`docker images`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen12.png)\n### 2. 修改镜像标签\n`docker tag ec99659d9677 registry.paas/library/qwen:v3.0`\n### 3. 将镜像推至仓库\n`docker push registry.paas/library/qwen:v3.0`\n### 4. 如果出现签名认证失败，需要修改docker守护进程配置文件\n`vim /etc/docker/daemon.json`\n增加如下配置：\n```json\n{\n\"insecure-registries\":[\"registry.paas\"]\n}\n```\n重启docker\n`systemctl daemon-reload && systemctl restart docker`\n### 5. 重新push至registry.paas/library/xxx:tags仓库\n二、修改配置文件\n### 1. qwentest.yaml\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: qwentest\nspec:\n  selector:\n    matchLabels:\n      app: qwentest\n  template:\n    metadata:\n      labels:\n        app: qwentest\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        model: qwen-7b\n      containers:\n      - name: qwentest\n        image: registry.paas/library/qwen:v3.0\n        imagePullPolicy: IfNotPresent\n        resources:\n         limits:\n           nvidia.com/gpu: \"4\"\n         requests:\n           nvidia.com/gpu: \"4\"\n        command:                                  # training command, which can be modified\n              - \"/bin/bash\"\n              - \"-c\"\n                #- sleep 10000\n              - |\n                cd /mnt/ &&\n                cp setRank.sh /qwen/ &&\n                cd /qwen/ &&\n                chmod +x setRank.sh &&\n                bash setRank.sh &&\n                chmod +x finetune_lora_ds.sh &&\n                bash finetune_lora_ds.sh\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: processeddata\n          mountPath: /mnt\n        - name: dshm\n          mountPath: /dev/shm\n        - name: tmp-volume\n          mountPath: /tmp\n      volumes:\n      - name: processeddata\n        hostPath:\n          path: /mnt/users/wangxiangbo/runk8s\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1G\n      - name: tmp-volume\n        hostPath:\n          path: /tmp\n```\n### 2. hostfile\n```shell\n192.168.0.20\n192.168.0.58\n```\n### 3. setRank.sh\n```shell\n#!/bin/bash\n\nshell_name=\"finetune_lora_ds.sh\"\n\nshell_dir=\"/mnt/\"\n\nlocal_dir=\"/qwen/\"\n\n## 复制脚本到/qwen/下\ncp $shell_dir$shell_name $local_dir\n\n\n## 读取hostfile\nreadarray -t ips < <(grep -vE '^[[:space:]]*$' \"$shell_dir\"hostfile)\n\n## 获取rank0 IP\nrank0_ip=$(echo \"${ips[0]}\" | tr -d '[:space:]')\n\nnodes=${#ips[@]}\n\n## 获取hostfile中配置的IP前缀\n## 使用cut提取IP地址的前三个数字部分\nip_prefix=$(echo \"${ips[0]}\" | cut -d '.' -f 1-3)\n\n## 获取本机IP\nip=$(hostname -I | grep -oE \"$ip_prefix\\.[0-9]+\")\n\nls\n# 初始化rank\nnode_rank=-1\n\n# 遍历数组\nfor i in \"${!ips[@]}\"; do\n    # 使用tr命令去除空白字符\n    clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n    if [[ \"$clean_string\" == \"$ip\" ]]; then\n        node_rank=$i\n        break\n    fi\ndone\n\nif [ $node_rank -ne -1 ]; then\n    ## 修改脚本中MASTER_ADDR\n    sed -i \"s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/\" $local_dir$shell_name\n\n    ## 修改NNODES\n    sed -i \"s/^NNODES=.*/NNODES=$nodes/\" $local_dir$shell_name\n\n    ## 修改NODE_RANK\n    sed -i \"s/^NODE_RANK=.*/NODE_RANK=$node_rank/\" $local_dir$shell_name\nfi\n\n```\n\n### 4. sh\n```shell\n#!/bin/bash\n/bin/bash -i <<'EOF'\nexport NCCL_IB_DISABLE=1\nexport NCCL_SOCKET_IFNAME=eth0\nexport NCCL_P2P_DISABLE=1\nexport NCCL_DEBUG=INFO\nsource ~/.bashrc\n\n. /opt/miniconda/etc/profile.d/conda.sh\nconda activate qwen\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\nDIR=`pwd`\n\n# Number of GPUs per GPU worker\nGPUS_PER_NODE=4\n# Number of GPU workers, for single-worker training, please set to 1\nNNODES=2\n# The rank of this worker, should be in {0, ..., WORKER_CNT-1}, for single-worker training, please set to 0\nNODE_RANK=0\n# The ip address of the rank-0 worker, for single-worker training, please set to localhost\nMASTER_ADDR=192.168.0.20\n# The port for communication\nMASTER_PORT=6003\n\nMODEL=\"/qwen/Qwen-7B-Chat\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"/qwen/train_data_law.json\"\nDS_CONFIG_PATH=\"/qwen/finetune/ds_config_zero2.json\"\n\nfunction usage() {\n    echo '\nUsage: bash finetune_lora_ds.sh [-m MODEL_PATH] [-d DATA_PATH] [--deepspeed DS_CONFIG_PATH]\n'\n}\n\nwhile [[ \"$1\" != \"\" ]]; do\n    case $1 in\n        -m | --model )\n            shift\n            MODEL=$1\n            ;;\n        -d | --data )\n            shift\n            DATA=$1\n            ;;\n        --deepspeed )\n            shift\n            DS_CONFIG_PATH=$1\n            ;;\n        -h | --help )\n            usage\n            exit 0\n            ;;\n        * )\n            echo \"Unknown argument ${1}\"\n            exit 1\n            ;;\n    esac\n    shift\ndone\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\n\ntorchrun $DISTRIBUTED_ARGS finetune.py \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 False \\\n    --output_dir output_qwen \\\n    --num_train_epochs 5 \\\n    --per_device_train_batch_size 8 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 8 \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 100 \\\n    --save_total_limit 10 \\\n    --learning_rate 3e-4 \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length 512 \\\n    --lazy_preprocess True \\\n    --use_lora \\\n    --gradient_checkpointing \\\n    --ddp_find_unused_parameters False \\\nEOF\n\n```\n三、拉起训练（3号和4号两机8卡）\n### 1. 切换到1号机器master节点上，给带训练得3号和4号机器打上标签\n`kubectl label nodes ecs-jhjs-1234-003 model=qwen-7b`\n`kubectl label nodes ecs-jhjs-1234-004 model=qwen-7b`\n### 2. 准备好启动脚本等文件后，在master节点1号机器上，利用修改好的qwentest.yaml文件拉起训练任务\n### 3. `kubectl apply -f qwentest.yaml`\n### 4. 通过kubectl查看pod节点启动信息\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen13.png)\n### 5. 查看两个节点pod的logs日志\n`kubectl logs qwentest-czm8n -f`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen14.png)\n`kubectl logs qwentest-qthsf -f`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen15.png)\n\n","tags":["模型训练调优","Nvidia","Qwen"],"categories":["模型训练调优 -NVIDIA"]},{"title":"5-Qwen多机多卡调优","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/5-Qwen多机多卡调优/","content":"### 1. lora\n#### 1.1 per_device_train_batch_size测试，最优为16\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 2 | 1 | 512 | 8 | true | 8.37 |\n| 4 | 1 | 512 | 8 | true | 9.07 |\n| 8 | 1 | 512 | 8 | true | 9.88 |\n| **16** | **1** | **512** | **8** | **true** | **10.32** |\n| 32 | 1 | 512 | 8 | true | OOM |\n\n\n单个GPU批次大小增加时，需要的内存也会增加，GPU内存不足以支持更大的批次，可能会导致溢出或效率降低。\n\n#### 1.2 per_device_eval_batch_size测试，最优为2\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 16 | 1 | 512 | 8 | true | 10.32 |\n| **16** | **2** | **512** | **8** | **true** | **10.33** |\n| 16 | 4 | 512 | 8 | true | 10.26 |\n\n\n#### 1.3 model_max_length测试，最优为512\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16 | flops |\n| --- | --- | --- | --- | --- | --- |\n| 16 | 2 | 128 | 8 | true | 8.95 |\n| 16 | 2 | 256 | 8 | true | 9.56 |\n| **16** | **2** | **512** | **8** | **true** | **10.33** |\n| 16 | 2 | 1024 | 8 | true | OOM |\n\n\n#### 1.4 gradient_accumulation_steps测试，最优为16\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 16 | 2 | 512 | 1 | true | 9.07 |\n| 16 | 2 | 512 | 2 | true | 9.24 |\n| 16 | 2 | 512 | 4 | true | 9.89 |\n| 16 | 2 | 512 | 8 | true | 10.33 |\n| **16** | **2** | **512** | **16** | **true** | **11.63** |\n\n\n#### 1.5 开启Fp16测试\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| **16** | **2** | **512** | **16** | **true** | **67.13** |\n\n\n#### 1.6 关闭gradient_checkpointing\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | gradient_checkpointing | tflops |\n| --- | --- | --- | --- | --- | --- | --- |\n| **16** | **2** | **512** | **16** | **true** | **true** | **67.13** |\n| 16 | 2 | 512 | 16 | true | false | OOM |\n\n\n| 参数配置 | per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16/fp16 | gradient_checkpointing | tflops |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| 默认 | 2 | 1 | 512 | 8 | bf16 | **true** | 8.32 |\n| 调优后 | **<font style=\"color:#DF2A3F;\">16</font>** | **<font style=\"color:#DF2A3F;\">2</font>** | **<font style=\"color:#DF2A3F;\">512</font>** | **<font style=\"color:#DF2A3F;\">16</font>** | **<font style=\"color:#DF2A3F;\">fp16</font>** | **<font style=\"color:#DF2A3F;\">true</font>** | **<font style=\"color:#DF2A3F;\">67.13</font>** |\n\n\n基于2机8卡的V100，Qwen-7B模型的lora微调训练中（采用deepspeed的zero2的内存优化并行方式），Tflops的值最高为<font style=\"color:#DF2A3F;\">67.13</font>**（8.32）\n\n最佳参数配置（per_device_train_batch_size：16，per_device_eval_batch_size：2，model_max_length：512，gradient_accumulation_steps：16，Fp16精度，gradient_checkpointing：True）\n\n### 2. qlora\n\nqlora使用4比特量化模型以及paged attention等技术实现更小的显存开销\n\n#### 2.1 per_device_train_batch_size测试，最优为32\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 2 | 1 | 512 | 8 | true | 31.71 |\n| 4 | 1 | 512 | 8 | true | 42.91 |\n| 8 | 1 | 512 | 8 | true | 51.14 |\n| 16 | 1 | 512 | 8 | true | 55.75 |\n| **32** | **1** | **512** | **8** | **true** | **58.44** |\n| 64 | 1 | 512 | 8 | true | OOM |\n\n\n#### 2.2 per_device_eval_batch_size测试，最优为4\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 32 | 1 | 512 | 8 | true | 58.44 |\n| 32 | 2 | 512 | 8 | true | 58.84 |\n| **32** | **4** | **512** | **8** | **true** | **59.05** |\n| 32 | 8 | 512 | 8 | true | 58.42 |\n\n\n#### 2.3 model_max_length测试，最优为512\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 32 | 4 | 128 | 8 | true | 48.68 |\n| 32 | 4 | 256 | 8 | true | 53.02 |\n| **32** | **4** | **512** | **8** | **true** | **59.05** |\n| 32 | 4 | 1024 | 8 | true | OOM |\n\n\n#### 2.4 gradient_accumulation_steps测试，最优为\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 32 | 4 | 512 | 1 | true | 52.66 |\n| 32 | 4 | 512 | 2 | true | 52.18 |\n| 32 | 4 | 512 | 4 | true | 54.99 |\n| 32 | 4 | 512 | 8 | true | 59.05 |\n| 32 | 4 | 512 | 16 | true | 64.26 |\n| 32 | 4 | 512 | 32 | true | 80.37 |\n| **32** | **4** | **512** | **64** | **true** | **103.89** |\n\n\n#### 2.5 开启BF16测试\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16/fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 32 | 4 | 512 | 64 | bf16 | 14.23 |\n| **32** | **4** | **512** | **64** | **fp16** | **103.89** |\n\n\n#### 2.6 关闭gradient_checkpointing\n\n| gradient_checkpointing | per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- | --- |\n| **<font style=\"color:#000000;\">True</font>** | **<font style=\"color:#000000;\">32</font>** | **<font style=\"color:#000000;\">4</font>** | **<font style=\"color:#000000;\">512</font>** | **<font style=\"color:#000000;\">64</font>** | **<font style=\"color:#000000;\">true</font>** | **<font style=\"color:#000000;\">103.89</font>** |\n| False | 32 | 4 | 512 | 64 | true | OOM |\n\n\nDeepSpeed ZeRO 3 对节点间通信速率的要求远大于 ZeRO 2，在多机微调的情况下会大幅降低训练速度。因此，我们不建议在多机微调的情况下使用 DeepSpeed ZeRO 3 配置。\n\n| 参数配置 | per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- | --- |\n| 默认 | 2 | 1 | 512 | 8 | true | 31.71 |\n| 调优后 | **<font style=\"color:#DF2A3F;\">32</font>** | **<font style=\"color:#DF2A3F;\">4</font>** | **<font style=\"color:#DF2A3F;\">512</font>** | **<font style=\"color:#DF2A3F;\">64</font>** | **<font style=\"color:#DF2A3F;\">true</font>** | **<font style=\"color:#DF2A3F;\">103.89</font>** |\n\n\n基于2机8卡的V100，Qwen-7B模型的qlora微调训练中（采用deepspeed的zero2的内存优化并行方式），Tflops的值最高为<font style=\"color:#DF2A3F;\">103.89 </font>\n\n默认参数配置\n\n最佳参数配置（gradient_checkpointing：True，per_device_train_batch_size：32，per_device_eval_batch_size：4，model_max_length：512，gradient_accumulation_steps：64，Fp16精度）\n\nqlora在拉起训练时，需要对模型权重进行INT-4量化，会损失掉权重小数部分精度，量化后的权重由于使用更少的位数来表示，存储上比原始权重高效，在拉起模型训练过程过中tflops有明显提升。\n\n","tags":["模型训练调优","Nvidia","Qwen"],"categories":["模型训练调优 -NVIDIA"]},{"title":"3-基于docker的Qwen单机单卡_多卡训练","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/3-基于docker的Qwen单机单卡_多卡训练/","content":"### 1. 查看容器\n`docker ps`\n### 2. exec进入容器\n`docker exec -it containerid bash`\n### 3. 进入qwen目录并修改finetune_lora_single_gpu.sh参数\n```bash\n#!/bin/bash\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\nMODEL=\"/qwen/Qwen-7B-Chat\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"/qwen/train_data_law.json\"\n\nfunction usage() {\n    echo '\nUsage: bash finetune/finetune_lora_single_gpu.sh [-m MODEL_PATH] [-d DATA_PATH]\n'\n}\n\nwhile [[ \"$1\" != \"\" ]]; do\n    case $1 in\n        -m | --model )\n            shift\n            MODEL=$1\n            ;;\n        -d | --data )\n            shift\n            DATA=$1\n            ;;\n        -h | --help )\n            usage\n            exit 0\n            ;;\n        * )\n            echo \"Unknown argument ${1}\"\n            exit 1\n            ;;\n    esac\n    shift\ndone\n\nexport CUDA_VISIBLE_DEVICES=0\n\npython finetune.py \\\n  --model_name_or_path $MODEL \\\n  --data_path $DATA \\\n  --bf16 True \\\n  --output_dir output_qwen \\\n  --num_train_epochs 5 \\\n  --per_device_train_batch_size 1 \\\n  --per_device_eval_batch_size 1 \\\n  --gradient_accumulation_steps 8 \\\n  --evaluation_strategy \"no\" \\\n  --save_strategy \"steps\" \\\n  --save_steps 100 \\\n  --save_total_limit 10 \\\n  --learning_rate 3e-4 \\\n  --weight_decay 0.1 \\\n  --adam_beta2 0.95 \\\n  --warmup_ratio 0.01 \\\n  --lr_scheduler_type \"cosine\" \\\n  --logging_steps 1 \\\n  --report_to \"none\" \\\n  --model_max_length 512 \\\n  --lazy_preprocess True \\\n  --gradient_checkpointing \\\n  --use_lora\n\n# If you use fp16 instead of bf16, you should use deepspeed\n# --fp16 True --deepspeed finetune/ds_config_zero2.json\n```\n### 4. 执行finetune_lora_single_gpu.sh单机单卡\n### 5. 修改finetune_lora_ds.sh参数\n```bash\n#!/bin/bash\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nDIR=`pwd`\n\n# Guide:\n# This script supports distributed training on multi-gpu workers (as well as single-worker training).\n# Please set the options below according to the comments.\n# For multi-gpu workers training, these options should be manually set for each worker.\n# After setting the options, please run the script on each worker.\n\n# Number of GPUs per GPU worker\nGPUS_PER_NODE=$(python -c 'import torch; print(torch.cuda.device_count())')\n\n# Number of GPU workers, for single-worker training, please set to 1\nNNODES=${NNODES:-1}\n\n# The rank of this worker, should be in {0, ..., WORKER_CNT-1}, for single-worker training, please set to 0\nNODE_RANK=${NODE_RANK:-0}\n\n# The ip address of the rank-0 worker, for single-worker training, please set to localhost\nMASTER_ADDR=${MASTER_ADDR:-localhost}\n\n# The port for communication\nMASTER_PORT=${MASTER_PORT:-6001}\n\nMODEL=\"/qwen/Qwen-7B-Chat\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"/qwen/train_data_law.json\"\nDS_CONFIG_PATH=\"finetune/ds_config_zero2.json\"\n\nfunction usage() {\n    echo '\nUsage: bash finetune/finetune_lora_ds.sh [-m MODEL_PATH] [-d DATA_PATH] [--deepspeed DS_CONFIG_PATH]\n'\n}\n\nwhile [[ \"$1\" != \"\" ]]; do\n    case $1 in\n        -m | --model )\n            shift\n            MODEL=$1\n            ;;\n        -d | --data )\n            shift\n            DATA=$1\n            ;;\n        --deepspeed )\n            shift\n            DS_CONFIG_PATH=$1\n            ;;\n        -h | --help )\n            usage\n            exit 0\n            ;;\n        * )\n            echo \"Unknown argument ${1}\"\n            exit 1\n            ;;\n    esac\n    shift\ndone\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\n#export CUDA_VISIBLE_DEVICES=2,3\n\ntorchrun $DISTRIBUTED_ARGS finetune.py \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 False \\\n    --output_dir output_qwen \\\n    --num_train_epochs 5 \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 8 \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 100 \\\n    --save_total_limit 10 \\\n    --learning_rate 3e-4 \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length 512 \\\n    --lazy_preprocess True \\\n    --use_lora \\\n    --gradient_checkpointing \\\n    --deepspeed ${DS_CONFIG_PATH}\n\n```\n### 6. 修改deepspeed中ds_config_zero2.json配置文件，增加TFlops显示\n其中具体增加的参数配置为flops_profiler\n```json\n\"flops_profiler\": {\n        \"enabled\": true,\n        \"profile_step\": 1,\n        \"module_depth\": -1,\n        \"top_modules\": 1,\n        \"detailed\": false,\n        \"output_file\": null\n    },\n```\n```json\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \"bf16\": {\n        \"enabled\": \"auto\"\n    },\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"none\",\n            \"pin_memory\": true\n        },\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": true\n    },\n\n    \"flops_profiler\": {\n        \"enabled\": true,\n        \"profile_step\": 1,\n        \"module_depth\": -1,\n        \"top_modules\": 1,\n        \"detailed\": false,\n        \"output_file\": null\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 100,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}\n\n```\n### 7. 执行finetune_lora_ds.sh单机多卡训练\n其中报了Error while creating shared memory segment /dev/shm/nccl-KXWrmA (size 9637888)导致在docker中单机多卡拉起失败\n问题原因：docker的shm共享内存不足，可以通过命令\n`df -h | grep shm`查看当前容器的shm大小，默认为64M，这是远远不够的，所以要增加该容器的shm共享内存大小。参考博文[https://blog.csdn.net/gg864461719/article/details/112466585](https://blog.csdn.net/gg864461719/article/details/112466585)\n#### 解决方法1：创建完容器之后，手动修改shm共享内存大小\n##### a. 首先要关闭docker, 否则下面的操作步骤会无效.\n`service docker stop`\n##### b. 进入宿主机中/docker/containers/容器id 修改该容器的hostconfig.json文件，把其中的ShmSize的大小后面增加22（就变为了6.3G）其默认的是67108864_KB_ 就约等于64M。\n##### c. 重启docker服务\n`systemctl start docker`\n##### d. 解决完之后，重新查看shm的共享内存大小，此时已经变为了6.3G\n#### 解决方法2：在通过镜像run容器时，就直接指定--shm-size 6G\n`docker run -it --name qwen --gpus all --shm-size 6G registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0 bash`\n### 重新拉起训练\n`bash finetune/finetune_lora_ds.sh`\n其中tflops在9.5~10.5之间\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen9.png)\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen10.png)\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%B0%83%E4%BC%98/qwen11.png)\n\n","tags":["模型训练调优","Nvidia","Qwen"],"categories":["模型训练调优 -NVIDIA"]},{"title":"Git1_git命令","url":"/2025/11/04/Git/Git1_git 命令/","content":"**<font style=\"background-color:#FBDE28;\">git commmit -v和git push origin develop之后，我怎么撤销本次commit提交的代码，将origin develop的仓库返回值提交之前的状态？</font>**\n\n在Git中，如果你想要撤销最近的一次提交并将仓库状态回退到提交之前，你可以使用`git reset`命令。这里有几种方法可以撤销提交：\n\n1. **使用**`git reset --soft`：\n    - 这个命令会将HEAD指针移动到指定的提交，但是不会改变工作目录和索引的状态。这意味着你的更改仍然在暂存区（staged），你可以重新提交。\n    - 命令：`git reset --soft HEAD~1`（将HEAD指针回退到上一个提交）\n2. **使用**`git reset --mixed`（默认选项）：\n    - 这个命令会将HEAD指针移动到指定的提交，并且将索引（staged changes）重置到HEAD的状态，但是不会改变工作目录。\n    - 命令：`git reset --mixed HEAD~1`（将HEAD和索引回退到上一个提交，但工作目录不变）\n3. **使用**`git reset --hard`：\n    - 这个命令会将HEAD指针、索引和工作目录都重置到指定的提交状态。这意味着所有未提交的更改都会丢失。\n    - 命令：`git reset --hard HEAD~1`（将HEAD、索引和工作目录都回退到上一个提交）\n\n如果你已经将代码推送到了远程仓库（比如`origin develop`），并且想要撤销这次推送，你可以使用`git push`命令配合`--force`选项来强制推送本地的更改到远程仓库。但是请注意，这会覆盖远程仓库的历史，所以只有在你确定远程仓库没有其他人的提交或者你不介意丢失这些提交时才应该使用。\n\n命令如下：\n\n```bash\ngit push origin develop --force\n```\n\n或者，如果你使用的是Git 2.0或更高版本，可以使用更安全的`--force-with-lease`选项，它会在远程分支没有新的提交时才允许强制推送。\n\n命令如下：\n\n```bash\ngit push origin develop --force-with-lease\n```\n\n**警告**：使用`--force`选项会覆盖远程仓库的历史，这是一个破坏性的操作。在团队协作的环境中，这可能会导致其他协作者的工作丢失，所以请谨慎使用。\n\n","tags":["开发","git"],"categories":["开发"]},{"title":"Git2_gerrit仓库代码提交流程","url":"/2025/11/04/Git/Git2_gerrit仓库代码提交流程/","content":"1. 生成个人密钥\n```bash\nssh-keygen -C wangxiangbo_JTAGI@cmss.chinamobile.com\n```\n2. 修改config文件，增加gerrit仓库配置，增加刚生成的个人密钥`id_ed25519`\n```bash\nhost gerrit\nport 29418\nhostname gerrit.cmss.com\nuser wangxiangbo_JTAGI\nIdentityFile ~/.ssh/id_ed25519\n```\n3. 在gerrit中setting设置中，增加ssh的公钥，即`id_ed25519.pub`\n4. 设置邮箱，邮箱名`wangxiangbo@cmss.chinamobile.com`为不带JTAGI后缀的邮箱。\n5. 用ssh clone代码仓库\n```bash\ngit clone \"ssh://wangxiangbo_JTAGI@gerrit.cmss.com:29418/AGI/CM_OPTIMUS\" && scp -p -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg \"CM_OPTIMUS/.git/hooks/\"\n```\n6. 查看分支\n```bash\ngit branch -a\n```\n7. 查看远端分支\n```bash\ngit branch -r\n```\n8. 通过远端origin/develop仓库，创建一个本地develop开发分支\n```bash\ngit checkout -b develop origin/develop\n```\n9. 修改代码后，add之后并commit提交\n```bash\ngit commit -v\n```\n10. 推送远端origin/develop仓库\n```bash\ngit push origin HEAD:refs/for/develop\n```\n11. 在推送过程中会出现缺失 `Change-Id `的错误\n```bash\ngitdir=$(git rev-parse --git-dir); scp -p -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg ${gitdir}/hooks/\n```\n执行完毕后如果出现subsystem request failed on channel 0，则将-p修改为-O\n```bash\ngitdir=$(git rev-parse --git-dir); scp -O -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg ${gitdir}/hooks/\n```\n将本次commit提交的末尾加上Change-Id \n```bash\ngit commit --amend --no-edit\n```\n再次push\n```bash\ngit push origin HEAD:refs/for/develop\n```\n12. 提交完成之后，打开gerrit，找到develop分支的gitweb\n13. 选择review\n14. 找到刚刚提交的代码\n15. 点击add reviewer，评审人要最少要两个以上\n16. 第一次之后提交代码步骤\n```bash\ngit pull origin\ngit add xxx\ngit commit -v \ngit push origin HEAD:refs/for/develop\n```\n\n","tags":["开发","gerrit"],"categories":["开发"]},{"title":"Git3_gitlab仓库代码提交流程","url":"/2025/11/04/Git/Git3_gitlab仓库代码提交流程/","content":"1. gitlab上fork主仓库，生成个人的远端仓库origin/develop\n2. 修改git提交用户配置为九天账号\n```bash\ngit config --global --list\ngit config --global user.name wangxiangbo_JTAGI\ngit config --global user.email wangxiangbo_JTAGI@cmss.chinamobile.com\n```\n3. 拉取个人的远端仓库\n```bash\ngit clone http://gitlab.cmss.com/wangxiangbo/CM_OPTIMUS.git \n```\n4. 查看分支\n```bash\ngit branch -a\n```\n5. 查看远程仓库分支\n```bash\ngit branch -r\n```\n6. 通过个人的远端origin/develop仓库，创建一个本地develop开发分支\n```bash\ngit checkout -b develop origin/develop\n```\n7. <font style=\"color:rgba(0, 0, 0, 0.85);\">列出所有的远程仓库以及对应的 URL</font>\n```bash\ngit remote -v\n```\n8. <font style=\"color:rgba(0, 0, 0, 0.85);\">将远程仓库地址添加到本地Git仓库的远程仓库列表中，本地的upstream/develop仓库会和gitlab远程仓库关联起来</font>\n```bash\ngit remote add upstream http://gitlab.cmss.com/AGI/CM_OPTIMUS.git\n```\n8. upstream/develop远端仓库拉取最新的代码\n```bash\ngit fetch upstream\n```\n9. 将upstream/develop远端仓库的最新代码合并到本地的develop分支中\n```bash\ngit merge upstream/develop\n```\n10. 将本地develop最新的代码推到个人的远端仓库origin/develop中\n```bash\ngit push origin develop\n```\n11. <font style=\"background-color:#FBDE28;\">提交develop到origin/develop代码之前，保证自己的本地个人的远端仓库origin/develop和本地gitlab的远端仓库upstream/develop保持一致再提交</font>，防止后续origin/develop合并打upstream/develop上出现冲突。\n```bash\n#本地代码修改后拉取最新代码\n#1.拉取源代码\ngit fetch upstream \n#2.暂存本地修改的代码\ngit stash\n#3.合并源代码到当前的develop仓库\ngit merge upstream/develop\n#4.将本地修改的推送到远程的origin分支，使origin/develop和upstream/develop保持一致\ngit push origin develop\n#5.将暂存的代码弹出来\ngit stash pop\n```\n12. souretree上add需要提交的代码\n13. 提交add之后的代码\n```bash\ngit commit -v\n```\n```bash\n#修改下方的提交信息，第一行为提交描述信息\ndeepseek-7B预训练\n\nCode Source From: Self Code\nDescription: deepseek-7B预训练\nJira: #CMOPTIMUS-1089\n市场项目编号（名称）：CM_OPTIMUS\n```\n14. <font style=\"color:rgb(55, 53, 47);\">将提交完之后的代码合并到origin/develop上</font>\n```bash\ngit push origin develop\n```\n15. <font style=\"color:rgb(55, 53, 47);\">gitlab中找的到自己仓库下CM_OPTIMUS</font>\n16. <font style=\"color:rgb(55, 53, 47);\">找到Merge requests，然后点击开始一个合并</font>\n17. <font style=\"color:rgb(55, 53, 47);\">然后选择自己的分支为develop，还有旁边的分支develop，然后点击最下方的merge即可。该操作是将自己个人远端的orgin/develop仓库提交修改后的代码合并到gitlab远端的主仓库中（upstream/develop已和其关联，并通过git fetch保持最新）</font>\n选中自己刚刚提交到origin/develop上的代码，合并到AGI/CM_OPTIMUS的develop分支上\n在Write里添加合并人信息\n\n\n\n","tags":["开发","gitlab"],"categories":["开发"]},{"title":"运维1_docker命令","url":"/2025/11/04/服务器运维/运维1_docker命令/","content":"1. 启动一个容器\n```bash\nsudo docker run -it --name qwen --gpus all nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04\n```\n启动一个容器后，又通过exit关闭。发现docker ps没有了，该如何重新启动这个qwen容器？\n```bash\ndocker ps -a\n\ndocker start qwen\n\ndocker exec -it 容器id bash\n```\n\n2. 拉取一个新镜像后，通过这个镜像创建一个容器。\n```bash\ndocker run -it --name qwen --gpus all 镜像名:镜像tags bash\n```\n\n3. 将此时qwen容器打成镜像\n```bash\ndocker commit -a \"wangxiangbo\" -m \"qwen 7B\" 02649afd9710 qwen-7b:v1.0\n```\n\n\n\n\n","tags":["运维","docker"],"categories":["运维"]},{"title":"Linux2_Linux命令","url":"/2025/11/04/Linux/Linux2_linux命令/","content":"1. 当执行训练任务时，手动关闭训练，此时gup资源仍然占用，需要手动kill掉进程\n查询正在运行的进程并gerp finetune_lora_single_gpu.sh\n`ps aux | grep finetune_lora_single_gpu.sh`\n`ps aux | grep finetune_lora_ds.sh`\n`kill -9 372813` 根据上一步查出来的进程号，用kill -9强制删除\n`nvidia-smi `查看是否gpu显存已经清空\n2. 查看某个文件的前20行\n`head -n 20 train_data_law.json`\n3. 查看当前文件夹的大小\n`du -sh .`\n4. 查看当前文件夹挂载的是哪一个盘\n`df -h .`\n从大到小查看当前文件夹的下的文件大小\n`du -sh * | sort -rh`\n4. 查看当前文件下所有文件的大小\n`ls -lh`\n5. <font style=\"color:#000000;\">把一个名为Yuan2的文件夹的所有内容（包括该文件夹名），cp到/mnt/users/wangxiangbo/nemo/model目录下，使最后的目录为 /mnt/users/wangxiangbo/nemo/model/Yuan2</font>\n`cp -a Yuan2 /mnt/users/wangxiangbo/nemo/model/`\n6. 通过pid查看某个进程的详细信息\n`<font style=\"color:rgb(56, 58, 66);background-color:rgb(250, 250, 250);\">ps -fp 167891</font>`\n7. 查看当前目录下所有文件的磁盘占用情况\n`du -ah | sort -hr | head -n 20`\n8.  使用以下命令将`megatron-core`文件夹压缩成一个名为`megatron-core.zip`的zip文件\n \t` zip -r megatron-core.zip megatron-core  `\n9.  使用以下命令将`megatron-core.zip`解压缩  \n` unzip megatron-core.zip  `\n11. hg上面下载模型\n```bash\npip install -U huggingface_hub\nhuggingface-cli download bigscience/bloom-560m --local-dir bloom-560m\nhuggingface-cli download Qwen/Qwen2-7B-Instruct --local-dir Qwen2-7B-Instruct\nhuggingface-cli download Qwen/Qwen2.5-7B-Instruct --local-dir Qwen2.5-7B-Instruct\nhuggingface-cli download BAAI/IndustryCorpus_computer --repo-type dataset --local-dir IndustryCorpus_computer\nhuggingface-cli download BAAI/IndustryCorpus2_current_affairs_government_administration --repo-type dataset --local-dir government_administration\nhuggingface-cli download ShengbinYue/DISC-Law-SFT --repo-type dataset --local-dir DISC-Law-SFT\nhuggingface-cli download TigerResearch/sft_zh\n```\n12. 归档压缩文件\n```bash\n#-c：创建一个新的压缩文件。\n#-z：通过 gzip 压缩文件。\n#-v：显示详细的压缩过程。\n#-f：指定压缩后的文件名，这里是 colossalai.tar.gz\ntar -czvf colossalai.tar.gz colossalai/  \n\n#-x：表示解压。\n#-z：表示解压 .gz 格式的文件。\n#-v：显示解压过程。\n#-f：指定解压的文件\ntar -xzvf file.tar.gz\n```\n\n\n\n","tags":["运维","Linux"],"categories":["Linux"]},{"title":"运维2_k8s命令","url":"/2025/11/04/服务器运维/运维2_k8s命令/","content":"1. 获取特定命名空间的详细信息\n```bash\nkubectl get ns namespace-name -o wide\n```\n2. kubectl通过ns获取所有的pods的详细信息\n```bash\nkubectl get pods -n <ns_name> -o wide\n```\n3. 删除启动失败的pod，可以直接删除启动时的yaml，否则pod会自动重启\n```bash\nkubectl delete -f qwentest.yaml\n```\n4. 查看所有的node的lable标签信息\n```bash\nkubectl get node --show-labels\n\nkubectl get node xxx --show-labels\n\nkubectl get node --show-labels | grep model\n```\n5. 打标签\n```bash\nkubectl label nodes ecs-jhjs-1234-003 key=vaule\n```\n6. 删除某节点的lable标签\n```bash\nkubectl label nodes cce100-64-29-79.cce-stack.com model-\n```\n6. 创建一个命名空间namespace\n```bash\nkubectl create namespace xxx\n```\n7. 查看所有节点的标签\n```bash\nkubectl get nodes --show-labels\nkubectl get nodes --show-labels | grep model=llama2-70b\n```\n","tags":["运维","k8s"],"categories":["运维"]},{"title":"Linux1_kimi问题","url":"/2025/11/04/Linux/Linux1_kimi问题/","content":"1. 权重转换代码换行符\nKimi: 出现这些错误通常是因为脚本文件在不同的操作系统环境下被编辑或创建，导致文件的换行符不一致。Unix/Linux系统使用LF（Line Feed，`\\n`）作为换行符，而Windows系统使用CR+LF（Carriage Return + Line Feed，`\\r\\n`）作为换行符。\n错误信息中的 `$'\\r': command not found` 表示脚本尝试执行一个以 `\\r` 开头的命令，这通常是因为脚本文件中的换行符是 `\\r\\n` 而不是 `\\n`。\n此外，`syntax error near unexpected token 'elif'` 错误表明脚本解析器在遇到 `elif` 时遇到了语法错误，这可能是因为 `elif` 之前的行（可能是第59行）以 `\\r\\n` 结尾，导致 `elif` 行实际上是从一个新的行开始，而不是在预期的位置。\n要解决这个问题，你可以使用 `dos2unix` 工具将脚本文件从Windows格式转换为Unix/Linux格式。如果你没有 `dos2unix` 工具，可以使用 `sed` 命令来移除 `\\r` 字符：\n```bash\nsed -i 's/\\r$//' hf2mcore_convertor.sh\n```\n这条命令会查找文件中的所有行，并移除每行末尾的 `\\r` 字符。\n如果你有 `dos2unix` 工具，可以使用以下命令：\n```bash\ndos2unix hf2mcore_convertor.sh\n```\n这将把文件从Windows格式转换为Unix/Linux格式。\n在运行这些命令之后，再次执行脚本应该就不会出现这些错误了。\n2. 修改文件遇到Read-only file system\nKimi: 当你遇到 \"Read-only file system\" 错误时，通常意味着你的文件系统出于某种原因被挂载为只读模式。这可能是由于文件系统损坏、磁盘硬件问题、或者挂载时指定了错误的参数。以下是一些解决步骤：\n1. **检查文件系统挂载状态**：  \n使用 `mount` 命令查看所有挂载的文件系统及其状态。你可以使用如下命令：\n```bash\nmount | grep /path/to/mount/point\n```\n查看挂载点是否为只读（ro）。\n2. **重新挂载文件系统为读写模式**：  \n如果文件系统被挂载为只读，你可以尝试重新挂载它为读写模式。使用以下命令：\n```bash\nsudo mount -o remount,rw /path/to/mount/point\n```\n请将 `/path/to/mount/point` 替换为实际的挂载点路径。\n3. **检查和修复文件系统**：  \n如果重新挂载不起作用，可能是文件系统损坏。你可以使用 `fsck` 命令来检查和修复文件系统。例如，对于 ext4 文件系统，可以使用：\n```bash\nsudo fsck -f /dev/sda1\n```\n请确保替换 `/dev/sda1` 为你的实际分区。\n\n","tags":["运维","Linux"],"categories":["Linux"]},{"title":"测试 Hexo 标签分类2","url":"/2025/10/31/页面测试/test copy/","content":"这是文章正文。","tags":["测试2","hexo"],"categories":["测试"]}]