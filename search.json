[{"title":"vllm+mindie推理实践","url":"/2026/01/08/LLM/推理/vllm+mindie推理实践/","content":"\n## vllm\n\n### 镜像\n\n```bash\nregistry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:vllm\n或\nregistry.paas/library/ray-vllm-inference:v2.0\n```\n\n### 容器\n\n```bash\ndocker run -it \\\n--name vllm-wxb \\\n--gpus all \\\n--network host -p 8010:8010 \\\n--shm-size 11g \\\n-v /mnt/users/wangxiangbo/:/mnt \\\n-v /usr/local/cuda-12.2:/usr/local/cuda-12.2 \\\n-w /mnt \\\nregistry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:vllm\n```\n\n### DeepSeek-R1-Distill-Qwen-1.5B\n\n#### 开启vllm推理服务\n\n为了探究所需的最低GPU显存占用，gpu-memory-utilization从0.1开始测试。\n\n```bash\npython3 -m vllm.entrypoints.openai.api_server \\\n--host 0.0.0.0 --port 8010 \\\n--gpu-memory-utilization 0.1 \\\n--max-model-len=1024 \\\n--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B \\\n--trust-remote-code \\\n--dtype float16 \\\n--tensor-parallel-size 1\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image1.png)\n\n```bash\npython3 -m vllm.entrypoints.openai.api_server \\\n--host 0.0.0.0 --port 8010 \\\n--gpu-memory-utilization 0.2 \\\n--max-model-len=1024 \\\n--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B \\\n--trust-remote-code \\\n--dtype float16 \\\n--tensor-parallel-size 1\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image2.png)\n\n#### api测试\n\n```bash\ncurl \"http://127.0.0.1:8010/v1/chat/completions\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n\"model\": \"/mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B\",\n\"messages\": [\n    {\n        \"role\": \"user\",\n        \"content\": \"I like Beijing, because\"\n    }\n],\n\"max_tokens\":512,\n\"presence_penalty\": 1.2,\n\"frequency_penalty\": 1.0,\n\"temperature\": 0.7,\n\"top_p\": 0.9\n}'\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image3.png)\n\n#### evalscope压测\n\n##### evalscope安装\n\n```bash\n#安装eval-scope测试环境，使用源码安装\ngit clone https://github.com/modelscope/evalscope.git\npip install -e .\n```\n\n##### 启动压测服务\n\n:::tips\n参考eval-scope文档：[https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/parameters.html](https://evalscope.readthedocs.io/zh-cn/latest/user_guides/stress_test/parameters.html)\n\n:::\n\n```bash\nevalscope perf --url 'http://127.0.0.1:8010/v1/chat/completions' \\\n--parallel 10 \\\n--model '/mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1___5B' \\\n--log-every-n-query 10 \\\n--read-timeout=120 \\\n--max-tokens 512 \\\n--dataset-path '/mnt/script/open_qa.jsonl' \\\n-n 30 \\\n--max-prompt-length 12800 \\\n--api openai \\\n--stream \\\n--stop '<｜end▁of▁sentence｜>' \\\n--dataset openqa \\\n--debug\n```\n\n### DeepSeek-R1-Distill-Qwen-7B\n\n```bash\npython3 -m vllm.entrypoints.openai.api_server \\\n--host 0.0.0.0 --port 8010 \\\n--gpu-memory-utilization 0.2 \\\n--max-model-len=1024 \\\n--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/deepseek_qwen_hf \\\n--trust-remote-code \\\n--dtype float16 \\\n--tensor-parallel-size 1\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image4.png)\n\n```bash\npython3 -m vllm.entrypoints.openai.api_server \\\n--host 0.0.0.0 --port 8010 \\\n--gpu-memory-utilization 0.5 \\\n--max-model-len=1024 \\\n--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B/deepseek_qwen_hf \\\n--trust-remote-code \\\n--dtype float16 \\\n--tensor-parallel-size 1\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image5.png)\n\n### DeepSeek-R1-Distill-Llama-8B\n\n```bash\npython3 -m vllm.entrypoints.openai.api_server \\\n--host 0.0.0.0 --port 8010 \\\n--gpu-memory-utilization 0.5 \\\n--max-model-len=1024 \\\n--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Llama-8B \\\n--trust-remote-code \\\n--dtype float16 \\\n--tensor-parallel-size 1\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image6.png)\n\n```bash\npython3 -m vllm.entrypoints.openai.api_server \\\n--host 0.0.0.0 --port 8010 \\\n--gpu-memory-utilization 0.6 \\\n--max-model-len=1024 \\\n--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Llama-8B \\\n--trust-remote-code \\\n--dtype float16 \\\n--tensor-parallel-size 1\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image7.png)\n\n### DeepSeek-R1-Distill-Qwen-14B\n\n```bash\npython3 -m vllm.entrypoints.openai.api_server \\\n--host 0.0.0.0 --port 8010 \\\n--gpu-memory-utilization 0.4 \\\n--max-model-len=1024 \\\n--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B \\\n--trust-remote-code \\\n--dtype float16 \\\n--tensor-parallel-size 2\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image8.png)\n\n```bash\npython3 -m vllm.entrypoints.openai.api_server \\\n--host 0.0.0.0 --port 8010 \\\n--gpu-memory-utilization 0.5 \\\n--max-model-len=1024 \\\n--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B \\\n--trust-remote-code \\\n--dtype float16 \\\n--tensor-parallel-size 2\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image9.png)\n\n### DeepSeek-R1-Distill-Qwen-32B\n\n```bash\npython3 -m vllm.entrypoints.openai.api_server \\\n--host 0.0.0.0 --port 8010 \\\n--gpu-memory-utilization 0.5 \\\n--max-model-len=1024 \\\n--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \\\n--trust-remote-code \\\n--dtype float16 \\\n--tensor-parallel-size 4\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image10.png)\n\n```bash\npython3 -m vllm.entrypoints.openai.api_server \\\n--host 0.0.0.0 --port 8010 \\\n--gpu-memory-utilization 0.6 \\\n--max-model-len=1024 \\\n--model /mnt/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \\\n--trust-remote-code \\\n--dtype float16 \\\n--tensor-parallel-size 4\n```\n\n\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image11.png)\n\n### 压测结果\n\n| **<font style=\"color:rgb(23, 43, 77);background-color:rgb(244, 245, 247);\">   </font>**<br/>**<font style=\"color:rgb(23, 43, 77);background-color:rgb(244, 245, 247);\">   </font>**<br/>**<font style=\"color:rgb(23, 43, 77);background-color:rgb(244, 245, 247);\">模型名称</font>** | **<font style=\"color:rgb(23, 43, 77);background-color:rgb(244, 245, 247);\">   </font>**<br/>**<font style=\"color:rgb(23, 43, 77);background-color:rgb(244, 245, 247);\">输入参数</font>** |                                                              | **<font style=\"color:rgb(23, 43, 77);background-color:rgb(244, 245, 247);\">   </font>**<br/>**<font style=\"color:rgb(23, 43, 77);background-color:rgb(244, 245, 247);\">输出参数</font>** | **<font style=\"color:rgb(23, 43, 77);background-color:rgb(244, 245, 247);\">   </font>**<br/>**<font style=\"color:rgb(23, 43, 77);background-color:rgb(244, 245, 247);\">资源</font>**<br/>**<font style=\"color:rgb(23, 43, 77);background-color:rgb(244, 245, 247);\">   </font>** |                                                              | **<font style=\"color:rgb(23, 43, 77);\">      </font>****<font style=\"color:rgb(23, 43, 77);\">性能      </font>** |\n| ------------------------------------------------------------ | :----------------------------------------------------------: | ------------------------------------------------------------ | :----------------------------------------------------------: | :----------------------------------------------------------: | ------------------------------------------------------------ | ------------------------------------------------------------ |\n|                                                              |  **<font style=\"color:rgb(23, 43, 77);\">batchsize</font>**   | **<font style=\"color:rgb(23, 43, 77);\">max-model-len</font>**<font style=\"color:rgb(23, 43, 77);\">   </font><br/> |  **<font style=\"color:rgb(23, 43, 77);\">max-tokens</font>**  | **<font style=\"color:rgb(23, 43, 77);\">GPU卡数（V100）</font>** | **<font style=\"color:rgb(6, 6, 7);\">实际显存占用</font>**    | **<font style=\"color:rgb(6, 6, 7);\">Concurrency / Total</font>** |\n| <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">DeepSeek-R1-Distill-Qwen-1.5B</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">自动调整</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">1024</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">512</font><br/><font style=\"color:rgb(23, 43, 77);\">   </font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">1卡</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(38, 38, 38);\">5.7GB</font><font style=\"color:rgb(6, 6, 7);\">(FP16 精度)</font><br/><font style=\"color:rgb(6, 6, 7);\">(gpu-memory-utilization 0.2)</font> | <font style=\"color:rgb(6, 6, 7);\">10/30</font>               |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">50/150</font>              |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">100/300</font>             |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">200/600</font>             |\n| <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">DeepSeek-R1-Distill-Qwen-7B</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">自动调整</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">1024</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">512</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">1卡</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(6, 6, 7);\">15.95G(FP16 精度)</font><br/><font style=\"color:rgb(6, 6, 7);\">(gpu-memory-utilization 0.5)</font><br/><font style=\"color:rgb(23, 43, 77);\">   </font> | <font style=\"color:rgb(6, 6, 7);\">10/30</font>               |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">50/150</font>              |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">100/300</font>             |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">200/600</font>             |\n| <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">DeepSeek-R1-Distill-llama-8B</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">自动调整</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">1024</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">512</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">1卡</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(6, 6, 7);\">18.55G(FP16 精度)</font><br/><font style=\"color:rgb(6, 6, 7);\">(gpu-memory-utilization 0.6)</font><br/><font style=\"color:rgb(23, 43, 77);\">   </font> | <font style=\"color:rgb(6, 6, 7);\">10/30</font>               |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">50/150</font>              |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">100/300</font>             |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">200/600</font>             |\n| <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">DeepSeek-R1-Distill-Qwen-14B</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">自动调整</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">1024</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">512</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">2卡</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(6, 6, 7);\">16.44G*2(FP16 精度)</font><br/><font style=\"color:rgb(6, 6, 7);\">(gpu-memory-utilization 0.5)</font> | <font style=\"color:rgb(0, 0, 0);\">10/30</font>               |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">50/150</font>              |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">100/300</font>             |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">200/600</font>             |\n| <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">DeepSeek-R1-Distill-Qwen-32B</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">自动调整</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">1024</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">512</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">4卡</font> | <font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(23, 43, 77);\">   </font><br/><font style=\"color:rgb(6, 6, 7);\">18.98G*4(FP16 精度)</font><br/><font style=\"color:rgb(6, 6, 7);\">(gpu-memory-utilization 0.6)</font><br/><font style=\"color:rgb(23, 43, 77);\">   </font> | <font style=\"color:rgb(6, 6, 7);\">10/30</font>               |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">50/150</font>              |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">100/300</font>             |\n|                                                              |                                                              |                                                              |                                                              |                                                              |                                                              | <font style=\"color:rgb(6, 6, 7);\">200/600</font>             |\n\n\n\n\n## mindie\n\n### 镜像\n\n```bash\naiops-8af5363b.ecis.huhehaote-1.cmecloud.cn/public/mindie-develop-inference-model:1.0.rc3-pytorch2.1.0-py3.10-npu-cann8.0.rc3-ubuntu22.04\n```\n\n### 容器\n\n```bash\nsudo docker run --name mindie-wxb -dit --net=host --shm-size=100g \\\n--privileged=true \\\n-w /workspace \\\n--device=/dev/davinci_manager \\\n--device=/dev/hisi_hdc \\\n--device=/dev/devmm_svm \\\n--entrypoint=bash \\\n-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n-v /usr/local/dcmi:/usr/local/dcmi \\\n-v /usr/local/bin/npu-smi:/usr/local/bin/npu-smi \\\n-v /usr/local/sbin:/usr/local/sbin \\\n-v /as-sharedata-1/wangxiangbo/mindie:/workspace \\\n-v /mnt/weight-1:/job \\\n-v /usr/share/zoneinfo/Asia/Shanghai:/etc/localtime \\\n-e http_proxy=$http_proxy \\\n-e https_proxy=$https_proxy \\\naiops-8af5363b.ecis.huhehaote-1.cmecloud.cn/public/mindie-develop-inference-model:1.0.rc3-pytorch2.1.0-py3.10-npu-cann8.0.rc3-ubuntu22.04\n```\n\n### 激活环境\n\n```bash\nsource /usr/local/Ascend/nnal/atb/set_env.sh\nsource /usr/local/Ascend/mindie/set_env.sh\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\nsource /usr/local/Ascend/llm_model/set_env.sh\n```\n\n### config推理配置修改\n\n```bash\nvim /usr/local/Ascend/mindie/latest/mindie-service/conf/config.json\n```\n\n```json\n{\n  \"Version\": \"1.0.0\",\n  \"LogConfig\": {\n    \"logLevel\": \"Info\",\n    \"logFileSize\": 20,\n    \"logFileNum\": 20,\n    \"logPath\": \"logs/mindservice.log\"\n  },\n  \"ServerConfig\": {\n    \"ipAddress\": \"10.208.200.67\",\n    \"managementIpAddress\": \"127.0.0.2\",\n    \"port\": 16384,\n    \"managementPort\": 1026,\n    \"metricsPort\": 1027,\n    \"allowAllZeroIpListening\": false,\n    \"maxLinkNum\": 1000,\n    \"httpsEnabled\": false,\n    \"fullTextEnabled\": false,\n    \"tlsCaPath\": \"security/ca/\",\n    \"tlsCaFile\": [\n      \"ca.pem\"\n    ],\n    \"tlsCert\": \"security/certs/server.pem\",\n    \"tlsPk\": \"security/keys/server.key.pem\",\n    \"tlsPkPwd\": \"security/pass/key_pwd.txt\",\n    \"tlsCrl\": \"security/certs/server_crl.pem\",\n    \"managementTlsCaFile\": [\n      \"management_ca.pem\"\n    ],\n    \"managementTlsCert\": \"security/certs/management/server.pem\",\n    \"managementTlsPk\": \"security/keys/management/server.key.pem\",\n    \"managementTlsPkPwd\": \"security/pass/management/key_pwd.txt\",\n    \"managementTlsCrl\": \"security/certs/management/server_crl.pem\",\n    \"kmcKsfMaster\": \"tools/pmt/master/ksfa\",\n    \"kmcKsfStandby\": \"tools/pmt/standby/ksfb\",\n    \"inferMode\": \"standard\",\n    \"interCommTLSEnabled\": false,\n    \"interCommPort\": 1121,\n    \"interCommTlsCaFile\": \"security/grpc/ca/ca.pem\",\n    \"interCommTlsCert\": \"security/grpc/certs/server.pem\",\n    \"interCommPk\": \"security/grpc/keys/server.key.pem\",\n    \"interCommPkPwd\": \"security/grpc/pass/key_pwd.txt\",\n    \"interCommTlsCrl\": \"security/certs/server_crl.pem\",\n    \"openAiSupport\": \"vllm\"\n  },\n  \"BackendConfig\": {\n    \"backendName\": \"mindieservice_llm_engine\",\n    \"modelInstanceNumber\": 1,\n    \"npuDeviceIds\": [\n      [\n        0\n      ]\n    ],\n    \"tokenizerProcessNumber\": 8,\n    \"multiNodesInferEnabled\": false,\n    \"multiNodesInferPort\": 1120,\n    \"interNodeTLSEnabled\": true,\n    \"interNodeTlsCaFile\": \"security/grpc/ca/ca.pem\",\n    \"interNodeTlsCert\": \"security/grpc/certs/server.pem\",\n    \"interNodeTlsPk\": \"security/grpc/keys/server.key.pem\",\n    \"interNodeTlsPkPwd\": \"security/grpc/pass/mindie_server_key_pwd.txt\",\n    \"interNodeTlsCrl\": \"security/grpc/certs/server_crl.pem\",\n    \"interNodeKmcKsfMaster\": \"tools/pmt/master/ksfa\",\n    \"interNodeKmcKsfStandby\": \"tools/pmt/standby/ksfb\",\n    \"ModelDeployConfig\": {\n      \"maxSeqLen\": 2048,\n      \"maxInputTokenLen\": 2048,\n      \"truncation\": false,\n      \"ModelConfig\": [\n        {\n          \"modelInstanceType\": \"Standard\",\n          \"modelName\": \"deepseek-r1-distill-qwen-7b\",\n          \"modelWeightPath\": \"/job/huggingface/DeepSeek-R1-Distill-qwen-7B\",\n          \"worldSize\": 1,\n          \"cpuMemSize\": 5,\n          \"npuMemSize\": 1,\n          \"backendType\": \"atb\"\n        }\n      ]\n    },\n    \"ScheduleConfig\": {\n      \"templateType\": \"Standard\",\n      \"templateName\": \"Standard_LLM\",\n      \"cacheBlockSize\": 128,\n      \"maxPrefillBatchSize\": 50,\n      \"maxPrefillTokens\": 48000,\n      \"prefillTimeMsPerReq\": 150,\n      \"prefillPolicyType\": 0,\n      \"decodeTimeMsPerReq\": 50,\n      \"decodePolicyType\": 0,\n      \"maxBatchSize\": 200,\n      \"maxIterTimes\": 512,\n      \"maxPreemptCount\": 0,\n      \"supportSelectBatch\": false,\n      \"maxQueueDelayMicroseconds\": 5000\n    }\n  }\n}\n\n```\n\n\n\n### 启动推理服务\n\n#### 增加权限\n\n```bash\nCONFIG_FILE=/usr/local/Ascend/mindie/latest/mindie-service/conf/config.json\nmodelWeightPath=/job/huggingface/DeepSeek-R1-Distill-qwen-7B\nchmod -R 640 $modelWeightPath\nchmod 640 $modelWeightPath\nchmod 640 $CONFIG_FILE\n```\n\n#### 启动推理\n\n```bash\ncd /usr/local/Ascend/mindie/latest/mindie-service\n\nnohup ./bin/mindieservice_daemon > output.log 2>&1 &\n\ntail -f output.log\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image12.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image13.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image14.png)\n\n\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image15.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image16.png)\n\n### evalscope压测\n\n#### evalscope安装\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image17.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/vllm/image18.png)\n\n#### 启动压测服务\n\n\n\n\n\n\n\n\n\n","tags":["模型推理","NVIDIA","昇腾","V100","910B","vllm+mindie"],"categories":["模型推理","vllm+mindie"]},{"title":"ColossalAI训练框架解读","url":"/2026/01/08/LLM/训练框架/ColossalAI/","content":"## 框架特点\n\nColossal-AI 框架中的 **Gemini** 是一种内存优化策略，旨在通过智能的内存管理和计算优化来减少显存占用，从而支持更大规模的模型训练。Gemini 的核心思想是通过 **动态内存管理** 和 **计算优化** 来最大化显存利用率，同时减少内存碎片和冗余计算。\n\n## 以llama2为例的验证\n\n### 增加每部迭代打印以及tflops打印\n\nColossalAI初始打印效果\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image1.png)\n\n## <!-- 这是一张图片，ocr 内容为： -->\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image2.png)<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image3.png)<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image4.png)\n\n### 脚本\n\n```bash\n#!/bin/bash\n# NCCL IB environment variables\nexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1\nexport NCCL_IB_DISABLE=0\nexport NCCL_SOCKET_IFNAME=eth0\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_IB_TIMEOUT=23\nexport NCCL_IB_RETRY_CNT=7\nexport OMP_NUM_THREADS=4\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nLOG_SAVE=\"/mnt/colossalai/output/log/${current_time}-llama2-pretrain.log\"\nPROJECT_NAME=\"llama2-7B-pretrain\"\nPARENT_SAVE_DIR=\"/mnt/colossalai/output/save/\"\nPARENT_TENSORBOARD_DIR=\"/mnt/colossalai/output/tensorboard/\"\nPARENT_CONFIG_FILE=\"/mnt/colossalai/output/config/\"\nPRETRAINED_MODEL_PATH=\"/mnt/colossalai/weight/Llama-2-7b-hf\"\ndeclare -a dataset=(\n \"/mnt/colossalai/dataset/llama2_pretraindata/arrow/part-00000\"\n)\n\nTIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)\nFULL_PROJECT_NAME=\"${PROJECT_NAME}-${TIMESTAMP}\"\nSAVE_DIR=\"${PARENT_SAVE_DIR}${FULL_PROJECT_NAME}\"\nTENSORBOARD_DIR=\"${PARENT_TENSORBOARD_DIR}${FULL_PROJECT_NAME}\"\nCONFIG_FILE=\"${PARENT_CONFIG_FILE}${FULL_PROJECT_NAME}.json\"\ncolossalai run --nproc_per_node 4 --master_port 30012 train.py \\\n    --pretrained $PRETRAINED_MODEL_PATH \\\n    --dataset ${dataset[@]} \\\n    --plugin \"gemini\" \\\n    --microbatch_size 1 \\\n    --batch_size 4 \\\n    --save_interval 400 \\\n    --save_dir $SAVE_DIR \\\n    --tensorboard_dir $TENSORBOARD_DIR \\\n    --config_file $CONFIG_FILE \\\n    --num_epochs 1 \\\n    --accumulation_steps 1 \\\n    --lr 1e-4 \\\n    --mixed_precision \"fp16\" \\\n    --grad_clip 1.0 \\\n    --weight_decay 0.01 \\\n    --warmup_steps 100 \\\n    --use_grad_checkpoint \\\n    --pad_token \"unk\" \\\n    --max_length 1024 2>&1 | tee -a \"$LOG_SAVE\"\n```\n\n### 超参设置/GPU利用率/TFlops\n\n#### mixed_precision：fp16/plugin：gemini /mbs：1/gbs：4\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image5.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image6.png)\n\n#### mixed_precision：fp16/plugin：gemini_auto /mbs：1/gbs：4\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image7.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image8.png)\n\n#### mixed_precision：bf16/plugin：gemini_auto /mbs：1/gbs：4\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image9.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image10.png)\n\n\n\n#### mixed_precision：fp16/plugin：zero2  /mbs：1/gbs：4\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image11.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image12.png)\n\n## Gemini并行策略分析\n\n###  传统并行策略 vs Gemini  \n\n| 并行策略                          | 主要特点                                                     | 主要问题                 |\n| --------------------------------- | ------------------------------------------------------------ | ------------------------ |\n| 数据并行（DP）                    | 每个 GPU 计算整个模型的一部分数据，最终聚合梯度              | 计算冗余，显存消耗大     |\n| 张量并行（TP）                    | 不同 GPU 处理矩阵乘法的不同部分，减少单卡负担                | 通信开销大，负载不均衡   |\n| 流水线并行（PP）                  | 将模型层划分到多个 GPU，流水线方式进行计算                   | 依赖严格的同步，调度复杂 |\n| ZeRO（Zero Redundancy Optimizer） | ZeRO-1: 仅切分优化器状态   ZeRO-2: 切分优化器+梯度   ZeRO-3: 进一步切分模型参数 | 计算和通信开销较高       |\n| **Gemini**                        | **混合 ZeRO-2/3 + 计算和数据自动调度**                       | 适用于更大规模模型       |\n\n\n### Gemini的关键优化点\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image13.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image14.png)\n\n####  优化了参数切分策略\n\n在 `GeminiPlugin` 里，`shard_param_frac` 和 `offload_optim_frac` 控制了参数切分和优化器状态的存储：\n\n```python\nshard_param_frac: float = 1.0  # 1.0 相当于 ZeRO-3, 0.0 相当于 ZeRO-2\noffload_optim_frac: float = 0.0  # 控制优化器状态是否下放到 CPU\n```\n\n+ **如果 **`**shard_param_frac=1.0**`，那么所有参数都会被 shard（类似 ZeRO-3）。\n+ **如果 **`**shard_param_frac=0.0**`，则不会对参数进行 shard（类似 ZeRO-2）。\n+ **如果 **`**offload_optim_frac=1.0**`，优化器状态可以下放到 CPU，减少 GPU 占用。\n\n#### 计算图的智能分配\n\n**Gemini 结合 TP（张量并行）+ DP（数据并行）+ ZeRO 进行计算优化**：\n\n```python\nself.pg_mesh = ProcessGroupMesh(self.zero_size, self.extra_dp_size, self.tp_size)\n```\n\n其中：\n\n+ `self.zero_group` 负责 **ZeRO 参数管理**（类似 ZeRO-2/3）。\n+ `self.extra_dp_group` 控制 **额外的数据并行组**。\n+ `self.tp_group` 负责 **张量并行**（TP）。\n\n这样，Gemini 允许 **动态切换并行方式**，避免静态分配导致的资源浪费。\n\n####  自适应内存管理  \n\n##### GPU-CPU 混合存储\n\nGemini 提供了 `placement_policy` 参数：\n\n```python\nplacement_policy: str = \"static\"  # 也可设为 \"auto\"\n```\n\n+ `\"static\"` 模式：手动设定 `shard_param_frac` 和 `offload_optim_frac` 来控制 ZeRO 级别。\n+ `\"auto\"` 模式：Gemini **自动检测显存使用情况**，智能分配参数到 CPU/GPU，防止 OOM。\n\n##### GPU 显存利用率优化\n\n```python\nwarmup_non_model_data_ratio: float = 0.8  # 训练前预留显存\nsteady_cuda_cap_ratio: float = 0.9  # 训练稳定后，模型最大使用显存比例\n```\n\n+ 训练初期，预留 80% 显存给非模型数据（如激活值缓存）。\n+ 训练稳定后，Gemini 允许模型占用 90% 显存，提高利用率。\n\n####  混合精度训练  \n\nGemini 通过 `precision` 参数支持 `fp16/bf16` 训练：\n\n```python\nprecision: str = \"fp16\"\n```\n\n+ `**fp16**`：减少显存占用，提升计算速度。\n+ `**bf16**`：训练稳定性更好，适用于 NVIDIA A100/H100。\n\n此外，它还支持 `master_weights`（优化器保留 FP32 权重）：\n\n```python\nmaster_weights: bool = True\n```\n\n+ 这样可以在 `fp16` 计算时避免精度下降。\n\n####  梯度同步优化  \n\nGemini 允许 **异步梯度同步**，减少通信瓶颈：\n\n```python\nenable_async_reduce: bool = True\n```\n\n当 `enable_async_reduce=True` 时：\n\n1. 反向传播时，梯度不会立即同步，而是 **累积多个 step 之后再同步**。\n2. 这样可以 **减少通信次数，提高 GPU 计算效率**。\n\n### ZeRO-3 vs Gemini\n\n| 特性               | ZeRO-3                | Gemini                       |\n| ------------------ | --------------------- | ---------------------------- |\n| **参数切分**       | 仅参数                | 参数 + 计算状态（更全面）    |\n| **优化器状态存储** | GPU / CPU（手动配置） | 动态调度（自动调整 GPU/CPU） |\n| **梯度同步**       | 每步梯度同步          | 支持异步梯度同步             |\n| **显存利用率优化** | 需要手动调整 `stage`  | 自动分配显存                 |\n| **计算图优化**     | 仅限 DP/TP/PP         | 自动适配 DP/TP/PP            |\n| **适用场景**       | 静态任务              | 适用于动态任务               |\n\n\n#### 代码示例\n\n**（1）传统 ZeRO-3**\n\n```python\nplugin = LowLevelZeroPlugin(stage=3, precision=\"fp16\")\n```\n\n**(2) Gemini 自动优化**\n\n```python\nplugin = GeminiPlugin(precision=\"fp16\", placement_policy=\"auto\", enable_async_reduce=True)\n```\n\n+ `placement_policy=\"auto\"`**自动调整参数在 CPU/GPU 之间的存放**。\n+ `enable_async_reduce=True`**开启异步梯度同步，减少通信开销**。\n\n### Gemini vs Gemini_auto\n\n#### 主要区别\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/colossalai/image15.png)\n\n`**gemini**`\n\n+ `placement_policy=\"static\"`：在这个模式下，Gemini 使用固定的参数切分策略，用户需要手动设置 `shard_param_frac`、`offload_optim_frac` 等参数来控制模型参数和优化器状态的存放位置（比如是否存放在 CPU 或 GPU 上）。\n+ 在这种模式下，**所有的参数切分、存储和计算资源的调度**都由用户指定。虽然能获得较高的精细控制，但也需要用户进行更多的调优工作。\n\n`**gemini_auto**`:\n\n+ `placement_policy=\"auto\"`：在这种模式下，Gemini 会自动根据硬件资源（如 GPU 显存、CPU 显存等）的情况动态调整**参数切分和存放**，而不需要用户手动指定。例如，当 GPU 内存不足时，它会自动将部分参数转移到 CPU。\n+ 自动的内存管理策略可以减少用户的配置工作，同时提高训练过程中的显存利用率，避免 OOM（内存溢出）错误。\n\n####  内存和显存分配策略  \n\n##### 在 `gemini` 模式下，用户需要手动设置内存和计算资源的分配。例如：\n\n```python\nshard_param_frac: float = 1.0  # 控制参数是否进行切分\noffload_optim_frac: float = 0.0  # 控制优化器状态是否转移到 CPU\n```\n\n+ `shard_param_frac=1.0` 会将所有参数切分到不同的 GPU 上，类似于 ZeRO-3。\n+ `offload_optim_frac=0.0` 则意味着优化器的状态仍然保留在 GPU 上。\n\n需要手动调节显存利用率（例如，通过设置 `shard_param_frac` 和 `offload_optim_frac` 来控制参数和优化器的存放位置）。适合那些 **对内存使用有较高控制需求** 的场景，但需要更多的人工调试和优化。\n\n##### 在 `gemini_auto` 模式下，Gemini 会自动优化内存的使用。例如：\n\n```python\nplacement_policy=\"auto\"\n```\n\n+ 自动根据硬件状态调整内存和参数的存放位置。\n+ 适合在 **没有足够硬件信息或不希望进行手动调优** 的情况下使用。特别是在多 GPU 或多个机器的训练场景中，自动的内存管理可以有效避免 **OOM** 和提升显存利用率。\n\n这种模式会根据当前 GPU 的负载、显存的剩余情况等因素，动态决定是否将部分模型参数或优化器状态从 GPU 转移到 CPU 上，减少 GPU 内存的占用，避免显存溢出。这种模式通常会在训练开始时进行预热，即根据模型和数据的特点自动调整参数的分配方式，以保证训练过程中内存的高效利用。\n\n##### 适用场景\n\n+ `**gemini**`：适合 **精确控制 **内存使用和训练过程的场景，尤其是当用户对硬件资源有深入了解并能进行手动调优时。适用于超大规模训练，特别是需要显式控制模型切分和存储位置的情况。\n+ `**gemini_auto**`：适合那些 **希望减少人工调优 **的用户，特别是在训练过程中动态调整资源和内存时。它使得训练过程更加灵活，特别是在面对 **GPU 内存瓶颈** 和 **多卡训练** 时，自动调整内存和计算资源分配能够提高训练效率。\n\n### 总结\n\n**Gemini 的关键改进点**：\n\n1. **参数切分更加灵活**：支持 ZeRO-2/3 之间的动态调整。\n2. **计算图智能优化**：结合 TP + DP + ZeRO 进行负载均衡。\n3. **显存管理更高效**：自动调整参数在 CPU/GPU 之间的存放，防止 OOM。\n4. **混合精度支持**：FP16/BF16 计算，提高计算效率。\n5. **异步梯度同步**：减少通信开销，提高 GPU 利用率。\n\n\n\n","tags":["NVIDIA","V100","模型训练调优","ColossalAI"],"categories":["模型训练框架","ColossalAI"]},{"title":"xinference推理实践","url":"/2026/01/08/LLM/推理/xinference推理实践/","content":"\n## NV生态\n\n### 容器创建\n\n```bash\ndocker run -d --restart=always --name=xinference_wxb \\\n -v /mnt/users/wangxiangbo/xinference:/opt/xinference -e XINFERENCE_HOME=/opt/xinference -e XINFERENCE_MODEL_SRC=modelscope \\\n -p 9998:9997 --gpus all registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:latest xinference-local -H 0.0.0.0\n```\n\n```bash\ndocker run -d --restart=always --name=xinference_wxb \\\n -v /mnt/users/wangxiangbo/xinference:/opt/xinference -e XINFERENCE_HOME=/opt/xinference -e XINFERENCE_MODEL_SRC=hf-mirror.com \\\n -p 9998:9997 --gpus all registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:latest xinference-local -H 0.0.0.0\n```\n\n```bash\ndocker run -d --restart=always --name=xinference_wxb \\\n -v /mnt/users/wangxiangbo/xinference:/opt/xinference -e XINFERENCE_HOME=/opt/xinference -p 9998:9997 --gpus all registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:latest xinference-local -H 0.0.0.0\n```\n\n\n\n进入容器\n\n```bash\ndocker exec -it xinference_wxb bash\n```\n\n### 服务创建\n\n开启xinference-local服务\n\n```bash\nroot@979723eca011:/opt/inference/examples# xinference-local\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/xinference/image1.png)\n\n### web端调用\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/xinference/image2.png)\n\n选择待推理的LLM模型\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/xinference/image3.png)\n\n控制台开始下载对应的Qwen1.5-Chat-1.8B模型\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/xinference/image4.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/xinference/image5.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/xinference/image6.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/xinference/image7.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/xinference/image8.png)\n\n### 终端API调用\n\n```python\nfrom gradio_client import Client\n\nclient = Client(\"http://127.0.0.1:54696/qwen1.5-chat/\")\nresult = client.predict(\n\t\tmessage=\"Hello!!\",\n\t\trequest=512,\n\t\tparam_3=1,\n\t\tparam_4=\"Hello!!\",\n\t\tapi_name=\"/chat\"\n)\nprint(result)\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/xinference/image9.png)\n\n## HW生态\n\n### 容器创建\n\n```bash\ndocker run -d --restart=always --name=xinference_wxb \\\n -v /reason-sharedata/training_inference/wangxiangbo/xinference:/opt/xinference -e XINFERENCE_HOME=/opt/xinference -e XINFERENCE_MODEL_SRC=modelscope \\\n -p 9998:9997 registry.cn-hangzhou.aliyuncs.com/xprobe_xinference/xinference:nightly-main-cpu xinference-local -H 0.0.0.0\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/xinference/image10.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/xinference/image11.png)\n\n由于Xinference框架的最新镜像需要配合GPU来使用，故本次利用cpu版本进行推理验证。\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/xinference/image12.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/reasoning/xinference/image13.png)\n\n### 服务创建\n\n","tags":["模型推理","NVIDIA","V100","xinference"],"categories":["模型推理","xinference"]},{"title":"微调数据集处理全流程解析","url":"/2026/01/08/LLM/数据处理/微调数据集处理全流程解析/","content":"\n## 行业模型微调流程图\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/paint.png)\n\n## 大模型微调全流程解决方案\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image1%23.png)\n\n\n\n\n\n## pdf数据提取处理\n\n### 政企类数据集.pdf\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image2.png)\n\n### pdf数据提取\n\n#### 采用mineru方式\n\n##### mineru环境安装\n\n[https://github.com/opendatalab/MinerU](https://github.com/opendatalab/MinerU)\n\n```bash\nconda create -n mineru python=3.10\nconda activate mineru\npip install -U \"magic-pdf[full]\" --extra-index-url https://wheels.myhloli.com -i https://mirrors.aliyun.com/pypi/simple\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image3.png)\n\n##### magic-pdf.json配置文件修改\n\n```json\n{\n    \"bucket_info\": {\n        \"bucket-name-1\": [\n            \"ak\",\n            \"sk\",\n            \"endpoint\"\n        ],\n        \"bucket-name-2\": [\n            \"ak\",\n            \"sk\",\n            \"endpoint\"\n        ]\n    },\n    \"models-dir\": \"/mnthp/users/wangxiangbo/model/opendatalab/PDF-Extract-Kit-1___0/models\",\n    \"layoutreader-model-dir\": \"/mnthp/users/wangxiangbo/model/ppaanngggg/layoutreader\",\n    \"device-mode\": \"cpu\",\n    \"layout-config\": {\n        \"model\": \"doclayout_yolo\"\n    },\n    \"formula-config\": {\n        \"mfd_model\": \"yolo_v8_mfd\",\n        \"mfr_model\": \"unimernet_small\",\n        \"enable\": true\n    },\n    \"table-config\": {\n        \"model\": \"rapid_table\",\n        \"sub_model\": \"slanet_plus\",\n        \"enable\": false,\n        \"max_time\": 400\n    },\n    \"llm-aided-config\": {\n        \"formula_aided\": {\n            \"api_key\": \"your_api_key\",\n            \"base_url\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n            \"model\": \"qwen2.5-7b-instruct\",\n            \"enable\": false\n        },\n        \"text_aided\": {\n            \"api_key\": \"your_api_key\",\n            \"base_url\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n            \"model\": \"qwen2.5-7b-instruct\",\n            \"enable\": false\n        },\n        \"title_aided\": {\n            \"api_key\": \"your_api_key\",\n            \"base_url\": \"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n            \"model\": \"qwen2.5-32b-instruct\",\n            \"enable\": false\n        }\n    },\n    \"config_version\": \"1.1.1\"\n}\n```\n\n##### py脚本方式处理pdf文件\n\n```python\nimport os\n\nfrom magic_pdf.data.data_reader_writer import FileBasedDataWriter, FileBasedDataReader\nfrom magic_pdf.data.dataset import PymuDocDataset\nfrom magic_pdf.model.doc_analyze_by_custom_model import doc_analyze\nfrom magic_pdf.config.enums import SupportedPdfParseMethod\n\n# args\npdf_file_name = \"/mnthp/users/wangxiangbo/data/pdf2.pdf\"  # replace with the real pdf path\n# name_without_suff = pdf_file_name.split(\".\")[0]\nname_without_suff = os.path.basename(pdf_file_name).split(\".\")[0]\n\n# prepare env\nlocal_image_dir, local_md_dir = \"/mnthp/users/wangxiangbo/mineru/images\", \"/mnthp/users/wangxiangbo/mineru/data\"\nimage_dir = str(os.path.basename(local_image_dir))\n\nos.makedirs(local_image_dir, exist_ok=True)\n\nimage_writer, md_writer = FileBasedDataWriter(local_image_dir), FileBasedDataWriter(\n    local_md_dir\n)\n\n# read bytes\nreader1 = FileBasedDataReader(\"\")\npdf_bytes = reader1.read(pdf_file_name)  # read the pdf content\n\n# proc\n## Create Dataset Instance\nds = PymuDocDataset(pdf_bytes)\n\n## inference\nif ds.classify() == SupportedPdfParseMethod.OCR:\n    infer_result = ds.apply(doc_analyze, ocr=True)\n\n    ## pipeline\n    pipe_result = infer_result.pipe_ocr_mode(image_writer)\n\nelse:\n    infer_result = ds.apply(doc_analyze, ocr=False)\n\n    ## pipeline\n    pipe_result = infer_result.pipe_txt_mode(image_writer)\n\n### draw model result on each page\ninfer_result.draw_model(os.path.join(local_md_dir, f\"{name_without_suff}_model.pdf\"))\n\n### get model inference result\nmodel_inference_result = infer_result.get_infer_res()\n\n### draw layout result on each page\npipe_result.draw_layout(os.path.join(local_md_dir, f\"{name_without_suff}_layout.pdf\"))\n\n### draw spans result on each page\npipe_result.draw_span(os.path.join(local_md_dir, f\"{name_without_suff}_spans.pdf\"))\n\n### get markdown content\nmd_content = pipe_result.get_markdown(image_dir)\n\n### dump markdown\npipe_result.dump_md(md_writer, f\"{name_without_suff}.md\", image_dir)\n\n### get content list content\ncontent_list_content = pipe_result.get_content_list(image_dir)\n\n### dump content list\npipe_result.dump_content_list(md_writer, f\"{name_without_suff}_content_list.json\", image_dir)\n\n### get middle json\nmiddle_json_content = pipe_result.get_middle_json()\n\n### dump middle json\npipe_result.dump_middle_json(md_writer, f'{name_without_suff}_middle.json')\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image4.png)\n\n##### 处理结果\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image5.png)\n\n### 利用Langchain切分md数据，并转为jsonl格式\n\n#### Langchain环境安装\n\n```bash\npip install langchain\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image6.png)\n\n#### py脚本切分转好的md数据\n\n:::tips\nLangChain 的 `**RecursiveCharacterTextSplitter**` 会 **按优先级** 依次尝试 **分割文本**：\n\n1. `**\\n**`（换行符）：如果能按段落分割，就不继续往下。\n2. `**\". \"**`** / **`**\"。\"**`（句号）：如果没有换行符，就按句号分割。\n3. `**\"!\"**`** / **`**\"?\"**`** / **`**\"；\"**`** / **`**\";\"**`（其他标点）：如果句号也没有，就按这些符号切分。\n\n**🚀**** 这样能保证**：\n\n+ **先按段落分割**，如果段落太长，再按 **句号** 细分；\n+ **如果句号也不够用，就继续用其他标点分割**，确保切片合理。\n\n:::\n\n```python\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport json \n# 读取 Markdown 文件\nwith open(\"/mnthp/users/wangxiangbo/mineru/data/pdf2.md\", \"r\", encoding=\"utf-8\") as f:\n    markdown_text = f.read()\n\n# 使用 RecursiveCharacterTextSplitter，按句号分割\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=300,   # 每个块最大长度\n    chunk_overlap=0, # 允许的重叠字符\n    separators=[\"\\n\", \". \", \"。\", \"!\", \"?\", \"；\", \";\"],  # 先按换行符分割，再按句号、感叹号等\n)\n\nchunks = splitter.split_text(markdown_text)\n\n# 保存为 JSONL 文件\noutput_path = \"/mnthp/users/wangxiangbo/mineru/data/output6.jsonl\"\nwith open(output_path, \"w\", encoding=\"utf-8\") as f:\n    for chunk in chunks:\n        f.write(json.dumps({\"text\": chunk}, ensure_ascii=False) + \"\\n\")\n\nprint(f\"文本已按句号分割并保存到 {output_path}\")\n\n```\n\n#### 切分结果\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image7.png)\n\n### Data-juicer进行数据清洗与质量分析\n\n#### data-juicer环境安装\n\n:::tips\n[https://github.com/modelscope/data-juicer](https://github.com/modelscope/data-juicer)\n\n:::\n\n依赖环境步骤略去，已推至镜像\n\n```bash\nregistry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:data-juicer\n```\n\n#### 数据清洗\n\n##### 清洗配置\n\n```yaml\n# Process config example for dataset\n\n# global parameters\nproject_name: 'test-process'\ndataset_path: '/mnt/mineru/data/output6.jsonl'  # path to your dataset directory or file\nnp: 4  # number of subprocess to process your dataset\n\nexport_path: './outputs/demo-process/test-processed4.jsonl'\n\n# process schedule\n# a list of several process operators with their arguments\nprocess:\n  - language_id_score_filter:\n      lang: 'zh'\n      min_score: 0.8\n  - clean_email_mapper:                                     # remove emails from text.\n  - clean_html_mapper:                                      # remove html formats form text.\n  - clean_ip_mapper:                                        # remove ip addresses from text.\n  - clean_links_mapper:                                     # remove web links from text.\n  - average_line_length_filter:                             # filter text with the average length of lines out of specific range.\n      min_len: 50                                             # the min length of filter range\n      max_len: 10000                                          # the max length of filter range\n#   - perplexity_filter:\n#       lang: 'zh'\n#       max_ppl: 5000                                             #过滤困惑度大于5000的文本  \n#   - alphanumeric_filter:                                    # filter text with alphabet/numeric ratio out of specific range.\n#       tokenization: false                                     # whether to count the ratio of alphanumeric to the total number of tokens.\n#       min_ratio: 0.0                                          # 确保文本包含足够多的字母数字内容，字母数字/总字符数 >0.7\n#       max_ratio: 0.9                                         # 允许高达90%的字母数字，过滤掉接近纯字母数字的异常文本。\n```\n\n```bash\npython tools/process_data.py --config configs/demo/process.yaml\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image8.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image9.png)\n\n\n\n#### 数据分析（质量）\n\n##### 分析配置\n\n```yaml\n# Process config example for dataset\n\n# global parameters\nproject_name: 'demo-analyzer'\ndataset_path: './outputs/demo-process/test-processed4.jsonl'  # path to your dataset directory or file\nnp: 4  # number of subprocess to process your dataset\n\nexport_path: './outputs/demo-analyzer/demo-analyzer-result4.jsonl'\n\n# process schedule\n# a list of several process operators with their arguments\nprocess:\n  - language_id_score_filter:\n      lang: 'zh'\n  - perplexity_filter:\n      lang: 'zh'\n    #   max_ppl: 5000                                             #过滤困惑度大于5000的文本                                        \n#   - llm_difficulty_score_filter:                            # filter to keep sample with high difficulty score estimated by LLM.\n#       api_or_hf_model: 'gpt-4o'                               # API or huggingface model name.\n#       min_score: 0.5                                          # The lowest difficulty score threshold to keep the sample.\n#       api_endpoint: null                                      # URL endpoint for the API.\n#       response_path: null                                     # Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n#       input_keys: ['text']                                    # Sub set of keys in the sample. Support data with multi fields such as 'query', 'analysis' and 'answer' in RFT data.\n#       field_names: ['text']                                   # Corresponding field names for input keys.\n#       system_prompt: null                                     # System prompt for the task.\n#       input_template: null                                    # The input template.\n#       field_template: null                                    # Template for each field in the prompt.\n#       try_num: 3                                              # The number of retry attempts when there is an API call error or output parsing error.\n#       enable_vllm: false                                      # If true, use VLLM for loading hugging face or local llm. Otherwise, use API for reference.\n#       model_params: {}                                        # Parameters for initializing the API model.\n#       sampling_params: {}                                     # Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}\n#   - llm_quality_score_filter:                               # filter to keep sample with high quality score estimated by LLM.\n#       api_or_hf_model: 'gpt-4o'                               # API or huggingface model name.\n#       min_score: 0.5                                          # The lowest quality score threshold to keep the sample.\n#       api_endpoint: https://dashscope.aliyuncs.com/compatible-mode/v1                                      # URL endpoint for the API.\n#       response_path: null                                     # Path to extract content from the API response. Defaults to 'choices.0.message.content'.\n#       input_keys: ['text']                                    # Sub set of keys in the sample. Support data with multi fields such as 'query', 'analysis' and 'answer' in RFT data.\n#       field_names: ['text']                                   # Corresponding field names for input keys.\n#       system_prompt: null                                     # System prompt for the task.\n#       input_template: null                                    # The input template.\n#       field_template: null                                    # Template for each field in the prompt.\n#       try_num: 3                                              # The number of retry attempts when there is an API call error or output parsing error.\n#       enable_vllm: false                                      # If true, use VLLM for loading hugging face or local llm. Otherwise, use API for reference.\n#       model_params: {\n#         'api_key': \"sk-312f14f4e65144f1b9f9240a31b584f9\",  # 你的 OpenAI API Key\n#         'base_url': \"https://dashscope.aliyuncs.com/compatible-mode/v1\"  # OpenAI API Base URL\n#       }                                        # Parameters for initializing the API model.\n#       sampling_params: {\n#         'temperature': 0.9,\n#         'top_p': 0.95\n#       }                                     # Extra parameters passed to the API call. e.g {'temperature': 0.9, 'top_p': 0.95}\n\n```\n\n```bash\npython tools/analyze_data.py --config configs/demo/analyzer.yaml\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image10.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image11.png)\n\n:::tips\n**1. 语言检测（lang）**\n\n+ count: 19.0（19 个样本都有语言标记）。\n+ unique: 1.0（只有一种语言）。\n+ top: zh（最常见的语言是中文）。\n+ freq: 19.0（所有 19 个样本都是中文）。\n+ 结论：这个数据集是纯中文数据集，没有其他语言混入。\n\n**2. 语言得分（lang_score）**\n\n定义：语言得分通常表示文本被识别为某种语言的可信度，分数范围一般是 0 到 1，越高表示越可能是该语言。\n\n统计结果： \n\n+ mean: 0.949412（平均得分约为 0.95，表明文本被识别为中文的置信度很高）。\n+ std: 0.02133（标准差较小，说明得分分布较为集中，样本间差异不大）。\n+ min: 0.909303（最低得分仍高于 0.9，说明即使是最差的样本，中文识别置信度也很高）。\n+ 25%: 0.937739（25% 分位数，说明大部分样本得分高于 0.94）。\n+ 50%: 0.95398（中位数，接近均值，分布较为对称）。\n+ 75%: 0.958543（75% 分位数，大部分样本得分集中在 0.96 以下）。\n+ max: 0.98278（最高得分接近 1，表明有些样本几乎被完美识别为中文）。\n\n结论：语言得分整体较高且分布稳定，说明数据集的文本语言一致性非常好，几乎可以确定为高质量的中文文本。\n\n**3. 困惑度（perplexity）**\n\n定义：困惑度是衡量语言模型对文本预测难度的指标，通常与文本的复杂性、可预测性相关。值越低，文本越容易被模型理解；值越高，文本可能更复杂或噪声更大。\n\n统计结果： \n\n+ mean: 4351.347368（平均困惑度约为 4351，偏高）。\n+ std: 1360.413693（标准差较大，说明样本间困惑度差异明显）。\n+ min: 1270.3（最低困惑度，表明有些样本较为简单或规律性强）。\n+ 25%: 3974.1（25% 分位数，大多数样本困惑度高于 3974）。\n+ 50%: 4415.3（中位数，与均值接近，分布相对均匀）。\n+ 75%: 5062.0（75% 分位数，表明 25% 的样本困惑度高于 5062）。\n+ max: 7357.8（最高困惑度，说明某些样本可能非常复杂或包含噪声）。\n\n结论：困惑度均值和范围都较高（1270.3 到 7357.8），且标准差较大，表明数据集文本复杂度差异显著。部分样本可能包含不规则内容、噪声或过于复杂的表达，这可能会影响模型训练效果。\n\n:::\n\n## jsonl格式数据->Q&A问答对格式\n\n### jsonl格式数据集\n\n:::tips\n采用huggingface开源的BAAI/IndustryCorpus和BAAI/IndustryCorpus2\n\n计算机数据集：[https://huggingface.co/datasets/BAAI/IndustryCorpus_computer](https://huggingface.co/datasets/BAAI/IndustryCorpus_computer)\n\n国家政治类数据集：[https://huggingface.co/datasets/BAAI/IndustryCorpus2_current_affairs_government_administration](https://huggingface.co/datasets/BAAI/IndustryCorpus2_current_affairs_government_administration)\n\n清洗处理后的政企类数据集：/mnthp/users/wangxiangbo/data-juicer-main/outputs/demo-process/test-processed4.jsonl\n\n:::\n\n#### 计算机类数据集jsonl抽取\n\n为了测试代码和节约api，仅测试其中10条数据\n\n```json\n{\"text\":\"推特怎样查看评论(推特怎么看评论区)\\n去年22月开始,推特开始测试类似于Reddit的对话树用户界面,以帮助用户更轻松地追踪转发和后续推文.在经过5个多月的测试之后,该功能终于面向所有用户开放.\\n类似于Reddit的对话树现在可以在Windows10的Twitter PWA和网页端中使用.如果你使用的是Android手机,并且手机上安装了Twitter Lite,你也会看到这个变化.值得注意的是,由于这是一个服务器端的更新,所以不需要你访问Google Play Store或Microsoft Store,也不需要你去更新Twitter web应用.\",\"alnum_ratio\":0.9172661871,\"avg_line_length\":92.6666666667,\"char_rep_ratio\":0.029739777,\"flagged_words_ratio\":0.0,\"industry_type\":\"计算机\",\"lang\":\"zh\",\"lang_score\":0.8875751495,\"max_line_length\":180,\"num_words\":73,\"perplexity\":1061.4,\"special_char_ratio\":0.1079136691,\"word_rep_ratio\":0.0,\"id\":3822520893459}\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image12.png)\n\n#### 国家政治类数据集parquet格式转为jsonl格式\n\n```python\nimport pandas as pd\n\n# 读取 parquet 文件\ndf = pd.read_parquet('/mnt/data/government_administration/chinese/middle/rank_00410.parquet')\n\n# 转换为 JSON Lines 格式并保存\ndf.to_json(\"data.json\", orient=\"records\", lines=True, force_ascii=False)\n```\n\n```json\n{\"text\":\"中国外交部发言人耿爽27日在北京表示,中方参与打击索马里海盗国际合作的行动,得到了有关国家的积极评价,为维护国际和地区的和平与安全作出重要贡献,也展现了中国负责任,有担当的大国形象.\\n中新社记者 侯宇 摄\\n  在当天举行的外交部例行记者会上,有记者提问:日前,联合国秘书长向安理会提交索马里海盗问题报告.报告多次提及中国在亚丁湾和索马里海域执行护航任务,为应对海盗威胁发挥了重要作用.中方对此有何评论?\\n  耿爽回应说,2008年12月,经联合国安理会授权,中国海军派出首批舰艇编队,赴亚丁湾,索马里海域执行护航任务.截至2017年7月,中国海军先后派出了26批护航编队,为中外6400余艘船只提供护航,驱离疑似海盗船只3000余艘次,有力保障了国际重要贸易航道安全.中方参与打击索马里海盗国际合作的行动,得到了有关国家的积极评价,为维护国际和地区的和平与安全作出重要贡献,也展现了中国负责任,有担当的大国形象.\\n  \\\"联合国秘书长的有关报告对中方所作工作和贡献给予了积极评价,我们对此表示赞赏.\\\"耿爽说,正如习近平总书记在十九大报告中所指出的,中国将恪守维护世界和平,促进共同发展的外交政策宗旨,始终做世界和平的建设者,全球发展的贡献者,国际秩序的维护者.中方将继续积极参与亚丁湾和索马里海域护航行动,拓展国际护航合作,履行国际责任和义务,为维护国际和地区和平与稳定,保障国际重要通道安全发挥更大作用.(完)\",\"alnum_ratio\":0.9024793388,\"avg_line_length\":121.0,\"char_rep_ratio\":0.0771812081,\"flagged_words_ratio\":0.0,\"max_line_length\":204,\"num_words\":213,\"perplexity\":701.0,\"quality_score\":3.79296875,\"special_char_ratio\":0.1371900826,\"word_rep_ratio\":0.1848184818,\"_id\":1005000038194,\"industry_type\":\"时政_政务_行政\"}\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image13.png)\n\n### 通过阿里api_key分析原始jsonl并生成QA微调类数据集\n\n#### py脚本\n\n```bash\nimport os\nimport json\nfrom openai import OpenAI\n\n# 初始化客户端\ntry:\n    client = OpenAI(\n        api_key=\"sk-312f14f4e65144f1b9f9240a31b584f9\",  # 请替换为你的实际API Key\n        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n    )\n\n    # 输入文件路径\n    input_file = \"/mnt/data/IndustryCorpus_computer/10.jsonl\"\n    \n    # 输出文件路径（只保存Q&A问答对）\n    output_file = \"/mnt/data/IndustryCorpus_computer/10_qa_pairs.jsonl\"\n\n    # 计数器\n    processed_count = 0\n\n    # 打开输入文件并逐行处理\n    with open(input_file, 'r', encoding='utf-8') as f:\n        for line in f:\n            try:\n                # 解析每一行的JSON数据\n                data = json.loads(line.strip())\n                \n                # 构造请求消息，要求返回特定JSON格式\n                completion = client.chat.completions.create(\n                    model=\"qwen-plus\",\n                    messages=[\n                        {\n                            'role': 'system', \n                            'content': '''你是一个擅长信息提取和问答对转换的助手。请根据提供的文本，分析其内容并转换为适合训练的问答对格式，一对问答对即可。输出必须是以下JSON格式：\n                            {\n                                \"instruction\": \"问题\",\n                                \"output\": \"答案\"\n                            }\n                            确保问题和答案内容准确反映输入文本信息。'''\n                        },\n                        {\n                            'role': 'user', \n                            'content': json.dumps(data, ensure_ascii=False)\n                        }\n                    ]\n                )\n                \n                # 获取大模型的输出（JSON格式的问答对）\n                qa_pair = completion.choices[0].message.content\n                \n                # 打印结果\n                print(f\"处理第 {processed_count + 1} 行:\")\n                print(qa_pair)\n                print(\"-\" * 50)\n                \n                # 保存JSON格式的问答对到文件\n                with open(output_file, 'a', encoding='utf-8') as out_f:\n                    out_f.write(qa_pair + \"\\n\")\n                \n                processed_count += 1\n                \n            except Exception as line_error:\n                print(f\"处理第 {processed_count + 1} 行时出错：{line_error}\")\n                continue\n    \n    print(f\"处理完成，总共处理 {processed_count} 行数据\")\n\nexcept Exception as e:\n    print(f\"初始化或主程序错误：{e}\")\n    print(\"请参考文档：https://help.aliyun.com/zh/model-studio/developer-reference/error-code\")\n\n```\n\n#### 处理结果\n\n##### 计算机类数据集\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image14.png)\n\n\n\n### 通过移动云api_key分析原始jsonl并生成QA微调类数据集\n\n#### py脚本\n\n```bash\nfrom openai import OpenAI\nimport json\nimport os\nimport re\nimport logging\n\n# 设置日志\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(\"/mnt/api/log/government.log\", encoding='utf-8'),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(__name__)\n\n# 初始化OpenAI客户端\nclient = OpenAI(\n    api_key=\"jn8P1B1vl7AbKZanw-rDQ9ahJmUH3eML_hAaKGP9wkA\",\n    base_url=\"https://zhenze-huhehaote.cmecloud.cn/inference-api/exp-api/inf-1336781912337387520/v1\"\n)\n\n# 文件路径\ninput_file_path = \"/mnt/data-juicer-main/outputs/demo-process/test-processed2.jsonl\"\noutput_file_path = \"/mnt/api/res/government_qa.jsonl\"\n\n# 确保输出目录存在\noutput_dir = os.path.dirname(output_file_path)\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# 系统提示\nsystem_prompt = {\n    \"role\": \"system\",\n    \"content\": \"\"\"你是一个擅长信息提取和问答对转换的助手。请根据提供的文本，分析其内容并转换为一个问答对，格式必须为：\n    {\"instruction\": \"问题\", \"input\": \"\", \"output\": \"答案\"}\n    只生成一对问答对，返回JSON字符串。\n    \"\"\"\n}\n\n# 计数器\nprocessed_count = 0\nerror_count = 0\n\n# 从响应中提取JSON的函数\ndef extract_json_from_response(response):\n    # 尝试直接解析整个响应为JSON\n    try:\n        qa_json = json.loads(response)\n        if all(key in qa_json for key in [\"instruction\", \"input\", \"output\"]):\n            if qa_json[\"input\"] != \"\":\n                qa_json[\"input\"] = \"\"  # 强制设为空字符串\n            return qa_json\n    except json.JSONDecodeError:\n        pass\n    \n    # 使用更精确的正则表达式匹配JSON\n    json_match = re.search(r'```json\\s*({.*?})\\s*```', response, re.DOTALL)\n    if json_match:\n        json_str = json_match.group(1)\n        try:\n            qa_json = json.loads(json_str)\n            if all(key in qa_json for key in [\"instruction\", \"input\", \"output\"]):\n                if qa_json[\"input\"] != \"\":\n                    qa_json[\"input\"] = \"\"  # 强制设为空字符串\n                return qa_json\n        except json.JSONDecodeError:\n            pass\n    \n    # 如果仍然失败，返回None\n    return None\n\n# 读取输入文件并处理\nwith open(input_file_path, 'r', encoding='utf-8') as input_file, \\\n     open(output_file_path, 'w', encoding='utf-8') as output_file:\n    \n    for line_num, line in enumerate(input_file, 1):\n        try:\n            # 解析每一行的JSON数据\n            data = json.loads(line.strip())\n            user_input = json.dumps(data, ensure_ascii=False)\n            \n            # 构造消息\n            messages = [\n                system_prompt,\n                {\"role\": \"user\", \"content\": user_input}\n            ]\n            \n            # 调用API\n            response = client.chat.completions.create(\n                model=\"default\",\n                messages=messages,\n                max_tokens=1024,\n                temperature=0.6,\n                stream=False\n            )\n            \n            # 获取结果\n            qa_result = response.choices[0].message.content.strip()\n            logger.info(f\"Line {line_num} - Raw response: {qa_result}\")\n            \n            # 从响应中提取JSON\n            qa_json = extract_json_from_response(qa_result)\n            \n            # 如果提取失败，记录原始响应\n            if qa_json is None:\n                qa_json = {\n                    \"instruction\": \"问题提取失败\",\n                    \"input\": \"\",\n                    \"output\": f\"原始响应: {qa_result}\"\n                }\n                logger.warning(f\"Line {line_num} - Failed to extract JSON, using fallback\")\n            else:\n                logger.info(f\"Line {line_num} - Successfully extracted JSON: {json.dumps(qa_json, ensure_ascii=False)}\")\n            \n            # 写入文件\n            output_file.write(json.dumps(qa_json, ensure_ascii=False) + '\\n')\n            processed_count += 1\n            \n            # 控制台显示进度\n            print(f\"已处理: {processed_count} 行\")\n            \n        except json.JSONDecodeError as e:\n            error_count += 1\n            logger.error(f\"Line {line_num} - JSON解析错误: {e}, Input: {line.strip()}\")\n            print(f\"Line {line_num} - JSON解析错误: {e}\")\n            print(\"-\" * 50)\n        except Exception as e:\n            error_count += 1\n            logger.error(f\"Line {line_num} - 处理出错: {e}, Input: {line.strip()}\")\n            print(f\"Line {line_num} - 处理出错: {e}\")\n            print(\"-\" * 50)\n\n# 打印总结\nprint(\"=\" * 50)\nprint(f\"处理完成！\")\nprint(f\"总计处理行数: {processed_count + error_count}\")\nprint(f\"成功生成问答对: {processed_count}\")\nprint(f\"错误行数: {error_count}\")\nprint(f\"结果已保存至: {output_file_path}\")\nprint(f\"日志已保存至: /mnt/api/log/government.log\")\n```\n\n#### 处理结果\n\n##### 国家政治类数据集\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image15.png)\n\n```json\n{\"instruction\": \"中国海军首批赴亚丁湾、索马里海域执行护航任务的舰艇编队是在何时派出的？\", \"input\": \"\", \"output\": \"2008年12月，经联合国安理会授权，中国海军派出首批舰艇编队赴亚丁湾、索马里海域执行护航任务。\"}\n{\"instruction\": \"闵行区七宝镇党委在提升党群服务阵地能级方面采取了哪些措施？\", \"input\": \"\", \"output\": \"七宝镇党委通过织好'理想信念网'强化政治建设，采用'线上+线下'相结合的立体化学习教育方式，推出包含七号桥碉堡、张充仁纪念馆等18个红色点位组成的行知路线，并创新开展情景党课，全面提升党群服务阵地能级。\"}\n{\"instruction\": \"汪辜会谈在两岸关系发展中具有怎样的历史意义？\", \"input\": \"\", \"output\": \"汪辜会谈是海峡两岸授权的民间机构最高负责人之间的首次会晤，也是两岸高层人士在长期隔断后的首次正式接触，标志着两岸走向和解的历史性突破，成为两岸关系发展进程中的\\\"重要里程碑\\\"。尽管会谈范围限于民间性、经济性和事务性领域，但成功启动了两岸对话谈判机制，推动两岸关系向更深层次发展，为后续两岸关系和平发展进入巩固深化期奠定了基础。\"}\n{\"instruction\": \"泰州市五届人大二次会议选举产生的市人大常委会主任和市监察委员会主任分别是谁？\", \"input\": \"\", \"output\": \"曲福田当选为市人大常委会主任，汤成快当选为市监察委员会主任。\"}\n```\n\n##### 清洗处理后的政企类数据集\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image16.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image17.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image18.png)\n\n```json\n{\"instruction\": \"优化政务服务、提升行政效能的重要抓手是什么？\", \"input\": \"\", \"output\": \"把\\\"高效办成一件事\\\"作为优化政务服务、提升行政效能的重要抓手，通过加强整体设计、推动模式创新、注重改革引领和数字赋能双轮驱动，实现办事方式多元化、流程最优化、材料最简化和成本最小化。\"}\n{\"instruction\": \"到2027年我国高效政务服务体系建设要实现什么目标？\", \"input\": \"\", \"output\": \"基本形成泛在可及、智慧便捷、公平普惠的高效政务服务体系，实现企业和个人两个全生命周期重要阶段'高效办成一件事'重点事项落地见效，大幅提升企业和群众办事满意度、获得感。\"}\n{\"instruction\": \"如何推进线下办事“只进一门”？\", \"input\": \"\", \"output\": \"通过完善集约高效的线下政务服务体系，县级以上政务服务中心应统一设置综合/专业服务窗口集中办理政务服务事项；整合部门单设窗口至本级中心或实行一体化管理；统筹建设乡镇(街道)便民服务中心和村(社区)便民服务站；鼓励在银行网点、邮政网点、园区设置便民服务点，并利用集成式自助终端提供24小时服务。\"}\n{\"instruction\": \"如何推进线上办事“一网通办”？\", \"input\": \"\", \"output\": \"加强国家政务服务平台为总枢纽的全国政务服务“一张网”建设，推动高频事项网上办、掌上办、一次办；建设省级政务服务平台统一受理端实现办件信息实时共享；整合各级平台资源推动政务服务事项和应用“应接尽接、应上尽上”，除法定或涉密情况外所有事项均纳入同级政务服务平台办理。\"}\n{\"instruction\": \"如何推进企业和群众诉求的“一线应答”？\", \"input\": \"\", \"output\": \"通过依托12345政务服务便民热线加强热线归并，设置重点领域专席提升接办效率，建立“接诉即办”机制及时响应诉求，推动12345与110高效对接联动，并加强热线与政务服务平台投诉建议体系、“好差评”体系的业务协同，打造智慧政务服务“总客服”。\"}\n{\"instruction\": \"推进关联事项集成办需要采取哪些具体措施？\", \"input\": \"\", \"output\": \"1. 建立'一件事'牵头部门与配合部门的职责分工体系；2. 重构跨部门业务流程，实现多表合一、一网通办和材料一次提交；3. 开展线上线下并联审批、联合评审和联合验收；4. 通过系统协同和数据共享压缩办理时间，降低办事成本。\"}\n{\"instruction\": \"推进容缺事项承诺办中提到的“告知承诺+容缺办理”审批服务模式的主要目标是什么？\", \"input\": \"\", \"output\": \"在风险可控、纠错成本低且能够通过事中事后监管有效防范风险的前提下，推进政务服务便利化，通过申请材料后补/免交、实质审查后置/豁免等方式实现政务服务事项承诺办理，并建立差异化的告知承诺事后核查和风险防范机制。\"}\n{\"instruction\": \"如何推进政务服务异地事项跨域办理？\", \"input\": \"\", \"output\": \"通过推动政务服务事项省内通办、跨省通办，依托全国一体化政务服务平台实现数据共享和网上办理；优化线下代收代办服务模式，建立跨域窗口协同机制；运用远程虚拟窗口技术提供帮办服务，并支持京津冀等重点区域探索更多跨区域办理事项。\"}\n{\"instruction\": \"如何推进政策服务免申办？\", \"input\": \"\", \"output\": \"通过全面梳理政策条件与规则，依托数据归集共享、模型算法和大数据分析，精准匹配符合条件的企业与群众，推动政策“免申即享”；对需申请的政策自动生成申请表并精准推送申请材料，同时优化政务服务平台专属服务空间，实现政策精准直达。\"}\n{\"instruction\": \"根据文本内容，推动政务服务平台的整合中，关于地市级以下政务服务平台的建设有什么规定？\", \"input\": \"\", \"output\": \"原则上不再单独建设地市级以下政务服务平台。\"}\n{\"instruction\": \"如何提升政务数据共享实效？\", \"input\": \"\", \"output\": \"通过完善政务数据共享责任清单机制，依托全国一体化政务服务平台数据共享枢纽推动部门数据回流和直达基层，编制'一企一档、一人一档'数据规范实现'一数一源一标准'，深化电子证照跨地区跨部门互通互认，推动政府部门核发材料免提交、共享核验事项免证明、电子证照免实体证照。\"}\n{\"instruction\": \"如何推动政务服务由人力服务型向人机交互型和数据分析型转变？\", \"input\": \"\", \"output\": \"通过创新应用大数据、区块链、人工智能等新技术，优化重构服务流程并完善智能预填、预审功能，同时在具体办事场景中应用自然语言大模型技术提升智能客服能力，推动政务服务向人机交互型和数据分析型转变。\"}\n{\"instruction\": \"如何增强在高新区、产业园区的项目帮办代办能力？\", \"input\": \"\", \"output\": \"通过加强项目全流程帮办代办，结合实际建立专班服务、专员跟进等机制，及时解决项目推进中的难点问题。\"}\n{\"instruction\": \"在丰富公共服务供给方面，新增了哪些与企业和群众生产生活密切相关的服务领域？\", \"input\": \"\", \"output\": \"新增了水电气热、网络通信等公用事业领域的高频办理服务事项。\"}\n{\"instruction\": \"如何拓展涉企增值服务内容？\", \"input\": \"\", \"output\": \"通过统筹行业协会、市场化专业服务机构等资源，一站式提供政策服务、法律服务、金融服务、人才服务、科创服务和国际贸易服务，具体包括政策推荐/申报、公证/合规指导、融资担保/产业基金对接、人才认定/住房安居、科技企业培育、报关退税咨询等服务。\"}\n{\"instruction\": \"如何健全政务服务标准体系？\", \"input\": \"\", \"output\": \"健全政务服务标准体系需加强总体设计，制定全国一体化政务服务平台、政务服务中心建设及12345热线运行标准，统一政务服务事项基础标准，编制集成办理\\\"一件事\\\"指南，并完善跨区域办事业务标准与操作规程。\"}\n{\"instruction\": \"如何强化政务服务制度供给？\", \"input\": \"\", \"output\": \"强化政务服务制度供给需通过三方面措施：1.加强国家层面立法研究并清理修订不适应性法规；2.破除集成办/承诺办/跨域办/免申办的制度障碍；3.完善数字化应用配套政策保障电子证照共享和电子档案效力。\"}\n{\"instruction\": \"如何健全政务服务工作体系？\", \"input\": \"\", \"output\": \"推进国家、省、市、县、乡五级政务服务体系建设，健全一体联动、高效便捷、权责清晰的工作机制；加强各级政务服务窗口从业人员配备、管理、培训和考核，推进综合服务窗口人员统一配备和职业化发展；创新政务服务人才引进、培养、选拔和评价机制，提升干部队伍法治思维、服务意识和数字素养，强化专业化队伍建设。\"}\n{\"instruction\": \"如何确保“高效办成一件事”取得实效？\", \"input\": \"\", \"output\": \"1.各地区要强化省级统筹，逐项制定工作方案并明确目标任务、改革措施和职责分工；\\n2.国务院各部门需加强条块联动，提供业务指导和数据共享支撑；\\n3.国务院办公厅要统筹协调，开展试点示范并逐年推出重点事项清单，推动政务服务提质增效。\"}\n\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image19.png)\n\n## 微调测试\n\n### 数据集\n\n按上述方法，重新增加了政企类数据集。\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image20.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image21.png)<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image22.png)\n\n#### 提取并切分后的数据\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image23.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image24.png)\n\n#### 清洗结果如下\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image25.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image26.png)\n\n#### 清洗结果对比 \n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image27.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image28.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image29.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image30.png)\n\n#### 问答对生成\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image31.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image32.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image33.png)\n\n### ms-swift 框架微调\n\n#### 环境搭建\n\nms-swift 训练环境已传至镜像\n\n```bash\nregistry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:swift\n```\n\n#### 训练脚本\n\n##### 基于政企类数据集的 qwen2.5-7B-Ins-lora 微调\n\n```bash\n# 22GB\nCUDA_VISIBLE_DEVICES=0,1,2,3 \\\nswift sft \\\n    --model /mnt/model/Qwen/Qwen2.5-7B-Instruct \\\n    --train_type lora \\\n    --dataset /mnt/ms-swift/data/train.jsonl \\\n    --torch_dtype float16 \\\n    --num_train_epochs 6 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --learning_rate 1e-4 \\\n    --lora_rank 8 \\\n    --lora_alpha 32 \\\n    --target_modules all-linear \\\n    --gradient_accumulation_steps 1 \\\n    --eval_steps 50 \\\n    --save_steps 50 \\\n    --save_total_limit 5 \\\n    --logging_steps 1 \\\n    --max_length 2048 \\\n    --output_dir /mnt/ms-swift/output \\\n    --system '你是一个苏州政企问答小助手' \\\n    --warmup_ratio 0.05 \\\n    --dataloader_num_workers 4 \\\n    --model_author swift \\\n    --model_name swift-qwen2.5-7b\n```\n\n##### 基于政企类数据集的 qwen2.5-0.5B-Ins-全参微调\n\n```bash\n# 76GiB\nCUDA_VISIBLE_DEVICES=0,1,2,3 \\\nswift sft \\\n    --model /mnt/model/Qwen2.5-0.5B-Instruct \\\n    --train_type full \\\n    --dataset /mnt/ms-swift/data/train.jsonl \\\n    --torch_dtype float32 \\\n    --num_train_epochs 6 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --learning_rate 1e-5 \\\n    --gradient_accumulation_steps 16 \\\n    --eval_steps 100 \\\n    --save_steps 100 \\\n    --save_total_limit 5 \\\n    --logging_steps 1 \\\n    --max_length 2048 \\\n    --output_dir /mnt/ms-swift/output \\\n    --system 'You are a helpful assistant.' \\\n    --warmup_ratio 0.05 \\\n    --dataloader_num_workers 4 \\\n    --model_author swift \\\n    --model_name swift-qwen2.5-0.5b\n\n```\n\n#### 训练\n\n##### 日志\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image34.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image35.png)\n\n##### 显存占用情况\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image36.png)\n\n##### loss 曲线\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image37.png)\n\n#### 推理测试\n\n##### 开启 swift web 界面服务\n\n```bash\nswift web-ui --lang zh --server_port 16686\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image38.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image39.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image40.png)<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/data/datajuicer/image41.png)\n\n\n\n\n\n","tags":["NVIDIA","V100","模型微调","数据集处理"],"categories":["模型微调","数据集处理"]},{"title":"ms-swift框架微调测试","url":"/2026/01/08/LLM/训练框架/ms-swift框架/","content":"### ms-swift 框架微调\n\n#### 环境搭建\n\n#### 训练脚本\n\n##### 基于政企类数据集的 qwen2.5-7B-Ins-lora 微调\n\n```bash\n# 22GB\nCUDA_VISIBLE_DEVICES=0,1,2,3 \\\nswift sft \\\n    --model /mnt/model/Qwen/Qwen2.5-7B-Instruct \\\n    --train_type lora \\\n    --dataset /mnt/ms-swift/data/train.jsonl \\\n    --torch_dtype float16 \\\n    --num_train_epochs 6 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --learning_rate 1e-4 \\\n    --lora_rank 8 \\\n    --lora_alpha 32 \\\n    --target_modules all-linear \\\n    --gradient_accumulation_steps 1 \\\n    --eval_steps 50 \\\n    --save_steps 50 \\\n    --save_total_limit 5 \\\n    --logging_steps 1 \\\n    --max_length 2048 \\\n    --output_dir /mnt/ms-swift/output \\\n    --system '你是一个苏州政企问答小助手' \\\n    --warmup_ratio 0.05 \\\n    --dataloader_num_workers 4 \\\n    --model_author swift \\\n    --model_name swift-qwen2.5-7b\n```\n\n##### 基于政企类数据集的 qwen2.5-0.5B-Ins-全参微调\n\n```bash\n# 76GiB\nCUDA_VISIBLE_DEVICES=0,1,2,3 \\\nswift sft \\\n    --model /mnt/model/Qwen2.5-0.5B-Instruct \\\n    --train_type full \\\n    --dataset /mnt/ms-swift/data/train.jsonl \\\n    --torch_dtype float32 \\\n    --num_train_epochs 6 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --learning_rate 1e-5 \\\n    --gradient_accumulation_steps 16 \\\n    --eval_steps 100 \\\n    --save_steps 100 \\\n    --save_total_limit 5 \\\n    --logging_steps 1 \\\n    --max_length 2048 \\\n    --output_dir /mnt/ms-swift/output \\\n    --system 'You are a helpful assistant.' \\\n    --warmup_ratio 0.05 \\\n    --dataloader_num_workers 4 \\\n    --model_author swift \\\n    --model_name swift-qwen2.5-0.5b\n\n```\n\n#### 训练\n\n##### 日志\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/ms-swift/image1.png)\n\n##### 显存占用情况\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/ms-swift/image2.png)\n\n##### loss 曲线\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/ms-swift/image3.png)\n\n#### 推理测试\n\n##### 开启 swift web 界面服务\n\n```bash\nswift web-ui --lang zh --server_port 16686\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/ms-swift/image4.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/ms-swift/image5.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/ms-swift/image6.png)<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/train_framework/ms-swift/image7.png)\n\n\n\n\n\n","tags":["NVIDIA","V100","模型微调","ms-swift"],"categories":["模型微调","ms-swift"]},{"title":"tulu3论文解读","url":"/2026/01/08/LLM/论文解读/tulu3论文解读/","content":"\n语言模型的后训练可以改进其性能并解锁新技能，为了缩小开源方法在这方面通常落后于专有闭源技术这一差距，研究者推出了TÜLU3，一个完全开源的高级后训练模型系列，包括其数据、代码和训练方法。基于Llama 3.1基础模型，TÜLU3的表现优于多个知名模型，如Llama 3.1、Qwen 2.5、Mistral等，并且在某些方面甚至超过了GPT-4o-mini和Claude 3.5-Haiku这样的闭源模型。TÜLU3采用**监督微调(SFT)、直接偏好优化(DPO)**以及一种**新的可验证奖励强化学习(RLVR)**方法进行训练。此外，还建立了一个全面的多任务评估框架，涵盖了开发与未见测试、标准基准及对现有开源数据集的大规模清理。最后，项目提供了模型权重、示例、数据集、工具包、训练代码和详细报告，以便他人复制结果并将这些技术应用于更广泛的领域。\n\n\n[TÜLU3 70B](https://huggingface.co/allenai/Llama-3.1-Tulu-3-70B)\n\n[TÜLU3 8B](https://huggingface.co/allenai/Llama-3.1-Tulu-3-8B)\n\n[TÜLU3 Data](https://huggingface.co/collections/allenai/tulu-3-datasets-673b8df14442393f7213f372)\n\n[TÜLU3 Code](https://github.com/allenai/open-instruct)\n\n[TÜLU3 Eval](https://github.com/allenai/olmes)\n\n\n#  Introduction\n\n_“正如骆驼与商队中的其他骆驼分担负担一样，智者也会分享他们的见解，以减轻无知的负担。” –_ TÜLU3产生的谚语。\n\n<!-- 这是一张图片，ocr 内容为： -->\n![图 1 TÜLU3 recipe概述。这包括：针对一般和目标能力的数据管理、训练策略以及用于开发和最终评估阶段的标准化评估套件。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733899427791-c9d0b79b-aaf6-4478-9be4-c71d0dc178a8.png)\n\n为了缩小开源式和闭源式后训练之间的差距，作者们推出了这是一系列开源的先进后训练模型，以及所有数据、训练方法、代码、基础设施和评估框架。\n\n为了构建 TÜLU3，作者们确定了一组后训练需要改进的核心技能（例如推理、数学、编码、安全、精确遵循指令、知识回忆等），并建立了一个评估框架，以建立明确的绩效目标并指导模型在一系列开发和未见过的任务上的改进。TÜLU3受益匪浅，它充分利用了公开可用的开源数据，在各个训练阶段生成多样化、特定技能的合成数据，并根据作者们的评估套件积极地净化这些数据。\n\nTÜLU3训练Recipe涉及多个阶段，每个阶段都以之前的模型为基础，并侧重于不同类型的数据——即监督微调的快速完成实例、偏好调整以及强化学习的可验证奖励。作者们的方法有助于识别技能缺陷并改进数据组合、方法和参数，确保在整个训练过程中核心技能的平衡表现。通过严格、有原则的实验，作者们确定了监督微调的最佳数据组合，从而得出 TÜLU3SFT 检查点。利用偏好调整方面的最新进展，作者们随后通过比较 TÜLU3SFT 完成情况与其他语言模型的输出，使用精心策划的策略偏好数据来训练模型。此外，作者们引入了一个新的最终微调阶段——可验证奖励的强化学习 (RLVR)——它采用了一种新颖的 RL 目标，旨在增强特定技能，具有可验证的答案，例如数学和精确的指令。\n\n作者们表现最佳的方案产生的 TÜLU3模型的表现优于同等规模的最先进的后训练开源权重模型，例如 Llama 3.1 Instruct、Qwen2.5 Instruct 或 Mistral-Instruct，并且在 70B 规模下，TÜLU可与 Claude 3.5 Haiku 和 GPT-4o mini 等闭源提供商的产品相媲美。\n\n闭源模型版本：GPT-3.5-Turbo-0125、GPT-4o-mini-2024-07-18、Claude 3.5 Haiku 20241022\n\n总而言之，TÜLU3代表了一系列最先进的开源语言模型，具有现代的后训练框架、完全开源的数据 TÜLU3D ATA 、评估 TÜLU3E VAL 、训练代码 TÜLU3C ODE 和开发Recipe TÜLU3R ECIPE 。以下是 TÜLU开发的一些关键贡献：\n\n\n\n+ 为评估、净化和Recipe设计提供广泛的指导和工具，\n+ 扩展的、新的合成教学数据集，\n+ 使用基于策略的生成来扩展偏好数据，\n+ 具有可验证奖励的强化学习，这是一种基于 RL 的方法，只有在模型的完成被验证为正确时才会获得奖励，\n+ 先进的基础设施、细节和代码，以促进大型模型的成功实施\n\n\n\n作者们的工作成果是完全开源的语言模型微调pipeline。作者们发布了在 Llama 3.1 基础版本基础上训练的最终模型，其中包含中间检查点、训练数据、训练代码和评估代码（发布的完整工件列表见表 1）。借助所有发布的资源，其他人可以采用开源的基础模型并对其进行微调，以在任何感兴趣的任务上实现高性能，为复杂、多目标和多阶段训练方案中的后训练研究奠定基础。\n\n<!-- 这是一张图片，ocr 内容为： -->\n![表 1 中间检查点、训练数据、训练代码和评估代码](https://cdn.nlark.com/yuque/0/2024/png/26978393/1734059834134-cf6cc423-eac6-4aaf-8d27-c088c6d206fa.png)\n\n# TÜLU3  Overview \n\n表 2：TÜLU3 EVAL由开发和隐藏的分割组成，用于评估核心技能。借助 TÜLU3 EVAL ，作者们发布了统一的标准化评估套件和工具包，用于根据基准对训练数据进行净化。下标显示了作者们用于评估的指标。TÜLU3 Safety 是安全评估的集合，详情请参阅第 7.2.1 节。\n\n<!-- 这是一张图片，ocr 内容为： -->\n![表 2 TÜLU3 EVAL 发布了统一的标准化评估套件和工具包](https://cdn.nlark.com/yuque/0/2024/png/26978393/1734059964094-89a71f4c-d505-4577-b6ab-3f5a05bfe9d7.png)\n\nTÜLU3突破了后训练研究的界限，缩小了开源式和闭源式微调方法之间的差距。借助 TÜLU3，作者们希望发现开源社区的哪些路径将取得成功，哪些路径不会（通过报告负面结果）。这是一个复杂的训练过程，将专有方法的部分细节与新技术和成熟的学术研究相结合。mdelname 3 成功的关键因素在于仔细的数据管理、严格的实验和评估、创新的方法和改进的训练基础设施。作者们遵循系统的指导方针，通过创建开发和测试集进行评估，对这一过程进行科学评估，并对公开可用的数据集进行仔细的净化。开源后训练的TÜLU3 不仅仅是一个工件，而是一套全面的数据和工具，旨在推动前沿技术的发展。通过公开分享作者们的数据、Recipe和发现，作者们旨在使社区能够探索新的和创新的后训练方法。\n\n## TÜLU3 Data\n\n核心技能TÜLU3 的工作首先确定了开源的后训练方法经常落后的关键领域，这些领域是通用语言模型所需的能力。表 3 概述了作者们旨在增强的核心能力以及为涵盖这些技能而选择的评估基准。在 TÜLU3 中，作者们专注于知识回忆、推理、数学、编码、指令遵循、一般聊天和安全等核心技能。\n\n作者们通过从公共数据中获取数据并综合整理数据来整理和收集 TÜLU3D ATA以针对这些核心技能。作者们在训练的不同阶段使用各种数据格式。表 6 概述了用于训练作者们模型的数据集集合，更多详细信息请参见第 3 节。\n\n## TÜLU3  Evaluation\n\n作者们的后训练方法取得成功的一个关键因素是建立明确的绩效目标和评估工具，以指导这些阶段的改进。借助 TÜLU3E VAL ，作者们发布了统一的标准化评估套件和工具包，以指导开发和评估最终模型并根据评估基准净化训练数据。\n\n作者们的框架包括一个用于可重复评估的开源评估工具包（第 7.1 节）、一套用于评估指令调整模型中核心技能的套件（具有单独开发（第 7.2 节）和保留评估（第 7.3 节），以及一组基于作者们对各种模型的实验在作者们的评估套件上进行评估的推荐设置。这两种划分都涵盖了所有已识别的技能。至关重要的是，作者们没有检查作者们未见集上的分数\n\n## TÜLU3  Recipe \n\n在本节中，作者们概述了 TÜLU3 Recipe  ，以获得最先进的后训练模型。作者们在预训练语言模型的基础上通过四阶段后训练Recipe生成 TÜLU3模型（见图 1）。TÜLU3RECIPE是一个先进的多阶段训练pipeline，结合了强化学习中的新算法进步、尖端基础设施和严格的实验，以整理数据并优化各个训练阶段的数据组合、方法和参数。在所有阶段，作者们使用精心选择的评估套件来衡量模型性能。阶段如下：\n\n\n\n+ **第 1 阶段：数据管理 (§3)，**作者们管理各种Prompts，以分配到多个优化阶段。作者们创建新的合成Prompts，或者在可用时从现有数据集中获取源Prompts，以针对特定功能。作者们确保Prompts不会受到作者们的评估套件 TÜLU3 EVAL 的污染。\n+ **第 2 阶段：监督微调 (§4)，**作者们对精心挑选的Prompts和完成内容进行监督微调 (SFT)。通过彻底的实验，在作者们的评估框架的指导下，确定最终的 SFT 数据和训练超参数，以增强目标核心技能，而不会显著影响其他技能的表现。\n+ **第 3 阶段：偏好调整 (§5)，**作者们将偏好调整（特别是 DPO）应用于新整理的基于策略的合成偏好数据（从选定的Prompts以及非策略数据中生成）。与 SFT 阶段一样，作者们通过彻底的实验确定最佳偏好数据组合，发现哪些格式的数据、方法或超参数可以带来改进。\n+ **第 4 阶段：具有可验证奖励的强化学习（§6）**，作者们引入了一个新的基于 RL 的后训练阶段，该阶段使用可验证奖励而不是奖励模型来训练模型，这在传统 RLHF PPO 训练中很常见。作者们选择具有可验证结果的任务，例如数学问题解决，并且仅在模型的生成被验证为正确时才提供奖励。然后作者们使用 RL 来训练这些奖励。\n\n\n\n作者们的 TÜLU3流程的主要贡献在于改进了数据、方法、基础设施和严格的评估。作者们流程的关键要素包括：\n\n\n\n+ **数据质量、来源和规模（§3）**作者们通过仔细调查可用的开源数据集、分析其来源并对其进行净化，以及策划针对核心技能的合成Prompts来获得Prompts。为了确保有效性，作者们进行了彻底的实验来研究它们对作者们的开发评估套件的影响。\n+ **创建多技能 SFT 数据集(§4.1) **通过对各种数据mixtures进行多轮监督微调，改进了“一般”和“特定技能”类别中Prompts的分布。例如，为了提高数学推理能力，作者们首先通过创建数学专业模型在评估套件中建立上限，然后混合数据以使一般模型更接近这个上限。\n+ **整理基于策略的偏好数据集(§5.2)** 作者们开发了基于策略的数据整理流程来扩展作者们的偏好数据集生成。具体来说，作者们从 TÜLU3-SFT 和其他模型中为给定的Prompts生成回答，并通过它们的成对比较获得偏好标签。作者们的方法扩展并改进了 Cui 等人提出的基于策略的偏好数据生成方法 [2023]。通过精心选择偏好数据，作者们获得了 354,192 个用于偏好调整的实例，在一系列任务中表现出显著的改进。\n+ **偏好调整算法设计(§5.4) **作者们试验了几种偏好调整算法，并观察到使用长度归一化的直接偏好优化可以提高性能。作者们在实验中优先考虑简单性和效率，并在整个开发过程和训练最终模型时使用长度归一化的 DPO，而不是对基于 PPO 的方法进行成本更高的研究。\n+ **具有可验证奖励的技能特定强化学习(§6) **作者们采用一种新方法，利用标准强化学习范式来针对可以根据真实结果（例如数学）进行评估的技能。作者们将此算法称为具有可验证奖励的强化学习 (RLVR)；如果完成成功，它将获得恒定的奖励值。作者们的结果表明，RLVR 可以提高 GSM8K、MATH 和 IFEval 性能。\n+ **强化学习的训练基础设施（§6.3）**作者们实施了异步 RL 设置：作者们通过 vLLM 高效运行 LLM 推理，同时学习者同时执行梯度更新。作者们的 RL 代码库也具有高度可扩展性，可以训练 70B RLVR 策略模型。\n+ **评估框架：TÜLU3EVAL（§7）**除了评估最终模型之外，作者们的评估框架是一个开源的评估工具包，旨在通过精心挑选的评估套件和净化工具来指导开发进度。\n\n\n\n##  Evaluation and Results \n\n在整个工作过程中报告分数时，作者们使用表 3 中确定的指标；分数越高越好。在计算整体性能时，作者们只是对所有评估取平均分数，对每个评估一视同仁。在作者们的开发评估套件中，在 Llama 3 基础模型上训练的 TÜLU3的表现优于其大小类别中的所有其他开源权重模型。与闭源模型相比，TÜLU370B 甚至超越了 GPT-3.5-Turbo-0125 或 GPT-4o-mini-2024-07-18 等闭源模型，同时接近 Claude 3.5 Haiku 20241022 的性能。表 3显示了在 80 亿和 700 亿个参数的 Llama 3 上训练的 TÜLU3与它们大小类别中的领先模型的总结。表 4 显示了 8B 版本按训练阶段的性能细分，表 5 显示了 70B 版本按训练阶段的性能细分。\n\n<!-- 这是一张图片，ocr 内容为： -->\n![表 3 TÜLU3 EVAL suite上 8B 和 70B 模型的结果概览。每个基准上每种模型大小表现最佳的模型以粗体显示。TÜLU3 的表现优于相同大小的最先进的后训练开放权重模型，并超越了 Claude Haiku、GPT-3.5 Turbo 和 GPT-4o Mini。 封闭模型版本：GPT-3.5-Turbo-0125、GPT-4o-mini-2024-07-18、Claude 3.5 Haiku 20241022](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733899395302-1cf556db-dbd8-4d7d-930e-245a85c8966e.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![表 4 TÜLU3 结果与同类 8B 模型的比较总结。每个基准上表现最佳的模型 （即每行）以粗体显示。TÜLU3-8B 的表现明显优于之前最先进的 8B 模型。各个检查点的进展凸显了训练的每个阶段在提高核心技能方面的贡献。许多最低值（例如 BigBenchHard）是由于未遵循评估所需的少样本格式或其他重复性错误造成的 - 有关更多详细信息，请参阅§7。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733899601980-166453fb-b02f-45df-92f4-dcea7ab31db1.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![表 5 TÜLU3 结果与同类 70B 模型的比较总结。每个基准上表现最佳的模型 （即在每一行）以粗体显示。TÜLU3-70B 的表现明显优于之前最先进的 70B 模型。各个检查点的进展凸显了训练的每个阶段对提高核心技能的贡献。Nemotron Llama 3.1 70B 是表中唯一一个从另一个后训练模型（在本例中为 Llama 3.1 70B Instruct）微调的模型，而其他模型则来自各自的基础模型。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733899628835-9acdbbe7-316a-48b3-89eb-127fd828fd56.png)\n\n# TÜLU3 Data\n\n Prompts代表用户与模型交互的各种方式，是所有后训练阶段的重要组成部分。作者们精心挑选了数百万个Prompts作为 TÜLU3后训练Recipe的起点。从这些Prompts中选择了用于下一阶段训练的数据。表 6 总结了这些Prompts的关键信息。在本节中，作者们描述了作者们的Prompts策划过程和净化工作，以确保作者们的评估不会泄露在这些Prompts中。在以下部分中，作者们将描述如何使用Prompts进行监督微调 §4 和偏好调整 §5。\n\n## Prompt 拓展\n\n为了瞄准所需的核心技能，作者们从具有明确来源的公开数据集中挑选出一组多样化、高质量的Prompts，并综合生成Prompts以填补任何空白。\n\n### 从公共数据集获取数据\n\n自作者们发布 TÜLU2 以来，社区见证了大量工作为后期训练创建数据集，包括监督微调和偏好调整。TÜLU3旨在整合和扩展这些资源以构建更强大的模型。作者们从对公共数据集的广泛调查开始这一过程，包括由专职人员注释的数据集、来自真实用户的数据集以及与模型合成的数据集。然后，作者们手动审查每个数据集，并根据以下考虑因素进行选择。\n\n<!-- 这是一张图片，ocr 内容为： -->\n![表 6 tulu3 Prompt数据样本摘要，α标记为污染，↓为在原始数据集上进行缩减采样，↑为上采样。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733901190484-e64749a8-54e0-4097-ae3d-df8bafb1b86e.png)\n\n\t\n\n### 针对目标技能进行合成\n\n为了满足对多样化和特定技能数据集日益增长的需求，作者们采用了合成数据生成作为补充方法。合成数据生成已成为人工编写数据的一种有前途的替代方案，因为它更容易获取、可针对不同目的进行定制，并且反映了底层模型的丰富知识 [Dubey et al., 2024]。然而，大规模生成多样化和高质量的数据并非易事，因为 LM 很容易陷入重复的模式或模式，称为“模式崩溃”。为了确保生成的多样性，作者们遵循 Chan 等人 [2024] 中最近采用的角色驱动方法来生成合成数据。关键思想是使用不同的角色（例如，“专注于神经网络的机器学习研究人员”）和数据合成Prompts（例如，“创建编码问题”）来引导 LLM 合成具有相应视角的数据。具体来说，作者们以 Persona Hub [Chan et al., 2024] 中的 ∼250K 个角色为条件，生成针对特定技能（例如精确遵循指令、数学和编码）的Prompts。\n\n\n\n+ **精确的指令遵循，**精确的指令遵循是指遵循自然语言中可验证的指令（例如“写 3 段话”）的能力，这些指令可以通过启发式方法自动验证。作者们使用角色驱动的方法综合生成可验证指令，涵盖 IFEval 基准中定义的 25 种不同约束类型。更具体地说，作者们首先为每个约束手动编写 1-2 个示例指令（用作seedPrompts。然后，作者们使用例如单词数来生成新指令），从而得到总共33 条可验证指令，作者们给出了数据合成Prompts、角色和单个可验证指令作为示例。\n+ **数学和编码，**作者们采用类似的角色驱动方法来综合生成各种数学单词和编码问题。数学问题包括需要高等数学技能的问题以及小学问题。对于编码，作者们生成入门到中级程序员都可以解决的 Python 编程问题。与精确的指令遵循不同，作者们零样本Prompts GPT-4o 生成特定于给定角色输入的独特问题。生成问题后，作者们生成多步骤使用 GPT-4o 编写数学解决方案，使用 claude-3-5-sonnet 编写 Python 程序。\n+ **不合规和安全**，随着作者们增强模型有效协助用户的能力，确保它们能够可靠地拒绝不安全查询并适当处理细微和超出范围的查询至关重要。\n\n\n\n## Prompt  清洗\n\n在拓展作者们的训练组合时，一个重要的考虑因素是训练集和评估集之间可能存在重叠。作者们按如下方式量化这种重叠，并根据需要从作者们的训练组合中删除实例，以防止测试集污染。\n\n\n\n+ **匹配方法。**作者们尝试了全字符串、n-gram 和基于嵌入的匹配，发现 n-gram 匹配产生了最有用的结果——虽然基于嵌入的方法原则上可以识别由于释义而产生的非平凡污染 [Yang et al., 2023]，但作者们发现很难区分单纯的分布相似性和实际的释义。此外，使用 n-gram 匹配的部分表面重叠成功地识别了污染的情况，其中实例略有不同，例如数学问题仅数字不同。\n+ **识别匹配实例。**由于训练数据集中的补全通常使用语言模型重新生成，因此作者们选择仅计算Prompts中的重叠（或更一般地计算多轮对话中的用户轮次）。\n+ **去污。** 如果训练集中的任何实例数量与作者们开发和未见套件中的任何评估中超过 2% 的实例重叠，则作者们认为该训练集受到污染。作者们会删除所有受到未见评估污染的训练集。对于受到开发评估污染的训练集，如果这样做不会显著影响最终模型的性能，作者们会删除整个数据集；否则，作者们会删除与任何测试实例匹配的特定实例。\n\n\n\n# SFT\n\n将预训练的基础模型调整为各种任务和用户请求通常依赖于监督微调 (SFT)，也称为指令微调。此过程中的一个关键挑战是平衡代表不同技能的混合训练数据集的比例。对于 TÜLU3，作者们进行了数据混合消融并探索了模型合并数据最初用于构建 Vicuna 模型，但确切的数据集尚未发布。\n\n<!-- 这是一张图片，ocr 内容为： -->\n![图 2：TÜLU3最终 SFT 混合了Prompts的来源和长度以及标记的完成情况（使用 Llama 3 标记器）。将此分布与图 24 中之前开源的 SFT 训练数据集进行比较。实例最多的数据集位于直方图的底部。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733904443024-d0833599-0580-4e15-80a4-5a698c63411a.png)\n\n## SFT 数据\n\n### 从Prompts到 SFT 数据\n\n为了创建作者们的 SFT 组合，作者们通过两种方式收集或创建第 3 节中描述的Prompts的回答：过滤现有回答和创建新回答。\n\n对于有现有回答的Prompts，如果原始回答是由人类或前沿模型（如 GPT-4o）编写的，作者们通常会保留原始回答。对于包含来自前沿模型子集的大型数据集（例如 WildChat），作者们使用来自最佳模型的子集。作者们还会过滤空回答和包含有关模型或其开发人员的信息的回答。如果一组Prompts没有回答（例如作者们的 Persona Prompts），或者原始回答来自较弱的模型（例如 WildGuardMix），作者们会使用 GPT-4o 生成新回答。作者们还手写了对硬编码Prompts的回答。\n\n### TÜLU3 SFT Mix\n\n为了开发作者们的 SFT 组合，作者们首先使用在 TÜLU2 上训练的 Llama3.1 作为基线，确定了落后于最先进模型的技能。针对每一项技能，作者们收集了高质量的公开数据集并创建了合成数据集（如第 3.1.2 节所述），同时删除了一些作者们的发现与其他较新的数据集相比，其质量相对较低的数据集。\n\n为了设计最终的 SFT 组合，作者们首先构建了特定技能的数据组合和模型，保留了在单个技能上表现最佳的混合数据，忽略了其他评估。这样做是为了根据作者们的设置估算出每个评估的上限。\n\n然后，作者们将这些mixtures组合起来，创建了作者们的初始 TÜLU3预览组合。然后，作者们继续通过添加或删除数据集来迭代mixtures，以提高滞后技能，并根据作者们的评估进行净化混合，以及作者们的中间和最终 TÜLU3SFT 混合。\n\n\n\n<!-- 这是一张图片，ocr 内容为： -->\n![图 3： 中间混合 1、2 和 3 是添加新数据集以提高性能的结果。中间混合 4 和 5 是运行多轮净化的结果，导致性能略有下降。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733906122721-e758bc75-18ee-4266-a158-8e9c65bed95d.png)\n\n\n\n<!-- 这是一张图片，ocr 内容为： -->\n![表 7 TÜLU3SFT 模型与可比基线的性能总结。作者们最终的 SFT mixtures表现出色，平均得分高于其他可比mixtures。所有模型（包括 TÜLU2 SFT）均在 Llama 3.0 或 3.1 上进行训练。作者们最终的 Tülu 3 70B 模型用于帮助格式化此表。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733906304611-025db79c-ed3f-491b-8a17-608f6a524f18.png)\n\n## 关键数据实验\n\n在开发最终的 SFT mixtures之后，作者们还进行了一系列控制实验，以探索在数据混合和训练期间做出的不同决策的重要性。\n\n\n\n+ **多样化的聊天数据。**在作者们的组合中，作者们还强调添加多样化的聊天数据，主要来自 WildChat。作者们在表9中展示了删除 WildChat 的影响，作者们发现大多数技能都出现了轻微但明显的退化，最明显的是 Alpaca Eval，这凸显了多样化现实世界数据的重要性。\n+ **安全性是正交的**。作者们发现，作者们的安全性 SFT 数据通常与作者们的其他数据集正交。作者们在表9中报告了删除特定于安全性的数据集的效果，并且作者们看到，除了安全性平均值之外，大多数技能大致保持不变。作者们还发现，添加诸如 CoCoNot 中的那些构造性Prompts有助于防止作者们的模型过度拒绝安全Prompts。\n+ **新人物角色数据。**作者们新的人物角色数据集是针对特定技能而构建的：数学、编码和遵循指令。在表9中，作者们显示删除人物角色数据集后，HumanEval(+)、GSM8K、MATH 和 IFEval 上的性能下降，显示了创建多样化、特定技能的 SFT 数据集的价值。针对特定技能。作者们还在表9 中展示了针对特定技能的影响，其中作者们展示了删除所有特定数学数据的效果。与作者们的最终组合相比，GSM8K 和 MATH 都显着下降，强调了在数据收集期间关注特定技能的积极影响。\n+ **针对特定技能。**作者们很大一部分精力都集中在收集或创建针对特定能力的数据集上。以数学推理为例，作者们在表8中展示了数学特定数据对 GSM8K 和 MATH 的影响。作者们发现数学特定的 SFT 数据显著提高了 GSM8K 和 MATH，显示了最终组合中包含的数据的价值。\n+ **SFT 数据量。**在图 4 中，作者们展示了对 SFT 组合进行分层子抽样的效果。作者们发现，随着更多 SFT 数据被纳入，作者们的模型平均水平继续提高，并且随着作者们将数据量增加到完整组合，作者们看到 GSM8K 等指标的巨大改进。有趣的是，随着组合中的数据量增加，TruthfulQA 性能实际上会下降。作者们不会将 SFT 数据大小增加到当前组合之外，因为作者们分配了其他Prompts以进行偏好优化。\n\n\n\n\n\n<!-- 这是一张图片，ocr 内容为： -->\n![表 9 SFT 消融期间的性能，显示了单独删除安全、WildChat、Persona 和数学数据的效果。作者们发现：1) 多样化的聊天数据对大多数技能都有好处，最明显的是 Alpaca Eval，2) 安全性能通常与一般性能正交，3) 作者们的新 Persona 数据集改进了它们针对的所有技能，4) 使用数学作为测试用例，添加高质量的技能特定数据可显着提高技能特定性能。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733907437046-5ad1e7b6-09a1-44ea-9572-b2de143786d7.png)\n\n\n\n## SFT  Recipe and Analyses\n\n**训练设置 **为了训练作者们的 TÜLU 3 模型，作者们使用了 4 到 16 个 8xH100 节点，并进行了高速互连。作者们使用了 128 的batch size  和 4,096 个 token 的最大序列长度。作者们训练了两个时期，对于 8B 模型，作者们使用 5e-6 的学习率，对于 70B 模型，作者们使用 2e-6 的学习率，这是作者们在超参数搜索后发现的。作者们的超参数设置也总结在表 10 中。\n\n<!-- 这是一张图片，ocr 内容为： -->\n![表 10 超参数设置](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733966749502-babedc02-8166-4694-b07d-6e74dede56b0.png)\n\n\n\n### 关键训练实验\n\n**基础模型的选择。**作者们还使用完整的 SFT 组合测试了训练不同的基础预训练模型对数学性能的影响。在表 11 中，作者们展示了通过在 Llama 3.1 8B 和 70B 上进行训练来改变模型大小的影响，以及通过在 Qwen 2.5 7B和 Qwen 2.5 Math 7B 上进行训练来添加特定领域的预训练数据的影响。在这两种情况下，作者们都看到 GSM8K 和 MATH 都有了显著的提升，凸显了模型大小和预训练数据对于下游技能的重要性。\n\n<!-- 这是一张图片，ocr 内容为： -->\n![表 11 在作者们的混合模型上训练的不同基础模型的数学性能。作者们发现 1) 在更大的模型上训练可以获得更好的性能，2) 即使对于相同大小的模型，添加特定于技能的预训练数据也可以提高性能。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733911004668-595e675d-44ef-4ec5-878e-f775b861b2a3.png)\n\n**聊天模板变化。**在创建 TÜLU3 期间，作者们探索了更改用于指导生成微调模型的聊天模板。作者们对以前 TÜLU版本中使用的聊天模板做了一些小改动，具体来说是删除了模板末尾的新行（在模型回答之前）。表 12 显示了作者们 SFT 设置的早期版本中对聊天模板的不同更改之间的性能。作者们发现用 eos token替换助手消息末尾的换行符可以获得最佳性能，但作者们选择不使用它，以避免与后训练的pipeline中的后续步骤生成不一致。\n\n<!-- 这是一张图片，ocr 内容为： -->\n![表 12  不同聊天模板对 SFT 模型性能的影响，使用 Llama 3.0 上的中间 SFT 混合进行训练。虽然替换换行符效果最好，但作者们选择简单地删除换行符以避免复杂性。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733911394844-5798dad0-b5de-43d1-9a4f-7fc33dfe7a6d.png)\n\n**随机seed和模型soups。**作者们还探索了在 SFT 期间更改随机seed，然后使用这些模型创建模型soups。在表 13 中，作者们将使用多个不同seed训练的 8B 和 70B 模型与最佳模型soups进行了比较。作者们发现 SFT 性能因seed的不同而明显不同，这凸显了多次训练运行的重要性，并且最佳模型soups并不总是优于最佳单次训练运行。因此，作者们将每种模型大小（8B和70B）的最佳单次 SFT 训练运行用作最终的 SFT 模型。\n\n<!-- 这是一张图片，ocr 内容为： -->\n![表13 使用随机seed的 8B 和 70B SFT 模型的平均性能，并与使用不同seed训练的模型的最佳模型soups进行了比较。作者们发现最佳随机seed与最佳模型soups相当，因此为了保持一致性，作者们使用最佳单次 SFT 运行作为最终的 SFT 模型。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733911575473-330bd2ab-2300-4906-b099-4c33b50a0915.png)\n\n### 批量聚合\n\n在训练 Tframework 和在其他设置（例如 TPU）上训练的模型的早期阶段。TÜLU 3 中，作者们注意到在作者们的 Open-Instruct 8上训练的 SFT 模型之间存在性能差距。作者们发现这个问题主要是由于Transformers 内部损失聚合问题造成的：在不考虑梯度累积或分布式训练设置的情况下对填充标记之间的损失进行平均。\n\n这里作者们用一个例子来说明这个问题。假设作者们在一个批次中有两个样本，其中有𝑛 1、𝑛 2 个 非填充标记和𝑚 1、 𝑚 2 个填充标记。如果作者们同时将两个样本传递到默认的 Transformers 前向传递中，作者们会得到：\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733970400478-a3eb2b6c-72d3-4a6b-ba85-2ac7b389bddc.png)\n\n当两个样本同时通过Transformer模型的前向传播时，损失函数 L 是两个样本的损失值之和除以两个样本中非填充tokens的总数。这里的 l<sub>n1 </sub>和 l<sub>n2</sub>_ _分别是第一个和第二个样本的损失值，它们是根据各自样本中的非填充tokens计算得出的。\n\n然而，如果作者们应用梯度积累，分别输入两个样本，计算损失，然后除以损失就会这样计算：\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733972383767-e5e737ce-5218-4da0-a268-e212672d124a.png)\n\n当使用梯度累积时，每个样本单独通过模型，分别计算损失，然后再将这两个损失值平均。在这种情况下，每个样本的损失值是按照其非填充tokens的数量来加权的。\n\n也就是说，在第二种情况下，作者们对每个示例赋予相同的权重，而在第一种情况下，作者们对每个标记赋予相同的权重。因此，改变梯度累积会对性能产生很大影响，因为实际上会改变样本权重。由于跨设备平均，分布式训练中也会出现类似的问题。\n\n为了解决这个问题，作者们通常选择在训练时使用总损失而不是平均损失。这只需从上述方程中删除分母即可消除问题，并需要调整学习率。这实际上对所有标记赋予了相同的权重（作者们发现这通常会使初始混合的性能更好）。作者们通过在 TÜLU2 SFT 混合上使用各种学习率、时期和损失类型对 Llama 3.0 进行微调来验证作者们设置的性能，如图 5 和图 6 所示。最终，作者们发现使用学习率为 5.00E-06 的总损失效果最好。令人惊讶的是，作者们还发现更长时间的训练并没有带来进一步的改进，因此使用了 2 个时期进行训练。\n\n<!-- 这是一张图片，ocr 内容为： -->\n![图 5 使用不同的损失类型和学习率在TÜLU2 mixture上微调 Llama3.0 时的平均性能。作者们发现 LR 为 5e-6 且带有总损失的效果最好。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733972663158-921e0ddd-4d3d-4df0-a2b7-eec1772e7fee.png)\n\n\n\n<!-- 这是一张图片，ocr 内容为： -->\n![图 6 在 TÜLU2 mixture上使用 sum loss 和 LR 5e-6 进行Llama3.0微调的不同数量的 epoch 测试。作者们发现使用 2 个 epoch 效果最好。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733972756530-41bc40f1-8a2e-4759-82e2-38ebd9f84c3d.png)\n\n#  Preference Finetuning \n\n对于TÜLU3，作者们探索了许多偏好微调方法，目的是改进整个评估套件。作者们探索了多种训练算法，从直接偏好优化 (DPO) 及其衍生算法到强化学习算法，例如近端策略优化 (PPO)。在本节中，作者们详细介绍了从人类偏好和作者们的优化器中学习的问题表述。接下来，作者们将解释如何将他们的Prompts转换为来自在线策略（TÜLU3套件）和离线策略模型（其他指导模型）的合成偏好数据。作者们展示了如何为感兴趣的特定技能创建偏好数据，以及如何使用 DPO 稳健地改进他们的模型。\n\n## 背景\n\n先前的研究已将偏好数据训练确立为提高模拟人类或合成偏好的基准模型性能的关键步骤 。典型的程序是从人类或合成反馈中进行强化学习。\n\n### 设置\n\n#### 偏好数据\n\n<!-- 这是一张图片，ocr 内容为： -->\n![图 7：偏好数据的pipeline。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733987369332-8d729cfc-ac61-4b51-890c-08c77bae27ba.png)\n\n图 7展示了<font style=\"background-color:#C1E77E;\">偏好数据集制作的流程，主要分为三个阶段：**<font style=\"background-color:#C1E77E;\">Prompt Selection（提示选择）**<font style=\"background-color:#C1E77E;\">、**<font style=\"background-color:#C1E77E;\">Response Generation（回答生成）**<font style=\"background-color:#C1E77E;\"> 和 **<font style=\"background-color:#C1E77E;\">Preference Annotation（偏好标注）**在这个阶段，会从以下三类来源中选择提示（Prompts）：\n\n\n\n+ Prompts used in SFT（监督微调中使用的提示）：用于模型的<font style=\"background-color:#C1E77E;\">监督微调训练的数据集中的提示。\n+ Prompts from datasets subsampled for SFT（从用于SFT的数据集中采样的提示）：从<font style=\"background-color:#C1E77E;\">更大的训练数据集中挑选出一部分提示。\n+ New OOD prompts（超出分布的提示，例如Ultrafeedback和Persona）：设计<font style=\"background-color:#C1E77E;\">新的、超出模型训练分布的提示，以测试模型在新领域上的表现。\n\n\n\n**Response Generation（回答生成）**这里提到了一个模型池，包含22个不同的模型，其中回答生成的过程包括：\n\n\n\n+ Off-policy data（离策略数据）：由其他模型生成的回答（如Meta、Gemma、技术创新研究所等）。这些模型生成的回答可能会被标注为“较差”或“被拒绝”。\n+ On-policy data（策略内数据）：由Tülu 3（SFT 8B或SFT 70B）生成的回答，这些回答可能是高质量、符合偏好的答案。\n\n\n\n**Preference Annotation（偏好标注）** <font style=\"background-color:#C1E77E;\"> 在这个阶段，使用**<font style=\"background-color:#C1E77E;\">GPT-4o**<font style=\"background-color:#C1E77E;\">协助标注回答质量，具体操作包括：\n\n\n\n+ 标注维度：对每个生成的回答，<font style=\"background-color:#C1E77E;\">从以下四个方面进行评分（1到5分）：\n  - <font style=\"background-color:#C1E77E;\">Helpfulness（有用性）：回答是否对用户有帮助。\n  - <font style=\"background-color:#C1E77E;\">Instruction Following（指令遵循性）：是否严格按照提示指令进行回答。\n  - <font style=\"background-color:#C1E77E;\">Truthfulness（真实性）：回答内容是否真实、准确。\n  - <font style=\"background-color:#C1E77E;\">Honesty（诚实性）：回答是否有误导性或隐瞒信息。\n+ Binarize（二值化）：将评分结果转化为“Chosen（选择）”或“Rejected（拒绝）”：\n  - Chosen：得分较高的回答。\n  - Rejected：得分较低的回答。\n\n**GPT-4o 不直接生成回答，而是作为标注助手，为每个回答在不同维度上给出具体评分。**除了判断“好”或“坏”，还可以提供更精细的质量维度（如有用性、指令遵循性等），以便后续更细粒度地优化模型。  \n\n\n\n\n\n<!-- 这是一张图片，ocr 内容为： -->\n![图 8 Huggingface上公开的TÜLU3 偏好数据集](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733984737378-80bae173-585a-4383-b69b-f00456fc771e.png)\n\n#### 奖励模型\n\n给定偏好数据集，训练奖励模型 (RM) 𝑟<sub> 𝜙</sub>，目标如下：\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://cdn.nlark.com/yuque/0/2024/png/26978393/1734075926463-03cc0b3f-b9f6-473b-847c-f87c6dc43276.png)\n\n\n\n+ 数据集 $ \\mathcal{D} $包含了一些提示（prompts） x 和两个候选回答 y和 y′。\n+ 对于每个提示 x，一些评审者会从 y 和 y′ 中选择一个作为更好的回答 y<sub>c</sub>，另一个会被标记为被拒绝的回答 y<sub>r</sub>。\n+ $ r_{\\phi}(x, y) $：奖励模型对于输入 x和回答 y的评分。\n+ $ \\sigma(z)\n  $：激活函数 Sigmoid  ，$ \\sigma(z)=\\frac{1}{1+e^{-z}} $ ，将分数差转化为一个概率值。\n+ $ (x,y_c,y_r)\\sim D $：从偏好数据集中采样的一个三元组，其中y<sub>c</sub>是选择的回答， y<sub>r</sub>是被拒绝的回答。\n+ $ r_ϕ(x,y_c)−r_ϕ(x,y_r) $：表示奖励模型认为选择回答y<sub>c</sub>与被拒绝回答 y<sub>r</sub>的分数差。\n+ $ logσ(r_ϕ(x,y_c)−r_ϕ(x,y_r)) $：表示根据分数差的对数似然，描述了模型将 y<sub>c</sub> 视为选择回答的概率， 它越接近 0，意味着对应的原始概率$ σ(z) $越高。  \n\n**优化目标**：最大化这一对数似然，强化奖励模型对选择回答与被拒绝回答的区分能力。\n\n\n\n<!-- 这是一张图片，ocr 内容为： -->\n![Sigmoid函数  𝜎 ( 𝑧 ) 的图像](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733986047166-8b0b8cf6-dc2d-4d7e-9d65-032b09a1655c.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![对数似然函数](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733986183839-46740210-40a0-4efd-9316-a44710befd6a.png)\n\n### 5.1.2\t策略优化\n\n#### PPO\n\n用 KL 散度作为正则化项，限制模型更新的步长。模型希望最大化奖励（生成更符合人类偏好的内容），但模型也需要受限于“不要偏离预训练模型太远”，以避免生成奇怪、不稳定的内容。<font style=\"background-color:#C1E77E;\">KL 散度提供了一个度量工具，$ \\beta $<font style=\"background-color:#C1E77E;\">则决定了偏离的允许程度。\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733987758189-f84ec593-4cf5-44e5-8585-1a2c050ae389.png)\n\n\n\n+ $ π_θ(y|x) $: <font style=\"background-color:#F1A2AB;\">当前训练的语言模型的策略（生成 y 的概率分布）。\n+ $ π_{ref}(y|x) $: 初始参考策略（预训练模型的策略，用于保持生成结果的稳定性）。\n+ $ KL[π_θ(y|x)∥π_{ref}(y|x)] $: <font style=\"background-color:#F1A2AB;\">Kullback-Leibler (KL) 散度，用来衡量当前策略$  \\pi_\\theta $<font style=\"background-color:#F1A2AB;\">偏离参考策略 $ \\pi_\\text{ref} $<font style=\"background-color:#F1A2AB;\">的程度。\n+ $ r_\\phi(x, y) $: 奖励模型的输出，表示对输入 x和生成结果 y的质量评分。\n+ $ \\beta $: 控制 KL 散度的权重，平衡模型对奖励最大化和与参考策略保持接近之间的取舍。\n\n**目标**：\n\n+ <font style=\"background-color:#F1A2AB;\">目标是优化策略$ \\pi_\\theta $<font style=\"background-color:#F1A2AB;\">，使得生成的结果 y 在给定 x 时能获得最高的奖励 R(x,y)。\n+ 奖励 R(x,y) 被分为两部分：\n  1. $ r_\\phi(x, y) $: 奖励模型给出的偏好评分。\n  2. $ \\beta \\text{KL}[\\pi_\\theta \\| \\pi_\\text{ref}] $: 惩罚项，限制 $ \\pi_\\theta $偏离$ \\pi_\\text{ref} $ 太远。\n\n**作用**：\n\n+ **奖励最大化**：通过$ r_\\phi(x, y) $让策略生成更符合人类偏好的内容。\n+ **稳定性控制**：通过 KL 散度，确保模型不会偏离初始策略太远，避免生成质量崩塌或出现极端行为。\n+ **权重 **$ \\beta $：\n+ $ \\beta $控制两部分的权衡：\n  - 如果 $ \\beta $较大，模型更倾向于保持和参考策略接近。\n  - 如果 $ \\beta $较小，模型更倾向于追求奖励最大化，但可能会导致生成不稳定。\n\n\n\n#### DPO\n\n直接用偏好数据优化策略，避免大幅偏离初始参考策略，其等效目标如下：\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733987815637-9132e8ab-b14d-4a37-8a4e-adaa71196b14.png)\n\n\n**输入数据**：\n\n+ 数据集$ \\mathcal{D} $包含一对偏好标注 $ (y_c, y_r) $：\n  - $ y_c $：优选项（chosen）。\n  - $ y_r $：非优选项（rejected）。\n+ $ x $是输入上下文。\n\n**目标函数的核心**：\n\n+ 分数差：<font style=\"background-color:#F1A2AB;\">分数的计算基于模型 $ \\pi_\\theta $<font style=\"background-color:#F1A2AB;\">和参考策略 $ \\pi_\\text{ref} $<font style=\"background-color:#F1A2AB;\">的对数比值（即对数概率比）。优选项 $ y_c $ 和非优选项 $ y_r $的分数差定义为：\n  - $ \\Delta = \\beta \\log \\frac{\\pi_\\theta(y_c|x)}{\\pi_\\text{ref}(y_c|x)} - \\beta \\log \\frac{\\pi_\\theta(y_r|x)}{\\pi_\\text{ref}(y_r|x)}. $\n  - $ \\beta $是一个权重参数，用于调整策略和参考策略之间的偏离程度。\n\n\n\n上面的$ \\Delta  $可进一步简化，最终为：\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733993906272-1fd6f914-badd-421b-8f35-8572a482754a.png)\n\n其中，假设 𝑦<sub>c </sub> 是由多个 token 组成的序列： 𝑦 = ( 𝑦<sub>1</sub> , 𝑦<sub>2 </sub>, … , 𝑦<sub>𝑇 </sub>)，那么条件概率可以被分解为：\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733994381498-0928e7aa-a9f8-4921-919e-7418c4b7b722.png)\n\n在生成第 t个 token y<sub>t </sub>时<font style=\"background-color:#F1A2AB;\">，模型不仅参考输入 x，还参考之前已经生成的 token $ y1,…,y_{t−1} $。  \n\n\n在 DPO的过程中，希望模型学会对好的回答 y<sub>c</sub> 赋予 **更高的概率**，也就是 $ \\pi_\\theta(y_c|x) $<font style=\"color:#DF2A3F;\">更大。   同时，对差的回答 y<sub>r </sub>赋予 **更低的概率**，也就是$ \\pi_\\theta(y_r|x) $<font style=\"color:#DF2A3F;\">更小。   在优化模型参数$ 𝜃 $的过程中， 逐步学习从输入 x到优质输出 y的映射关系，即优化 $ \\pi_\\theta(y|x) $的分布，使其更贴合偏好数据  ，逐步让 **优质回答的生成概率**$ \\pi_\\theta(y_c|x) $超过劣质回答的生成概率$ \\pi_\\theta(y_r|x) $。\n\n\n\n#### Length-Normalized DPO\n\n作者在上述DPO的基础上，进一步加入了归一化来消除，因回答的序列长度（对比来说很长的话）带来的概率偏好影响。\n\n\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733987832023-fcaccdf2-3f2d-47b9-82ea-0313deae8697.png)\n\n其中，假设 𝑦<sub>c </sub> 是由多个 token 组成的序列： 𝑦 = ( 𝑦<sub>1</sub> , 𝑦<sub>2 </sub>, … , 𝑦<sub>𝑇 </sub>) ，那么条件概率可以被分解为：\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733994381498-0928e7aa-a9f8-4921-919e-7418c4b7b722.png)\n\n其对数概率为：\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://cdn.nlark.com/yuque/0/2024/png/26978393/1733995424182-517e89a3-a0f5-429d-bd5c-b259989cd77e.png)\n\n\n\n+ <font style=\"background-color:#F1A2AB;\">较长的序列 y 会累积更多的概率项，即累积更多的对数值，即使单个词的概率较低，其整体对数概率可能仍然较高。因此，较长的序列可能会在优化目标中获得更大的得分，进而被模型优先选择。\n\n\n\n####  DOP和Length-Normalized DPO 的区别\n\n\n**长度归一化**：\n\n+ 在分数计算中，加入了长度归一化项$ \\frac{1}{|y|} $，分别对 y<sub>c</sub>和 y<sub>r</sub>的分数进行长度调整。\n+ 直观理解：<font style=\"background-color:#F1A2AB;\">较长的输出序列通常会累积更大的对数概率，从而可能产生长度偏差。长度归一化可以减轻这种偏差。\n\n**优化目标**：\n\n+ 在 DPO 的基础上，引入长度归一化，适配更广泛的人类偏好标注数据，特别是在输出长度差异较大的情况下。\n\n\n\n#### DPO 和 Length-Normalized DPO 的特点\n\n\n**DPO**：\n\n+ 不需要显式利用训练奖励模型评分，直接优化策略 $ \\pi_\\theta $。\n+ 通过分数差 $ \\Delta $表达偏好关系。\n+ 简洁高效，但可能受输出长度偏差影响。\n\n**Length-Normalized DPO**：\n\n+ <font style=\"background-color:#F1A2AB;\">在 DPO 基础上加入长度归一化，减轻模型生成偏好较长输出的倾向。\n+ 更加适配包含长度偏差的标注数据。\n\n\n\n#### DPO和PPO对比\n\n| 对比维度         | DPO                        | PPO                                        |\n| ---------------- | -------------------------- | ------------------------------------------ |\n| 否需要奖励模型   | 不需要（直接用偏好对）     | 需要训练一个奖励模型                       |\n| 训练复杂度       | 较低（只优化策略模型）     | 较高（需要优化策略、奖励、价值等多个组件） |\n| 依赖强化学习     | 不需要                     | 需要                                       |\n| 对人类偏好的利用 | 更直接（通过概率对比）     | 间接（通过奖励模型评估）                   |\n| 实现难度         | 实现简单                   | 实现复杂                                   |\n| KL 控制          | 内置在偏好对比中（更直观） | 需要显式控制 KL 惩罚项                     |\n\n\n## TÜLU3偏好数据\n\n### 从Prompts到偏好数据\n\n根据 §3 中的Prompts，作者们通过调整和改进偏好数据的pipeline创建符合策略的偏好数据 ( 𝑥,𝑦,𝑦 ′,𝑙𝑎𝑏𝑒𝑙 ) 。作者们的早期实验表明，该流程在创建偏好数据方面具有优势，可以生成高质量的合成偏好数据集。作者们的数据创建流程（如图 7 所示）包括三个阶段：Prompts选择、从模型池生成回答以及使用 LLM-as-a-judge 进行偏好注释以创建（偏好、拒绝）对。\n\n+ **第 1 阶段：Prompts选择**。准备数据集以进行偏好微调的第一步是选择Prompts或用户指令以生成回答并获取偏好。鉴于表 6 中的Prompts集，作者们精心挑选了作者们的选择，以包括 SFT 期间使用的Prompts，以及从相同来源抽样但未用于 SFT 的Prompts。作者们还包括来自其他来源的Prompts，例如没有 TruthfulQA 实例的 Ultrafeedback 版本，或者通过向Prompts添加新的 IF 约束。\n+ **第 2 阶段：回答生成。**对于给定的Prompts，作者们从模型池中随机抽取四个模型来生成回答。作者们的模型选择受到 Ultrafeedback pipeline的启发，该pipeline由开源和专有模型组成，这些模型在参数大小和模型系列上有所不同。作者们通过使用某些模型的最新版本（Llama 2 → Llama 3.1）来更新 Ultrafeedback 的模型池，添加性能最佳的模型以增加池大小，并用开源替代方案替换当前无法访问的模型（例如 WizardLM）。最后，作者们还通过从 TÜLU SFT 模型中抽样完成来包含在线策略数据。作者们通过添加一系列Prompts来实现这一点，其中一个回答来自在线策略模型，另一个回答来自非策略模型。\n+ **第 3 阶段：偏好注释**。在为每个Prompts生成四个回答后，作者们使用 LLM-as-a-judge，特别是 GPT-4o-2024-0806，从四个不同方面对每个回答从 1 到 5 进行评分：有帮助性、遵循指示、诚实和真实性。\n\n### TÜLU3偏好组合\n\n<!-- 这是一张图片，ocr 内容为： -->\n![表 14：作者们对 TÜLU3 8B DPO 和 TÜLU3 70B DPO的最佳偏好数据集组合的总结。IF 是“Instruction Following”的缩写。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1734056115830-02fe37f0-af83-469e-838e-3c71315c0771.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![表 15 显示了偏好数据集的包含或排除如何影响平均性能。](https://cdn.nlark.com/yuque/0/2024/png/26978393/1734056509663-f53c60ca-05af-40ee-8a98-8274b49a3a06.png)\n\n作者们为 8b 和 70b 模型选择了最终的偏好组合，这可以最大限度地提高开发评估的平均表现，同时在目标技能方面也表现出色。大多数偏好数据组合消融都是针对 8b 模型运行的，作者们从用于 SFT 的Prompts开始，生成在策略和离策略偏好数据，从而产生 96911 个（拒绝策略）和 19444 个（接受策略）偏好实例。根据这个偏好基础，作者们向组合中添加其他Prompts源，以及这些添加如何影响下游评估性能，特别是针对诸如精确指令遵循、数学和 alpacaeval 上的一般聊天表现等技能。总之，作者们的偏好组合来自不同的Prompts源，例如 SFT 数据、WildChat 和 Persona IF。它包括 SFT 训练期间看到的Prompts，也包括新的、未见过的Prompts。最后，它包含在线和离线策略完成的混合。\n\n### 数据消融的主要发现\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n### \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["NVIDIA","V100","论文解读","tulu3"],"categories":["论文解读","tulu3"]},{"title":"基于Easy-Dataset的律所数据集处理","url":"/2026/01/08/LLM/客户支撑/某律所大模型微调/基于Easy-Dataset的律所数据集处理/","content":"\n### 环境\n\n```bash\ngit clone https://github.com/ConardLi/easy-dataset.git\ncd easy-dataset\nnpm install\nnpm run build\nnpm run start\n```\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/law/easydataset/image1.png)\n\n### 模型测试\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/law/easydataset/image2.png)\n\n\n\n\n\n","tags":["昇腾","910B","数据集处理","Easy-Dataset"],"categories":["数据集处理","Easy-Dataset"]},{"title":"llama-factory环境搭建-昇腾","url":"/2026/01/08/LLM/客户支撑/某律所大模型微调/llama-factory环境搭建-昇腾/","content":"\n### 训练\n\n```bash\nconda create --name llamafactory python=3.10\n\nconda activate llamafactory\n\npip install -e \".[torch-npu,metrics]\"\n\npip install --upgrade torch==2.6.0 -i https://pypi.tuna.tsinghua.edu.cn/simple\n\npip install --upgrade torch-npu==2.6.0rc1 -i https://pypi.tuna.tsinghua.edu.cn/simple\n\npip uninstall scikit-learn\n\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n\nllamafactory-cli train examples/train_lora/qwen3_lora_sft.yaml\n```\n\n### 推理\n\n```bash\nllamafactory-cli chat examples/inference/qwen3_lora_sft.yaml\n\nvllm推理\nsudo apt update\n\nsudo apt install libnuma-dev\n\npip install vllm==0.8.5 -i https://pypi.tuna.tsinghua.edu.cn/simple\n\npip install vllm-ascend==\n\nASCEND_RT_VISIBLE_DEVICES=2,4 llamafactory-cli chat examples/inference/qwen3_lora_sft.yaml\n\n\nbash Ascend-cann-nnal_8.0.0_linux-aarch64.run --install\n\nsource /usr/local/Ascend/nnal/atb/set_env.sh\n```\n\n\n\n\n\n","tags":["昇腾","910B","训练环境搭建","llama-factory"],"categories":["训练环境搭建","llama-factory"]},{"title":"基于llama-factory的法律模型微调","url":"/2026/01/08/LLM/客户支撑/某律所大模型微调/基于llama-factory的律所模型微调/","content":"\n## 数据集\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/law/finetune/image1.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/law/finetune/image2.png)\n\n## 微调\n\n### lora\n\n#### deepseek-r1-distill-qwen-7b\n\n##### 训练 log\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/law/finetune/image3.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/law/finetune/image4.png)\n\n##### loss 曲线\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/law/finetune/image5.png)\n\n##### lora 优化器与权重合并\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/law/finetune/image6.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/law/finetune/image7.png)\n\n#### qwen3-8b\n\n##### 训练 log\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/law/finetune/image8.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/law/finetune/image9.png)\n\n\n\n##### loss 曲线\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/law/finetune/image10.png)\n\n##### lora 优化器与权重合并\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/law/finetune/image11.png)\n\n<!-- 这是一张图片，ocr 内容为： -->\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/llm/law/finetune/image12.png)\n\n## 推理\n\n\n\n","tags":["昇腾","910B","模型微调","llama-factory"],"categories":["模型微调","llama-factory"]},{"title":"2.1 数据集处理","url":"/2025/11/13/模型训练调优/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.1 Deepseek-7B数据集处理/","content":"#### 下载初始数据集\n1. 下载WuDaoCorpora2.0开源数据集到/mnt/workspace/llama3-datasets工作目录下\n\n```bash\nwget https://atp-modelzoo.oss-cn-hangzhou.aliyuncs.com/release/datasets/WuDaoCorpus2.0_base_sample.tgz\ntar zxvf WuDaoCorpus2.0_base_sample.tgz \n```\n\n#### Megatron-LM预训练数据清洗\nmmap数据是一种预先执行tokenize处理的数据格式，可以极大减少训练微调过程中等待数据读入的时间，当数据量极大时，优势显著。\n\n```bash\n#! /bin/bash\nset -ex\n# 请在此处设置原始数据所在路径\ndata_dir=/mnt/users/wangxiangbo/dataset/WuDaoCorpus2.0_base_sample\n\n#开始数据清洗流程\ndataset_dir=$(dirname $data_dir)\nmkdir -p ${dataset_dir}/cleaned_wudao_dataset\ncd ${dataset_dir}/cleaned_wudao_dataset\nwget https://atp-modelzoo-wlcb-pai.oss-cn-wulanchabu.aliyuncs.com/release/models/pai-megatron-patch/llama2-codes/preprocess_wudao2.py\n# 此处与上一节不同，增加了key参数设为text\npython3 preprocess_wudao2.py -i ${data_dir} -o ${dataset_dir}/cleaned_wudao_dataset -k text -p 32\n\n# 合并清洗后的数据\nmkdir ${dataset_dir}/wudao\ncd ${dataset_dir}/wudao\nfind ${dataset_dir}/cleaned_wudao_dataset -name \"*.json\" -exec cat {} + > ${dataset_dir}/wudao/merged_wudao_cleaned.json\nrm -rf ${dataset_dir}/cleaned_wudao_dataset\n```\n\n脚本执行完成后，会得到一个合并的json数据集格式\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image1.png)\n\n#### 执行下方的处理数据脚本\n```bash\n#! /bin/bash\nSTART_TIME=$SECONDS\n\nMEGATRON_PATCH_PATH=/data/code\nMEGATRON_PATH=${MEGATRON_PATCH_PATH}/Megatron-LM\n\nexport PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATCH_PATH}:$PYTHONPATH\n\ninput_data_dir=/mnt/dataset/wudao/merged_wudao_cleaned.json\ntokenizer=DeepseekTokenizer\njson_keys=text\noutput_data_dir=/data/code/temp/deepseek/deepseek-datasets\nload_dir=/data/code/temp/deepseek/deepseek-ckpt\nTOKENIZER_PATH=/data/code/temp/deepseek/deepseek-ckpt\n\nINPUT=\"${input_data_dir}\"\n\nif [ $tokenizer = \"Qwen2Tokenizer\" ]; then\n  python preprocess_data_megatron.py \\\n  --input ${INPUT} \\\n  --output-prefix ${output_data_dir}/mmap_qwen2_datasets \\\n  --patch-tokenizer-type Qwen2Tokenizer \\\n  --json-keys ${json_keys} \\\n  --load ${load_dir} \\\n  --workers 2 \\\n  --partitions 2 \\\n  --keep-sequential-samples \\\n  --append-eod\n\nelif [ $tokenizer = \"DeepSeekV2Tokenizer\" ]; then\n  python preprocess_data_megatron.py \\\n  --input ${INPUT} \\\n  --output-prefix ${output_data_dir}/mmap_deepseekv2_datasets \\\n  --patch-tokenizer-type DeepSeekV2Tokenizer \\\n  --json-keys ${json_keys} \\\n  --load ${load_dir} \\\n  --workers 8 \\\n  --partitions 1 \\\n  --keep-sequential-samples \\\n  --append-eod\n\nelif [ $tokenizer = \"LLamaTokenizer\" ]; then\n  python preprocess_data_megatron.py \\\n  --tokenizer-model $TOKENIZER_PATH \\\n  --input ${INPUT} \\\n  --output-prefix ${output_data_dir}/mmap_llama_datasets \\\n  --patch-tokenizer-type LLamaTokenizer \\\n  --load ${load_dir} \\\n  --workers 16 \\\n  --partitions 1 \\\n  --keep-sequential-samples \\\n  --append-eod\n\nelif [ $tokenizer = \"DeepseekTokenizer\" ]; then\n  python preprocess_data_megatron.py \\\n  --tokenizer-model $TOKENIZER_PATH \\\n  --input ${INPUT} \\\n  --output-prefix ${output_data_dir}/mmap_deepseek_datasets \\\n  --patch-tokenizer-type LLamaTokenizer \\\n  --load ${load_dir} \\\n  --workers 16 \\\n  --partitions 1 \\\n  --keep-sequential-samples \\\n  --append-eod\n\nelif [ $tokenizer = \"LLama2Tokenizer\" ]; then\n  python preprocess_data_megatron.py \\\n  --tokenizer-model $TOKENIZER_PATH \\\n  --extra-vocab-size 0 \\\n  --input ${INPUT} \\\n  --output-prefix ${output_data_dir}/mmap_llama2_datasets \\\n  --patch-tokenizer-type LLama2Tokenizer \\\n  --load ${load_dir} \\\n  --workers 16 \\\n  --partitions 1 \\\n  --keep-sequential-samples \\\n  --append-eod\n\nelif [ $tokenizer = \"LLama3Tokenizer\" ]; then\n  python preprocess_data_megatron.py \\\n  --input ${INPUT} \\\n  --output-prefix ${output_data_dir}/mmap_llama3_datasets \\\n  --patch-tokenizer-type LLama3Tokenizer \\\n  --load ${load_dir} \\\n  --workers 16 \\\n  --partitions 1 \\\n  --keep-sequential-samples \\\n  --append-eod\n\nfi\n\nELAPSED_TIME=$(($SECONDS - $START_TIME))\necho \"$(($ELAPSED_TIME/60)) min $(($ELAPSED_TIME%60)) sec\"\n```\n\n在python preprocess_data_megatron.py中，加入DeepseekTokenizer，此外还需要在/data/code/megatron_patch/tokenizer/__init__.py中加入DeepseekTokenizer配置。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image2.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image3.png)\n\n#### 数据处理完成\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image4.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image5.png)\n\n","tags":["V100","模型适配","Nvidia","Deepseek","Megatron-LM"],"categories":["模型适配","NVIDIA"]},{"title":"2.3 Deepseek-7B权重转换","url":"/2025/11/13/模型训练调优/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.3 Deepseek-7B权重转换/","content":"#### Hg-Core权重转换\n##### huggingface开源权重准备\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image17.png)\n\n##### 权重转换脚本\n```bash\n#!/bin/bash\n\nexport CUDA_VISIBLE_DEVICES=0\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nSTART_TIME=$SECONDS\nMASTER_ADDR=localhost\nMASTER_PORT=6666\n\nMODEL_SIZE=7B\nHG_CKPT_PATH=/mnt/model/deepseek-ai/deepseek-llm-7b-base\nMEGATRON_PATCH_PATH=/data/code/\nMEGATRON_PATH=${MEGATRON_PATCH_PATH}/Megatron-LM\nexport PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATCH_PATH}:$PYTHONPATH\nSOURCE_CKPT_PATH=/mnt/model/deepseek-ai/deepseek-llm-7b-base\nTARGET_CKPT_PATH=/mnt/model/deepseek-ai/deepseek-llm-7b-core\nTP=4\nPP=1\nEXTRA_VOCAB_SIZE=2400\nNUM_EXPERTS=0\nEXPERTS_TOPK=0\nEP=0\nNUM_EXPERT_SPLITS=0\nmg2hf=false\n\nif [ $MODEL_SIZE = 7B ]; then\n\nNUM_LAYERS=30\nHIDDEN_SIZE=4096\nNUM_ATTN_HEADS=32\nINTERMEDIATE_SIZE=11008\nNUM_KV_HEADS=32\nVOCAB_SIZE=102400\nROPE_THETA=10000\nRMS_NORM_EPS=1e-6\ngqa_options=\"\"\n\nfi\n\nif [ $NUM_EXPERT_SPLITS -gt 0 ]; then\n\nINTERMEDIATE_SIZE=$(( ${INTERMEDIATE_SIZE} / ${NUM_EXPERT_SPLITS}))\n\nfi\n\nif [ $NUM_EXPERTS -gt 0 ]; then\n    expert_options=\"\n                --moe-router-topk ${EXPERTS_TOPK} \\\n                --num-experts ${NUM_EXPERTS} \\\n                --expert-model-parallel-size 1 \\\n                --target_expert_model_parallel_size ${EP} \\\n                --num_expert_split_size ${NUM_EXPERT_SPLITS} \\\n    \"\nfi\n\nif [ $mg2hf = true ]; then\n    convert_options=\"\n                --convert_checkpoint_from_megatron_to_transformers\n    \"\nelif [ $mg2hf = false ]; then\n    convert_options=\"\"\nfi\n\n\n\nDISTRIBUTED_ARGS=\"--nproc_per_node 1 --nnodes 1 --node_rank 0 --master_addr $MASTER_ADDR --master_port $MASTER_PORT\"\n\nif [ $MODEL_SIZE != 70B ]; then\n\ntorchrun ${DISTRIBUTED_ARGS} hf2mcore.py \\\n    --load_path ${SOURCE_CKPT_PATH} \\\n    --save_path ${TARGET_CKPT_PATH} \\\n    --tokenizer-model ${HG_CKPT_PATH} \\\n    --huggingface_model_path ${HG_CKPT_PATH} \\\n    --megatron-path ${MEGATRON_PATH} \\\n    --target_tensor_model_parallel_size ${TP} \\\n    --target_pipeline_model_parallel_size ${PP} \\\n    --micro-batch-size 1 \\\n    --fp16 \\\n    --swiglu \\\n    --num-layers ${NUM_LAYERS} \\\n    --hidden-size ${HIDDEN_SIZE} \\\n    --ffn-hidden-size ${INTERMEDIATE_SIZE} \\\n    --norm-epsilon ${RMS_NORM_EPS}\\\n    --num-attention-heads ${NUM_ATTN_HEADS} \\\n    --max-position-embeddings 4096 \\\n    --seq-length 1 \\\n    --no-async-tensor-model-parallel-allreduce \\\n    --patch-tokenizer-type DeepseekTokenizer \\\n    --extra-vocab-size ${EXTRA_VOCAB_SIZE} \\\n    --untie-embeddings-and-output-weights \\\n    --no-rope-fusion \\\n    --use-rotary-position-embeddings \\\n    --rotary-base ${ROPE_THETA} \\\n    --transformer-impl transformer_engine \\\n    --disable-bias-linear \\\n    --normalization RMSNorm \\\n    --use-mcore-models \\\n    --attention-dropout 0.0 \\\n    --hidden-dropout 0.0 \\\n    ${expert_options} \\\n    ${convert_options} \\\n    ${gqa_options}\n\nelse\npython hf2mcore_70b.py \\\n  --load ${HG_CKPT_PATH} \\\n  --megatron-path ${MEGATRON_PATH} \\\n  --load_path ${SOURCE_CKPT_PATH} \\\n  --save_path ${TARGET_CKPT_PATH} \\\n  --target_params_dtype bf16 \\\n  --target_tensor_model_parallel_size ${TP} \\\n  --target_pipeline_model_parallel_size ${PP} \\\n${convert_options} \\\n\nfi\n\nELAPSED_TIME=$(($SECONDS - $START_TIME))\necho \"$(($ELAPSED_TIME/60)) min $(($ELAPSED_TIME%60)) sec\"\n```\n\n##### tokenizer配置\n在/data/code/megatron_patch/tokenizer/__init__.py中加入Deepseek配置\n\n```python\nelif args.patch_tokenizer_type == 'DeepseekTokenizer':\n        from megatron.core.datasets.megatron_tokenizer import MegatronTokenizer\n        class _DeepseekTokenizer(MegatronTokenizer):\n            def __init__(self, tokenizer_path, extra_vocab_size):\n                super().__init__(tokenizer_path)\n                self.tokenizer = AutoTokenizer.from_pretrained(\n                    tokenizer_path,\n                    padding_side=\"right\",\n                    use_fast=False,\n                    trust_remote_code=True,\n                )\n                self.extra_vocab_size = extra_vocab_size\n\n            def __call__(self, text, return_tensors=None,\n                         padding=None, max_length=None, truncation=None, add_special_tokens=True):\n\n                return self.tokenizer(text, return_tensors=return_tensors, padding=padding,\n                        max_length=max_length, truncation=truncation, add_special_tokens=add_special_tokens)\n\n            @property\n            def vocab_size(self):\n                return self.tokenizer.vocab_size + self.extra_vocab_size\n\n            @property\n            def vocab(self):\n                return self.tokenizer.encoder\n\n            @property\n            def inv_vocab(self):\n                return self.tokenizer.decoder\n\n            def tokenize(self, text):\n                return self.tokenizer.encode(text)\n\n            def detokenize(self, token_ids):\n                return self.tokenizer.decode(token_ids)\n\n            @property\n            def eod(self):\n                return self.tokenizer.eos_token_id\n\n            @property\n            def eos_token(self):\n                return self.tokenizer.eos_token\n\n            @property\n            def pad_token_id(self):\n                return self.tokenizer.pad_token_id\n\n            @property\n            def eos_token_id(self):\n                return self.tokenizer.eos_token_id\n\n        tokenizer = _DeepseekTokenizer(args.tokenizer_model, args.extra_vocab_size)\n        # tokenizer = _DeepseekTokenizer(args.load, args.extra_vocab_size)\n        args.padded_vocab_size = tokenizer.vocab_size\n```\n\n##### hf2mcore.py修改\n在/data/code/toolkits/model_checkpoints_convertor/deepseek_core/hf2mcore.py中修改导包位置\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image18.png)\n\n##### 执行转换脚本，转换流程\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image19.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image20.png)\n\n##### 转换结束，得到core权重\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image21.png)\n\n#### core权重的推理结果验证\n##### core权重推理结果\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image22.png)\n\n##### huggingface权重推理结果\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image23.png)\n\n\n\n","tags":["V100","模型适配","Nvidia","Deepseek","Megatron-LM"],"categories":["模型适配","NVIDIA"]},{"title":"2.2 Deepseek-7B预训练适配","url":"/2025/11/13/模型训练调优/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.2 Deepseek-7B预训练适配/","content":"#### 预训练物料及代码准备\n##### Deepseek-7B的Tokenizer下载\n位置位于/data/code/temp/deepseek/deepseek-ckpt\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image6.png)\n\n##### 数据集准备\n位置位于/data/code/temp/deepseek/deepseek-datasets\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image7.png)\n\n##### pretrain_mcore_deepseek.sh预训练脚本\n```bash\n\nexport NCCL_P2P_DISABLE=1\nexport NCCL_DEBUG=INFO\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\n\nMEGATRON_PATCH_PATH=/data/code \nMEGATRON_PATH=${MEGATRON_PATCH_PATH}/Megatron-LM\n\nexport PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATCH_PATH}:$PYTHONPATH\n\nNNODES=2\nNODE_RANK=0\nGPUS_PER_NODE=4\nMASTER_ADDR=192.168.0.22\nMASTER_PORT=29500\n\nDISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT\"\n\nMODEL_SIZE=7B\nBATCH_SIZE=1\nGLOBAL_BATCH_SIZE=8\nLR=1e-5\nMIN_LR=1e-6\nSEQ_LEN=1024\nPAD_LEN=1024\nEXTRA_VOCAB_SIZE=0\nPR=fp16\nTP=4\nPP=1\nAC=sel\nDO=true\nFL=false\nSP=true\nTE=true\nMOE=false\nSAVE_INTERVAL=1000\nDATASET_PATH=/data/code/temp/deepseek/deepseek-datasets/mmap_deepseek_datasets_text_document\n#PRETRAIN_CHECKPOINT_PATH=none\nTOKENIZER_PATH=/data/code/temp/deepseek/deepseek-ckpt\nTRAIN_TOKENS=10000000\nWARMUP_TOKENS=0\nOUTPUT_BASEPATH=/data/code/temp/output/deepseek/pretrain_output\n\nif [ $MODEL_SIZE = 7B ]; then\n\nNUM_LAYERS=32\nHIDDEN_SIZE=4096\nNUM_ATTN_HEADS=32\nINTERMEDIATE_SIZE=11008\nMAX_POSITION_EMBEDDINGS=4096\n\ngqa_options=\"\"\n\n\nfi\n\nif [ $AC = full ]; then\n    activation_checkpoint_options=\" \\\n\t\t    --recompute-method uniform \\\n\t\t    --recompute-granularity full\"\nelif [ $AC = sel ]; then\n    activation_checkpoint_options=\" \\\n        --recompute-activations\"\nelif [ $AC = none ]; then\n    activation_checkpoint_options=\" \\\n    \"\nfi\n\nif [ $PR = fp16 ]; then\n    pr_options=\" \\\n\t\t    --fp16 \\\n            --apply-query-key-layer-scaling\"\n    export NVTE_APPLY_QK_LAYER_SCALING=1\nelif [ $PR = bf16 ]; then\n    pr_options=\" \\\n        --bf16\"\nelif [ $PR = fp8 ]; then\n    pr_options=\" \\\n        --bf16 \\\n        --fp8-hybrid \\\n        --fp8-amax-compute-algo max \\\n        --fp8-amax-history-len 1024 \\\n        --transformer-impl transformer_engine\"\nfi\n\nif [ $DO = true ]; then\n    do_options=\" \\\n\t\t    --use-distributed-optimizer\"\n\nelif [ $DO = false ]; then\n    do_options=\" \\\n                    \"\nfi\n\nif [ $FL = true ]; then\n    flash_options=\" \\\n\t\t    --use-flash-attn\"\n\nelif [ $FL = false ]; then\n    flash_options=\" \\\n                    \"\nfi\n\nif [ $TE = true ]; then\n    te_options=\" \\\n\t\t    --transformer-impl transformer_engine\"\n\nelif [ $TE = false ]; then\n    te_options=\" \\\n                    \"\nfi\n\n\nif [ $MOE = true ]; then\n    moe_options=\" \\\n\t\t    --moe-router-topk 1 \\\n\t\t    --num-experts 4 \\\n\t\t    --expert-model-parallel-size 2\"\n\nelif [ $MOE = false ]; then\n    moe_options=\" \\\n                    \"\nfi\n\nif [ $SP = true ] && [ $TP -gt 1 ]; then\n    sp_options=\" \\\n\t\t    --sequence-parallel\"\n\nelif [ $SP = false ]; then\n    sp_options=\" \\\n                    \"\nfi\n\nif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then\n    load_options=\" \\\n            --load $PRETRAIN_CHECKPOINT_PATH\"\nfi\n\nTRAIN_ITERS=$(( ${TRAIN_TOKENS} / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\nLR_WARMUP_ITERS=$(( ${WARMUP_TOKENS}  / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\nLR_DECAY_ITERS=$(( ${TRAIN_TOKENS} /  ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\n\nNAME=\"${ENV}-pretrain-mcore-deepseek-${MODEL_SIZE}-lr-${LR}-bs-${BATCH_SIZE}-seqlen-${SEQ_LEN}-pr-${PR}-tp-${TP}-pp-${PP}-ac-${AC}-do-${DO}-sp-${SP}-moe-${MOE}-tt-${TRAIN_TOKENS}-wt-${WARMUP_TOKENS}\"\nmkdir -p \"${OUTPUT_BASEPATH}/tensorboard/\"\nmkdir -p \"${OUTPUT_BASEPATH}/checkpoint/\"\nmkdir -p \"${OUTPUT_BASEPATH}/log/\"\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nTENSORBOARD_DIR=\"${OUTPUT_BASEPATH}/tensorboard/${NAME}_${current_time}\"\nmkdir -p ${TENSORBOARD_DIR}\n\nSAVED_PRETRAIN_CHECKPOINT_PATH=\"${OUTPUT_BASEPATH}/checkpoint/${NAME}\"\n\nmegatron_options=\"  \\\n        --tokenizer-model $TOKENIZER_PATH \\\n        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \\\n        --data-path ${DATASET_PATH} \\\n        --lr ${LR} \\\n        --min-lr ${MIN_LR} \\\n        --lr-decay-style linear \\\n        --adam-beta1 0.9 \\\n        --adam-beta2 0.95 \\\n        --weight-decay 0.1 \\\n        --clip-grad 1.0 \\\n        --init-method-std 0.006 \\\n        --attention-dropout 0.0 \\\n        --hidden-dropout 0.0  \\\n        --lr-decay-iters ${LR_DECAY_ITERS} \\\n        --lr-warmup-iters ${LR_WARMUP_ITERS} \\\n        --train-iters ${TRAIN_ITERS} \\\n        --split 99,1,0 \\\n        --micro-batch-size ${BATCH_SIZE} \\\n        --global-batch-size ${GLOBAL_BATCH_SIZE} \\\n        --num-layers ${NUM_LAYERS} \\\n        --hidden-size ${HIDDEN_SIZE} \\\n        --num-attention-heads ${NUM_ATTN_HEADS} \\\n        --ffn-hidden-size ${INTERMEDIATE_SIZE} \\\n        --seq-length ${SEQ_LEN} \\\n        --max-position-embeddings ${MAX_POSITION_EMBEDDINGS} \\\n        --max-padding-length ${PAD_LEN} \\\n        --log-throughput \\\n        --log-interval 1 \\\n        --eval-interval 10000 \\\n        --eval-iters 10 \\\n        --save-interval ${SAVE_INTERVAL} \\\n        --tensorboard-queue-size 1 \\\n        --tensorboard-dir ${TENSORBOARD_DIR} \\\n        --log-timers-to-tensorboard \\\n        --log-batch-size-to-tensorboard \\\n        --log-validation-ppl-to-tensorboard \\\n        --tensor-model-parallel-size ${TP} \\\n        --pipeline-model-parallel-size ${PP} \\\n        --no-load-optim \\\n        --no-load-rng \\\n        --num-workers 8 \\\n        --seed 1234 \\\n        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \\\n        --patch-tokenizer-type DeepseekTokenizer \\\n        --dataset LLama-Pretrain-Idxmap \\\n        --swiglu \\\n        --normalization RMSNorm \\\n        --norm-epsilon 1e-05 \\\n        --use-rotary-position-embeddings \\\n        --no-rope-fusion \\\n        --position-embedding-type rope \\\n        --untie-embeddings-and-output-weights \\\n        --disable-bias-linear \\\n        --rotary-base 10000 \\\n        --use-mcore-models \n        \"\nLOG_SAVE=\"/data/code/temp/output/deepseek/pretrain_output/log/\"${current_time}-deepseek-${NODE_RANK}.log\"\" \nrun_cmd=\"torchrun $DISTRIBUTED_ARGS pretrain_mcore_deepseek.py\n ${megatron_options} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} ${do_options} ${flash_options} ${sp_options} ${gqa_options} ${moe_options} > ${LOG_SAVE} 2>&1\"\n\necho ${run_cmd}\neval ${run_cmd}\nset +x\n```\n\n##### pretrain_mcore_deepseek.py训练代码\n```python\n# Copyright (c) 2023 Alibaba PAI and Nvidia Megatron-LM Team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport torch\nfrom torch import Tensor\nfrom functools import partial\nfrom typing import Union\n\nfrom megatron import get_args\nfrom megatron import get_timers\nfrom megatron.core import mpu, tensor_parallel\nfrom megatron.core.enums import ModelType\nimport megatron.model\nfrom megatron.utils import (\n    get_batch_on_this_tp_rank,\n    get_batch_on_this_cp_rank,\n    average_losses_across_data_parallel_group\n)\nfrom megatron.core.datasets.blended_megatron_dataset_builder import BlendedMegatronDatasetBuilder\nfrom megatron.training import pretrain\nfrom megatron.core.datasets.gpt_dataset import GPTDatasetConfig\nfrom megatron.core.datasets.gpt_dataset import GPTDataset\n# from megatron.core.models.gpt import GPTModel\nfrom megatron.core.transformer.spec_utils import import_module\nfrom megatron.arguments import core_transformer_config_from_args\n\nfrom megatron_patch.model.deepseek_core.model import GPTModel\nfrom megatron_patch.data import build_pretrain_dataset_from_original\nfrom megatron_patch.data.utils import get_batch_on_this_tp_rank_original\nfrom megatron_patch.tokenizer import get_tokenizer, build_tokenizer\nfrom megatron_patch.arguments import get_patch_args\nfrom megatron_patch.model.deepseek_core.transformer_config import DeepseekTransformerConfig\nfrom megatron_patch.model.deepseek_core.layer_specs import get_gpt_layer_with_transformer_engine_spec\n\nimport torch._dynamo\ntorch._dynamo.config.suppress_errors = True\n\ndef model_provider(\n    pre_process=True, post_process=True\n) -> Union[GPTModel, megatron.model.GPTModel]:\n    \"\"\"Builds the model.\n\n    If you set the use_mcore_models to True, it will return the mcore GPT model and if not the legacy GPT model.\n\n    Args:\n        pre_process (bool, optional): Set to true if you need to compute embedings. Defaults to True.\n        post_process (bool, optional): Set to true if you need to want to compute output logits/loss. Defaults to True.\n\n\n    Returns:\n        Union[GPTModel, megatron.model.GPTModel]: The returned model\n    \"\"\"\n    args = get_args()\n    build_tokenizer(args)\n    config = core_transformer_config_from_args(args, DeepseekTransformerConfig)\n    #config = core_transformer_config_from_args(get_args())\n    if args.use_mcore_models:\n        if args.spec is not None:\n            transformer_layer_spec = import_module(args.spec)\n        else:\n            transformer_layer_spec = get_gpt_layer_with_transformer_engine_spec(args.num_experts, args.moe_grouped_gemm)\n        model = GPTModel(\n            config=config,\n            transformer_layer_spec=transformer_layer_spec,\n            vocab_size=args.padded_vocab_size,\n            max_sequence_length=args.max_position_embeddings,\n            pre_process=pre_process,\n            post_process=post_process,\n            fp16_lm_cross_entropy=args.fp16_lm_cross_entropy,\n            parallel_output=True,\n            share_embeddings_and_output_weights=not args.untie_embeddings_and_output_weights,\n            position_embedding_type=args.position_embedding_type,\n            rotary_percent=args.rotary_percent,\n            rotary_base=args.rotary_base,\n        )\n    return model\n\n\ndef get_batch(data_iterator):\n    \"\"\"Generate a batch.\"\"\"\n\n    # TODO: this is pretty hacky, find a better way\n    if (not mpu.is_pipeline_first_stage()) and (not mpu.is_pipeline_last_stage()):\n        return None, None, None, None, None\n\n    args = get_args()\n\n    if \"-Raw\" in args.dataset:\n        # get batches based on the TP rank you are on\n        batch = get_batch_on_this_tp_rank_original(data_iterator)\n        # slice batch along sequence dimension for context parallelism\n        batch = get_batch_on_this_cp_rank(batch)\n\n    elif \"-Idxmap\" in args.dataset:\n        # get batches based on the TP rank you are on\n        batch = get_batch_on_this_tp_rank(data_iterator)\n        # slice batch along sequence dimension for context parallelism\n        batch = get_batch_on_this_cp_rank(batch)\n\n    else:\n        raise ValueError(\"please set correct --dataset \")\n\n    return batch.values()\n\n\ndef loss_func(loss_mask: Tensor, output_tensor: Tensor):\n    \"\"\"Loss function.\n\n    Args:\n        loss_mask (Tensor): Used to mask out some portions of the loss\n        output_tensor (Tensor): The tensor with the losses\n    \"\"\"\n    args = get_args()\n\n    losses = output_tensor.float()\n    loss_mask = loss_mask.view(-1).float()\n    if args.context_parallel_size > 1:\n        loss = torch.cat([torch.sum(losses.view(-1) * loss_mask).view(1), loss_mask.sum().view(1)])\n        torch.distributed.all_reduce(loss, group=mpu.get_context_parallel_group())\n        loss = loss[0] / loss[1]\n    else:\n        loss = torch.sum(losses.view(-1) * loss_mask) / loss_mask.sum()\n\n    # Check individual rank losses are not NaN prior to DP all-reduce.\n    if args.check_for_nan_in_loss_and_grad:\n        global_rank = torch.distributed.get_rank()\n        assert not loss.isnan(), (\n            f'Rank {global_rank}: found NaN in local forward loss calculation. '\n            f'Device: {torch.cuda.current_device()}, node: {os.uname()[1]}'\n        )\n\n    # Reduce loss for logging.\n    averaged_loss = average_losses_across_data_parallel_group([loss])\n\n    return loss * args.context_parallel_size, {'lm loss': averaged_loss[0]}\n\n\ndef forward_step(data_iterator, model):\n    \"\"\"Forward step.\"\"\"\n    timers = get_timers()\n\n    # Get the batch.\n    timers('batch-generator', log_level=2).start()\n    tokens, labels, loss_mask, attention_mask, position_ids = get_batch(\n        data_iterator)\n    timers('batch-generator').stop()\n\n    output_tensor = model(tokens, position_ids, attention_mask,\n                          labels=labels)\n\n    return output_tensor, partial(loss_func, loss_mask)\n\n\n\ndef is_dataset_built_on_rank():\n    return (mpu.is_pipeline_first_stage() or mpu.is_pipeline_last_stage()) and mpu.get_tensor_model_parallel_rank() == 0\n\ndef core_gpt_dataset_config_from_args(args):\n    tokenizer = get_tokenizer()\n    return GPTDatasetConfig(\n        is_built_on_rank=is_dataset_built_on_rank,\n        random_seed=args.seed,\n        sequence_length=args.seq_length,\n        blend=args.data_path,\n        split=args.split,\n        path_to_cache=args.data_cache_path,\n        reset_attention_mask=args.reset_attention_mask,\n        eod_mask_loss=args.eod_mask_loss,\n        eod_id=tokenizer.eod\n    )\n\ndef train_valid_test_datasets_provider(train_val_test_num_samples):\n    \"\"\"Build train, valid, and test datasets.\"\"\"\n    args = get_args()\n    if \"-Raw\" in args.dataset:\n                train_ds, valid_ds, test_ds = \\\n                                    build_pretrain_dataset_from_original(args.dataset)\n    else:\n        train_ds, valid_ds, test_ds = BlendedMegatronDatasetBuilder(\n            GPTDataset,\n            train_val_test_num_samples,\n            core_gpt_dataset_config_from_args(args)\n        ).build()\n\n    return train_ds, valid_ds, test_ds\n\n\n\nif __name__ == \"__main__\":\n    train_valid_test_datasets_provider.is_distributed = True\n    pretrain(\n        train_valid_test_datasets_provider,\n        model_provider,\n        ModelType.encoder_or_decoder,\n        forward_step,\n        extra_args_provider=get_patch_args,\n    )\n```\n\n##### 在/data/code/megatron_patch/model在加入deepseek_core的相关配置\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image8.png)\n\n##### DeepseekTokenizer配置\n在/data/code/megatron_patch/tokenizer/__init__.py下加入DeepseekTokenizer配置\n\n```python\nelif args.patch_tokenizer_type == 'DeepseekTokenizer':\n        from megatron.core.datasets.megatron_tokenizer import MegatronTokenizer\n        class _DeepseekTokenizer(MegatronTokenizer):\n            def __init__(self, tokenizer_path, extra_vocab_size):\n                super().__init__(tokenizer_path)\n                self.tokenizer = AutoTokenizer.from_pretrained(\n                    tokenizer_path,\n                    padding_side=\"right\",\n                    use_fast=False,\n                    trust_remote_code=True,\n                )\n                self.extra_vocab_size = extra_vocab_size\n\n            def __call__(self, text, return_tensors=None,\n                         padding=None, max_length=None, truncation=None, add_special_tokens=True):\n\n                return self.tokenizer(text, return_tensors=return_tensors, padding=padding,\n                        max_length=max_length, truncation=truncation, add_special_tokens=add_special_tokens)\n\n            @property\n            def vocab_size(self):\n                return self.tokenizer.vocab_size + self.extra_vocab_size\n\n            @property\n            def vocab(self):\n                return self.tokenizer.encoder\n\n            @property\n            def inv_vocab(self):\n                return self.tokenizer.decoder\n\n            def tokenize(self, text):\n                return self.tokenizer.encode(text)\n\n            def detokenize(self, token_ids):\n                return self.tokenizer.decode(token_ids)\n\n            @property\n            def eod(self):\n                return self.tokenizer.eos_token_id\n\n            @property\n            def eos_token(self):\n                return self.tokenizer.eos_token\n\n            @property\n            def pad_token_id(self):\n                return self.tokenizer.pad_token_id\n\n            @property\n            def eos_token_id(self):\n                return self.tokenizer.eos_token_id\n\n        tokenizer = _DeepseekTokenizer(args.tokenizer_model, args.extra_vocab_size)\n        args.padded_vocab_size = tokenizer.vocab_size\n```\n\n##### 启动预训练k8s的yaml\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: deepseek-core\n  namespace: deepseek\nspec:\n  selector:\n    matchLabels:\n      app: deepseek-core\n  template:\n    metadata:\n      labels:\n        app: deepseek-core\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        deepseek: deepseek-7B  \n      containers:\n      - name: deepseek\n        image: registry.paas/cmss/nemo:24.05_v1.1\n        imagePullPolicy: IfNotPresent\n        resources:\n         limits:\n           nvidia.com/gpu: 4\n         requests:\n           nvidia.com/gpu: 4\n        command:\n              - \"/bin/bash\"\n              - \"-c\"\n              - |\n                cd /data/code/temp/conf/deepseek_core/pretrain &&\n                bash setRank.sh &&\n                cd /data/ &&\n                bash pretrain_mcore_deepseek.sh\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - mountPath: /data/code\n          name: code   \n        - mountPath: /etc/localtime\n          name: localtime\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: code\n        hostPath:\n          path: /mnt/users/lihai/distribute/core/megatron-core\n      - name: localtime\n        hostPath:\n          path: /etc/localtime\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 20G\n\n```\n\n##### setRank.sh\n```bash\n#!/bin/bash\n\norigin_shell=\"/data/code/examples/deepseek_core/pretrain_mcore_deepseek.sh\"\n\nconf_dir=\"/data/code/temp/conf/\"\n\nlocal_dir=\"/data/\"\n\nshell_name=pretrain_mcore_deepseek.sh\n\n## 复制脚本到/workspace下\ncp $origin_shell $local_dir\n\n\n## 读取hostfile\nreadarray -t ips < <(grep -vE '^[[:space:]]*$' \"$conf_dir\"hostfile-67)\n\n## 获取rank0 IP\nrank0_ip=$(echo \"${ips[0]}\" | tr -d '[:space:]')\n\nnodes=${#ips[@]}\n\n## 获取hostfile中配置的IP前缀\n## 使用cut提取IP地址的前三个数字部分\nip_prefix=$(echo \"${ips[0]}\" | cut -d '.' -f 1-3)\n\n## 获取本机IP\nip=$(hostname -I | grep -oE \"$ip_prefix\\.[0-9]+\")\n\n\n# 初始化rank\nnode_rank=-1\n\n# 遍历数组\nfor i in \"${!ips[@]}\"; do\n    # 使用tr命令去除空白字符\n    clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n    if [[ \"$clean_string\" == \"$ip\" ]]; then\n        node_rank=$i\n        break\n    fi\ndone\n\nif [ $node_rank -ne -1 ]; then\n    ## 修改脚本中MASTER_ADDR\n    sed -i \"s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/\" $local_dir$shell_name\n\n    ## 修改NNODES\n    sed -i \"s/^NNODES=.*/NNODES=$nodes/\" $local_dir$shell_name\n\n    ## 修改NODE_RANK\n    sed -i \"s/^NODE_RANK=.*/NODE_RANK=$node_rank/\" $local_dir$shell_name\nfi\n\n```\n\n#### 多机多卡拉起测试\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image9.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image10.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image11.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image12.png)\n\n#### 断点续训测试\n#### loss曲线对比实验\n##### patch框架训练代码准备\n###### patch的预训练yaml\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: deepseek-core\n  namespace: deepseek\nspec:\n  selector:\n    matchLabels:\n      app: deepseek-core\n  template:\n    metadata:\n      labels:\n        app: deepseek-core\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        deepseek: deepseek-7B\n      containers:\n        - name: deepseek\n          image: registry.paas/library/megatron-lm:v1\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              nvidia.com/gpu: 4\n            requests:\n              nvidia.com/gpu: 4\n          command:\n            - \"/bin/bash\"\n            - \"-c\"\n            - |\n              cd /data/code/examples/deepseek/conf && \n              bash setRank-patch.sh &&\n              cd /data/ &&\n              bash pretrain_patch_deepseek.sh\n          securityContext:\n            privileged: true\n          volumeMounts:\n            - mountPath: /data/code\n              name: code\n            - mountPath: /etc/localtime\n              name: localtime\n            - mountPath: /dev/shm\n              name: dshm\n      volumes:\n        - name: code\n          hostPath:\n            path: /mnt/users/wangxiangbo/Pai-Megatron-Patch\n        - name: localtime\n          hostPath:\n            path: /etc/localtime\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 20G\n```\n\n###### patch的预训练脚本\n```shell\n#!/bin/bash\n#sh run_pretrain_megatron_deepseek.sh dsw /workspace/Pai-Megatron-Patch 7B 1 8 1e-5 1e-6 2048 2048 0 bf16 1 1 sel true true true false 100000 /mnt/deepseek-datasets/wudao_llamabpe_text_document /mnt/deepseek-ckpts/Llama-2-7b-hf-to-mg-tp1-pp1/ 10000000000 100000000 /mnt/output_patch_test\nexport NCCL_P2P_DISABLE=1\n#export NCCL_DEBUG=INFO\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\n\nMEGATRON_PATCH_PATH=/data/code \nMEGATRON_PATH=${MEGATRON_PATCH_PATH}/Megatron-LM-231007\nexport PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATCH_PATH}:$PYTHONPATH\n\nNNODES=2\nNODE_RANK=0\nGPUS_PER_NODE=4\nMASTER_ADDR=192.168.0.22\nMASTER_PORT=8889\n\n\n\nDISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT\"\n\nMODEL_SIZE=7B\nBATCH_SIZE=1\nGLOBAL_BATCH_SIZE=8\nLR=1e-5\nMIN_LR=1e-6\nSEQ_LEN=1024\nPAD_LEN=1024\nEXTRA_VOCAB_SIZE=0\nPR=fp16\nTP=4\nPP=1\nAC=sel\nDO=true\nFL=false\nSP=true\nTE=false\nSAVE_INTERVAL=1000\nDATASET_PATH=/data/code/examples/deepseek/dataset/wudao_deepseekbpe_text_document\nPRETRAIN_CHECKPOINT_PATH=/data/code/examples/deepseek/ckpt\nTRAIN_TOKENS=10000000\nWARMUP_TOKENS=0\nOUTPUT_BASEPATH=/data/code/examples/deepseek/output/pretrain_output\n\n\nif [ $MODEL_SIZE = 7B ]; then\n\nNUM_LAYERS=30\nHIDDEN_SIZE=4096\nNUM_ATTN_HEADS=32\nINTERMEDIATE_SIZE=11008\n\ngqa_options=\"\"\n\n\nfi\n\nif [ $AC = full ]; then\n    activation_checkpoint_options=\" \\\n\t\t    --recompute-method uniform \\\n\t\t    --recompute-granularity full\"\nelif [ $AC = sel ]; then\n    activation_checkpoint_options=\" \\\n        --recompute-activations\"\nelif [ $AC = none ]; then\n    activation_checkpoint_options=\" \\\n                    \"\nfi\n\nif [ $PR = fp16 ]; then\n    pr_options=\" \\\n\t\t    --fp16\"\nelif [ $PR = bf16 ]; then\n    pr_options=\" \\\n        --bf16\"\nelif [ $PR = fp8 ]; then\n    pr_options=\" \\\n        --bf16\n        --fp8-hybrid \\\n        --fp8-amax-compute-algo max \\\n        --fp8-amax-history-len 1024 \\\n        --transformer-impl transformer_engine\"\nfi\n\nif [ $DO = true ]; then\n    do_options=\" \\\n\t\t    --use-distributed-optimizer\"\n\nelif [ $DO = false ]; then\n    do_options=\" \\\n                    \"\nfi\n\nif [ $FL = true ]; then\n    flash_options=\" \\\n\t\t    --use-flash-attn\"\n\nelif [ $FL = false ]; then\n    flash_options=\" \\\n                    \"\nfi\n\nif [ $TE = true ]; then\n    te_options=\" \\\n\t\t    --transformer-impl transformer_engine\"\n\nelif [ $TE = false ]; then\n    te_options=\" \\\n                    \"\nfi\n\nif [ $SP = true ] && [ $TP -gt 1 ]; then\n    sp_options=\" \\\n\t\t    --sequence-parallel\"\n\nelif [ $SP = false ]; then\n    sp_options=\" \\\n                    \"\nfi\n\nif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then\n    load_options=\" \\\n            --load $PRETRAIN_CHECKPOINT_PATH\"\nfi\n\nTRAIN_ITERS=$(( ${TRAIN_TOKENS} / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\nLR_WARMUP_ITERS=$(( ${WARMUP_TOKENS}  / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\nLR_DECAY_ITERS=$(( ${TRAIN_TOKENS} /  ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\n\nNAME=\"${ENV}-pretrain-patch-deepseek-${MODEL_SIZE}-lr-${LR}-bs-${BATCH_SIZE}-seqlen-${SEQ_LEN}-pr-${PR}-tp-${TP}-pp-${PP}-ac-${AC}-do-${DO}-sp-${SP}-tt-${TRAIN_TOKENS}-wt-${WARMUP_TOKENS}\"\nmkdir -p \"${OUTPUT_BASEPATH}/tensorboard/\"\nmkdir -p \"${OUTPUT_BASEPATH}/checkpoint/\"\nmkdir -p \"${OUTPUT_BASEPATH}/log/\"\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nTENSORBOARD_DIR=\"${OUTPUT_BASEPATH}/tensorboard/${NAME}_${current_time}\"\nmkdir -p ${TENSORBOARD_DIR}\n\nSAVED_PRETRAIN_CHECKPOINT_PATH=\"${OUTPUT_BASEPATH}/checkpoint/${NAME}\"\n\nmegatron_options=\"  \\\n        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \\\n        --split 99,1,0 \\\n        --train-data-path ${DATASET_PATH}\n        --lr ${LR} \\\n        --min-lr ${MIN_LR} \\\n        --lr-decay-style linear \\\n        --adam-beta1 0.9 \\\n        --adam-beta2 0.95 \\\n        --weight-decay 0.1 \\\n        --clip-grad 1.0 \\\n        --init-method-std 0.006 \\\n        --lr-decay-iters ${LR_DECAY_ITERS} \\\n        --lr-warmup-iters ${LR_WARMUP_ITERS} \\\n        --train-iters ${TRAIN_ITERS} \\\n        --micro-batch-size ${BATCH_SIZE} \\\n        --global-batch-size ${GLOBAL_BATCH_SIZE} \\\n        --num-layers ${NUM_LAYERS} \\\n        --hidden-size ${HIDDEN_SIZE} \\\n        --num-attention-heads ${NUM_ATTN_HEADS} \\\n        --ffn-hidden-size ${INTERMEDIATE_SIZE} \\\n        --seq-length ${SEQ_LEN} \\\n        --max-position-embeddings ${SEQ_LEN} \\\n        --log-interval 1 \\\n        --eval-interval 10000 \\\n        --eval-iters 10 \\\n        --save-interval ${SAVE_INTERVAL} \\\n        --tensorboard-queue-size 1 \\\n        --tensorboard-dir ${TENSORBOARD_DIR} \\\n        --log-timers-to-tensorboard \\\n        --log-batch-size-to-tensorboard \\\n        --log-validation-ppl-to-tensorboard \\\n        --tensor-model-parallel-size ${TP} \\\n        --pipeline-model-parallel-size ${PP} \\\n        --dataset LLama-Pretrain-Idxmap \\\n        --no-load-optim \\\n        --no-load-rng \\\n        --num-workers 8 \\\n        --seed 1234 \\\n        --max-padding-length ${PAD_LEN} \\\n        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \\\n        --patch-tokenizer-type LLamaTokenizer \\\n        --swiglu \\\n        --normalization RMSNorm \\\n        --use-llama2-rotary-position-embeddings \\\n        --position-embedding-type rope \\\n        --untie-embeddings-and-output-weights \\\n        --rotary-base 10000 \\\n        --rotary-scale-factor 4 \\\n        --loss-scale 16384 \\\n        --disable-bias-linear\n        \"\n#--no-query-key-layer-scaling \\\n#/data/code/examples/llama2/pretrain_megatron_llama.py\nLOG_SAVE=\"/data/code/examples/deepseek/output/pretrain_output/log/\"${current_time}-deepseek-patch-${NODE_RANK}.log\"\" \nrun_cmd=\"torchrun $DISTRIBUTED_ARGS /data/code/examples/llama2/pretrain_megatron_llama.py\n ${megatron_options} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} ${do_options} ${flash_options} ${sp_options} ${gqa_options} > ${LOG_SAVE} 2>&1\"\n\necho ${run_cmd}\neval ${run_cmd}\nset +x\n\n```\n\n###### patch的预训练setrank\n```shell\n#!/bin/bash\n\norigin_shell=\"/data/code/examples/deepseek/pretrain_patch_deepseek.sh\"\n\nconf_dir=\"/data/code/examples/deepseek/conf/\"\n\nlocal_dir=\"/data/\"\n\nshell_name=pretrain_patch_deepseek.sh\n\n## 复制脚本到/workspace下\ncp $origin_shell $local_dir\n\n\n## 读取hostfile\nreadarray -t ips < <(grep -vE '^[[:space:]]*$' \"$conf_dir\"hostfile)\n\n## 获取rank0 IP\nrank0_ip=$(echo \"${ips[0]}\" | tr -d '[:space:]')\n\nnodes=${#ips[@]}\n\n## 获取hostfile中配置的IP前缀\n## 使用cut提取IP地址的前三个数字部分\nip_prefix=$(echo \"${ips[0]}\" | cut -d '.' -f 1-3)\n\n## 获取本机IP\nip=$(hostname -I | grep -oE \"$ip_prefix\\.[0-9]+\")\n\n\n# 初始化rank\nnode_rank=-1\n\n# 遍历数组\nfor i in \"${!ips[@]}\"; do\n    # 使用tr命令去除空白字符\n    clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n    if [[ \"$clean_string\" == \"$ip\" ]]; then\n        node_rank=$i\n        break\n    fi\ndone\n\nif [ $node_rank -ne -1 ]; then\n    ## 修改脚本中MASTER_ADDR\n    sed -i \"s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/\" $local_dir$shell_name\n\n    ## 修改NNODES\n    sed -i \"s/^NNODES=.*/NNODES=$nodes/\" $local_dir$shell_name\n\n    ## 修改NODE_RANK\n    sed -i \"s/^NODE_RANK=.*/NODE_RANK=$node_rank/\" $local_dir$shell_name\nfi\n```\n\n##### patch框架预训练测试\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image13.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image14.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image15.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image16.png)\n\n","tags":["V100","模型适配","Nvidia","Deepseek","Megatron-LM"],"categories":["模型适配","NVIDIA"]},{"title":"2.4 Deepseek-7B微调适配","url":"/2025/11/13/模型训练调优/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.4 Deepseek-7B微调适配/","content":"#### 微调物料及代码准备\n##### 数据集准备\n/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-train.json\n\n/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-valid.json\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image24.png)\n\n##### 微调脚本\n```shell\n#!/bin/bash\nexport NCCL_P2P_DISABLE=1\n#export NCCL_DEBUG=INFO\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\n\nMEGATRON_PATCH_PATH=/data/code\nMEGATRON_PATH=${MEGATRON_PATCH_PATH}/Megatron-LM\n\nexport PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATCH_PATH}:$PYTHONPATH\n\nNNODES=2\nNODE_RANK=0\nGPUS_PER_NODE=4\nMASTER_ADDR=192.168.0.65\nMASTER_PORT=8889\n\nDISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT\"\n\n\nMODEL_SIZE=7B\nBATCH_SIZE=1\nGLOBAL_BATCH_SIZE=8 \nLR=1e-5\nMIN_LR=1e-6\nSEQ_LEN=1024\nPAD_LEN=1024\nEXTRA_VOCAB_SIZE=2400\nPR=fp16\nTP=4\nPP=1\nAC=sel\nDO=true\nFL=false\nSP=true\nTE=true\nMOE=false\nSAVE_INTERVAL=1000\nDATASET_PATH=/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-train.json\nVALID_DATASET_PATH=/data/code/temp/deepseek/deepseek-datasets/alpaca_zh-deepseek-valid.json\nPRETRAIN_CHECKPOINT_PATH=/data/code/temp/deepseek/deepseek-ckpt\nTOKENIZER_PATH=/data/code/temp/deepseek/deepseek-ckpt\nTRAIN_TOKENS=100000000   \nWARMUP_TOKENS=10000\nOUTPUT_BASEPATH=/data/code/temp/output/deepseek/finetune_output\n\n\nif [ $MODEL_SIZE = 7B ]; then\n\nNUM_LAYERS=30\nHIDDEN_SIZE=4096\nNUM_ATTN_HEADS=32\nINTERMEDIATE_SIZE=11008\nMAX_POSITION_EMBEDDINGS=4096\n\ngqa_options=\"\"\n\nfi\n\nif [ $AC = full ]; then\n    activation_checkpoint_options=\" \\\n\t\t    --recompute-method uniform \\\n\t\t    --recompute-granularity full\"\nelif [ $AC = sel ]; then\n    activation_checkpoint_options=\" \\\n        --recompute-activations\"\nelif [ $AC = none ]; then\n    activation_checkpoint_options=\" \\\n                    \"\nfi\n\nif [ $PR = fp16 ]; then\n    pr_options=\" \\\n\t\t    --fp16 \\\n            --apply-query-key-layer-scaling\"\n    export NVTE_APPLY_QK_LAYER_SCALING=1\nelif [ $PR = bf16 ]; then\n    pr_options=\" \\\n        --bf16\"\nelif [ $PR = fp8 ]; then\n    pr_options=\" \\\n        --bf16 \\\n        --fp8-hybrid \\\n        --fp8-amax-compute-algo max \\\n        --fp8-amax-history-len 1024 \\\n        --transformer-impl transformer_engine\"\nfi\n\nif [ $DO = true ]; then\n    do_options=\" \\\n\t\t    --use-distributed-optimizer\"\n\nelif [ $DO = false ]; then\n    do_options=\" \\\n                    \"\nfi\n\nif [ $FL = true ]; then\n    flash_options=\" \\\n\t\t    --use-flash-attn\"\n\nelif [ $FL = false ]; then\n    flash_options=\" \\\n                    \"\nfi\n\nif [ $TE = true ]; then\n    te_options=\" \\\n\t\t    --transformer-impl transformer_engine\"\n\nelif [ $TE = false ]; then\n    te_options=\" \\\n                    \"\nfi\n\nif [ $MOE = true ]; then\n    moe_options=\" \\\n\t\t    --moe-router-topk 1 \\\n\t\t    --num-experts 4 \\\n\t\t    --moe-aux-loss-coeff 1e-2 \\\n\t\t    --expert-model-parallel-size 2\"\n\nelif [ $MOE = false ]; then\n    moe_options=\" \\\n                    \"\nfi\n\nif [ $SP = true ] && [ $TP -gt 1 ]; then\n    sp_options=\" \\\n\t\t    --sequence-parallel\"\n\nelif [ $SP = false ]; then\n    sp_options=\" \\\n                    \"\nfi\n\nif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then\n    load_options=\" \\\n            --load $PRETRAIN_CHECKPOINT_PATH\"\nfi\n\nTRAIN_ITERS=$(( ${TRAIN_TOKENS} / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\nLR_WARMUP_ITERS=$(( ${WARMUP_TOKENS}  / ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\nLR_DECAY_ITERS=$(( ${TRAIN_TOKENS} /  ${GLOBAL_BATCH_SIZE} / ${SEQ_LEN} ))\n# LR_DECAY_ITERS=$((${TRAIN_ITERS} - ${LR_WARMUP_ITERS}))\n\nNAME=\"${ENV}-finetune-mcore-deepseek-${MODEL_SIZE}-lr-${LR}-bs-${BATCH_SIZE}-seqlen-${SEQ_LEN}-pr-${PR}-tp-${TP}-pp-${PP}-ac-${AC}-do-${DO}-sp-${SP}-tt-${TRAIN_TOKENS}-wt-${WARMUP_ITERS}\"\nmkdir -p \"${OUTPUT_BASEPATH}/tensorboard/\"\nmkdir -p \"${OUTPUT_BASEPATH}/checkpoint/\"\nmkdir -p \"${OUTPUT_BASEPATH}/log/\"\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nTENSORBOARD_DIR=\"${OUTPUT_BASEPATH}/tensorboard/${NAME}_${current_time}\"\nmkdir -p ${TENSORBOARD_DIR}\n\nSAVED_PRETRAIN_CHECKPOINT_PATH=\"${OUTPUT_BASEPATH}/checkpoint/${NAME}\"\n\nmegatron_options=\"  \\\n        --tokenizer-model ${TOKENIZER_PATH} \\\n        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \\\n        --split 99,1,0 \\\n        --train-data-path ${DATASET_PATH} \\\n        --valid-data-path ${VALID_DATASET_PATH} \\\n        --test-data-path ${VALID_DATASET_PATH} \\\n        --lr ${LR} \\\n        --min-lr ${MIN_LR} \\\n        --lr-decay-style linear \\\n        --adam-beta1 0.9 \\\n        --adam-beta2 0.95 \\\n        --weight-decay 0.1 \\\n        --clip-grad 1.0 \\\n        --init-method-std 0.006 \\\n        --dataloader-type cyclic \\\n        --lr-decay-iters ${LR_DECAY_ITERS} \\\n        --lr-warmup-iters ${LR_WARMUP_ITERS} \\\n        --train-iters ${TRAIN_ITERS} \\\n        --micro-batch-size ${BATCH_SIZE} \\\n        --global-batch-size ${GLOBAL_BATCH_SIZE} \\\n        --num-layers ${NUM_LAYERS} \\\n        --hidden-size ${HIDDEN_SIZE} \\\n        --num-attention-heads ${NUM_ATTN_HEADS} \\\n        --ffn-hidden-size ${INTERMEDIATE_SIZE} \\\n        --seq-length ${SEQ_LEN} \\\n        --max-position-embeddings ${MAX_POSITION_EMBEDDINGS} \\\n        --max-padding-length ${PAD_LEN} \\\n        --log-interval 1 \\\n        --eval-interval 10000 \\\n        --eval-iters 10 \\\n        --save-interval ${SAVE_INTERVAL} \\\n        --tensorboard-queue-size 1 \\\n        --tensorboard-dir ${TENSORBOARD_DIR} \\\n        --log-timers-to-tensorboard \\\n        --log-batch-size-to-tensorboard \\\n        --log-validation-ppl-to-tensorboard \\\n        --tensor-model-parallel-size ${TP} \\\n        --pipeline-model-parallel-size ${PP} \\\n        --no-load-optim \\\n        --no-load-rng \\\n        --num-workers 8 \\\n        --seed 1234 \\\n        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \\\n        --patch-tokenizer-type DeepseekTokenizer \\\n        --dataset LLama-Pretrain-Raw \\\n        --swiglu \\\n        --normalization RMSNorm \\\n        --use-rotary-position-embeddings \\\n        --no-rope-fusion \\\n        --position-embedding-type rope \\\n        --untie-embeddings-and-output-weights \\\n        --disable-bias-linear \\\n        --use-mcore-models \\\n        --rotary-base 10000 \\\n        --attention-dropout 0.0 \\\n        --hidden-dropout 0.0 \\\n        --norm-epsilon 1e-05 \\\n        --eod-mask-loss \\\n        --log-throughput \\\n        --loss-scale 2048      \n        \"\n        # --loss-scale 2048  \n\nLOG_SAVE=\"/data/code/temp/output/deepseek/finetune_output/log/\"${current_time}-deepseek-finetune-${NODE_RANK}.log\"\" \nrun_cmd=\"torchrun $DISTRIBUTED_ARGS /data/code/examples/deepseek_core/pretrain_mcore_deepseek.py\n ${megatron_options} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} ${do_options} ${flash_options} ${sp_options} ${gqa_options} ${moe_options} > ${LOG_SAVE} 2>&1\"\n\necho ${run_cmd}\neval ${run_cmd}\nset +x\n```\n\n##### 微调yaml\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: deepseek-core\n  namespace: deepseek\nspec:\n  selector:\n    matchLabels:\n      app: deepseek-core\n  template:\n    metadata:\n      labels:\n        app: deepseek-core\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        deepseek: deepseek-7B\n      containers:\n        - name: deepseek\n          image: registry.paas/library/megatron-lm:v1\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              nvidia.com/gpu: 4\n            requests:\n              nvidia.com/gpu: 4\n          command:\n            - \"/bin/bash\"\n            - \"-c\"\n            - |\n              cd /data/code/temp/conf/deepseek_core/finetune &&\n              bash setRank.sh &&\n              cd /data/ &&\n              bash finetune_mcore_deepseek.sh\n          securityContext:\n            privileged: true\n          volumeMounts:\n            - mountPath: /data/code\n              name: code\n            - mountPath: /etc/localtime\n              name: localtime\n            - mountPath: /dev/shm\n              name: dshm\n            - name: weight\n              mountPath: /data/code/temp/deepseek/deepseek-ckpt/\n      volumes:\n        - name: code\n          hostPath:\n            path: /mnt/users/lihai/distribute/core/megatron-core\n        - name: weight\n          hostPath:\n            path: /mnt/users/wangxiangbo/model/deepseek-ai/deepseek-llm-7b-core\n        - name: localtime\n          hostPath:\n            path: /etc/localtime\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 20G\n\n```\n\n##### setrank\n```shell\n#!/bin/bash\n\norigin_shell=\"/data/code/examples/deepseek_core/finetune_mcore_deepseek.sh\"\n\nconf_dir=\"/data/code/temp/conf/\"\n\nlocal_dir=\"/data/\"\n\nshell_name=finetune_mcore_deepseek.sh\n\n## 复制脚本到/workspace下\ncp $origin_shell $local_dir\n\n\n## 读取hostfile\nreadarray -t ips < <(grep -vE '^[[:space:]]*$' \"$conf_dir\"hostfile-23)\n\n## 获取rank0 IP\nrank0_ip=$(echo \"${ips[0]}\" | tr -d '[:space:]')\n\nnodes=${#ips[@]}\n\n## 获取hostfile中配置的IP前缀\n## 使用cut提取IP地址的前三个数字部分\nip_prefix=$(echo \"${ips[0]}\" | cut -d '.' -f 1-3)\n\n## 获取本机IP\nip=$(hostname -I | grep -oE \"$ip_prefix\\.[0-9]+\")\n\n\n# 初始化rank\nnode_rank=-1\n\n# 遍历数组\nfor i in \"${!ips[@]}\"; do\n    # 使用tr命令去除空白字符\n    clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n    if [[ \"$clean_string\" == \"$ip\" ]]; then\n        node_rank=$i\n        break\n    fi\ndone\n\nif [ $node_rank -ne -1 ]; then\n    ## 修改脚本中MASTER_ADDR\n    sed -i \"s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/\" $local_dir$shell_name\n\n    ## 修改NNODES\n    sed -i \"s/^NNODES=.*/NNODES=$nodes/\" $local_dir$shell_name\n\n    ## 修改NODE_RANK\n    sed -i \"s/^NODE_RANK=.*/NODE_RANK=$node_rank/\" $local_dir$shell_name\nfi\n\n\n```\n\n#### 多机多卡拉起测试\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image25.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image26.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image27.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image28.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image29.png)\n\n#### 断点续训测试\n#### loss曲线对比\n##### hugging-patch权重转换\n###### patch转换脚本\n```bash\n#!/bin/bash\n\nset -e\nSTART_TIME=$SECONDS\n\nMEGATRON_PATH=/data/code/\nexport PYTHONPATH=$PYTHONPATH:${MEGATRON_PATH}:${MEGATRON_PATH}/Megatron-LM-231007\nSOURCE_CKPT_PATH=/mnt/deepseek-ai/deepseek-llm-7b-base\nTARGET_CKPT_PATH=/mnt/deepseek-ai/deepseek-llm-7b-patch\nTP=4\nPP=1\nMN=llama-7b #llama-7b, llama-13b, llama-30b, llama-65b, llama2-7b, llama2-13b, llama2-70b\nEXTRA_VOCAB_SIZE=2400\nmg2hf=false\n\nif [ $mg2hf = true ]; then\n    do_options=\"\n                --convert_checkpoint_from_megatron_to_transformers\n    \"\nelif [ $mg2hf = false ]; then\n    do_options=\"\"\nfi\n\n\npython hf2megatron.py \\\n    --load_path ${SOURCE_CKPT_PATH} \\\n    --save_path ${TARGET_CKPT_PATH} \\\n    --target_params_dtype fp16 \\\n    --megatron-path ${MEGATRON_PATH} \\\n    --target_tensor_model_parallel_size ${TP} \\\n    --target_pipeline_model_parallel_size ${PP} \\\n    --model_name ${MN} \\\n    --extra_num_vocabs ${EXTRA_VOCAB_SIZE} \\\n${do_options}\n\nELAPSED_TIME=$(($SECONDS - $START_TIME))\necho \"$(($ELAPSED_TIME/60)) min $(($ELAPSED_TIME%60)) sec\"\n\n```\n\n###### patch权重转换过程\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image30.png)\n\n###### 得到转成patch的权重\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image31.png)\n\n##### patch框架微调代码准备\n###### patch的微调yaml\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: deepseek-core\n  namespace: deepseek\nspec:\n  selector:\n    matchLabels:\n      app: deepseek-core\n  template:\n    metadata:\n      labels:\n        app: deepseek-core\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        deepseek: deepseek-7B\n      containers:\n        - name: deepseek\n          image: registry.paas/library/megatron-lm:v1\n          imagePullPolicy: IfNotPresent\n          resources:\n            limits:\n              nvidia.com/gpu: 4\n            requests:\n              nvidia.com/gpu: 4\n          command:\n            - \"/bin/bash\"\n            - \"-c\"\n            - |\n              cd /data/code/examples/deepseek/conf &&\n              bash setRank.sh &&\n              cd /data/ &&\n              bash finetune_patch_deepseek.sh\n          securityContext:\n            privileged: true\n          volumeMounts:\n            - mountPath: /data/code\n              name: code\n            - mountPath: /etc/localtime\n              name: localtime\n            - mountPath: /dev/shm\n              name: dshm\n            - name: weight\n              mountPath: /data/code/examples/deepseek/ckpt/\n      volumes:\n        - name: code\n          hostPath:\n            path: /mnt/users/wangxiangbo/Pai-Megatron-Patch\n        - name: weight\n          hostPath:\n            path: /mnt/users/wangxiangbo/model/deepseek-ai/deepseek-llm-7b-patch\n        - name: localtime\n          hostPath:\n            path: /etc/localtime\n        - name: dshm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 20G\n\n```\n\n###### patch的微调脚本\n```python\n#!/bin/bash\n#sh run_pretrain_megatron_deepseek.sh dsw /workspace/Pai-Megatron-Patch 7B 1 8 1e-5 1e-6 2048 2048 0 bf16 1 1 sel true true true false 100000 /mnt/deepseek-datasets/wudao_llamabpe_text_document /mnt/deepseek-ckpts/Llama-2-7b-hf-to-mg-tp1-pp1/ 10000000000 100000000 /mnt/output_patch_test\nexport NCCL_P2P_DISABLE=1\n#export NCCL_DEBUG=INFO\nexport NCCL_SOCKET_IFNAME=eth0\nexport GLOO_SOCKET_IFNAME=eth0\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\n\nMEGATRON_PATCH_PATH=/data/code \nMEGATRON_PATH=${MEGATRON_PATCH_PATH}/Megatron-LM-231007\nexport PYTHONPATH=${MEGATRON_PATH}:${MEGATRON_PATCH_PATH}:$PYTHONPATH\n\nNNODES=2\nNODE_RANK=0\nGPUS_PER_NODE=4\nMASTER_ADDR=192.168.0.65\nMASTER_PORT=8889\n\n\n\nDISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT\"\n\nMODEL_SIZE=7B\nBATCH_SIZE=1\nGLOBAL_BATCH_SIZE=8\nLR=1e-5\nMIN_LR=1e-6\nSEQ_LEN=1024\nPAD_LEN=1024\nEXTRA_VOCAB_SIZE=4800\nPR=fp16\nTP=4\nPP=1\nAC=sel\nDO=true\nFL=false\nSP=true\nTE=false\nSAVE_INTERVAL=1000\nDATASET_PATH=/data/code/examples/deepseek/dataset/alpaca_zh-deepseek-train.json\nVALID_DATASET_PATH=/data/code/examples/deepseek/dataset/alpaca_zh-deepseek-valid.json\nPRETRAIN_CHECKPOINT_PATH=/data/code/examples/deepseek/ckpt\nTRAIN_ITERS=10000\nLR_WARMUP_ITERS=0\nOUTPUT_BASEPATH=/data/code/examples/deepseek/output/finetune_output\n\n\nif [ $MODEL_SIZE = 7B ]; then\n\nNUM_LAYERS=30\nHIDDEN_SIZE=4096\nNUM_ATTN_HEADS=32\nINTERMEDIATE_SIZE=11008\n\ngqa_options=\"\"\n\nfi\n\nif [ $AC = full ]; then\n    activation_checkpoint_options=\" \\\n\t\t    --recompute-method uniform \\\n\t\t    --recompute-granularity full \\\n            --recompute-num-layers ${NUM_LAYERS}\"\nelif [ $AC = sel ]; then\n    activation_checkpoint_options=\" \\\n        --recompute-activations\"\nelif [ $AC = none ]; then\n    activation_checkpoint_options=\" \\\n                    \"\nfi\n\nif [ $PR = fp16 ]; then\n    pr_options=\" \\\n\t\t    --fp16\"\nelif [ $PR = bf16 ]; then\n    pr_options=\" \\\n        --bf16\"\nelif [ $PR = fp8 ]; then\n    pr_options=\" \\\n        --bf16\n        --fp8-hybrid \\\n        --fp8-amax-compute-algo max \\\n        --fp8-amax-history-len 1024 \\\n        --transformer-impl transformer_engine\"\nfi\n\nif [ $DO = true ]; then\n    do_options=\" \\\n\t\t    --use-distributed-optimizer\"\n\nelif [ $DO = false ]; then\n    do_options=\" \\\n                    \"\nfi\n\nif [ $FL = true ]; then\n    flash_options=\" \\\n\t\t    --use-flash-attn\"\n\nelif [ $FL = false ]; then\n    flash_options=\" \\\n                    \"\nfi\n\nif [ $TE = true ]; then\n    te_options=\" \\\n\t\t    --transformer-impl transformer_engine\"\n\nelif [ $TE = false ]; then\n    te_options=\" \\\n                    \"\nfi\n\nif [ $SP = true ] && [ $TP -gt 1 ]; then\n    sp_options=\" \\\n\t\t    --sequence-parallel\"\n\nelif [ $SP = false ]; then\n    sp_options=\" \\\n                    \"\nfi\n\nif [ $PRETRAIN_CHECKPOINT_PATH != none ]; then\n    load_options=\" \\\n            --load $PRETRAIN_CHECKPOINT_PATH\"\nfi\n\nLR_DECAY_ITERS=$(( ${TRAIN_ITERS} - ${LR_WARMUP_ITERS}))\n\nNAME=\"${ENV}-finetune-patch-deepseek-${MODEL_SIZE}-lr-${LR}-bs-${BATCH_SIZE}-seqlen-${SEQ_LEN}-pr-${PR}-tp-${TP}-pp-${PP}-ac-${AC}-do-${DO}-sp-${SP}-tt-${TRAIN_TOKENS}-wt-${WARMUP_TOKENS}\"\nmkdir -p \"${OUTPUT_BASEPATH}/tensorboard/\"\nmkdir -p \"${OUTPUT_BASEPATH}/checkpoint/\"\nmkdir -p \"${OUTPUT_BASEPATH}/log/\"\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nTENSORBOARD_DIR=\"${OUTPUT_BASEPATH}/tensorboard/${NAME}_${current_time}\"\nmkdir -p ${TENSORBOARD_DIR}\n\nSAVED_PRETRAIN_CHECKPOINT_PATH=\"${OUTPUT_BASEPATH}/checkpoint/${NAME}\"\n\nmegatron_options=\"  \\\n        --save ${SAVED_PRETRAIN_CHECKPOINT_PATH} \\\n        --split 99,1,0 \\\n        --train-data-path ${DATASET_PATH} \\\n        --valid-data-path ${VALID_DATASET_PATH} \\\n        --test-data-path ${VALID_DATASET_PATH} \\\n        --lr ${LR} \\\n        --min-lr ${MIN_LR} \\\n        --lr-decay-style linear \\\n        --adam-beta1 0.9 \\\n        --adam-beta2 0.95 \\\n        --weight-decay 0.1 \\\n        --clip-grad 1.0 \\\n        --init-method-std 0.006 \\\n        --dataloader-type cyclic \\\n        --lr-decay-iters ${LR_DECAY_ITERS} \\\n        --lr-warmup-iters ${LR_WARMUP_ITERS} \\\n        --train-iters ${TRAIN_ITERS} \\\n        --micro-batch-size ${BATCH_SIZE} \\\n        --global-batch-size ${GLOBAL_BATCH_SIZE} \\\n        --num-layers ${NUM_LAYERS} \\\n        --hidden-size ${HIDDEN_SIZE} \\\n        --num-attention-heads ${NUM_ATTN_HEADS} \\\n        --ffn-hidden-size ${INTERMEDIATE_SIZE} \\\n        --seq-length ${SEQ_LEN} \\\n        --max-position-embeddings ${SEQ_LEN} \\\n        --log-interval 1 \\\n        --eval-interval 10000 \\\n        --eval-iters 10 \\\n        --save-interval ${SAVE_INTERVAL} \\\n        --tensorboard-queue-size 1 \\\n        --tensorboard-dir ${TENSORBOARD_DIR} \\\n        --log-timers-to-tensorboard \\\n        --log-batch-size-to-tensorboard \\\n        --log-validation-ppl-to-tensorboard \\\n        --tensor-model-parallel-size ${TP} \\\n        --pipeline-model-parallel-size ${PP} \\\n        --dataset LLama-Pretrain-Raw \\\n        --no-save-optim \\\n        --no-load-optim \\\n        --no-load-rng \\\n        --num-workers 8 \\\n        --seed 1234 \\\n        --max-padding-length ${PAD_LEN} \\\n        --extra-vocab-size ${EXTRA_VOCAB_SIZE} \\\n        --patch-tokenizer-type LLamaTokenizer \\\n        --swiglu \\\n        --normalization RMSNorm \\\n        --use-llama2-rotary-position-embeddings \\\n        --position-embedding-type rope \\\n        --untie-embeddings-and-output-weights \\\n        --rotary-base 10000 \\\n        --rotary-scale-factor 4 \\\n        --loss-scale 2048 \\\n        --disable-bias-linear\n        \"\nLOG_SAVE=\"/data/code/examples/deepseek/output/finetune_output/log/\"${current_time}-deepseek-patch-${NODE_RANK}.log\"\" \nrun_cmd=\"torchrun $DISTRIBUTED_ARGS /data/code/examples/llama2/pretrain_megatron_llama.py\n ${megatron_options} ${pr_options} ${load_options} ${te_options} ${activation_checkpoint_options} ${do_options} ${flash_options} ${sp_options} ${gqa_options} > ${LOG_SAVE} 2>&1\"\n\necho ${run_cmd}\neval ${run_cmd}\nset +x\n```\n\n###### patch的微调setrank\n```shell\n#!/bin/bash\n\norigin_shell=\"/data/code/examples/deepseek/finetune_patch_deepseek.sh\"\n\nconf_dir=\"/data/code/examples/deepseek/conf/\"\n\nlocal_dir=\"/data/\"\n\nshell_name=finetune_patch_deepseek.sh\n\n## 复制脚本到/workspace下\ncp $origin_shell $local_dir\n\n\n## 读取hostfile\nreadarray -t ips < <(grep -vE '^[[:space:]]*$' \"$conf_dir\"hostfile)\n\n## 获取rank0 IP\nrank0_ip=$(echo \"${ips[0]}\" | tr -d '[:space:]')\n\nnodes=${#ips[@]}\n\n## 获取hostfile中配置的IP前缀\n## 使用cut提取IP地址的前三个数字部分\nip_prefix=$(echo \"${ips[0]}\" | cut -d '.' -f 1-3)\n\n## 获取本机IP\nip=$(hostname -I | grep -oE \"$ip_prefix\\.[0-9]+\")\n\n\n# 初始化rank\nnode_rank=-1\n\n# 遍历数组\nfor i in \"${!ips[@]}\"; do\n    # 使用tr命令去除空白字符\n    clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n    if [[ \"$clean_string\" == \"$ip\" ]]; then\n        node_rank=$i\n        break\n    fi\ndone\n\nif [ $node_rank -ne -1 ]; then\n    ## 修改脚本中MASTER_ADDR\n    sed -i \"s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/\" $local_dir$shell_name\n\n    ## 修改NNODES\n    sed -i \"s/^NNODES=.*/NNODES=$nodes/\" $local_dir$shell_name\n\n    ## 修改NODE_RANK\n    sed -i \"s/^NODE_RANK=.*/NODE_RANK=$node_rank/\" $local_dir$shell_name\nfi\n\n```\n\n##### patch框架微调测试\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image32.png)\n\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image33.png)\n\n","tags":["V100","模型适配","Nvidia","Deepseek","Megatron-LM"],"categories":["模型适配","NVIDIA"]},{"title":"2.5 pytorchjob+eki断点续训","url":"/2025/11/13/模型训练调优/NVIDIA/Deepseek-R1-Distill-Qwen-7b/2.5 pytorchjob+eki断点续训/","content":"#### 环境准备\npytorchjob训练环境已由小组同事搭建完成\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image34.png)\n\n#### 代码准备\n##### 断点续训yaml\n需要注意的是，不能设定自定义的namespace，需要在default下启动pod\n\n```yaml\napiVersion: \"ai.cmss.chinamobile.com/v1alpha1\"\nkind: PyTorchJob\nmetadata:\n  name: deepseek-core\n  labels:\n    service: deepseek-core\nspec:\n  nprocPerNode: \"4\"\n  pytorchReplicaSpecs:\n    Master:\n      replicas: 1\n      template:\n        metadata:\n          labels:\n            app: ckpt\n        spec:\n          nodeSelector:\n            model-ckpt: deepseek-master\n          restartPolicy: Always\n          hostNetwork: true\n          dnsPolicy: ClusterFirstWithHostNet\n          affinity:\n            podAntiAffinity:\n              requiredDuringSchedulingIgnoredDuringExecution:\n                - labelSelector:\n                    matchExpressions:\n                      - key: app\n                        operator: In\n                        values:\n                          - ckpt\n                  topologyKey: \"kubernetes.io/hostname\"\n          containers:\n            - name: pytorch\n              image: registry.paas/library/megatron-lm:v1\n              imagePullPolicy: IfNotPresent\n              securityContext:\n                capabilities:\n                  add:\n                    - SYS_ADMIN\n              resources:\n                limits:\n                  nvidia.com/gpu: 4 # Request access to all GPU\n                requests:\n                  nvidia.com/gpu: 4\n              env:\n                - name: OMP_NUM_THREADS\n                  value: \"4\"\n              command:\n                - \"/bin/bash\"\n                - \"-c\"\n                - |\n                  cd /data/code/temp/conf/deepseek_core/pretrain &&\n                  bash SetRank.sh &&\n                  cd /data/ &&\n                  bash pretrain_ckpt_deepseek.sh\n              volumeMounts:\n                - mountPath: /data/code\n                  readOnly: false\n                  name: code\n                - name: shm-volume\n                  mountPath: /dev/shm\n                - name: pod-gpu-resources\n                  mountPath: /tmp\n                  readOnly: false\n          volumes:\n            - name: code\n              hostPath:\n                path: /mnt/users/lihai/distribute/core/megatron-core\n            - name: shm-volume\n              emptyDir:\n                medium: Memory\n                sizeLimit: 1G\n            - name: pod-gpu-resources\n              hostPath:\n                path: /var/lib/kubelet/pod-resources\n          schedulerName: ai-scheduler\n\n    Worker:\n      replicas: 1\n      restartPolicy: OnFailure\n      template:\n        metadata:\n          labels:\n            app: ckpt\n        spec:\n          nodeSelector:\n            model-ckpt: deepseek-worker\n          restartPolicy: Always\n          hostNetwork: true\n          dnsPolicy:\n            ClusterFirstWithHostNet\n            #affinty:\n            #podAntiAffinity:\n            # requiredDuringSchedulingIgnoredDuringExecution:\n            # - labelSelector:\n            #     matchExpressions:\n            #     - key: app\n            #       operator: In\n            #       values:\n            #       - ckpt\n            #   topologyKey: \"kubernetes.io/hostname\"\n          containers:\n            - name: pytorch\n              image: registry.paas/library/megatron-lm:v1\n              imagePullPolicy: IfNotPresent\n              # workingDir: /workspace/model/glm3/chatglm3/finetune_demo\n              securityContext:\n                capabilities:\n                  add:\n                    - SYS_ADMIN\n              resources:\n                limits:\n                  nvidia.com/gpu: 4 # Request access to all GPU\n                requests:\n                  nvidia.com/gpu: 4\n              env:\n                - name: OMP_NUM_THREADS\n                  value: \"4\"\n              command:\n                - \"/bin/bash\"\n                - \"-c\"\n                - |\n                  cd /data/code/temp/conf/deepseek_core/pretrain &&\n                  bash SetRank.sh &&\n                  cd /data/ &&\n                  bash pretrain_ckpt_deepseek.sh\n              volumeMounts:\n                - mountPath: /data/code\n                  readOnly: false\n                  name: code\n                - name: shm-volume\n                  mountPath: /dev/shm\n                - name: pod-gpu-resources\n                  mountPath: /tmp\n                  readOnly: false\n          volumes:\n            - name: code\n              hostPath:\n                path: /mnt/users/lihai/distribute/core/megatron-core\n                type: Directory\n            - name: shm-volume\n              emptyDir:\n                medium: Memory\n                sizeLimit: 1G\n            - name: pod-gpu-resources\n              hostPath:\n                path: /var/lib/kubelet/pod-resources\n          schedulerName: ai-scheduler\n\n```\n\n##### setrank\n```shell\n#!/bin/bash\n\norigin_shell=\"/data/code/examples/deepseek_core/pretrain_ckpt_deepseek.sh\"\n\nconf_dir=\"/data/code/temp/conf/\"\n\nlocal_dir=\"/data/\"\n\nshell_name=pretrain_ckpt_deepseek.sh\n\n## 复制脚本到/workspace下\ncp $origin_shell $local_dir\n\n\n## 读取hostfile\nreadarray -t ips < <(grep -vE '^[[:space:]]*$' \"$conf_dir\"hostfile-236)\n\n## 获取rank0 IP\nrank0_ip=$(echo \"${ips[0]}\" | tr -d '[:space:]')\n\nnodes=${#ips[@]}\n\n## 获取hostfile中配置的IP前缀\n## 使用cut提取IP地址的前三个数字部分\nip_prefix=$(echo \"${ips[0]}\" | cut -d '.' -f 1-3)\n\n## 获取本机IP\nip=$(hostname -I | grep -oE \"$ip_prefix\\.[0-9]+\")\n\n\n# 初始化rank\nnode_rank=-1\n\n# 打印调试信息\necho \"本机IP: $ip\"\necho \"rank0 IP: $rank0_ip\"\necho \"所有节点IP: ${ips[@]}\"\n\n# 遍历数组\n# for i in \"${!ips[@]}\"; do\n#     # 使用tr命令去除空白字符\n#     clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n#     if [[ \"$clean_string\" == \"$ip\" ]]; then\n#         node_rank=$i\n#         break\n#     fi\n# done\n\n# TODO 测试断点续训\nfor i in \"${!ips[@]}\"; do\n    clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n    if [[ \"$clean_string\" == \"$ip\" ]]; then\n        if [[ \"$ip\" == \"$rank0_ip\" ]]; then\n            node_rank=0\n        else\n            node_rank=1\n        fi\n        break\n    fi\ndone\n\n# 打印调试信息\necho \"设置的NODE_RANK: $node_rank\"\n\nif [ $node_rank -ne -1 ]; then\n    ## 修改脚本中MASTER_ADDR\n    sed -i \"s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/\" $local_dir$shell_name\n\n    ## 修改NNODES\n    # sed -i \"s/^NNODES=.*/NNODES=$nodes/\" $local_dir$shell_name\n\n    ## 修改NODE_RANK\n    sed -i \"s/^NODE_RANK=.*/NODE_RANK=$node_rank/\" $local_dir$shell_name\nfi\n\n```\n\n##### hostfile\n多机节点故障需要选取3个节点来模拟，其中2台用来拉起训练，之后模拟其中1台发生故障，之后再调度到另一个备用机器上继续训练。\n\n其中将192.168.0.65作为master节点，192.168.0.20和192.168.0.63作为worker节点。\n\n```plain\n192.168.0.65\n192.168.0.20\n192.168.0.63\n```\n\n#### 节点故障实践\n##### 训练拉起\n```bash\nkubectl apply -f deepseek-ckpt.yaml\nkubectl get node -owide\nkubectl get pod -owide\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image35.png)\n\n可以看出训练再在master节点192.168.0.65和worker节点192.168.0.63上拉起。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image36.png)\n\n##### ckpt保存\n持续训练，并在interval50步保存ckpt信息，之后继续训练\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image37.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image38.png)\n\n##### 模拟节点故障\n进入192.168.0.63机器，模拟节点故障，停掉k8s服务\n\n```bash\nsystemctl stop kubelet\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image39.png)\n\n停掉后在k8s的master节点查看node状态，是否为NotReady\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image40.png)\n\n此时192.168.0.63已经出现故障，再去查看拉起训练的pod信息\n\n由于192.168.0.63出现故障，pytorchjob会根据hostfile的节点信息，再重新尝试调度到另外的机器上192.168.0.20，可以看出在192.168.0.20重新进行了Init、PodInitializing、Running的过程。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image41.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image42.png)\n\n##### 查看续训log\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image43.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image44.png)\n\n至此，节点故障断点续训实践完成。\n\n模拟完成后，重新将192.168.0.63节点恢复\n\n```bash\nsystemctl start kubelet\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/deepseek/image45.png)\n\n\n\n#### \n\n\n","tags":["V100","模型适配","Nvidia","Deepseek","Megatron-LM"],"categories":["模型适配","NVIDIA"]},{"title":"A100集群6机48卡-基于k8s的Colossal-AI llama2 70B训练操作文档","url":"/2025/11/13/模型训练调优/NVIDIA/Llama2/A100集群6机48卡-基于k8s的Colossal-AI llama2 70B训练操作文档/","content":"<font style=\"color:#DF2A3F;background-color:#28d837;font-weight: bold;\">注：该项目为本人支撑客户项目，文档内涉及到的客户、设备信息等已脱敏处理。</font>\n### 基础环境说明\n#### 概述\n模型名称: Colossal-AI llama2 70B  \n模型参数: 70B参数  \n硬件需求: 6台服务器，每台服务器配备8张NVIDIA A100 GPU\n\n#### 硬件配置\nGPU型号: NVIDIA A100  \nGPU数量: 6台服务器 x 8张/台 = 48张GPU  \n内存容量: 每张A100 GPU拥有80GB显存  \nCPU: Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz，x86_64  \n存储: 高速SSD存储，用于数据读写  \n网络: 高速网络连接，支持多机训练\n\n#### GPU环境说明\n驱动版本: 470.141.03  \nCUDA版本: CUDA 11.4  \nNvidia-smi版本：470.141.03  \nNCCL: 用于多GPU和多节点通信\n持久性模式: Persistence-Ml\n功率使用上限: 功率上限为400W\n显存：显存总量为81920MiB (80GB)\n\n#### 软件环境\n操作系统: Linux (CentOS Linux 7 (Core))  \nPython版本: Python 3.8 或更高版本  \n依赖库:  \nPyTorch 2.1  \nColossal-AI: 支持70B参数模型版本  \n其他依赖: NumPy, SciPy, Pandas等\n\n#### 模型参数大小\n参数总数: 70B  \n模型占用内存: 由于GPU显存占用会根据模型的参数量、模型的内存占用、训练时的batch size、并行优化技术等不同而发生变化，</font>现参数未定，暂不能量化GPU占用情况。\n\n### 模型训练流程\n<font style=\"background-color:#c50e0e;\">注：由于xxx.xx.xx.141机器日常有业务在跑，无法空闲出完整的8块gpu资源，故以下最多以</font>**<font style=\"color:#cddf2a;background-color:#c50e0e;\">5机40卡</font>**<font style=\"background-color:#c50e0e;\">来拉起训练。</font>\n\n#### 查看A100机器的ip信息\n`cat /root/a100.txt`\n\n#### 查看待训练节点的GPU占用情况\n通过137机器，分别ssh进入以上6台机器（已设置免密登录），查看对应机器的gpu使用情况`nvidia-smi`、网卡配置信息`ifconfig`等。\n\n示例一：\n\n`ssh root@xxx.xx.xx.141` ssh进入机器141\n\n`nvidia-smi`141机器的gpu已经被占用，若是有需求要使用该机器，需要提前沟通\n\n`exit` 从141机器退出到137机器上\n\n示例二：\n\n`ssh root@xxx.xx.xx.78`\n\n`nvidia-smi` 78机器的gpu没有被使用，可以进行模型训练任务\n\n#### 物料准备工作\n<font style=\"background-color:#c50e0e;\">所有的物料位置存储在xxx.xx.xx137机器上的</font>`/home/disk_sdb0/llama2`<font style=\"background-color:#c50e0e;\">下。</font>\n\n物料包含原始数据集文件夹`raw_data`，离线切分完成的数据集文件夹`datas`、镜像`llama2-70b.tar`、70B模型权重`Llama-2-70b-hf`、13B模型权重`Llama-2-13b-hf`、7B模型权重`Llama-2-7b-hf`、k8s拉起预训练的配置文件`llama2-70b-test.yaml`、k8s拉起微调训练的配置文件`llama2-70b-finetune.yaml`，预训练/微调的训练代码`train.py`、预训练的配置脚本`train.example.sh`、微调训练的配置脚本`finetune.example.sh`、节点ip的信息配置文件`hostfile.txt`，此外还需要创建一个目录`output`来存放训练过程中的输出日志及checkpoint信息`mkdir output`。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image1.png)\n\n#### 训练环境配置准备\n##### /etc/hosts配置\n6台机器都需要对`vim /etc/hosts`文件进行修改，加入对应的<font style=\"color:rgba(0, 0, 0, 0.85);\"> IP 地址和与其对应的主机名或域名的映射关系，以此保证各主机之间通信正常。此外，还需要加入百度镜像仓库地址的ip映射。否则会报socket通信错误。</font>\n\n<font style=\"color:#DF2A3F;background-color:#c50e0e;\">socket.gaierror: [Errno -2] Name or service not known</font>\n\n```plain\nxxx.xx.xx.103 xxx.xx.xx..baidu.com\nxxx.xx.xx137 xxx-xx-xx-xxx-137\nxxx.xx.xx.78 xxx-xx-xx-78.XXX-stack.com\nxxx.xx.xx.79 xxx-xx-xx-79.XXX-stack.com\nxxx.xx.xx.80 xxx-xx-xx-80.XXX-stack.com\nxxx.xx.xx.141 xxx-xx-xx-141.XXX-stack.com\nxxx.xx.xx.142 xxx-xx-xx-142.XXX-stack.com\nxxx.xx.xx.143 xxx-xx-xx-143.XXX-stack.com\n```\n\n##### k8s命名空间创建\n因为未使用百度的测试平台来拉训练，通过k8s来拉起，所以需要自定义一个`namespace`，并在改ns下启动pod拉起训练任务。该ns需要与`llama2-70b-test.yaml`中的`namespace: llama2-70b-test`对应。\n\n`kubectl create namespace llama2-70b-test`\n\n`kubectl get ns`\n\n##### 镜像push至仓库\n镜像需push到百度指定的仓库中\n\n将镜像load之后，并将镜像push至 <font style=\"background-color:#c50e0e;\">xxx.xx.xx..baidu.com </font>仓库中。\n\n        1. `docker load -i llama2-70b.tar`\n        2. `docker images`\n        3. `docker tag 9cb8e972304f xxx.xx.xx..baidu.com/public/llama2-70b:v1.0`\n        4. `docker login xxx.xx.xx..baidu.com -u admin -p XXXXXXX`\n        5. `docker push xxx.xx.xx..baidu.com/public/llama2-70b:v1.0`\n\n#####   hostfile配置\n通过ssh登录到5台机器，查看是否有gpu占用，拉起训练时要保证节点的8个gpu都没有被占用。将空闲的机器bond4的ip写入到hostfile中。bond4的ip地址可以通过`ifconfig`来进行查看，有的机器使用的是以太网eth0的ip配置。\n\n```plain\nxxx.xx.xx.142\nxxx.xx.xx.143\nxxx.xx.xx.78\nxxx.xx.xx.79\nxxx.xx.xx.80\n```\n\n#####  训练脚本\n修改预训练启动的脚本`train.example.sh`，网卡的socket通信配置要修改为bond4，并在拉起的训练的节点中选取一个作为matser主节点`--master_addr`\n\n```bash\n#!/bin/bash\n\n# NCCL IB environment variables\nexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1\nexport NCCL_IB_DISABLE=0\nexport NCCL_SOCKET_IFNAME=bond4\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_IB_TIMEOUT=23\nexport NCCL_IB_RETRY_CNT=7\nexport OMP_NUM_THREADS=8\n\nPROJECT_NAME=\"llama2-70b-pt\"\nPARENT_SAVE_DIR=\"/opt/tmp/ColossalAI/output/save/\"\nPARENT_TENSORBOARD_DIR=\"/opt/tmp/ColossalAI/output/tensorboard/\"\nPARENT_CONFIG_FILE=\"/opt/tmp/ColossalAI/output/config/\"\nPRETRAINED_MODEL_PATH=\"/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf\"\ndeclare -a dataset=(\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009\"\n)\n\nTIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)\nFULL_PROJECT_NAME=\"${PROJECT_NAME}-${TIMESTAMP}\"\nSAVE_DIR=\"${PARENT_SAVE_DIR}${FULL_PROJECT_NAME}\"\nTENSORBOARD_DIR=\"${PARENT_TENSORBOARD_DIR}${FULL_PROJECT_NAME}\"\nCONFIG_FILE=\"${PARENT_CONFIG_FILE}${FULL_PROJECT_NAME}.json\"\n\ncolossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.78 --master_port 30015 train.py \\\n    --pretrained $PRETRAINED_MODEL_PATH \\\n    --dataset ${dataset[@]} \\\n    --plugin \"gemini_auto\" \\\n    --save_interval 400 \\\n    --save_dir $SAVE_DIR \\\n    --tensorboard_dir $TENSORBOARD_DIR \\\n    --config_file $CONFIG_FILE \\\n    --num_epochs 1 \\\n    --micro_batch_size 1 \\\n    --lr 1e-4 \\\n    --mixed_precision \"fp16\" \\\n    --grad_clip 1.0 \\\n    --weight_decay 0.01 \\\n    --warmup_steps 100 \\\n    --use_grad_checkpoint \\\n    --max_length 4096 \\\n    --accumulation_steps 1 \\\n    --pad_token \"unk\"\n```\n\n#####  启动k8s的yaml\n修改k8s拉起的配置文件`llama2-70b-test.yaml`\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: llama2-70b-test\n  name: llama2-70b-pt # Deployment 的名称\nspec:\n  replicas: 2 # 副本数量\n  selector:\n    matchLabels:\n      app: llama2-70b-pt\n  template:\n    metadata:\n      labels:\n        app: llama2-70b-pt\n    spec:\n      hostNetwork: true\n      containers:\n      - name: llama2-70b-container # 容器的名称\n        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/bash\", \"-c\"]\n        args: [\"cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ && chmod +x start.sh && ./start.sh && chmod +x train.example.sh && ./train.example.sh\"] # 启动容器后执行的命令，切换目录并运行训练脚本\n        resources:\n          limits:\n            nvidia.com/gpu: 8 # 限制使用 8 个 GPU，根据实际情况调整\n          requests:\n            nvidia.com/gpu: 8\n        volumeMounts:\n        - name: colossalai-orgdataset\n          mountPath: /opt/tmp/ColossalAI/datas/org_dataset # 将容器内数据目录挂载\n        - name: colossalai-weight\n          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载\n        - name: colossalai-train\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py\n          subPath: train.py\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: colossalai-output\n          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载\n        - name: host-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile\n          subPath: hostfile.txt\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: train-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # 容器内train_example.sh\n          subPath: train.example.sh\n        - name: shm-volume\n          mountPath: /dev/shm\n      nodeSelector:\n        model: llama2-70b\n      volumes:\n      - name: colossalai-orgdataset\n        hostPath:\n          path: /home/disk_sdb0/llama2/datas/org_dataset # 宿主机中 ColossalAI 数据的路径\n          type: Directory\n      - name: colossalai-weight\n        hostPath:\n          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # 宿主机中 ColossalAI 权重的路径\n          type: Directory\n      - name: colossalai-train\n        hostPath:\n          path: /home/disk_sdb0/llama2\n          type: Directory\n      - name: colossalai-output\n        hostPath:\n          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径\n          type: Directory\n      - name: host-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: train-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 8Gi\n```\n\n#####  node节点打标签\n给待拉起训练的node打上标签（首先需要确认待拉起训练的node的状态是否Ready）\n\n打标签时，其中的model=llama2-70b对应的是llama2-70b-test.yaml文件中的nodeSelector部分，需保持一致。\n\n`kubectl label nodes xxx-xx-xx-141.XXX-stack.com model=llama2-70b`\n\n`kubectl label nodes xxx-xx-xx-142.XXX-stack.com model=llama2-70b`\n\n`kubectl label nodes xxx-xx-xx-143.XXX-stack.com model=llama2-70b`\n\n`kubectl label nodes xxx-xx-xx-78.XXX-stack.com model=llama2-70b`\n\n`kubectl label nodes xxx-xx-xx-79.XXX-stack.com model=llama2-70b`\n\n`kubectl label nodes xxx-xx-xx-80.XXX-stack.com model=llama2-70b`\n\n#####  训练物料传输与同步\n因为多机拉起训练时，每个node节点都需要有一份训练所需的物料（权重、数据集、训练脚本等文件），故需要通过`rsync`或`scp`命令将137机器上的物料文件传输至待训练的节点中。(除去启动k8s所需的`llama2-70b-test.yaml`配置文件要在matser主节点上，其余的所有物料在待训练的node的同路径中`/home/disk_sdb0/llama2`都需要同步拷贝一份)\n\n137节点传输至79节点的示例：\n\n`scp -r /home/disk_sdb0/llama2/Llama-2-70b-hf root@xxx.xx.xx.79:/home/disk_sdb0/llama2/Llama-2-70b-hf`\n\n`scp -r /home/disk_sdb0/llama2/datas root@xxx.xx.xx.79:/home/disk_sdb0/llama2/datas`\n\n`scp -r /home/disk_sdb0/llama2/output root@xxx.xx.xx.79:/home/disk_sdb0/llama2/output`\n\n`scp -r /home/disk_sdb0/llama2/hostfile.txt root@xxx.xx.xx.79:/home/disk_sdb0/llama2`\n\n`scp -r /home/disk_sdb0/llama2/train.py root@xxx.xx.xx.79:/home/disk_sdb0/llama2`\n\n`scp -r /home/disk_sdb0/llama2/train.example.sh root@xxx.xx.xx.79:/home/disk_sdb0/llama2`\n\n#### 训练拉起\n打完标签之后，注意切换至`cd /home/disk_sdb0/llama2`，apply启动训练脚本。\n\n`kubectl apply -f llama2-70b-test.yaml` 启动yaml。\n\n`kubectl get pod -n llama2-70b-test -owide` 查看ns为llama2-70b-test下的pod信息。\n\n`kubectl describe pod -n llama2-70b-test xxx` 查看每个正在运行的pod信息，用来追踪pod的运行状态，看是否有error报错。\n\n`kubectl logs xxx -n llama2-70b-test -f` 实时跟踪pod的训练日志（最终的训练日志会在设定的主节点master上打印）。\n\n`kubectl delete -f llama2-70b-test.yaml`删除yaml后，将终止训练任务。\n\n### 模型训练测试记录\n#### 报错记录\n##### nvidia driver 与 cuda版本问题\n（nvidia driver 470 和 cuda11.4版本过低，故一直调不到机器GPU的资源，所以在并行策略zero和gemini时候会去调度CPU的资源，然后CPU撑不起来，直接就down掉了）<font style=\"background-color:#c50e0e;\">该错误通过升级nvidia driver至525.125.06，cuda升级为12.0时，问题解决。</font>\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image2.png)\n\n##### docker容器拉起测试问题\n尝试通过该镜像run一个容器来测试，由于缺失<font style=\"color:#000000;\">nvidia-container-toolkit，后续安装完成后，再次run，发现--gpus all 未能调用（nvidia驱动太低导致）。去掉--gpus all之后，容器启动后，识别不到GPU。</font>\n\n`dpkg -l | grep nvidia-container-toolkit` 查看是否安装了<font style=\"color:#000000;\">nvidia-container-toolkit</font>\n\n##### A100需要nvidia-fabricmanager支持\n驱动升级完成后，在xxx.xx.xx.78机器上尝试拉起单机8卡的llama2-7B模型的预训练，报了错误经查阅资料后发现<font style=\"background-color:#c50e0e;\">A100需要nvidia-fabricmanager支持，后续安装对应nvidia driver驱动525.125.06一致的nvidia-fabricmanager后成功拉起训练。</font>\n\n错误解决参考博客：[https://www.cnblogs.com/huadongw/p/16504137.html](https://www.cnblogs.com/huadongw/p/16504137.html)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image3.png)\n\n单机8卡的llama2-7B的预训练成功拉起\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image4.png)\n\n##### 5机40卡预训练时存在某进程随机占用gpu资源问题\n在拉起5机40卡时，一直存在一个ns为single-gpu的pod被调度并占用一个gpu资源（删除之后会随机在6台机器上占用一个gpu）\n\n`kubectl describe node xxx-xx-xx-142.XXX-stack.com`查看142节点情况\n\n正常情况下，一台机器的8台gpu都处于空闲状态下，nvidia.com/gpu的占用应该是0\n\n<font style=\"background-color:#c50e0e;\">解决方法：删除掉启动该pod的对应的label（gpu-a100）并删除掉该pod</font>\n\n`kubectl label nodes xxx-xx-xx-141.XXX-stack.com gpu-`\n\n`kubectl label nodes xxx-xx-xx-142.XXX-stack.com gpu-`\n\n`kubectl label nodes xxx-xx-xx-143.XXX-stack.com gpu-`\n\n`kubectl label nodes xxx-xx-xx-78.XXX-stack.com gpu-`\n\n`kubectl label nodes xxx-xx-xx-79.XXX-stack.com gpu-`\n\n`kubectl label nodes xxx-xx-xx-80.XXX-stack.com gpu-`\n\n#### 训练拉起测试\n##### 预训练\n###### llama2-13B 2机16卡\n1. 启动k8s的yaml配置文件\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: llama2-70b-test\n  name: llama2-70b-pt # Deployment 的名称\nspec:\n  replicas: 2 # 副本数量\n  selector:\n    matchLabels:\n      app: llama2-70b-pt\n  template:\n    metadata:\n      labels:\n        app: llama2-70b-pt\n    spec:\n      hostNetwork: true\n      containers:\n      - name: llama2-70b-container # 容器的名称\n        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/bash\", \"-c\"]\n        args: [\"cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ && chmod +x train.example.sh && ./train.example.sh\"] # 启动容器后执行的命令，切换目录并运行训练脚本\n        resources:\n          limits:\n            nvidia.com/gpu: 8 # 限制使用 8 个 GPU，根据实际情况调整\n          requests:\n            nvidia.com/gpu: 8\n        volumeMounts:\n        - name: colossalai-weight\n          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载\n        - name: colossalai-train\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py\n          subPath: train.py\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: colossalai-output\n          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载\n        - name: host-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile\n          subPath: hostfile.txt\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: train-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # 容器内train_example.sh\n          subPath: train.example.sh\n        - name: shm-volume\n          mountPath: /dev/shm\n      nodeSelector:\n        model: llama2-70b\n      volumes:\n      - name: colossalai-weight\n        hostPath:\n          path: /home/disk_sdb0/llama2/Llama-2-13b-hf # 宿主机中 ColossalAI 权重的路径\n          type: Directory\n      - name: colossalai-train\n        hostPath:\n          path: /home/disk_sdb0/llama2\n          type: Directory\n      - name: colossalai-output\n        hostPath:\n          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径\n          type: Directory\n      - name: host-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: train-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: train-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 8Gi\n```\n\n2. 训练脚本\n\n```bash\n#!/bin/bash\n\n# NCCL IB environment variables\nexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1\nexport NCCL_IB_DISABLE=0\nexport NCCL_SOCKET_IFNAME=bond4\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_IB_TIMEOUT=23\nexport NCCL_IB_RETRY_CNT=7\nexport OMP_NUM_THREADS=8\n\nPROJECT_NAME=\"llama2-70b-pt\"\nPARENT_SAVE_DIR=\"/opt/tmp/ColossalAI/output/save/\"\nPARENT_TENSORBOARD_DIR=\"/opt/tmp/ColossalAI/output/tensorboard/\"\nPARENT_CONFIG_FILE=\"/opt/tmp/ColossalAI/output/config/\"\nPRETRAINED_MODEL_PATH=\"/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf\"\ndeclare -a dataset=(\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009\"\n)\n\nTIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)\nFULL_PROJECT_NAME=\"${PROJECT_NAME}-${TIMESTAMP}\"\nSAVE_DIR=\"${PARENT_SAVE_DIR}${FULL_PROJECT_NAME}\"\nTENSORBOARD_DIR=\"${PARENT_TENSORBOARD_DIR}${FULL_PROJECT_NAME}\"\nCONFIG_FILE=\"${PARENT_CONFIG_FILE}${FULL_PROJECT_NAME}.json\"\n\ncolossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.78 --master_port 30015 train.py \\\n    --pretrained $PRETRAINED_MODEL_PATH \\\n    --dataset ${dataset[@]} \\\n    --plugin \"gemini_auto\" \\\n    --save_interval 400 \\\n    --save_dir $SAVE_DIR \\\n    --tensorboard_dir $TENSORBOARD_DIR \\\n    --config_file $CONFIG_FILE \\\n    --num_epochs 1 \\\n    --micro_batch_size 8 \\\n    --lr 1e-4 \\\n    --mixed_precision \"fp16\" \\\n    --grad_clip 1.0 \\\n    --weight_decay 0.01 \\\n    --warmup_steps 100 \\\n    --use_grad_checkpoint \\\n    --max_length 512 \\\n    --accumulation_steps 1 \\\n    --pad_token \"unk\"\n```\n\n3. hostfile.txt\n\n```plain\nxxx.xx.xx.78\nxxx.xx.xx.79\n```\n\n4. 训练拉起截图\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image5.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image6.png)\n\n\n5. 78，79显存占用情况\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image7.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image8.png)\n\n###### llama2-70B 2机16卡\n1. 参数配置\n\n| micro_batch_size | max_length | plugin | mixed_precision | tflops |\n| --- | --- | --- | --- | --- |\n| 8 | 512 | gemini_auto | fp16 | 64.4685 |\n\n\n2. 启动k8s的yaml配置文件\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: llama2-70b-test\n  name: llama2-70b-pt # Deployment 的名称\nspec:\n  replicas: 2 # 副本数量\n  selector:\n    matchLabels:\n      app: llama2-70b-pt\n  template:\n    metadata:\n      labels:\n        app: llama2-70b-pt\n    spec:\n      hostNetwork: true\n      containers:\n      - name: llama2-70b-container # 容器的名称\n        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/bash\", \"-c\"]\n        args: [\"cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ && chmod +x train.example.sh && ./train.example.sh\"] # 启动容器后执行的命令，切换目录并运行训练脚本\n        resources:\n          limits:\n            nvidia.com/gpu: 8 # 限制使用 4 个 GPU，根据实际情况调整\n          requests:\n            nvidia.com/gpu: 8\n        volumeMounts:\n        - name: colossalai-weight\n          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载\n        - name: colossalai-train\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py\n          subPath: train.py\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: colossalai-output\n          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载\n        - name: host-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile\n          subPath: hostfile.txt\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: train-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # 容器内train_example.sh\n          subPath: train.example.sh\n        - name: profiler-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/performance_evaluator.py  # 容器内train_example.sh\n          subPath: performance_evaluator.py\n        - name: shm-volume\n          mountPath: /dev/shm\n      nodeSelector:\n        model: llama2-70b\n      volumes:\n      - name: colossalai-weight\n        hostPath:\n          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # 宿主机中 ColossalAI 权重的路径\n          type: Directory\n      - name: colossalai-train\n        hostPath:\n          path: /home/disk_sdb0/llama2\n          type: Directory\n      - name: colossalai-output\n        hostPath:\n          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径\n          type: Directory\n      - name: host-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: train-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: profiler-script\n        hostPath:\n            path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n            type: Directory\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 8Gi\n```\n\n3. 训练脚本\n\n```bash\n#!/bin/bash\n\n# NCCL IB environment variables\nexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1\nexport NCCL_IB_DISABLE=0\nexport NCCL_SOCKET_IFNAME=bond4\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_IB_TIMEOUT=23\nexport NCCL_IB_RETRY_CNT=7\nexport OMP_NUM_THREADS=8\n\nPROJECT_NAME=\"llama2-70b-pt\"\nPARENT_SAVE_DIR=\"/opt/tmp/ColossalAI/output/save/\"\nPARENT_TENSORBOARD_DIR=\"/opt/tmp/ColossalAI/output/tensorboard/\"\nPARENT_CONFIG_FILE=\"/opt/tmp/ColossalAI/output/config/\"\nPRETRAINED_MODEL_PATH=\"/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf\"\ndeclare -a dataset=(\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009\"\n)\n\nTIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)\nFULL_PROJECT_NAME=\"${PROJECT_NAME}-${TIMESTAMP}\"\nSAVE_DIR=\"${PARENT_SAVE_DIR}${FULL_PROJECT_NAME}\"\nTENSORBOARD_DIR=\"${PARENT_TENSORBOARD_DIR}${FULL_PROJECT_NAME}\"\nCONFIG_FILE=\"${PARENT_CONFIG_FILE}${FULL_PROJECT_NAME}.json\"\n\ncolossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.78 --master_port 30015 train.py \\\n    --pretrained $PRETRAINED_MODEL_PATH \\\n    --dataset ${dataset[@]} \\\n    --plugin \"gemini_auto\" \\\n    --save_interval 400 \\\n    --save_dir $SAVE_DIR \\\n    --tensorboard_dir $TENSORBOARD_DIR \\\n    --config_file $CONFIG_FILE \\\n    --num_epochs 1 \\\n    --micro_batch_size 8 \\\n    --lr 1e-4 \\\n    --mixed_precision \"fp16\" \\\n    --grad_clip 1.0 \\\n    --weight_decay 0.01 \\\n    --warmup_steps 100 \\\n    --use_grad_checkpoint \\\n    --max_length 512 \\\n    --accumulation_steps 1 \\\n    --pad_token \"unk\"\n```\n\n4. hostfile.txt\n\n```plain\nxxx.xx.xx.78\nxxx.xx.xx.79\n```\n\n5. 训练拉起截图\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image9.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image10.png)\n\n6. 78，79显存占用情况\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image11.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image12.png)\n\n###### llama2-70B 5机40卡\n1. 预训练数据集\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image13.png)\n\n由于模型需要对通过脚本对raw原始数据集进行切分，这个步骤会特别耗时，会随着数据集大小的增加而增加切分时长，由于每次拉起训练时都需要切分并消耗很多时间，所以采用了<font style=\"background-color:#c50e0e;\">离线切分的方式将数据集离线切分好并直接挂载读取使用</font>。\n\n<font style=\"background-color:#c50e0e;\">离线切分好的数据集文件共211G，其中会将原始数据集切分成10份并为arrow格式文件</font>。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image14.png)\n\n2. 启动k8s的yaml配置文件\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: llama2-70b-test\n  name: llama2-70b-pt # Deployment 的名称\nspec:\n  replicas: 5 # 副本数量\n  selector:\n    matchLabels:\n      app: llama2-70b-pt\n  template:\n    metadata:\n      labels:\n        app: llama2-70b-pt\n    spec:\n      hostNetwork: true\n      containers:\n      - name: llama2-70b-container # 容器的名称\n        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/bash\", \"-c\"]\n        args: [\"cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ && chmod +x train.example.sh && ./train.example.sh\"] # 启动容器后执行的命令，切换目录并运行训练脚本\n        resources:\n          limits:\n            nvidia.com/gpu: 8 # 限制使用 8 个 GPU，根据实际情况调整\n          requests:\n            nvidia.com/gpu: 8\n        volumeMounts:\n        - name: colossalai-orgdataset\n          mountPath: /opt/tmp/ColossalAI/datas/org_dataset # 将容器内数据目录挂载\n        - name: colossalai-predataset\n          mountPath: /opt/tmp/ColossalAI/datas/pre_dataset2\n        - name: colossalai-weight\n          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载\n        - name: colossalai-train\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py\n          subPath: train.py\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: colossalai-output\n          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载\n        - name: host-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile\n          subPath: hostfile.txt\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: train-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.example.sh  # 容器内train_example.sh\n          subPath: train.example.sh\n        - name: shm-volume\n          mountPath: /dev/shm\n      nodeSelector:\n        model: llama2-70b\n      volumes:\n      - name: colossalai-orgdataset\n        hostPath:\n          path: /home/disk_sdb0/llama2/datas/org_dataset # 宿主机中 ColossalAI 数据的路径\n          type: Directory\n      - name: colossalai-predataset\n        hostPath:\n          path: /home/disk_sdb0/llama2/datas/pre_dataset2\n          type: Directory\n      - name: colossalai-weight\n        hostPath:\n          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # 宿主机中 ColossalAI 权重的路径\n          type: Directory\n      - name: colossalai-train\n        hostPath:\n          path: /home/disk_sdb0/llama2\n          type: Directory\n      - name: colossalai-output\n        hostPath:\n          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径\n          type: Directory\n      - name: host-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: train-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: profiler-script\n        hostPath:\n            path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n            type: Directory\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 8Gi\n```\n\n3. 训练脚本\n\n```bash\n#!/bin/bash\n\n# NCCL IB environment variables\nexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1\nexport NCCL_IB_DISABLE=0\nexport NCCL_SOCKET_IFNAME=bond4\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_IB_TIMEOUT=23\nexport NCCL_IB_RETRY_CNT=7\nexport OMP_NUM_THREADS=8\n\nPROJECT_NAME=\"llama2-70b-pt\"\nPARENT_SAVE_DIR=\"/opt/tmp/ColossalAI/output/save/\"\nPARENT_TENSORBOARD_DIR=\"/opt/tmp/ColossalAI/output/tensorboard/\"\nPARENT_CONFIG_FILE=\"/opt/tmp/ColossalAI/output/config/\"\nPRETRAINED_MODEL_PATH=\"/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf\"\ndeclare -a dataset=(\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009\"\n)\n\nTIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)\nFULL_PROJECT_NAME=\"${PROJECT_NAME}-${TIMESTAMP}\"\nSAVE_DIR=\"${PARENT_SAVE_DIR}${FULL_PROJECT_NAME}\"\nTENSORBOARD_DIR=\"${PARENT_TENSORBOARD_DIR}${FULL_PROJECT_NAME}\"\nCONFIG_FILE=\"${PARENT_CONFIG_FILE}${FULL_PROJECT_NAME}.json\"\n\ncolossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.142 --master_port 30015 train.py \\\n    --pretrained $PRETRAINED_MODEL_PATH \\\n    --dataset ${dataset[@]} \\\n    --plugin \"gemini_auto\" \\\n    --save_interval 1600 \\\n    --save_dir $SAVE_DIR \\\n    --tensorboard_dir $TENSORBOARD_DIR \\\n    --config_file $CONFIG_FILE \\\n    --num_epochs 1 \\\n    --micro_batch_size 16 \\\n    --lr 1e-4 \\\n    --mixed_precision \"fp16\" \\\n    --grad_clip 1.0 \\\n    --weight_decay 0.01 \\\n    --warmup_steps 100 \\\n    --use_grad_checkpoint \\\n    --max_length 512 \\\n    --accumulation_steps 1 \\\n    --pad_token \"unk\"\n```\n\n4. hostfile.txt\n\n```plain\nxxx.xx.xx.142\nxxx.xx.xx.143\nxxx.xx.xx.78\nxxx.xx.xx.79\nxxx.xx.xx.80\n```\n\n5. 参数配置调优\n\n| micro_batch_size | max_length | plugin | mixed_precision | tflops |\n| --- | --- | --- | --- | --- |\n| 1 | 512 | gemini_auto | fp16 | 10.8231 |\n| 2 | 512 | gemini_auto | fp16 | 20.4685 |\n| 4 | 512 | gemini_auto | fp16 | 45.6885 |\n| 8 | 512 | gemini_auto | fp16 | 78.6885 |\n| **16** | **512** | **gemini_auto** | **fp16** | **136.5975** |\n| 16 | 512  | zero2 | fp16 | OOM |\n| 16 | 512 | 3d | fp16 | OOM |\n| 1 | 1024 | gemini_auto | fp16 | OOM |\n| 4 | 2048 | gemini_auto | fp16 | OOM |\n| 16 | 1024 | gemini_auto | fp16 | OOM |\n\n\n6. 训练拉起截图\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image15.png)\n\n总体step1519\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image16.png)\n\n7. 142，143，78，79，80的GPU显存占用\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image17.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image18.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image19.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image20.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image21.png)\n\n8. XXX平台监控\n\n6台机器的GPU性能监控\n涉及客户信息，图片略去。\n单机器141的GPU性能监控\n涉及客户信息，图片略去。\n\n9. 断点续训\n\n7.4日晚上由于142、143、78、79、80五台机器的ssh没有设置免密登录，模型预训练过程中在主节点142保存的checkpoint文件不能及时同步给其他143、78、79、80四个节点，故导致了训练中断，现五台机器的ssh已互相免密，checkpoint信息可以通过主节点142来下发共享。\n\n之后设置读取断点400步后保存的checkpoint与权重信息，拉起训练时会从第400步开始继续训练。\n\n加入`--load_checkpoint $PARENT_LOAD_DIR \\`参数，设定加载的checkpoint位置`PARENT_LOAD_DIR=\"/opt/tmp/ColossalAI/output/save/llama2-70b-pt-2024-07-03-14-11-44/epoch-0_step-400\"`\n\n```bash\n#!/bin/bash\n\n# NCCL IB environment variables\nexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1\nexport NCCL_IB_DISABLE=0\nexport NCCL_SOCKET_IFNAME=bond4\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_IB_TIMEOUT=23\nexport NCCL_IB_RETRY_CNT=7\nexport OMP_NUM_THREADS=8\n\nPROJECT_NAME=\"llama2-70b-pt\"\nPARENT_SAVE_DIR=\"/opt/tmp/ColossalAI/output/save/\"\nPARENT_LOAD_DIR=\"/opt/tmp/ColossalAI/output/save/llama2-70b-pt-2024-07-03-14-11-44/epoch-0_step-400\"\nPARENT_TENSORBOARD_DIR=\"/opt/tmp/ColossalAI/output/tensorboard/\"\nPARENT_CONFIG_FILE=\"/opt/tmp/ColossalAI/output/config/\"\nPRETRAINED_MODEL_PATH=\"/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf\"\ndeclare -a dataset=(\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00000\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00001\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00002\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00003\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00004\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00005\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00006\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00007\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00008\"\n    \"/opt/tmp/ColossalAI/datas/pre_dataset2/arrow/part-00009\"\n)\n\nTIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)\nFULL_PROJECT_NAME=\"${PROJECT_NAME}-${TIMESTAMP}\"\nSAVE_DIR=\"${PARENT_SAVE_DIR}${FULL_PROJECT_NAME}\"\nTENSORBOARD_DIR=\"${PARENT_TENSORBOARD_DIR}${FULL_PROJECT_NAME}\"\nCONFIG_FILE=\"${PARENT_CONFIG_FILE}${FULL_PROJECT_NAME}.json\"\n\ncolossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.142 --master_port 30015 train.py \\\n    --pretrained $PRETRAINED_MODEL_PATH \\\n    --dataset ${dataset[@]} \\\n    --plugin \"gemini_auto\" \\\n    --save_interval 1600 \\\n    --save_dir $SAVE_DIR \\\n    --load_checkpoint $PARENT_LOAD_DIR \\\n    --tensorboard_dir $TENSORBOARD_DIR \\\n    --config_file $CONFIG_FILE \\\n    --num_epochs 1 \\\n    --micro_batch_size 16 \\\n    --lr 1e-4 \\\n    --mixed_precision \"fp16\" \\\n    --grad_clip 1.0 \\\n    --weight_decay 0.01 \\\n    --warmup_steps 100 \\\n    --use_grad_checkpoint \\\n    --max_length 512 \\\n    --accumulation_steps 1 \\\n    --pad_token \"unk\"\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image22.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image23.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image24.png)\n\n8. 预训练结束\n\n结束后会在设定的主节点142上生成对应权重文件\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image25.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image26.png)\n\n\n#####  微调\n###### llama2-70B 5机40卡\n1. 数据集准备\n\n微调数据集参照Colossalai官网要求的数据对话格式进行准备\n\n数据样式\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image27.png)\n\n数据集的离线切分，需要通过kubectl进入pod内执行切分脚本`prepare_sft_dataset.py`进行切分\n\n```bash\n#!/bin/bash\npython3 prepare_sft_dataset.py \\\n--data_input_dirs \"/opt/tmp/ColossalAI/applications/Colossal-LLaMA/data\" \\\n--tokenizer_dir \"/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf\" \\\n--data_output_dirs \"/opt/tmp/ColossalAI/applications/Colossal-LLaMA/sftdata\" \\\n--max_length 4096 \\\n--num_spliced_dataset_bins 10 \\\n--llama_version 2\n```\n\n执行切分脚本后，开始进行切分，会将原始数据集格式jsonl转为arrow格式\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image28.png)\n\n`tar -cvf sftdata.tar sftdata` 压缩切分好的数据集sftdata\n\n`kubectl cp llama2-70b-test/llama2-70b-pt-f5877d7c9-n7448:/opt/tmp/ColossalAI/applications/Colossal-LLaMA/sftdata.tar /home/disk_sdb0/llama2/sftdata.tar`通过`kubectl cp`将压缩好的sftdata.tar传输至宿主机137上\n\n`tar -xvf sftdata.tar`解压\n\n切分好的数据集为52G\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image29.png)\n\n2. 启动k8s的yaml的配置文件\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: llama2-70b-test\n  name: llama2-70b-pt # Deployment 的名称\nspec:\n  replicas: 5 # 副本数量\n  selector:\n    matchLabels:\n      app: llama2-70b-pt\n  template:\n    metadata:\n      labels:\n        app: llama2-70b-pt\n    spec:\n      hostNetwork: true\n      containers:\n      - name: llama2-70b-container # 容器的名称\n        image: xxx.xx.xx..baidu.com/public/llama2-70b:v1.0\n        imagePullPolicy: IfNotPresent\n        command: [\"/bin/bash\", \"-c\"]\n        args: [\"cd /opt/tmp/ColossalAI/applications/Colossal-LLaMA/ && chmod +x finetune.example.sh && ./finetune.example.sh\"] # 启动容器后执行的命令，切换目录并运行训练脚本\n        resources:\n          limits:\n            nvidia.com/gpu: 8 # 限制使用 8 个 GPU，根据实际情况调整\n          requests:\n            nvidia.com/gpu: 8\n        volumeMounts:\n        - name: colossalai-sftdataset\n          mountPath: /opt/tmp/ColossalAI/datas/sftdata\n        - name: colossalai-weight\n          mountPath: /opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf # 将容器内权重目录挂载\n        - name: colossalai-train\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/train.py\n          subPath: train.py\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: colossalai-output\n          mountPath: /opt/tmp/ColossalAI/output # 将容器内输出目录挂载\n        - name: host-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/hostfile.txt  # 将容器内hostfile\n          subPath: hostfile.txt\n          readOnly: true # 表示将挂载的文件设置为只读模式，这意味着容器内的进程只能读取文件，而不能修改它\n        - name: finetune-script\n          mountPath: /opt/tmp/ColossalAI/applications/Colossal-LLaMA/finetune.example.sh  # 容器内finetune.example.sh\n          subPath: finetune.example.sh\n        - name: shm-volume\n          mountPath: /dev/shm\n      nodeSelector:\n        model: llama2-70b\n      volumes:\n      - name: colossalai-sftdataset\n        hostPath:\n          path: /home/disk_sdb0/llama2/datas/sftdata\n          type: Directory\n      - name: colossalai-weight\n        hostPath:\n          path: /home/disk_sdb0/llama2/Llama-2-70b-hf # 宿主机中 ColossalAI 权重的路径\n          type: Directory\n      - name: colossalai-train\n        hostPath:\n          path: /home/disk_sdb0/llama2\n          type: Directory\n      - name: colossalai-output\n        hostPath:\n          path: /home/disk_sdb0/llama2/output # 宿主机中 ColossalAI 输出的路径\n          type: Directory\n      - name: host-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: finetune-script\n        hostPath:\n          path: /home/disk_sdb0/llama2 # 宿主机中包含训练脚本的路径\n          type: Directory\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 8Gi\n```\n\n3. 微调脚本\n\n```bash\n#!/bin/bash\n\n# NCCL IB environment variables\nexport NCCL_IB_HCA=mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1\nexport NCCL_IB_DISABLE=0\nexport NCCL_SOCKET_IFNAME=bond4\nexport NCCL_IB_GID_INDEX=3\nexport NCCL_IB_TIMEOUT=23\nexport NCCL_IB_RETRY_CNT=7\nexport OMP_NUM_THREADS=8\n#export NCCL_P2P_DISABLE=1\nexport NCCL_DEBUG=INFO\n\nPROJECT_NAME=\"llama2-70b-ft\"\nPARENT_SAVE_DIR=\"/opt/tmp/ColossalAI/output/save/\"\nPARENT_TENSORBOARD_DIR=\"/opt/tmp/ColossalAI/output/tensorboard/\"\nPARENT_CONFIG_FILE=\"/opt/tmp/ColossalAI/output/config/\"\nPRETRAINED_MODEL_PATH=\"/opt/tmp/ColossalAI/datas/weight/Llama-2-7b-hf\"\ndeclare -a dataset=(\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00000\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00001\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00002\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00003\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00004\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00005\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00006\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00007\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00008\"\n    \"/opt/tmp/ColossalAI/datas/sftdata/arrow/part-00009\"\n)\n\nTIMESTAMP=$(date +%Y-%m-%d-%H-%M-%S)\nFULL_PROJECT_NAME=\"${PROJECT_NAME}-${TIMESTAMP}\"\nSAVE_DIR=\"${PARENT_SAVE_DIR}${FULL_PROJECT_NAME}\"\nTENSORBOARD_DIR=\"${PARENT_TENSORBOARD_DIR}${FULL_PROJECT_NAME}\"\nCONFIG_FILE=\"${PARENT_CONFIG_FILE}${FULL_PROJECT_NAME}.json\"\n\ncolossalai run --nproc_per_node 8 --hostfile hostfile.txt --master_addr xxx.xx.xx.142 --master_port 30013 train.py \\\n    --pretrained $PRETRAINED_MODEL_PATH \\\n    --dataset ${dataset[@]} \\\n    --plugin \"gemini\" \\\n    --save_interval 400 \\\n    --save_dir $SAVE_DIR \\\n    --tensorboard_dir $TENSORBOARD_DIR \\\n    --config_file $CONFIG_FILE \\\n    --num_epochs 1 \\\n    --accumulation_steps 1 \\\n    --micro_batch_size 1 \\\n    --lr 5e-5 \\\n    --mixed_precision \"bf16\" \\\n    --grad_clip 1.0 \\\n    --weight_decay 0.01 \\\n    --warmup_steps 100 \\\n    --use_grad_checkpoint \\\n    --use_neft \\\n    --pad_token \"eos\"\n```\n\n4. hostfile.txt\n\n```plain\nxxx.xx.xx.142\nxxx.xx.xx.143\nxxx.xx.xx.78\nxxx.xx.xx.79\nxxx.xx.xx.80\n```\n\n5. 拉起微调训练\n\n```bash\n#查看pod的详细信息\nkubectl get pod -n llama2-70b-test -owide\n#删除启动微调训练的yaml\nkubectl delete -f llama2-70b-finetune.yaml\n#启动微调训练的yaml\nkubectl apply -f llama2-70b-finetune.yaml\n#查看pod日志\nkubectl logs -n llama2-70b-test llama2-70b-pt-7b5884d87c-pmd76 -f\n#查看pod状态信息\nkubectl describe pod -n llama2-70b-test llama2-70b-pt-6599d788c7-7nngz\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image30.png)\n\n6. 参数配置及调优\n\n| micro_batch_size | accumulation_steps | plugin | mixed_precision | tflops |\n| --- | --- | --- | --- | --- |\n| 1 | 1 | gemini | bf16 | 38.8488 |\n| 4 | 1 | gemini | bf16 | OOM |\n| 8 | 1 | gemini | bf16 | OOM |\n| 1 | 1 | gemini_auto | bf16 | OOM |\n| 8 | 1 | gemini_auto | bf16 | OOM |\n| 16 | 1 | gemini_auto | bf16 | OOM |\n| 1 | 1 | zero2 | bf16 | OOM |\n| 1 | 1 | 3d | bf16 | OOM |\n| 1 | 4 | gemini | bf16 | OOM |\n| 1 | 8 | gemini | bf16 | OOM |\n\n\n7. 142，143，78，79，80的GPU显存占用\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image31.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image32.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image33.png)  \n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image34.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image35.png)\n\n8. 备注\n\n在拉起微调训练时，尝试调整了训练参数micro_batch_size、accumulation_steps、并行策略gemini、gemini_auto、zero2等参数来尽量缩小总的迭代step步数（数据集的大小也会影响step数），但尝试增加了micro_batch_size等参数后，发现训练会拉不起来（out of memory），当前5机40卡可以成功拉起的参数只能将micro_batch_size调整为1，accumulation_steps为1、并行策略为gemini，总的迭代步数为<font style=\"color:#DF2A3F;\">35132</font>步（<font style=\"background-color:#c50e0e;\">每步迭代耗时需要3分钟左右，</font><font style=\"color:rgb(6, 6, 7);background-color:#c50e0e;\">在每一步迭代中，首先进行前向传播来计算预测输出和损失，然后进行反向传播来计算梯度，最后根据梯度更新模型参数。每个批次都会进行一次梯度和损失的计算，直到完成一个epoch的训练。</font>）\n\n类似参照某模型175B（千卡规模）的拉起的训练进度，总的迭代步数为<font style=\"color:#DF2A3F;\">574833。</font>\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image36.png)\n\n类似参照某模型57B（千卡规模）的拉起训练进度，总的迭代步数<font style=\"color:#DF2A3F;\">574636。</font>\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Llama2/image37.png)\n\n<font style=\"background-color:#c50e0e;\">故完成整套大模型的训练会持续较长的训练周期</font>，并在现有的卡数限制上，在短期内实现所有流程存在难度。\n\n### 结论与说明\n#### 现有环境训练模型参数大小及显存占用\n现有的5机40卡预训练/微调llama2-70B，模型参数及显存占用如下表格所示（<font style=\"background-color:#c50e0e;\">注：由于xxx.xx.xx.141机器日常有业务在跑，无法空闲出完整的8块gpu资源，故最多以</font>**<font style=\"color:#DF2A3F;background-color:#c50e0e;\">5机40卡</font>**<font style=\"background-color:#c50e0e;\">来拉起训练。</font>）\n\n| | 模型参数 | 机器 | GPU卡 | 显存占用 | GPU利用率 |\n| --- | --- | --- | --- | --- | --- |\n| 预训练 | 70B | 142 | 0 | 77886MiB/81920MiB | 99% |\n| | | | 1 | 77898MiB/81920MiB | 99% |\n| | | | 2 | 77898MiB/81920MiB | 99% |\n| | | | 3 | 77898MiB/81920MiB | 99% |\n| | | | 4 | 79946MiB/81920MiB | 99% |\n| | | | 5 | 77898MiB/81920MiB | 99% |\n| | | | 6 | 77898MiB/81920MiB | 99% |\n| | | | 7 | 77874MiB/81920MiB | 100% |\n| | | 143 | 0 | 77886MiB/81920MiB | 100% |\n| | | | 1 | 77898MiB/81920MiB | 100% |\n| | | | 2 | 77898MiB/81920MiB | 100% |\n| | | | 3 | 77898MiB/81920MiB | 99% |\n| | | | 4 | 77898MiB/81920MiB | 99% |\n| | | | 5 | 77886MiB/81920MiB | 99% |\n| | | | 6 | 77898MiB/81920MiB | 99% |\n| | | | 7 | 77886MiB/81920MiB | 99% |\n| | | 78 | 0 | 78764MiB/81920MiB | 99% |\n| | | | 1 | 78776MiB/81920MiB | 99% |\n| | | | 2 | 78776MiB/81920MiB | 99% |\n| | | | 3 | 78776MiB/81920MiB | 99% |\n| | | | 4 | 78776MiB/81920MiB | 100% |\n| | | | 5 | 78776MiB/81920MiB | 100% |\n| | | | 6 | 78776MiB/81920MiB | 100% |\n| | | | 7 | 78764MiB/81920MiB | 99% |\n| | | 79 | 0 | 77886MiB/81920MiB | 99% |\n| | | | 1 | 77898MiB/81920MiB | 99% |\n| | | | 2 | 77886MiB/81920MiB | 99% |\n| | | | 3 | 77898MiB/81920MiB | 100% |\n| | | | 4 | 77898MiB/81920MiB | 99% |\n| | | | 5 | 77898MiB/81920MiB | 99% |\n| | | | 6 | 77898MiB/81920MiB | 100% |\n| | | | 7 | 77886MiB/81920MiB | 100% |\n| | | 80 | 0 | 77886MiB/81920MiB | 99% |\n| | | | 1 | 77898MiB/81920MiB | 99% |\n| | | | 2 | 79948MiB/81920MiB | 99% |\n| | | | 3 | 77898MiB/81920MiB | 99% |\n| | | | 4 | 78350MiB/81920MiB | 99% |\n| | | | 5 | 79386MiB/81920MiB | 99% |\n| | | | 6 | 79374MiB/81920MiB | 100% |\n| | | | 7 | 78338MiB/81920MiB | 99% |\n| 微调 | 70B | 142 | 0 | 53278MiB/81920MiB | 100% |\n| | | | 1 | 52160MiB/81920MiB | 100% |\n| | | | 2 | 53660MiB/81920MiB | 100% |\n| | | | 3 | 53164MiB/81920MiB | 100% |\n| | | | 4 | 52382MiB/81920MiB | 100% |\n| | | | 5 | 53260MiB/81920MiB | 100% |\n| | | | 6 | 53660MiB/81920MiB | 100% |\n| | | | 7 | 54652MiB/81920MiB | 100% |\n| | | 143 | 0 | 53494MiB/81920MiB | 100% |\n| | | | 1 | 53254MiB/81920MiB | 100% |\n| | | | 2 | 53950MiB/81920MiB | 100% |\n| | | | 3 | 53172MiB/81920MiB | 100% |\n| | | | 4 | 67234MiB/81920MiB | 100% |\n| | | | 5 | 53212MiB/81920MiB | 100% |\n| | | | 6 | 53160MiB/81920MiB | 100% |\n| | | | 7 | 53604MiB/81920MiB | 100% |\n| | | 78 | 0 | 51978MiB/81920MiB | 100% |\n| | | | 1 | 51992MiB/81920MiB | 100% |\n| | | | 2 | 51410MiB/81920MiB | 100% |\n| | | | 3 | 51492MiB/81920MiB | 100% |\n| | | | 4 | 51992MiB/81920MiB | 100% |\n| | | | 5 | 51488MiB/81920MiB | 100% |\n| | | | 6 | 52154MiB/81920MiB | 100% |\n| | | | 7 | 47638MiB/81920MiB | 100% |\n| | | 79 | 0 | 48468MiB/81920MiB | 100% |\n| | | | 1 | 49280MiB/81920MiB | 100% |\n| | | | 2 | 49182MiB/81920MiB | 100% |\n| | | | 3 | 48980MiB/81920MiB | 100% |\n| | | | 4 | 49160MiB/81920MiB | 100% |\n| | | | 5 | 49966MiB/81920MiB | 100% |\n| | | | 6 | 49160MiB/81920MiB | 100% |\n| | | | 7 | 49858MiB/81920MiB | 100% |\n| | | 80 | 0 | 49366MiB/81920MiB | 100% |\n| | | | 1 | 48740MiB/81920MiB | 100% |\n| | | | 2 | 49400MiB/81920MiB | 99% |\n| | | | 3 | 47470MiB/81920MiB | 99% |\n| | | | 4 | 36662MiB/81920MiB | 100% |\n| | | | 5 | 37262MiB/81920MiB | 100% |\n| | | | 6 | 37660MiB/81920MiB | 99% |\n| | | | 7 | 37148MiB/81920MiB | 100% |\n\n\n#### AI平台训练可视化分析能力\n4.1中描述的关于GPU的显存使用及利用率情况，主要是通过`nvidia-smi`NVIDIA提供的命令行工具监控分析GPU设备信息，其中的信息包括\n\n1. GPU 利用率（GPU-Util）：显示 GPU 核心的利用率百分比。\n2. 显存使用情况（Memory-Usage）：显示 GPU 当前显存的使用量/总显存量。\n3. 功耗（Pwr:Usage/Cap）：显示 GPU 的当前/最大功耗。\n4. 温度（GPU Temperature）：显示 GPU 的当前温度。\n5. 进程 ID（PID）：显示当前使用 GPU 的进程的进程 ID。\n6. 进程名称：显示使用 GPU 的进程的名称。\n7. 使用 GPU 的应用程序：显示哪些应用程序正在使用 GPU。\n8. GPU 型号：显示 GPU 的型号和版本信息。\n9. 驱动版本：显示当前安装的 NVIDIA 驱动版本。\n10. CUDA 版本：显示当前安装的 CUDA 版本。\n11. ECC 状态：显示错误校正码（ECC）的状态。\n\n对比XXX监控平台的数据，两者在GPU显存使用和GPU利用率上可能会存在差异，由于`nvidia-smi`是实时监控当前时间下的GPU信息，XXX监控平台的信息与`nvidia-smi`监控信息存在时间差，故存在某些数据不同步，这也在合理的范围内。\n\nXXX监控平台包含了各机器的单个GPU的显存使用、GPU利用率、温度、功耗等信息，基本覆盖了模型训练监控所需的一些指标。此外，XXX中还显示了GPU的时钟频率，也代表了GPU性能的关键指标。\n\nXXX监控平台可以进一步监控机间网卡的通信带宽、机内单向通信带宽、Leaf层/spine层交换机信息、模型训练参数等等大模型训练的物理层与软件层信息。\n\n#### 用户的易用性,长时间训练,任务断点,监控告警,自动化拉起\n1. 本次训练采用k8s的方式来管理和拉起集群pod节点，其中涉及到一些拉起k8s的yaml配置文件、以及集群机器的ssh通信配置、etc/hosts主机ip名映射配置、标签管理等步骤，可能需要用户熟悉并操作。此外，机器的驱动环境可以按期更新迭代，以此适应和匹配最新系列大模型训练所需要的新特性。\n2. 本次预训练任务长达70h+，其中存在训练中断问题，当前的方法是通过手动拉起和利用训练脚本中save_interval的间隔步数保存的checkpoint信息来恢复至间隔步数的训练状态。\n3. 训练的监控告警可以通过kubectl命令查看pod`kubectl describe pod`、node`kubectl describe node`、训练日志`kubectl logs`里的错误信息进行排错处理。\n4. 自动化拉起可能需要EKI（k8s管理）平台与PyTorchJob 支持，EKI可以自动化处理pod节点异常并自动拉起新的节点。\n\n","tags":["模型训练调优","Nvidia","Llama2","A100","Colossal-AI"],"categories":["模型训练调优","NVIDIA"]},{"title":"Tulu3-8B-后训练实践","url":"/2025/11/12/模型训练调优/NVIDIA/Tulu3/Tulu3-8B-后训练实践/","content":"# 训练环境\n## 镜像\n```bash\nregistry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:tulu3\n```\n\n镜像中已包含tulu3后训练所需要的所有依赖环境。\n\n## 代码结构\n```bash\n/mnt/users/wangxiangbo/model/tulu3/open-instruct-main\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image1.png)\n\n### data\n包含了`sft`、`dpo`、`rlvr`所需的数据集\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image2.png)\n\n`eval`相关\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image3.png)\n\n### scripts/configs\n训练脚本\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image4.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image5.png)\n\n```bash\ntulu3_sft.sh #tulu3 sft单机脚本\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image6.png)\n\n```bash\nhostfile\nhostfile-12\nsetRank.sh\ntulu3_dpo.sh #多机dpo脚本\ntulu3_dpo_single.sh #单机4卡dpo脚本\ntulu3_dpo.yaml #多机yaml\ntulu3_sft.sh #多机sft脚本\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image7.png)\n\n```bash\ntulu3_rlvr_single.sh #tulu3 rlvr单机脚本\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image8.png)\n\n```bash\nloss_plot.py #loss曲线绘图脚本\n```\n\n### open_instruct\nsft、dpo、rlvr相关训练代码\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image9.png)\n\n\n\n## sft实践\n### 单机4卡测试\n#### sft数据集\nsft数据集采用tulu3官网提供的数据集\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image10.png)\n\nsft数据集样例：\n\n```json\n{\"id\":\"oasst1_2644\",\"messages\":[{\"content\":\"有缓解社交恐惧症的实用方法吗？\",\"role\":\"user\"},{\"content\":\"以下是一些实用的方法，可能有助于缓解社交恐惧症：\\n\\n1.  寻求专业帮助：寻求心理医生或心理治疗师的帮助是缓解社交恐惧症的最佳方法之一。专业的治疗师可以帮助你识别和处理恐惧的源头，并提供有效的治疗方案。\\n\\n2. 练习深呼吸和放松技巧：当你感到焦虑和紧张时，尝试通过深呼吸和放松技巧来帮助你冷静下来。深呼吸可以帮助你减轻身体上的压力和紧张感，从而使你更放松和镇定。\\n\\n3. 学习积极思考：避免消极的自我对话，例如“我会失败”、“我不行”等等。改变自己的思考方式，尝试以积极、自信的态度面对社交场合。\\n\\n4.  採取柔性方法：為了對抗焦慮症狀，建議使用一些方法來提醒你的身體是安全的，例如：  \\n\\n＊ 呼吸訓練（用鼻子深深的吸一口氣再用嘴巴將氣慢慢地吐出，當你專注在吐氣時，你就會被帶出抵抗或逃避的情緒中。） \\n\\n ＊ 專注於現在（這麼做讓你能夠專注於自己所經歷的每分每秒。試著說出你現在能看見、聞到、碰觸到並聽得到的東西，或是坐在椅子上並感受自己的腳觸碰地面的感覺，或身體正被支撐著的感覺。）\\n\\n5. 給自己一點從容：在過渡時期練習自我同情，並提醒自己感到不確定與不安是很正常的。\\n\\n这些方法可以帮助你缓解社交恐惧症，但不同的人可能需要不同的方法和治疗方案。如果你感到严重的焦虑和恐惧，请及时寻求专业帮助。\",\"role\":\"assistant\"},{\"content\":\"好吧，可是我要去哪裡尋求專業幫助？\",\"role\":\"user\"},{\"content\":\"您可以先參考美國精神疾病診斷標準 **DSM-5** 中的6點標準，或者進行一些專業的心理測試。如果您符合這些標准或測試結果顯示您有社交恐懼症，您應儘早尋求專業的幫助。\\n\\n尋求專業幫助的途徑有以下幾種：\\n\\n- **諮詢醫生或心理健康專家**。他們可以為您提供診斷，藥物治療，心理治療等服務。\\n- **參加心理諮詢或治療機構**。這些機構通常有專業的心理諮詢師或治療師，可以為您提供個人或團體的心理諮詢或治療服務。\\n- **加入互助小組或社區**。這些小組或社區通常由有相同或類似問題的人組成，可以為您提供支持，鼓勵，交流和分享的平台。\\n\\n以上是我為您搜索到的一些信息，希望對您有所幫助。如果您還有其他問題，歡迎繼續提問。\",\"role\":\"assistant\"}],\"source\":\"ai2-adapt-dev\\/oasst1_converted\"}\n```\n\n由于是.parquet格式数据，转成json后一共有93w条数据。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image11.png)\n\n由于机器数量有限，训练其完整数据集需要较长时间，故将.parquet转为json后将数据集缩减至5k条，并再次转回.parquet格式数据。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image12.png)\n\n#### 单机脚本配置\n```bash\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\nexport WANDB_MODE=disabled\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nLOG_SAVE=\"/mnt/open-instruct-main/output/sft/${current_time}-tulu3-sft.log\"\nMODEL_SIZE=8B\nNUM_GPUS=4\nBATCH_SIZE_PER_GPU=2\nTOTAL_BATCH_SIZE=8\nGRADIENT_ACC_STEPS=$(($TOTAL_BATCH_SIZE/($NUM_GPUS * $BATCH_SIZE_PER_GPU)))\necho \"Training llama model ${MODEL_SIZE} using $NUM_GPUS GPUs, $BATCH_SIZE_PER_GPU batch size per GPU, $GRADIENT_ACC_STEPS gradient accumulation steps\"\n\naccelerate launch \\\n    --mixed_precision fp16 \\\n    --num_machines 1 \\\n    --num_processes $NUM_GPUS \\\n    --use_deepspeed \\\n    --deepspeed_config_file /mnt/open-instruct-main/configs/ds_configs/stage3_offloading_accelerate.conf \\\n    /mnt/open-instruct-main/open_instruct/finetune.py \\\n    --model_name_or_path /mnt/LLM-Research/Meta-Llama-31-8B \\\n    --tokenizer_name /mnt/LLM-Research/Meta-Llama-31-8B \\\n    --use_slow_tokenizer \\\n    --train_file /mnt/open-instruct-main/data/sft_data_json/sft_dataset_5k.json \\\n    --max_seq_length 4096 \\\n    --preprocessing_num_workers 4 \\\n    --per_device_train_batch_size $BATCH_SIZE_PER_GPU \\\n    --gradient_accumulation_steps $GRADIENT_ACC_STEPS \\\n    --learning_rate 2e-5 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.03 \\\n    --weight_decay 0. \\\n    --num_train_epochs 1 \\\n    --output_dir /mnt/open-instruct-main/output/sft \\\n    --gradient_checkpointing true \\\n    --report_to none \\\n    --use_flash_attn false 2>&1 | tee -a \"$LOG_SAVE\"\n```\n\n为了尽可能地还原tulu3原论文的实验结果，其中部分超参与论文中最优保持一致。\n\n`--max_seq_length 4096`\n\n`--learning_rate 2e-5`\n\n`--lr_scheduler_type linear`\n\n`--warmup_ratio 0.03`\n\n其中的模型权重利用llama3.1-8B-base模型。\n\n#### 训练测试\n##### 训练结束\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image13.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image14.png)\n\n| 资源类型 | 利用率 | 内存/显存占用量 |\n| :---: | :---: | :---: |\n| CPU | 53.2% | 80.7% |\n| GPU | 85.5% | 66.4% |\n\n\n##### loss曲线\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image15.png)\n\n##### eval\nllama3.1-8B通过tulu3提供的sft数据集（删减至5k条）后，得到的tulu3_sft权重\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image16.png)\n\n| 类别 | Llama-31-8B | tulu3-sft-weight | Llama-31-Tulu-3-8B-SFT |\n| :---: | :---: | :---: | :---: |\n| 平均准确率 | 0.6014 | 0.5729 | **<font style=\"color:#DF2A3F;\">0.6356</font>** |\n| 子类别准确率 | - | - | **<font style=\"color:#000000;\">-</font>** |\n| 数学 | 0.4098 | 0.3778 | **0.4192** |\n| 健康 | 0.6561 | 0.6183 | **0.6720** |\n| 物理 | 0.5063 | 0.4891 | **0.5328** |\n| 商业 | 0.7643 | 0.7346 | **0.8101** |\n| 生物 | 0.7555 | 0.6982 | **0.7775** |\n| 化学 | 0.4851 | 0.4851 | **0.5186** |\n| 计算机科学 | 0.5752 | 0.5194 | **0.6092** |\n| 经济 | 0.5930 | 0.5809 | **0.6213** |\n| 工程 | 0.5517 | 0.5172 | **0.5517** |\n| 哲学 | 0.4881 | 0.4627 | **0.5775** |\n| 其他 | 0.6524 | 0.6532<font style=\"color:#000000;\">⬆</font> | **0.7090** |\n| 历史 | 0.7452 | 0.7172 | **0.7774** |\n| 地理 | 0.7323 | 0.7273 | **0.7677** |\n| 政治 | 0.7546 | 0.7099 | **0.7762** |\n| 心理学 | 0.7174 | 0.6845 | **0.7485** |\n| 文化 | **0.8102** | 0.7440 | 0.7922 |\n| 法律 | 0.4878 | 0.4589 | **0.5020** |\n| 类别准确率 | - | - | - |\n| STEM | 0.5192 | 0.4864 | **0.5394** |\n| 人文学科 | 0.5388 | 0.5116 | **0.5887** |\n| 社会科学 | 0.7062 | 0.6740 | **0.7296** |\n| 其他（商业、健康等） | 0.6693 | 0.6465 | **0.7039** |\n\n\n##### eval结果分析\n+ SFT后权重的平均准确率: 0.5729\n+ SFT前权重的平均准确率: 0.6014\n\n整体平均准确率下降了 ~2.85%。\n\n原因：本次实验（由于机器环境限制）将原始的tulu3 SFT数据集（93w）删减到5k条，导致模型未能充分学习各个领域的知识和任务特性，泛化能力下降。\n\n## dpo实践\n### 单机4卡测试\n#### 偏好数据集\n偏好数据集采用tulu3官网提供的数据集。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image17.png)\n\n偏好数据集样例：\n\n```json\n{\"id\":\"allenai\\/tulu-3-sft-reused-off-policy_0\",\"prompt\":\"Peter has 5 cages with rabbits (the cages are arranged in a row). It is known that there is at least one rabbit in each cage. We will call two rabbits neighbors if they are either in the same cage or in adjacent cages. It turned out that each rabbit has either 3 or 7 neighbors. How many rabbits are in the central cage?\",\"chosen\":[{\"content\":\"Peter has 5 cages with rabbits (the cages are arranged in a row). It is known that there is at least one rabbit in each cage. We will call two rabbits neighbors if they are either in the same cage or in adjacent cages. It turned out that each rabbit has either 3 or 7 neighbors. How many rabbits are in the central cage?\",\"role\":\"user\"},{\"content\":\"Here's how to solve this problem:\\n\\n**Understanding the Information**\\n\\n* **Each rabbit has 3 or 7 neighbors:** This means there must be a pattern in how the rabbits are distributed within the cages.\\n* **At least one rabbit in each cage:**  We can't have cages without any rabbits.\\n\\n**Reasoning**\\n\\n* **Corner Cages:**  Rabbits in the corner cages (cages 1 and 5) can only have 3 neighbors (two adjacent cages and one within their own cage).\\n* **Middle Cages:** Rabbits in the middle cages (cages 2, 3, and 4) must have 7 neighbors to balance out the setup. \\n\\n**The Solution**\\n\\nSince rabbits in the middle cages need 7 neighbors, and there's at least one rabbit in each cage, the central cage (cage 3) must have **4 rabbits**. \\n\\n\\n\\nLet me know if you'd like to explore other logic puzzles! \\n\",\"role\":\"assistant\"}],\"rejected\":[{\"content\":\"Peter has 5 cages with rabbits (the cages are arranged in a row). It is known that there is at least one rabbit in each cage. We will call two rabbits neighbors if they are either in the same cage or in adjacent cages. It turned out that each rabbit has either 3 or 7 neighbors. How many rabbits are in the central cage?\",\"role\":\"user\"},{\"content\":\"The central cage has the same number of rabbits as any other cage, so it also has either 3 or 7 neighbors.\\nThere are (at most) 7 rabbits in the central cage and (at most) 3 rabbits in any cage on either side. Hence, there are (at most) 7 rabbits in each of those cages. The total number of rabbits in the 5 cages is (at most) 5 * 7 + 1 = 36 + 1 = 37.\\nOne cage must have exactly the right number of neighbors, but two cages cannot have the right number.\\nIf each cage has exactly 3 neighbors, the total number of rabbits in all 5 cages is 1 + 5 * 3 = 16\\nIf each cage has exactly 7 neighbors, the total number of rabbits in all 5 cages is 1 + 5 * 7 = 32\\nNow that we know there are at most 36 rabbits, we can safely conclude that there is 1 rabbit in the central cage, that there are 5 cages with 3 rabbits each and one cage with 7 rabbits and that the central cage has 3 neighbors.\\nThe central cage has exactly 3 rabbits and 3 neighbors.\\nIn the other 4 cages, there are (at most) 5 * 3 = 15 rabbits.\\nIn the other 5 cages, there are (at least) 32 – 16 = 16 rabbits.\",\"role\":\"assistant\"}],\"source\":\"allenai\\/tulu-3-sft-reused-off-policy\"}\n```\n\n由于是.parquet格式数据，转成json后一共有67w条数据。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image18.png)\n\n由于机器数量有限，训练其完整数据集需要较长时间，故将.parquet转为json后将数据集缩减至5k条，并再次转回.parquet格式数据。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image19.png)\n\n#### 单机脚本配置\n单机脚本，部分配置OOM，采用deepseed中zero3以及cpu_offloading，开启梯度累积。\n\n```bash\n# you need 8 GPUs for full finetuning\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\nexport WANDB_MODE=disabled\n\nNUM_GPUS=4\nBATCH_SIZE_PER_GPU=1\nTOTAL_BATCH_SIZE=8\nGRADIENT_ACC_STEPS=$(($TOTAL_BATCH_SIZE/$NUM_GPUS/$BATCH_SIZE_PER_GPU))\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nLOG_SAVE=\"/mnt/open-instruct-main/output/dpo/${current_time}-tulu3-dpo.log\"\necho \"Training model using $NUM_GPUS GPUs, $BATCH_SIZE_PER_GPU batch size per GPU, $GRADIENT_ACC_STEPS gradient accumulation steps\"\n\n#stage3_no_offloading_accelerate.conf\naccelerate launch \\\n    --mixed_precision fp16 \\\n    --num_machines 1 \\\n    --num_processes $NUM_GPUS \\\n    --use_deepspeed \\\n    --deepspeed_config_file /mnt/open-instruct-main/configs/ds_configs/stage3_offloading_accelerate.conf \\\n    /mnt/open-instruct-main/open_instruct/dpo_tune.py \\\n    --model_name_or_path /mnt/LLM-Research/Llama-31-Tulu-3-8B-SFT \\\n    --use_flash_attn  false\\\n    --gradient_checkpointing \\\n    --tokenizer_name /mnt/LLM-Research/Llama-31-Tulu-3-8B-SFT \\\n    --use_slow_tokenizer \\\n    --dataset_name /mnt/open-instruct-main/data/dpo_data_5k \\\n    --max_seq_length 2048 \\\n    --preprocessing_num_workers 4 \\\n    --per_device_train_batch_size $BATCH_SIZE_PER_GPU \\\n    --gradient_accumulation_steps $GRADIENT_ACC_STEPS \\\n    --learning_rate 5e-7 \\\n    --lr_scheduler_type linear \\\n    --warmup_ratio 0.1 \\\n    --weight_decay 0. \\\n    --num_train_epochs 1 \\\n    --output_dir /mnt/open-instruct-main/output/dpo \\\n    --with_tracking False \\\n    --logging_steps 1 2>&1 | tee -a \"$LOG_SAVE\"\n```\n\n为了尽可能地还原tulu3原论文的实验结果，其中部分超参与论文中最优保持一致。\n\n`--max_seq_length 2048`\n\n`--learning_rate 5e-7`\n\n`--lr_scheduler_type linear`\n\n`--warmup_ratio 0.1`\n\n其中的模型权重利用tulu3开源的经过llama3.1-8B sft得到的权重。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image20.png)\n\n#### 训练测试\n##### 关闭cpu_offloading\n注：关闭cpu_offloading之后，迭代10步左右会OOM\n\n\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image21.png)\n\n| 资源类型 | 利用率 | 内存/显存占用量 |\n| :---: | :---: | :---: |\n| CPU | 16.5% | 12.1% |\n| GPU | 100% | 93.6% |\n\n\n##### 开启cpu_offloading\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image22.png)\n\n| 资源类型 | 利用率 | 内存/显存占用量 |\n| :---: | :---: | :---: |\n| CPU | 55.0% | 58.2% |\n| GPU | 85.8% | 68.7% |\n\n\n##### 训练完成\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image23.png)\n\n得到的dpo权重\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image24.png)\n\n##### loss曲线\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image25.png)\n\n##### eval\n评测数据集采用MMLU，一个包含来自各个知识领域的多项选择题的巨大多任务测试。 测试涵盖了人文学科、社会科学、自然科学以及其他对某些人来说重要的学习领域。 数据集中的问题是由研究生和本科生从在线免费资源中手动收集的。这包括研究生入学考试和美国医学执照考试等考试的练习题。还包括为本科生课程设计的题目，以及为牛津大学出版社书籍读者设计的题目。 一些任务涵盖一个主题，如心理学，但难度级别特定，例如“初阶”、“高中”、“大学”或“专业”。 例如，“专业心理学”任务借鉴了心理学专业实践考试的免费练习题中的问题，而“高中心理学”任务则包含类似高级 Placement 心理学考试中的问题。\n\n:::tips\nMMLU数据集样例如下，摘自MMLU论文[https://arxiv.org/abs/2009.03300](https://arxiv.org/abs/2009.03300)\n\n:::\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image26.png)\n\neg：以下是MMLU上<font style=\"color:rgb(0,0,0);\">College Biology类问题及答案</font>：\n\n```plain\n\"Based on the characteristic population curves that result from plotting population growth of a species, the most effective means of controlling the mosquito population is to\",\nmaintain the population at a point corresponding to the midpoint of its logistic curve,\nopt for zero population control once the K value of the curve has been reached,\nreduce the carrying capacity cif the environment to lower the K value,\nincrease the mortality rate,\nC\n```\n\neg：以下是`dpo_weight`在MMLU上<font style=\"color:rgb(0,0,0);\">College Biology类问题的回答</font>：\n\n```plain\n0,1,2,3,4,5,correct,choiceA_probs,choiceB_probs,choiceC_probs,choiceD_probs\n\"Based on the characteristic population curves that result from plotting population growth of a species, the most effective means of controlling the mosquito population is to\",\nmaintain the population at a point corresponding to the midpoint of its logistic curve,\nopt for zero population control once the K value of the curve has been reached,\nreduce the carrying capacity cif the environment to lower the K value,\nincrease the mortality rate,\nC,\nTrue,\n0.07324660569429398,\n0.0071399579755961895,\n0.6528399586677551,\n0.1929791122674942\n```\n\n\n\n本次dpo后得到的权重`dpo_weight`对比tulu3开源的sft权重`Llama-31-Tulu-3-8B-SFT`。\n\n`Llama-31-Tulu-3-8B-SFT`在MMLU上的表现：\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image27.png)\n\n`dpo_weight`在MMLU上的表现：\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image28.png)\n\n| 类别 | Llama-31-8B | Llama-31-Tulu-3-8B-SFT | dpo_weight | Llama-31-Tulu-3-8B-DPO |\n| :---: | :---: | :---: | :---: | :---: |\n| 平均准确率 | 0.6014 | 0.6356 | **<font style=\"color:#DF2A3F;\">0.6377</font>**⬆ | 0.6352 |\n| 子类别准确率 | - | **<font style=\"color:#000000;\">-</font>** | **<font style=\"color:#000000;\">-</font>** | - |\n| 数学 | 0.4098 | 0.4192 | **0.4352**⬆ | 0.4258 |\n| 健康 | 0.6561 | 0.6720 | 0.6726⬆ | **0.6915** |\n| 物理 | 0.5063 | 0.5328 | **0.5500**⬆ | 0.5391 |\n| 商业 | 0.7643 | **0.8101** | 0.7941 | 0.7849 |\n| 生物 | 0.7555 | 0.7775 | **0.7775**⬆ | 0.7731 |\n| 化学 | 0.4851 | 0.5186 | 0.5182 | **0.5248** |\n| 计算机科学 | 0.5752 | 0.6092 | **0.6189**⬆ | 0.6117 |\n| 经济 | 0.5930 | 0.6213 | 0.6173 | **0.6429** |\n| 工程 | 0.5517 | 0.5517 | **0.5655**⬆ | 0.5586 |\n| 哲学 | 0.4881 | **0.5775** | 0.5626 | 0.5388 |\n| 其他 | 0.6524 | 0.7090 | **0.7107**⬆ | 0.7082 |\n| 历史 | 0.7452 | 0.7774 | **0.7828**⬆ | 0.7720 |\n| 地理 | 0.7323 | 0.7677 | 0.7778⬆ | **0.7778** |\n| 政治 | 0.7546 | 0.7762 | **0.7793**⬆ | 0.7762 |\n| 心理学 | 0.7174 | 0.7485 | **0.7519**⬆ | 0.7476 |\n| 文化 | **0.8102** | 0.7922 | 0.7892 | 0.8042 |\n| 法律 | 0.4878 | 0.5020 | **0.5133**⬆ | 0.5156 |\n| 父类别准确率 | - | - | **-** | - |\n| STEM | 0.5192 | 0.5394 | **0.5507**⬆ | 0.5437 |\n| 人文学科 | 0.5388 | **0.5887** | 0.5877 | 0.5762 |\n| 社会科学 | 0.7062 | 0.7296 | 0.7309⬆ | **0.7364** |\n| 其他（商业、健康、杂项） | 0.6693 | 0.7039 | 0.7027 | **0.7101** |\n\n\n### dpo代码实现分析\n#### Length-Normalized DPO 的实现\n##### `_get_batch_logps` 函数  \ntulu3论文中说明了，tulu3-8B的dpo使用了长度归一化的DPO（Length-Normalized DPO），以此来消除<font style=\"color:#000000;background-color:#F1A2AB;\">因回答的序列长度（对比来说很长的话）带来的概率偏好影响。</font> 当 `average_log_prob=True` 时，会将每个 token 的 log 概率进行平均处理，实现序列长度归一化。 如果`average_log_prob=False`，则是直接求和。\n\n代码具体实现如下：\n\n```python\ndef _get_batch_logps(\n    logits: torch.FloatTensor, labels: torch.LongTensor, average_log_prob: bool = False\n) -> torch.FloatTensor:\n    # 确保logits和labels的形状在除了最后一个维度外是相同的。\n    assert logits.shape[:-1] == labels.shape\n    # 两行代码分别对labels和logits进行切片操作，去掉了每个序列的第一个和最后一个元素。\n    labels = labels[:, 1:].clone()\n    logits = logits[:, :-1, :]\n    # 这行代码创建了一个掩码loss_mask，用于标识哪些标签不是-100（即不是要被忽略的标签）。\n    loss_mask = labels != -100\n\n    # 这行代码将labels中值为-100的元素替换为0，因为在PyTorch中，0可以作为无效索引。\n    labels[labels == -100] = 0\n\n    # 计算每个token的对数概率。首先对logits应用log_softmax函数，然后在最后一个维度上使用torch.gather根据labels索引来选择对应的对数概率。\n    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n\n    # 根据average_log_prob的值决定返回值。如果average_log_prob为True，则返回每个样本的平均对数概率；否则，返回每个样本的对数概率之和\n    if average_log_prob:\n        return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n    else:\n        return (per_token_logps * loss_mask).sum(-1)\n```\n\n:::tips\n<font style=\"color:rgba(0, 0, 0, 0.85);\">函数_get_batch_logps用于计算给定标签（labels）在给定的模型输出（logits）下的对数概率（log probabilities）。</font>\n\n<font style=\"color:rgba(0, 0, 0, 0.85);\">参数说明：</font>\n\n+ <font style=\"color:rgba(0, 0, 0, 0.85);\">logits</font><font style=\"color:rgba(0, 0, 0, 0.85);\">：模型的输出（未归一化的）。形状为</font><font style=\"color:rgba(0, 0, 0, 0.85);\">(batch_size, sequence_length, vocab_size)</font><font style=\"color:rgba(0, 0, 0, 0.85);\">，其中</font><font style=\"color:rgba(0, 0, 0, 0.85);\">batch_size</font><font style=\"color:rgba(0, 0, 0, 0.85);\">是批次大小，</font><font style=\"color:rgba(0, 0, 0, 0.85);\">sequence_length</font><font style=\"color:rgba(0, 0, 0, 0.85);\">是序列长度，</font><font style=\"color:rgba(0, 0, 0, 0.85);\">vocab_size</font><font style=\"color:rgba(0, 0, 0, 0.85);\">是词汇表的大小。</font>\n+ <font style=\"color:rgba(0, 0, 0, 0.85);\">labels</font><font style=\"color:rgba(0, 0, 0, 0.85);\">：要计算对数概率的标签。值为-100的标签标记将被忽略。形状为</font><font style=\"color:rgba(0, 0, 0, 0.85);\">(batch_size, sequence_length)</font><font style=\"color:rgba(0, 0, 0, 0.85);\">。</font>\n+ <font style=\"color:rgba(0, 0, 0, 0.85);\">average_log_prob</font><font style=\"color:rgba(0, 0, 0, 0.85);\">：一个布尔值，默认为</font><font style=\"color:rgba(0, 0, 0, 0.85);\">False</font><font style=\"color:rgba(0, 0, 0, 0.85);\">。如果为</font><font style=\"color:rgba(0, 0, 0, 0.85);\">True</font><font style=\"color:rgba(0, 0, 0, 0.85);\">，则返回每个（未被掩码的）token的平均对数概率；如果为</font><font style=\"color:rgba(0, 0, 0, 0.85);\">False</font><font style=\"color:rgba(0, 0, 0, 0.85);\">，则返回（未被掩码的）token的对数概率之和。</font>\n\n<font style=\"color:rgba(0, 0, 0, 0.85);\">返回值：</font>\n\n+ <font style=\"color:rgba(0, 0, 0, 0.85);\">返回一个形状为</font><font style=\"color:rgba(0, 0, 0, 0.85);\">(batch_size,)</font><font style=\"color:rgba(0, 0, 0, 0.85);\">的张量，包含给定标签在给定logits下的对数概率的平均值或总和。</font>\n\n<font style=\"color:rgba(0, 0, 0, 0.85);\">该函数是用来计算模型预测的概率，并根据标签来确定哪些预测是有效的（即标签值不为-100）。如果average_log_prob参数为True，则函数返回的是平均对数概率；如果为False，则返回的是总和。</font>\n\n:::\n\n##### `concatenated_forward` 函数  \n 在训练中，`concatenated_forward` 负责对模型的 `logps` 进行计算：  \n\n```python\ndef concatenated_forward(\n    model: nn.Module,\n    batch: Dict[str, Union[List, torch.LongTensor]],\n    average_log_prob: bool = False,\n    output_router_logits: bool = False,\n) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n    concatenated_batch = concatenated_inputs(batch)\n    if output_router_logits:\n        outputs = model(\n            input_ids=concatenated_batch[\"concatenated_input_ids\"],\n            attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n            output_router_logits=True,\n        )\n        logits = outputs.logits.to(torch.float32)\n        aux_loss = outputs.aux_loss\n    else:\n        logits = model(\n            input_ids=concatenated_batch[\"concatenated_input_ids\"],\n            attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n        ).logits.to(torch.float32)\n        aux_loss = None\n    all_logps = _get_batch_logps(logits, concatenated_batch[\"concatenated_labels\"], average_log_prob=average_log_prob)\n    chosen_logps = all_logps[: batch[\"chosen_input_ids\"].shape[0]]\n    rejected_logps = all_logps[batch[\"chosen_input_ids\"].shape[0] :]\n    return chosen_logps, rejected_logps, aux_loss\n```\n\n 参数 `average_log_prob` 是从主脚本`dpo_tune.py`中传递的，当设置为 `True` 时，启用长度归一化。  \n\n##### `dpo_loss` 函数  \n`dpo_loss` 中计算 logits 的差值：  \n\n```python\npi_logratios = policy_chosen_logps - policy_rejected_logps\nref_logratios = reference_chosen_logps - reference_rejected_logps\nlogits = pi_logratios - ref_logratios\n```\n\n 如果 `average_log_prob=True`，则 `policy_chosen_logps` 和 `policy_rejected_logps` 都是归一化的值，从而影响最终的 logits 和损失计算。  \n\n##### 控制参数  \n 在主脚本dpo_tune.py中，通过以下代码控制是否启用长度归一化：  \n\n```python\naverage_log_prob_loss_types = [\"simpo\", \"dpo_norm\"]\naverage_log_prob = args.dpo_loss_type in average_log_prob_loss_types\n```\n\n 当 `dpo_loss_type` 设置为 `\"dpo_norm\"` 时，`average_log_prob`归一化被启用，进而在 `_get_batch_logps` 和 `concatenated_forward` 中触发长度归一化逻辑。  \n\n## rlvr实践\n### 单机4卡测试\n####  训练数据集\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image29.png)\n\n.parquet转为json后的数据集样例：\n\n```json\n{\"messages\":[{\"content\":\"Question: Find the domain of the expression $\\\\frac{\\\\sqrt{x-2}}{\\\\sqrt{5-x}}$.}\\nAnswer:The expressions inside each square root must be non-negative.\\nTherefore, $x-2 \\\\ge 0$, so $x\\\\ge2$, and $5 - x \\\\ge 0$, so $x \\\\le 5$.\\nAlso, the denominator cannot be equal to zero, so $5-x>0$, which gives $x<5$.\\nTherefore, the domain of the expression is $\\\\boxed{[2,5)}$.\\n\\nQuestion: If $\\\\det \\\\mathbf{A} = 2$ and $\\\\det \\\\mathbf{B} = 12,$ then find $\\\\det (\\\\mathbf{A} \\\\mathbf{B}).$\\nAnswer:We have that $\\\\det (\\\\mathbf{A} \\\\mathbf{B}) = (\\\\det \\\\mathbf{A})(\\\\det \\\\mathbf{B}) = (2)(12) = \\\\boxed{24}.$\\n\\nQuestion: Terrell usually lifts two 20-pound weights 12 times. If he uses two 15-pound weights instead, how many times must Terrell lift them in order to lift the same total weight?\\nAnswer:If Terrell lifts two 20-pound weights 12 times, he lifts a total of $2\\\\cdot 12\\\\cdot20=480$ pounds of weight.  If he lifts two 15-pound weights instead for $n$ times, he will lift a total of $2\\\\cdot15\\\\cdot n=30n$ pounds of weight.  Equating this to 480 pounds, we can solve for $n$: \\\\begin{align*}\\n30n&=480\\\\\\\\\\n\\\\Rightarrow\\\\qquad n&=480\\/30=\\\\boxed{16}\\n\\\\end{align*}\\n\\nQuestion: If the system of equations\\n\\n\\\\begin{align*}\\n6x-4y&=a,\\\\\\\\\\n6y-9x &=b.\\n\\\\end{align*}has a solution $(x, y)$ where $x$ and $y$ are both nonzero, find $\\\\frac{a}{b},$ assuming $b$ is nonzero.\\nAnswer:If we multiply the first equation by $-\\\\frac{3}{2}$, we obtain\\n\\n$$6y-9x=-\\\\frac{3}{2}a.$$Since we also know that $6y-9x=b$, we have\\n\\n$$-\\\\frac{3}{2}a=b\\\\Rightarrow\\\\frac{a}{b}=\\\\boxed{-\\\\frac{2}{3}}.$$\\n\\nQuestion: What is the modulo $13$ residue of $247+5 \\\\cdot 39 + 7 \\\\cdot 143 +4 \\\\cdot 15?$\",\"role\":\"user\"}],\"ground_truth\":\"8\",\"dataset\":\"MATH\",\"constraint_type\":null,\"constraint\":null}\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image30.png)\n\n#### 验证数据集\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image31.png)\n\n```json\n{\"messages\":[{\"content\":\"Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nAnswer:There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\\n\\nQuestion: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nAnswer:There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\\n\\nQuestion: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\nAnswer:Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\\n\\nQuestion: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\nAnswer:Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\\n\\nQuestion: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\nAnswer:Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\\n\\nQuestion: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\\nAnswer:There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\\n\\nQuestion: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\nAnswer:Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33.\\n\\nQuestion: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\nAnswer:Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is 8.\\n\\nQuestion: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?\",\"role\":\"user\"}],\"ground_truth\":\"3\",\"dataset\":\"gsm8k\"}\n{\"messages\":[{\"content\":\"Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\\nAnswer:There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. So the answer is 6.\\n\\nQuestion: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\\nAnswer:There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. So the answer is 5.\\n\\nQuestion: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\\nAnswer:Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. So the answer is 39.\\n\\nQuestion: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\\nAnswer:Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. So the answer is 8.\\n\\nQuestion: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\\nAnswer:Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. So the answer is 9.\\n\\nQuestion: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?\\nAnswer:There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. So the answer is 29.\\n\\nQuestion: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?\\nAnswer:Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. So the answer is 33.\\n\\nQuestion: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\\nAnswer:Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. So the answer is 8.\\n\\nQuestion: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repairs.  This increased the value of the house by 150%.  How much profit did he make?\",\"role\":\"user\"}],\"ground_truth\":\"70000\",\"dataset\":\"gsm8k\"}\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image32.png)\n\n#### 单机脚本配置\n```bash\nexport WANDB_MODE=disabled\nexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n\ncurrent_time=$(date \"+%Y.%m.%d-%H.%M.%S\")\nLOG_SAVE=\"/mnt/open-instruct-main/output/rlvr/${current_time}-tulu3-rlvr.log\"\n\npython /mnt/open-instruct-main/open_instruct/ppo_vllm_thread_ray_gtrl.py \\\n    --dataset_mixer '{\"/mnt/open-instruct-main/data/gsm8k_math_ground_truth_mixed\": 1.0}' \\\n    --dataset_train_splits train \\\n    --dataset_eval_mixer '{\"/mnt/open-instruct-main/data/gsm8k_math_ground_truth\": 1.0}' \\\n    --dataset_eval_splits test \\\n    --max_token_length 2048 \\\n    --max_prompt_token_length 2048 \\\n    --response_length 2048 \\\n    --model_name_or_path /mnt/LLM-Research/Llama-31-Tulu-3-8B-DPO \\\n    --reward_model_path /mnt/LLM-Research/LLama-31-Tulu3-8B-RM \\\n    --non_stop_penalty \\\n    --stop_token eos \\\n    --temperature 1.0 \\\n    --ground_truths_key ground_truth \\\n    --chat_template tulu \\\n    --sft_messages_key messages \\\n    --learning_rate 3e-7 \\\n    --total_episodes 10000000 \\\n    --penalty_reward_value -10.0 \\\n    --deepspeed_stage 3 \\\n    --per_device_train_batch_size 1 \\\n    --local_rollout_forward_batch_size 1 \\\n    --local_mini_batch_size 16 \\\n    --local_rollout_batch_size 16 \\\n    --actor_num_gpus_per_node 3 \\\n    --vllm_tensor_parallel_size 2 \\\n    --beta 0.05 \\\n    --apply_verifiable_reward true \\\n    --output_dir /mnt/open-instruct-main/output/rlvr \\\n    --seed 3 \\\n    --num_evals 3 \\\n    --save_freq 100 \\\n    --reward_model_multiplier 0.0 \\\n    --gradient_checkpointing \\\n    --with_tracking False 2>&1 | tee -a \"$LOG_SAVE\"\n```\n\n为了尽可能地还原tulu3原论文的实验结果，其中部分超参与论文中最优保持一致。\n\n`--max_token_length 2048`\n\n`--max_prompt_token_length 2048` \n\n`--response_length 2048`\n\n`--learning_rate 3e-7`\n\n`--penalty_reward_value -10.0`\n\n`--reward_model_multiplier 0.0`\n\n其中的模型权重利用tulu3开源的经过tulu3-sft dpo得到的Llama-31-Tulu-3-8B-DPO权重，奖励模型采用tulu3开源的LLama-31-Tulu3-8B-RM。\n\n#### 训练测试\n加载ckpt时一直卡住不动，尚未解决。\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Tulu3/image33.png)\n\n","tags":["V100","模型训练调优","Nvidia","Tulu3"],"categories":["模型训练调优","NVIDIA"]},{"title":"前端1_调用api接口的写法","url":"/2025/11/06/前端/前端1_调用api接口的写法/","content":"\n### Get请求写法\n#### 1. 看Parameters中是否有需要带参数，其中`Authorization`为授权认证的token可以不用考虑。\n1.1 若是没有其余的参数限定，那么SyncRequestFuncType<请求参数类型,响应返回参数类型>，第一个参数就为void或者undefined。返回类型可以使用any，等获取到响应参数之后，在对其通过Interface/type进行明确。\n\n```javascript\nexport const getApi: SyncRequestFuncType<\nvoid,\nresponseType\n> = () => {\nreturn javaAxios({\n method: \"get\",\n url: \"xxx/xxxx\",\n});\n};\n```\n\n1.2 若是有其余参数限定，这时需要箭头函数中需要带着params来进行请求，且reuturn中也需要将params带着。\n\n\t1.2.1 `单个参数`，例如id，这时在请求参数类型中，需要明确出请求参数的类型<{id:string},responseType>，这时请求参数的类型为对象{}的形式，可以直接在尖括号中写出来，也可以通过引入定义的Interface/type类型来写<IdType,responseType>。\n\n```javascript\nexport const getApi: SyncRequestFuncType<\n {id:string},\n void\n> = (params) => {\nreturn javaAxios({\n method: \"get\",\n url: \"xxx/xxxx\",\n params,\n});\n};\n```\n\n\t1.2.2 `多个参数`，例如email，type...，因为参数比较多，最好通过Interface/type的方式SendEmailCaptchaForUserInfoModifyType来明确请求参数类型。\n\n```javascript\nexport type SendEmailCaptchaForUserInfoModifyType = {\ntype: ModifyType;\nemail?: string;\n};\n```\n\n```javascript\nexport const getApi: SyncRequestFuncType<\nSendEmailCaptchaForUserInfoModifyType,\nvoid\n> = (params) => {\nreturn javaAxios({\n method: \"get\",\n url: \"xxx/xxx\",\n params,\n});\n};\n```\n\n### Post请求写法\n#### 1. 看看Parameters中是否有需要带参数，其中`Authorization`为授权认证的token可以不用考虑。\n1.1 若是没有其余的参数限定，那么SyncRequestFuncType<请求参数类型,响应返回参数类型>，第一个参数就为void或者undefined。返回类型可以使用any，等获取到响应参数之后，在对其通过Interface/type进行明确。\n\n```javascript\nexport const clearVideoRecycle: SyncRequestFuncType<void, void> = () => {\nreturn javaAxios({\n method: \"post\",\n url: \"xxx/xxx\",\n});\n};\n```\n\n1.2 若是有其余参数限定，这时需要箭头函数中需要带着params来进行请求，且reuturn中也需要将params带着。\n\n\t1.2.1 单个参数时\n\n```javascript\nexport const cancelSubscribeVideo: SyncRequestFuncType<\n{ themeId: string },\nany\n> = (params) => {\nreturn javaAxios({\n method: \"post\",\n url: \"xxx/xxx\",\n params,\n});\n};\n```\n\n\t1.2.2 多个参数时\n\n```javascript\nexport type SendEmailCaptchaForUserInfoModifyType = {\ntype: ModifyType;\nemail?: string;\n};\n```\n\n```javascript\nexport const sendEmailCaptchaForUserInfoModify: SyncRequestFuncType<\nSendEmailCaptchaForUserInfoModifyType,\nvoid\n> = (params) => {\nreturn javaAxios({\n method: \"get\",\n url: \"xxx/xxx\",\n params,\n});\n};\n```\n\n#### 2. 当出现请求体`Request body`时，这时就需要在return的javaAxios中添加键值对`data: params`，来将参数添加到请求体中传递过去。\n2.1 Request body为：\n\n```javascript\n[\n\"string\"\n]\n```\n\n```javascript\nexport const batchDeleteVideo: SyncRequestFuncType<string[], void> = (\nparams\n) => {\nreturn javaAxios({\n method: \"post\",\n url: \"videos/themes/batchDel\",\n data: params,\n});\n};\n```\n\n2.2 Request body为：这时候需要在data中进一步在约束一下，使其对应api的请求体的格式。\n\n```javascript\n{\n\"themes\": [\n \"string\"\n]\n}\n```\n\n```javascript\nexport const sortVideos: SyncRequestFuncType<string[], void> = (params) => {\nreturn javaAxios({\n method: \"post\",\n url: \"videos/themes/changeVideoThemesSort\",\n data: { themes: params },\n});\n};\n```\n\n","tags":["前端","Javascript"],"categories":["前端"]},{"title":"基于Mindformers的llama3.1模型微调","url":"/2025/11/06/模型训练调优/昇腾/Mindformers/llama3.1微调/","content":"## 启动容器\n```bash\ndocker run -dit --ipc=host --net=host \\\n--name=wxb_mindformers \\\n--device=/dev/davinci0 \\\n--device=/dev/davinci1 \\\n--device=/dev/davinci2 \\\n--device=/dev/davinci3 \\\n--device=/dev/davinci4 \\\n--device=/dev/davinci5 \\\n--device=/dev/davinci6 \\\n--device=/dev/davinci7 \\\n--device=/dev/davinci_manager \\\n--device=/dev/devmm_svm \\\n--device=/dev/hisi_hdc \\\n-v /etc/ascend_install.info:/etc/ascend_install.info \\\n-v /etc/hccn.conf:/etc/hccn.conf \\\n-v /etc/localtime:/etc/localtime \\\n-v /var/log/npu/:/usr/slog \\\n-v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \\\n-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n-v /reason-sharedata/training_inference/wangxiangbo/mindformers/code:/job/code \\\n-v /reason-sharedata/training_inference/wangxiangbo/mindformers/data:/job/data \\\n-v /reason-sharedata/training_inference/wangxiangbo/mindformers/output:/job/output \\\n-v /mnt/weight-1:/job/mnt \\\nswr.cn-central-221.ovaijisuan.com/mindformers/mindformers1.3_mindspore2.4:20241114 \\\n/bin/bash\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Mindformers/image1.png)\n\n## 代码准备\n```bash\ngit clone https://gitee.com/mindspore/mindformers.git\n```\n\n## 数据集\n```bash\npython research/llama3/llama_preprocess.py \\\n--dataset_type qa \\\n--input_glob /job/data/alpaca/alpaca-data-conversation.json \\\n--model_file /job/mnt/huggingface/llama-3-8b-hf \\\n--seq_length 8192 \\\n--output_file /job/data/alpaca\n```\n\n## 权重转换\n```bash\npython convert_weight.py --model llama --input_path /job/mnt/huggingface/llama-3-8b-hf --output_path /job/data/mindformers/llama3_8b_ckpt\n```\n\n## 训练脚本\n```bash\nseed: 0\noutput_dir: '/job/output' # path to save checkpoint/strategy\nload_checkpoint: '/job/data/mindformers/llama3_8b_ckpt/'\nsrc_strategy_path_or_dir: ''\nauto_trans_ckpt: False  # If true, auto transform load_checkpoint to load in distributed model\nonly_save_strategy: False\nresume_training: False\nrun_mode: 'finetune'\n \n# trainer config\ntrainer:\n  type: CausalLanguageModelingTrainer\n  model_name: 'llama3_8b'\n \n# runner config\nrunner_config:\n  epochs: 2\n  batch_size: 1\n  sink_mode: True\n  sink_size: 2\n \n# optimizer\noptimizer:\n  type: FP32StateAdamWeightDecay\n  beta1: 0.9\n  beta2: 0.95\n  eps: 1.e-8\n \n# lr sechdule\nlr_schedule:\n  type: CosineWithWarmUpLR\n  learning_rate: 1.e-5\n  lr_end: 0.0\n  warmup_ratio: 0.03\n  total_steps: -1 # -1 means it will load the total steps of the dataset\n \n# dataset\ntrain_dataset: &train_dataset\n  data_loader:\n    type: MindDataset\n    dataset_dir: \"/job/data/alpaca/alpaca_llama3_8192/alpaca-fastchat8192.mindrecord\"\n    shuffle: True\n  input_columns: [\"input_ids\",\"labels\"]  # \"input_ids\", \"labels\" , labels are used in instruction finetune.\n  num_parallel_workers: 8\n  python_multiprocessing: False\n  drop_remainder: True\n  batch_size: 6\n  repeat: 1\n  numa_enable: False\n  prefetch_size: 1\ntrain_dataset_task:\n  type: CausalLanguageModelDataset\n  dataset_config: *train_dataset\n# if True, do evaluate during the training process. if false, do nothing.\n# note that the task trainer should support _evaluate_in_training function.\ndo_eval: False\n \n# eval dataset\neval_dataset: &eval_dataset\n  data_loader:\n    type: MindDataset\n    dataset_dir: \"\"\n    shuffle: False\n  input_columns: [\"input_ids\"]\n  num_parallel_workers: 8\n  python_multiprocessing: False\n  drop_remainder: False\n  repeat: 1\n  numa_enable: False\n  prefetch_size: 1\neval_dataset_task:\n  type: CausalLanguageModelDataset\n  dataset_config: *eval_dataset\n \nuse_parallel: True\n# parallel context config\nparallel:\n  parallel_mode: 1 # 0-data parallel, 1-semi-auto parallel, 2-auto parallel, 3-hybrid parallel\n  gradients_mean: False\n  enable_alltoall: False\n  full_batch: True\n  search_mode: \"sharding_propagation\"\n  enable_parallel_optimizer: True\n  strategy_ckpt_save_file: \"./ckpt_strategy.ckpt\"\n  parallel_optimizer_config:\n    gradient_accumulation_shard: False\n    parallel_optimizer_threshold: 64\n# default parallel of device num = 8 for Atlas 800T A2\nparallel_config:\n  data_parallel: 1\n  model_parallel: 4\n  pipeline_stage: 2\n  use_seq_parallel: False\n  micro_batch_num: 8\n  vocab_emb_dp: True\n  gradient_aggregation_group: 4\n# when model parallel is greater than 1, we can set micro_batch_interleave_num=2, that may accelerate the train process.\nmicro_batch_interleave_num: 1\n \n# recompute config\nrecompute_config:\n  recompute: True\n  select_recompute: False\n  parallel_optimizer_comm_recompute: True\n  mp_comm_recompute: True\n  recompute_slice_activation: True\n \n# callbacks\ncallbacks:\n  - type: MFLossMonitor\n  - type: CheckpointMointor\n    prefix: \"llama3_8b\"\n    save_checkpoint_steps: 10000\n    integrated_save: False\n    async_save: False\n  - type: ObsMonitor\n \n# mindspore context init config\ncontext:\n  mode: 0 #0--Graph Mode; 1--Pynative Mode\n  device_target: \"Ascend\"\n  enable_graph_kernel: False\n  graph_kernel_flags: \"--disable_expand_ops=Softmax,Dropout --enable_parallel_fusion=true --reduce_fuse_depth=8 --enable_auto_tensor_inplace=true\"\n  max_call_depth: 10000\n  max_device_memory: \"26GB\"\n  save_graphs: False\n  save_graphs_path: \"./graph\"\n  device_id: 0\n  runtime_num_threads: 1\n \n# model config\nmodel:\n  model_config:\n    type: LlamaConfig\n    batch_size: 1 # add for increase predict\n    seq_length: 8192\n    hidden_size: 4096\n    num_layers: 32\n    num_heads: 32\n    n_kv_heads: 8\n    vocab_size: 128256\n    intermediate_size: 14336\n    rms_norm_eps: 1.0e-5\n    bos_token_id: 128000\n    eos_token_id: 128001\n    pad_token_id: 128002\n    ignore_token_id: -100\n    compute_dtype: \"bfloat16\"\n    layernorm_compute_type: \"float32\"\n    softmax_compute_type: \"float32\"\n    rotary_dtype: \"float32\"\n    param_init_type: \"bfloat16\"\n    use_past: False\n    scaling_factor: 1.0\n    theta: 500000\n    extend_method: \"None\" # support \"None\", \"PI\", \"NTK\"\n    use_flash_attention: True # FA can accelerate training or finetune\n    offset: 0\n    fine_grain_interleave: 1\n    checkpoint_name_or_path: \"\"\n    repetition_penalty: 1\n    max_decode_length: 512\n    top_k: 3\n    top_p: 1\n    do_sample: False\n  arch:\n    type: LlamaForCausalLM\n \n# metric\nmetric:\n  type: PerplexityMetric\n \n# wrapper cell config\nrunner_wrapper:\n  type: MFTrainOneStepCell\n  scale_sense: 1.0\n  use_clip_grad: True\n \neval_callbacks:\n  - type: ObsMonitor\n \nauto_tune: False\nfilepath_prefix: './autotune'\nautotune_per_step: 10\n \nprofile: False\nprofile_start_step: 5\nprofile_stop_step: 7\ninit_start_profile: True\nprofile_communication: True\nprofile_memory: True\nlayer_scale: False\nlayer_decay: 0.65\nlr_scale_factor: 256\n \n# aicc\nremote_save_url: \"Please input obs url on AICC platform.\"\n```\n\n## 训练拉起\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Mindformers/image2.png)\n\n<font style=\"color:transparent;\">git</font>\n\n<font style=\"color:transparent;\"> clone https://gitee.com/mindspore/mindformers.gitgit</font>\n\n<font style=\"color:transparent;\"> clone https://gitee.com/mindspore/mindformers.git</font>\n\n","tags":["昇腾","910B","模型训练调优","Mindformers"],"categories":["模型训练调优","昇腾"]},{"title":"基于Modellink的llama2-7b和Mistral-7b模型微调","url":"/2025/11/06/模型训练调优/昇腾/Modellink/llama2-7B+Mistral-7B/","content":"## 基于docker的mistral-7B微调\n### 容器挂载\n```bash\nsudo docker run -dit --ipc=host --net=host \\\n--name=modellink_wxb \\\n--device=/dev/davinci0 \\\n--device=/dev/davinci1 \\\n--device=/dev/davinci2 \\\n--device=/dev/davinci3 \\\n--device=/dev/davinci4 \\\n--device=/dev/davinci5 \\\n--device=/dev/davinci6 \\\n--device=/dev/davinci7 \\\n--device=/dev/davinci_manager \\\n--device=/dev/devmm_svm \\\n--device=/dev/hisi_hdc \\\n-v /etc/ascend_install.info:/etc/ascend_install.info \\\n-v /etc/hccn.conf:/etc/hccn.conf \\\n-v /etc/localtime:/etc/localtime \\\n-v /var/log/npu/:/usr/slog \\\n-v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \\\n-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n-v /reason-sharedata/training_inference/wangxiangbo/modellink/code/Modellink:/job/code \\\n-v /reason-sharedata/training_inference/wangxiangbo/modellink/data:/job/data \\\n-v /reason-sharedata/training_inference/wangxiangbo/modellink/output:/job/output \\\n-v /mnt/weight-1:/job/mnt \\\nregistry.paas/cmss/modellink-cann8.0-torch2.1-mindspeed-0.7:v1.0 \\\n/bin/bash\n```\n\n### 权重转换\n```bash\n# 修改 ascend-toolkit 路径\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n```\n\n```bash\npython  tools/checkpoint/convert_ckpt.py \\\n    --model-type GPT \\\n    --loader llama2_hf \\\n    --saver megatron \\\n    --load-dir /job/mnt/huggingface/mistral-7b-hf/ \\\n    --save-dir /job/data/megatron/mistral-7b-tp4-pp2 \\\n    --tokenizer-model /job/mnt/huggingface/mistral-7b-hf/tokenizer.model \\\n    --target-tensor-parallel-size 4 \\\n    --target-pipeline-parallel-size 2\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image1.png)\n\n### 数据集\n#### 下载\n```bash\nsudo wget https://hf-mirror.com/datasets/silk-road/alpaca-data-gpt4-chinese/resolve/main/Alpaca_data_gpt4_zh.jsonl\n```\n\n#### 处理\n```bash\npython tools/preprocess_data.py \\\n    --input /job/data/dataset/mistral/Alpaca_data_gpt4_zh.jsonl \\\n    --output-prefix /job/data/dataset/mistral/Alpaca_finetune/ \\\n    --tokenizer-type PretrainedFromHF \\\n    --tokenizer-name-or-path /job/mnt/huggingface/mistral-7b-hf/ \\\n    --append-eod \\\n    --tokenizer-not-use-fast \\\n    --handler-name GeneralInstructionHandler \\\n    --workers 4\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image2.png)\n\n处理完成\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image3.png)\n\n### 微调\n#### 微调脚本\n```bash\n#!/bin/bash\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport PYTORCH_NPU_ALLOC_CONF=expandable_segments:True\n\nGPUS_PER_NODE=8\nMASTER_ADDR=localhost\nMASTER_PORT=6006\nNNODES=1\nNODE_RANK=0\nWORLD_SIZE=$(($GPUS_PER_NODE * $NNODES))\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\n\necho \"NODE_RANK ${NODE_RANK}\"\n\nDATA_PATH=\"/job/data/dataset/mistral/Alpaca_finetune/\"\nTOKENIZER_MODEL=\"/job/mnt/huggingface/mistral-7b-hf\"\nCKPT_SAVE_DIR=\"/job/output/mistral_ckpt/\"\nCKPT_LOAD_DIR=\"/job/data/megatron/mistral-7b-tp4-pp2/\"\n\n\nTP=4\nPP=2\nNUM_LAYERS=32\n\nGPT_ARGS=\"\n    --tensor-model-parallel-size ${TP} \\\n    --pipeline-model-parallel-size ${PP} \\\n    --sequence-parallel \\\n    --sliding-window 4096 \\\n    --num-layers ${NUM_LAYERS} \\\n    --hidden-size 4096 \\\n    --ffn-hidden-size 14336 \\\n    --num-attention-heads 32 \\\n    --group-query-attention \\\n    --num-query-groups 8 \\\n    --tokenizer-type PretrainedFromHF \\\n    --tokenizer-name-or-path ${TOKENIZER_MODEL} \\\n    --seq-length 32768 \\\n    --max-position-embeddings 32768 \\\n    --micro-batch-size 1 \\\n    --global-batch-size 32 \\\n    --make-vocab-size-divisible-by 1 \\\n    --lr 1.25e-6 \\\n    --train-iters 1000 \\\n    --lr-decay-style cosine \\\n    --untie-embeddings-and-output-weights \\\n    --disable-bias-linear \\\n    --attention-dropout 0.0 \\\n    --init-method-std 0.01 \\\n    --hidden-dropout 0.0 \\\n    --position-embedding-type rope \\\n    --normalization RMSNorm \\\n    --use-fused-rmsnorm \\\n    --use-fused-swiglu \\\n    --use-rotary-position-embeddings \\\n    --use-fused-rotary-pos-emb \\\n    --use-mc2 \\\n    --swiglu \\\n    --use-flash-attn \\\n    --no-masked-softmax-fusion \\\n    --attention-softmax-in-fp32 \\\n    --min-lr 1.25e-7 \\\n    --weight-decay 1e-1 \\\n    --lr-warmup-fraction 0.01 \\\n    --clip-grad 1.0 \\\n    --adam-beta1 0.9 \\\n    --initial-loss-scale 65536 \\\n    --adam-beta2 0.95 \\\n    --no-gradient-accumulation-fusion \\\n    --no-load-optim \\\n    --no-load-rng \\\n    --use-distributed-optimizer \\\n    --overlap-grad-reduce \\\n    --load ${CKPT_LOAD_DIR} \\\n    --save ${CKPT_SAVE_DIR} \\\n    --bf16 \\\n    --finetune \\\n    --is-instruction-dataset \\\n    --log-throughput \\\n    --recompute-granularity full \\\n    --recompute-method block \\\n    --recompute-num-layers 32\n\"\n\nDATA_ARGS=\"\n    --data-path $DATA_PATH  \\\n    --split 100,0,0 \\\n\"\n\nOUTPUT_ARGS=\"\n    --log-interval 1 \\\n    --save-interval 1000 \\\n    --eval-interval 1000 \\\n    --eval-iters 0 \\\n\"\n\ntorchrun $DISTRIBUTED_ARGS ../pretrain_gpt.py \\\n  $GPT_ARGS \\\n  $DATA_ARGS \\\n  $OUTPUT_ARGS \\\n  --distributed-backend nccl \\\n  | tee /job/output/logs/train_mistral_7B.log\n```\n\n#### 拉起训练\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image4.png)\n\n\n\n## 基于docker的llama2-7B微调\n### 容器挂载\n```bash\nsudo docker run -dit --ipc=host --net=host \\\n--name=modellink_wxb \\\n--device=/dev/davinci0 \\\n--device=/dev/davinci1 \\\n--device=/dev/davinci2 \\\n--device=/dev/davinci3 \\\n--device=/dev/davinci4 \\\n--device=/dev/davinci5 \\\n--device=/dev/davinci6 \\\n--device=/dev/davinci7 \\\n--device=/dev/davinci_manager \\\n--device=/dev/devmm_svm \\\n--device=/dev/hisi_hdc \\\n-v /etc/ascend_install.info:/etc/ascend_install.info \\\n-v /etc/hccn.conf:/etc/hccn.conf \\\n-v /etc/localtime:/etc/localtime \\\n-v /var/log/npu/:/usr/slog \\\n-v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \\\n-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n-v /reason-sharedata/training_inference/wangxiangbo/modellink/code/Modellink:/job/code \\\n-v /reason-sharedata/training_inference/wangxiangbo/modellink/data:/job/data \\\n-v /reason-sharedata/training_inference/wangxiangbo/modellink/output:/job/output \\\n-v /mnt/weight-1:/job/mnt \\\nregistry.paas/cmss/modellink-cann8.0-torch2.1-mindspeed-0.7:v1.0 \\\n/bin/bash\n```\n\n### 权重转换\n```bash\n# 修改 ascend-toolkit 路径\nsource /usr/local/Ascend/ascend-toolkit/set_env.sh\n```\n\n```bash\npython  tools/checkpoint/convert_ckpt.py \\\n       --model-type GPT \\\n       --loader llama2_hf \\\n       --saver megatron \\\n       --target-tensor-parallel-size 8 \\\n       --target-pipeline-parallel-size 1 \\\n       --load-dir /job/mnt/huggingface/Llama-2-7b-hf// \\\n       --save-dir /job/data/megatron/llama-2-7b-hf-v0.1-tp8-pp1/ \\\n       --tokenizer-model /job/mnt/huggingface/Llama-2-7b-hf/tokenizer.json\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image5.png)\n\n### 数据集\n```bash\npython tools/preprocess_data.py \\\n    --input /job/data/dataset/mistral/Alpaca_data_gpt4_zh.jsonl \\\n    --output-prefix /job/data/dataset/llama2/Alpaca_finetune/ \\\n    --tokenizer-type PretrainedFromHF \\\n    --tokenizer-name-or-path /job/mnt/huggingface/Llama-2-7b-hf/ \\\n    --append-eod \\\n    --tokenizer-not-use-fast \\\n    --handler-name GeneralInstructionHandler \\\n    --workers 4\n```\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image6.png)\n\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image7.png)\n\n### 微调\n#### 微调脚本\n```bash\n\n#!/bin/bash\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport PYTORCH_NPU_ALLOC_CONF=expandable_segments:True\n\nGPUS_PER_NODE=8\nMASTER_ADDR=localhost\nMASTER_PORT=6006\nNNODES=1\nNODE_RANK=0\nWORLD_SIZE=$(($GPUS_PER_NODE * $NNODES))\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\n\necho \"NODE_RANK ${NODE_RANK}\"\n\nDATA_PATH=\"/job/data/dataset/llama2/Alpaca_finetune\"\nTOKENIZER_MODEL=\"/job/mnt/huggingface/Llama-2-7b-hf/\"\nCKPT_SAVE_DIR=\"/job/output/llama2_ckpt/\"\nCKPT_LOAD_DIR=\"/job/data/megatron/llama-2-7b-hf-v0.1-tp8-pp1\"\n\n\nTP=8\nPP=1\nNUM_LAYERS=32\n\nGPT_ARGS=\"\n    --tensor-model-parallel-size ${TP} \\\n    --pipeline-model-parallel-size ${PP} \\\n    --sequence-parallel \\\n    --num-layers 32 \\\n    --hidden-size 4096 \\\n    --ffn-hidden-size 11008 \\\n    --num-attention-heads 32 \\\n    --tokenizer-type Llama2Tokenizer \\\n    --tokenizer-model ${TOKENIZER_MODEL} \\\n    --seq-length 4096 \\\n    --max-position-embeddings 4096 \\\n    --micro-batch-size 1 \\\n    --global-batch-size 256 \\\n    --make-vocab-size-divisible-by 1 \\\n    --lr 1.25e-6 \\\n    --train-iters 5000 \\\n    --lr-decay-style cosine \\\n    --untie-embeddings-and-output-weights \\\n    --disable-bias-linear \\\n    --attention-dropout 0.0 \\\n    --init-method-std 0.01 \\\n    --hidden-dropout 0.0 \\\n    --position-embedding-type rope \\\n    --normalization RMSNorm \\\n    --use-fused-rmsnorm \\\n    --swiglu \\\n    --use-flash-attn \\\n    --no-masked-softmax-fusion \\\n    --attention-softmax-in-fp32 \\\n    --min-lr 1.25e-7 \\\n    --weight-decay 1e-1 \\\n    --lr-warmup-fraction 0.01 \\\n    --clip-grad 1.0 \\\n    --adam-beta1 0.9 \\\n    --initial-loss-scale 65536 \\\n    --adam-beta2 0.95 \\\n    --no-gradient-accumulation-fusion \\\n    --no-load-optim \\\n    --no-load-rng \\\n    --use-distributed-optimizer \\\n    --use-fused-swiglu \\\n    --use-fused-rotary-pos-emb \\\n    --overlap-grad-reduce \\\n    --bf16\n\"\n\nDATA_ARGS=\"\n    --data-path $DATA_PATH  \\\n    --split 100,0,0 \\\n\"\n\nOUTPUT_ARGS=\"\n    --log-interval 1 \\\n    --save-interval 1000 \\\n    --eval-interval 1000 \\\n    --eval-iters 0 \\\n\"\n\ntorchrun $DISTRIBUTED_ARGS ../pretrain_gpt.py \\\n  $GPT_ARGS \\\n  $DATA_ARGS \\\n  $OUTPUT_ARGS \\\n  --distributed-backend nccl \\\n  | tee /job/output/logs/train_llama2_7B.log\n\n```\n\n#### 拉起训练\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/Modellink/image8.png)\n\n\n","tags":["昇腾","910B","模型训练调优","Mindformers"],"categories":["模型训练调优","昇腾"]},{"title":"1.4-基于k8s拉起Qwen模型的多机多卡微调","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/1.4-基于k8s拉起Qwen模型的多机多卡微调/","content":"一、镜像准备\n### 1. 查看镜像\n`docker images`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen12.png)\n### 2. 修改镜像标签\n`docker tag ec99659d9677 registry.paas/library/qwen:v3.0`\n### 3. 将镜像推至仓库\n`docker push registry.paas/library/qwen:v3.0`\n### 4. 如果出现签名认证失败，需要修改docker守护进程配置文件\n`vim /etc/docker/daemon.json`\n增加如下配置：\n```json\n{\n\"insecure-registries\":[\"registry.paas\"]\n}\n```\n重启docker\n`systemctl daemon-reload && systemctl restart docker`\n### 5. 重新push至registry.paas/library/xxx:tags仓库\n二、修改配置文件\n### 1. qwentest.yaml\n```yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: qwentest\nspec:\n  selector:\n    matchLabels:\n      app: qwentest\n  template:\n    metadata:\n      labels:\n        app: qwentest\n    spec:\n      hostNetwork: true\n      nodeSelector:\n        model: qwen-7b\n      containers:\n      - name: qwentest\n        image: registry.paas/library/qwen:v3.0\n        imagePullPolicy: IfNotPresent\n        resources:\n         limits:\n           nvidia.com/gpu: \"4\"\n         requests:\n           nvidia.com/gpu: \"4\"\n        command:                                  # training command, which can be modified\n              - \"/bin/bash\"\n              - \"-c\"\n                #- sleep 10000\n              - |\n                cd /mnt/ &&\n                cp setRank.sh /qwen/ &&\n                cd /qwen/ &&\n                chmod +x setRank.sh &&\n                bash setRank.sh &&\n                chmod +x finetune_lora_ds.sh &&\n                bash finetune_lora_ds.sh\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: processeddata\n          mountPath: /mnt\n        - name: dshm\n          mountPath: /dev/shm\n        - name: tmp-volume\n          mountPath: /tmp\n      volumes:\n      - name: processeddata\n        hostPath:\n          path: /mnt/users/wangxiangbo/runk8s\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1G\n      - name: tmp-volume\n        hostPath:\n          path: /tmp\n```\n### 2. hostfile\n```shell\n192.168.0.20\n192.168.0.58\n```\n### 3. setRank.sh\n```shell\n#!/bin/bash\n\nshell_name=\"finetune_lora_ds.sh\"\n\nshell_dir=\"/mnt/\"\n\nlocal_dir=\"/qwen/\"\n\n## 复制脚本到/qwen/下\ncp $shell_dir$shell_name $local_dir\n\n\n## 读取hostfile\nreadarray -t ips < <(grep -vE '^[[:space:]]*$' \"$shell_dir\"hostfile)\n\n## 获取rank0 IP\nrank0_ip=$(echo \"${ips[0]}\" | tr -d '[:space:]')\n\nnodes=${#ips[@]}\n\n## 获取hostfile中配置的IP前缀\n## 使用cut提取IP地址的前三个数字部分\nip_prefix=$(echo \"${ips[0]}\" | cut -d '.' -f 1-3)\n\n## 获取本机IP\nip=$(hostname -I | grep -oE \"$ip_prefix\\.[0-9]+\")\n\nls\n# 初始化rank\nnode_rank=-1\n\n# 遍历数组\nfor i in \"${!ips[@]}\"; do\n    # 使用tr命令去除空白字符\n    clean_string=$(echo \"${ips[$i]}\" | tr -d '[:space:]')\n    if [[ \"$clean_string\" == \"$ip\" ]]; then\n        node_rank=$i\n        break\n    fi\ndone\n\nif [ $node_rank -ne -1 ]; then\n    ## 修改脚本中MASTER_ADDR\n    sed -i \"s/^MASTER_ADDR=.*/MASTER_ADDR=$rank0_ip/\" $local_dir$shell_name\n\n    ## 修改NNODES\n    sed -i \"s/^NNODES=.*/NNODES=$nodes/\" $local_dir$shell_name\n\n    ## 修改NODE_RANK\n    sed -i \"s/^NODE_RANK=.*/NODE_RANK=$node_rank/\" $local_dir$shell_name\nfi\n\n```\n\n### 4. sh\n```shell\n#!/bin/bash\n/bin/bash -i <<'EOF'\nexport NCCL_IB_DISABLE=1\nexport NCCL_SOCKET_IFNAME=eth0\nexport NCCL_P2P_DISABLE=1\nexport NCCL_DEBUG=INFO\nsource ~/.bashrc\n\n. /opt/miniconda/etc/profile.d/conda.sh\nconda activate qwen\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\nDIR=`pwd`\n\n# Number of GPUs per GPU worker\nGPUS_PER_NODE=4\n# Number of GPU workers, for single-worker training, please set to 1\nNNODES=2\n# The rank of this worker, should be in {0, ..., WORKER_CNT-1}, for single-worker training, please set to 0\nNODE_RANK=0\n# The ip address of the rank-0 worker, for single-worker training, please set to localhost\nMASTER_ADDR=192.168.0.20\n# The port for communication\nMASTER_PORT=6003\n\nMODEL=\"/qwen/Qwen-7B-Chat\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"/qwen/train_data_law.json\"\nDS_CONFIG_PATH=\"/qwen/finetune/ds_config_zero2.json\"\n\nfunction usage() {\n    echo '\nUsage: bash finetune_lora_ds.sh [-m MODEL_PATH] [-d DATA_PATH] [--deepspeed DS_CONFIG_PATH]\n'\n}\n\nwhile [[ \"$1\" != \"\" ]]; do\n    case $1 in\n        -m | --model )\n            shift\n            MODEL=$1\n            ;;\n        -d | --data )\n            shift\n            DATA=$1\n            ;;\n        --deepspeed )\n            shift\n            DS_CONFIG_PATH=$1\n            ;;\n        -h | --help )\n            usage\n            exit 0\n            ;;\n        * )\n            echo \"Unknown argument ${1}\"\n            exit 1\n            ;;\n    esac\n    shift\ndone\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\n\ntorchrun $DISTRIBUTED_ARGS finetune.py \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 False \\\n    --output_dir output_qwen \\\n    --num_train_epochs 5 \\\n    --per_device_train_batch_size 8 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 8 \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 100 \\\n    --save_total_limit 10 \\\n    --learning_rate 3e-4 \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length 512 \\\n    --lazy_preprocess True \\\n    --use_lora \\\n    --gradient_checkpointing \\\n    --ddp_find_unused_parameters False \\\nEOF\n\n```\n三、拉起训练（3号和4号两机8卡）\n### 1. 切换到1号机器master节点上，给带训练得3号和4号机器打上标签\n`kubectl label nodes ecs-jhjs-1234-003 model=qwen-7b`\n`kubectl label nodes ecs-jhjs-1234-004 model=qwen-7b`\n### 2. 准备好启动脚本等文件后，在master节点1号机器上，利用修改好的qwentest.yaml文件拉起训练任务\n### 3. `kubectl apply -f qwentest.yaml`\n### 4. 通过kubectl查看pod节点启动信息\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen13.png)\n### 5. 查看两个节点pod的logs日志\n`kubectl logs qwentest-czm8n -f`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen14.png)\n`kubectl logs qwentest-qthsf -f`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen15.png)\n\n","tags":["V100","模型训练调优","Nvidia","Qwen","Deepspeed"],"categories":["模型训练调优","NVIDIA"]},{"title":"1.3-基于docker的Qwen单机单卡_多卡训练","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/1.3-基于docker的Qwen单机单卡_多卡训练/","content":"### 1. 查看容器\n`docker ps`\n### 2. exec进入容器\n`docker exec -it containerid bash`\n### 3. 进入qwen目录并修改finetune_lora_single_gpu.sh参数\n```bash\n#!/bin/bash\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\nMODEL=\"/qwen/Qwen-7B-Chat\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"/qwen/train_data_law.json\"\n\nfunction usage() {\n    echo '\nUsage: bash finetune/finetune_lora_single_gpu.sh [-m MODEL_PATH] [-d DATA_PATH]\n'\n}\n\nwhile [[ \"$1\" != \"\" ]]; do\n    case $1 in\n        -m | --model )\n            shift\n            MODEL=$1\n            ;;\n        -d | --data )\n            shift\n            DATA=$1\n            ;;\n        -h | --help )\n            usage\n            exit 0\n            ;;\n        * )\n            echo \"Unknown argument ${1}\"\n            exit 1\n            ;;\n    esac\n    shift\ndone\n\nexport CUDA_VISIBLE_DEVICES=0\n\npython finetune.py \\\n  --model_name_or_path $MODEL \\\n  --data_path $DATA \\\n  --bf16 True \\\n  --output_dir output_qwen \\\n  --num_train_epochs 5 \\\n  --per_device_train_batch_size 1 \\\n  --per_device_eval_batch_size 1 \\\n  --gradient_accumulation_steps 8 \\\n  --evaluation_strategy \"no\" \\\n  --save_strategy \"steps\" \\\n  --save_steps 100 \\\n  --save_total_limit 10 \\\n  --learning_rate 3e-4 \\\n  --weight_decay 0.1 \\\n  --adam_beta2 0.95 \\\n  --warmup_ratio 0.01 \\\n  --lr_scheduler_type \"cosine\" \\\n  --logging_steps 1 \\\n  --report_to \"none\" \\\n  --model_max_length 512 \\\n  --lazy_preprocess True \\\n  --gradient_checkpointing \\\n  --use_lora\n\n# If you use fp16 instead of bf16, you should use deepspeed\n# --fp16 True --deepspeed finetune/ds_config_zero2.json\n```\n### 4. 执行finetune_lora_single_gpu.sh单机单卡\n### 5. 修改finetune_lora_ds.sh参数\n```bash\n#!/bin/bash\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\nDIR=`pwd`\n\n# Guide:\n# This script supports distributed training on multi-gpu workers (as well as single-worker training).\n# Please set the options below according to the comments.\n# For multi-gpu workers training, these options should be manually set for each worker.\n# After setting the options, please run the script on each worker.\n\n# Number of GPUs per GPU worker\nGPUS_PER_NODE=$(python -c 'import torch; print(torch.cuda.device_count())')\n\n# Number of GPU workers, for single-worker training, please set to 1\nNNODES=${NNODES:-1}\n\n# The rank of this worker, should be in {0, ..., WORKER_CNT-1}, for single-worker training, please set to 0\nNODE_RANK=${NODE_RANK:-0}\n\n# The ip address of the rank-0 worker, for single-worker training, please set to localhost\nMASTER_ADDR=${MASTER_ADDR:-localhost}\n\n# The port for communication\nMASTER_PORT=${MASTER_PORT:-6001}\n\nMODEL=\"/qwen/Qwen-7B-Chat\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"/qwen/train_data_law.json\"\nDS_CONFIG_PATH=\"finetune/ds_config_zero2.json\"\n\nfunction usage() {\n    echo '\nUsage: bash finetune/finetune_lora_ds.sh [-m MODEL_PATH] [-d DATA_PATH] [--deepspeed DS_CONFIG_PATH]\n'\n}\n\nwhile [[ \"$1\" != \"\" ]]; do\n    case $1 in\n        -m | --model )\n            shift\n            MODEL=$1\n            ;;\n        -d | --data )\n            shift\n            DATA=$1\n            ;;\n        --deepspeed )\n            shift\n            DS_CONFIG_PATH=$1\n            ;;\n        -h | --help )\n            usage\n            exit 0\n            ;;\n        * )\n            echo \"Unknown argument ${1}\"\n            exit 1\n            ;;\n    esac\n    shift\ndone\n\nDISTRIBUTED_ARGS=\"\n    --nproc_per_node $GPUS_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n\"\n#export CUDA_VISIBLE_DEVICES=2,3\n\ntorchrun $DISTRIBUTED_ARGS finetune.py \\\n    --model_name_or_path $MODEL \\\n    --data_path $DATA \\\n    --bf16 False \\\n    --output_dir output_qwen \\\n    --num_train_epochs 5 \\\n    --per_device_train_batch_size 2 \\\n    --per_device_eval_batch_size 1 \\\n    --gradient_accumulation_steps 8 \\\n    --evaluation_strategy \"no\" \\\n    --save_strategy \"steps\" \\\n    --save_steps 100 \\\n    --save_total_limit 10 \\\n    --learning_rate 3e-4 \\\n    --weight_decay 0.1 \\\n    --adam_beta2 0.95 \\\n    --warmup_ratio 0.01 \\\n    --lr_scheduler_type \"cosine\" \\\n    --logging_steps 1 \\\n    --report_to \"none\" \\\n    --model_max_length 512 \\\n    --lazy_preprocess True \\\n    --use_lora \\\n    --gradient_checkpointing \\\n    --deepspeed ${DS_CONFIG_PATH}\n\n```\n### 6. 修改deepspeed中ds_config_zero2.json配置文件，增加TFlops显示\n其中具体增加的参数配置为flops_profiler\n```json\n\"flops_profiler\": {\n        \"enabled\": true,\n        \"profile_step\": 1,\n        \"module_depth\": -1,\n        \"top_modules\": 1,\n        \"detailed\": false,\n        \"output_file\": null\n    },\n```\n```json\n{\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \"bf16\": {\n        \"enabled\": \"auto\"\n    },\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n\n    \"zero_optimization\": {\n        \"stage\": 2,\n        \"offload_optimizer\": {\n            \"device\": \"none\",\n            \"pin_memory\": true\n        },\n        \"allgather_partitions\": true,\n        \"allgather_bucket_size\": 2e8,\n        \"overlap_comm\": true,\n        \"reduce_scatter\": true,\n        \"reduce_bucket_size\": 2e8,\n        \"contiguous_gradients\": true\n    },\n\n    \"flops_profiler\": {\n        \"enabled\": true,\n        \"profile_step\": 1,\n        \"module_depth\": -1,\n        \"top_modules\": 1,\n        \"detailed\": false,\n        \"output_file\": null\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 100,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": false\n}\n\n```\n### 7. 执行finetune_lora_ds.sh单机多卡训练\n其中报了Error while creating shared memory segment /dev/shm/nccl-KXWrmA (size 9637888)导致在docker中单机多卡拉起失败\n问题原因：docker的shm共享内存不足，可以通过命令\n`df -h | grep shm`查看当前容器的shm大小，默认为64M，这是远远不够的，所以要增加该容器的shm共享内存大小。参考博文[https://blog.csdn.net/gg864461719/article/details/112466585](https://blog.csdn.net/gg864461719/article/details/112466585)\n#### 解决方法1：创建完容器之后，手动修改shm共享内存大小\n##### a. 首先要关闭docker, 否则下面的操作步骤会无效.\n`service docker stop`\n##### b. 进入宿主机中/docker/containers/容器id 修改该容器的hostconfig.json文件，把其中的ShmSize的大小后面增加22（就变为了6.3G）其默认的是67108864_KB_ 就约等于64M。\n##### c. 重启docker服务\n`systemctl start docker`\n##### d. 解决完之后，重新查看shm的共享内存大小，此时已经变为了6.3G\n#### 解决方法2：在通过镜像run容器时，就直接指定--shm-size 6G\n`docker run -it --name qwen --gpus all --shm-size 6G registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0 bash`\n### 重新拉起训练\n`bash finetune/finetune_lora_ds.sh`\n其中tflops在9.5~10.5之间\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen9.png)\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen10.png)\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen11.png)\n\n","tags":["V100","模型训练调优","Nvidia","Qwen","Deepspeed"],"categories":["模型训练调优","NVIDIA"]},{"title":"1.2-Qwen模型镜像制作","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/1.2-Qwen模型镜像制作/","content":"由于新机器挂载文件存储速度非常慢，通过dockerfile文件来直接生成镜像非常慢，所以本镜像在自己的V100云主机中进行打包。\n### 1. 首先docker pull拉取一个ubuntu基础环境\n[https://hub.docker.com/r/nvidia/cuda/tags?page=11&page_size=&name=&ordering=](https://hub.docker.com/r/nvidia/cuda/tags?page=11&page_size=&name=&ordering=)\n`docker pull nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04`\n### 2. 安装nvidia-container-toolkit，使docker可以调用宿主机gpu资源\n  #### 2.1 下载nvidia-container-toolkit\n `distribution=$(. /etc/os-release;echo $ID$VERSION_ID) && \\ curl -fsSL [https://nvidia.github.io/libnvidia-container/gpgkey](https://nvidia.github.io/libnvidia-container/gpgkey) | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg && \\ curl -s -L [https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list](https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list) | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list`\n  #### 2.2 安装nvidia-container-toolkit\n  `sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit`\n  #### 2.3 添加nvidia-docker源\n  `curl -s -L [https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list](https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list) |   sudo tee /etc/apt/sources.list.d/nvidia-docker.list`\n  #### 2.4 更新并重新执行安装\n  `sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit`\n如果执行过程中报W: GPG error: [https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64](https://nvidia.github.io/libnvidia-container/stable/ubuntu18.04/amd64)  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY DDCAE044F796ECB0，则需要确认你的系统是否信任NVIDIA的GPG密钥。如果没有，你需要导入它。可以通过以下命令导入GPG密钥：\n`curl -s [https://nvidia.github.io/libnvidia-container/gpgkey](https://nvidia.github.io/libnvidia-container/gpgkey) | sudo apt-key add -`\n  #### 2.5 完成 nvidia-container-toolkit 的安装之后，我们继续执行 nvidia-ctk runtime configure 命令，为 Docker 添加 nvidia 这个运行时。完成后，我们的应用就能在容器中使用显卡资源了\n  `sudo nvidia-ctk runtime configure --runtime=docker`\n\n  ![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen5.png)\n  #### 2.6 重启docker\n  `sudo systemctl restart docker`\n  #### 2.7 查看是否安装成功  \n  `dpkg -l | grep nvidia-container-toolkit`\n\n  ![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen6.png)\n\n### 3. 拉取基础镜像成功后，创建一个docker容器\n`docker images`\n`sudo docker run -it --name qwen --gpus all nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04`\n`exit`\n#### 4. 退出容器后，将本地跑同的qwen模型代码/权重/数据集/环境cp到创建的qwen镜像中\n`docker cp /opt/tmp/Qwen/ 02649afd9710:/qwen`\n#### 5. 重启docker，exec执行\n`docker ps -a`\n`docker start qwen`\n`docker exec -it 02649afd9710 bash`\n#### 6. 因为想在容器中执行自身的python环境，不借用宿主机的环境，所以需要单独再安装conda、pytorch等环境。\n#### 7. 安装完基础环境后，需要安装qwen模型的依赖\n`pip install -r requirements.txt`\n#### 8. 没有vim编辑器还需安装vim\n`apt-get update`\n`apt-get install -y vim`\n#### 9. deepspeed安装\n`pip install \"peft<0.8.0\" deepspeed`\n#### 10. 此时在qwen容器中执行训练脚本，拉起训练。\n`bash finetune/finetune_lora_single_gpu.sh`\n#### 11. 将此时qwen容器打成镜像\n`docker commit -a \"wangxiangbo\" -m \"qwen 7B\" 02649afd9710 qwen-7b:v1.0`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen7.png)\n#### 12. 将打好的镜像转成tar包，供新机器解压使用\n`docker save -o qwen-7b.tar qwen-7b:v1.0`\n#### 13. 加载tar镜像, 使用load进行从tar文件导出镜像\n`docker load -i qwen-7b.tar`\n#### 14. 由于新机器挂载文件存储的原因，镜像解压速度太慢，于是打算将打好的qwen镜像push到阿里云个人仓库中，在新机器中直接pull该镜像。\n#### 15. 将镜像推送到Registry\n`docker login --username=aliyun9599911612 registry.cn-shanghai.aliyuncs.com`\n`docker tag 37c7b97b67f6 registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0`\n`docker push registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0`\n#### 16. 在3号机器中，拉取该镜像\n`docker pull registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen8.png)\n#### 17. 通过该镜像，run一个容器\n`docker run -it --name qwen --gpus all registry.cn-shanghai.aliyuncs.com/aliyun_repository_wxb/repository_wxb:v1.0 bash`\n\n","tags":["V100","模型训练调优","Nvidia","Qwen","Deepspeed"],"categories":["模型训练调优","NVIDIA"]},{"title":"1.1-Qwen模型本地部署与单机单卡/多卡训练","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/1.1-Qwen模型本地部署与单机单卡_多卡训练/","content":"一、V100环境部署\n1. 项目地址： [https://github.com/QwenLM/Qwen](https://github.com/QwenLM/Qwen)\n2. 下载到本地\n`git clone  https://github.com/QwenLM/Qwen.git `\n3. 基础环境搭建\n`conda create -n qwen python=3.10`\n`conda activate qwen`\n4. 安装pytorch\n`conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia`\n5. 安装依赖环境\n`pip install -r requirements.txt `\n7. 7B模型下载\n`git clone [https://www.modelscope.cn/qwen/Qwen-7B-Chat.git](https://www.modelscope.cn/qwen/Qwen-7B-Chat.git)`\n在使用sdk的python脚本下载权重时，需要pip安装modelscope\n`pip install modelscope`\n使用git clone发现权重未下载成功，使用modelscope官方sdk脚本下载。\n将以下代码写入download.py文件中，并执行`python download.py`\n```python\n#模型下载\nfrom modelscope import snapshot_download\nmodel_dir = snapshot_download('qwen/Qwen-7B-Chat', cache_dir='/opt/tmp/Qwen', revision='v1.1.9')\n```\n8. 数据集下载\n进入modelscope下载数据集\n`https://modelscope.cn/datasets/Robin021/DISC-Law-SFT/files`\n9. 数据格式处理\n数据集处理后会生成train_data_law.json文件\n`head -n 20 train_data_law.json`\n处理之后的数据格式如下：\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen1.png)\n```python\nimport json\n# 读取以.jsonl结尾的文件\njson_data = []\nwith open('/opt/tmp/Qwen/dataset/DISC-Law-SFT-Triplet-released.jsonl', 'r', encoding='utf-8') as file:\n    for line in file:\n        data = json.loads(line)\n        json_data.append(data)\n# 待填入的模板\ntemplate = []\n\n# 遍历json数据集\nfor idx, data in enumerate(json_data[:]):\n    conversation = [\n        {\n            \"from\":\"user\",\n            \"value\": data[\"input\"]\n        },\n        {\n            \"from\": \"assistant\",\n            \"value\": data[\"output\"]\n        }\n    ]\n    template.append({\n        \"id\": f\"identity_{idx}\",\n        \"conversations\": conversation\n    })\nprint(len(template))\n# 输出填充数据后的模板\nprint(json.dumps(template[2], ensure_ascii=False, indent=2))\n# 将template写入到本地文件\noutput_file_path = \"/opt/tmp/Qwen/train_data_law.json\"\nwith open(output_file_path, 'w', encoding='utf-8') as f:\n    json.dump(template, f, ensure_ascii=False, indent=2)\nprint(f\"处理好的数据已写入到本地文件: {output_file_path}\")\n```\n10. 训练依赖安装\ndeepspeed安装\n`pip install \"peft<0.8.0\" deepspeed`\n11. 修改模型微调脚本参数\n修改MODEL和DATA的路径，及per_device_train_batch_size\n```bash\n#!/bin/bash\nexport CUDA_DEVICE_MAX_CONNECTIONS=1\n\nMODEL=\"/opt/tmp/Qwen/Qwen-7B-Chat\" # Set the path if you do not want to load from huggingface directly\n# ATTENTION: specify the path to your training data, which should be a json file consisting of a list of conversations.\n# See the section for finetuning in README for more information.\nDATA=\"/opt/tmp/Qwen/train_data_law.json\"\n\nfunction usage() {\n    echo '\nUsage: bash finetune/finetune_lora_single_gpu.sh [-m MODEL_PATH] [-d DATA_PATH]\n'\n}\n\nwhile [[ \"$1\" != \"\" ]]; do\n    case $1 in\n        -m | --model )\n            shift\n            MODEL=$1\n            ;;\n        -d | --data )\n            shift\n            DATA=$1\n            ;;\n        -h | --help )\n            usage\n            exit 0\n            ;;\n        * )\n            echo \"Unknown argument ${1}\"\n            exit 1\n            ;;\n    esac\n    shift\ndone\n\nexport CUDA_VISIBLE_DEVICES=0\n\npython finetune.py \\\n  --model_name_or_path $MODEL \\\n  --data_path $DATA \\\n  --bf16 False \\\n  --output_dir output_qwen \\\n  --num_train_epochs 5 \\\n  --per_device_train_batch_size 2 \\\n  --per_device_eval_batch_size 1 \\\n  --gradient_accumulation_steps 8 \\\n  --evaluation_strategy \"no\" \\\n  --save_strategy \"steps\" \\\n  --save_steps 100 \\\n  --save_total_limit 10 \\\n  --learning_rate 3e-4 \\\n  --weight_decay 0.1 \\\n  --adam_beta2 0.95 \\\n  --warmup_ratio 0.01 \\\n  --lr_scheduler_type \"cosine\" \\\n  --logging_steps 1 \\\n  --report_to \"none\" \\\n  --model_max_length 512 \\\n  --lazy_preprocess True \\\n  --gradient_checkpointing \\\n  --use_lora\n\n# If you use fp16 instead of bf16, you should use deepspeed\n# --fp16 True --deepspeed finetune/ds_config_zero2.json\n```\n\n12. 开启单机单卡训练\n`bash finetune/finetune_lora_single_gpu.sh`\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen2.png)\n13. 单机多卡训练（在分配的3号机器上执行）\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen3.png)\ngpu显存利用情况\n![](https://lyapunov-1312469874.cos.ap-shanghai.myqcloud.com/img/categories/model/qwen/qwen4.png)\n\n\n\n","tags":["V100","模型训练调优","Nvidia","Qwen","Deepspeed"],"categories":["模型训练调优","NVIDIA"]},{"title":"1.5-Qwen多机多卡调优","url":"/2025/11/06/模型训练调优/NVIDIA/Qwen/1.5-Qwen多机多卡调优/","content":"### 1. lora\n#### 1.1 per_device_train_batch_size测试，最优为16\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 2 | 1 | 512 | 8 | true | 8.37 |\n| 4 | 1 | 512 | 8 | true | 9.07 |\n| 8 | 1 | 512 | 8 | true | 9.88 |\n| **16** | **1** | **512** | **8** | **true** | **10.32** |\n| 32 | 1 | 512 | 8 | true | OOM |\n\n\n单个GPU批次大小增加时，需要的内存也会增加，GPU内存不足以支持更大的批次，可能会导致溢出或效率降低。\n\n#### 1.2 per_device_eval_batch_size测试，最优为2\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 16 | 1 | 512 | 8 | true | 10.32 |\n| **16** | **2** | **512** | **8** | **true** | **10.33** |\n| 16 | 4 | 512 | 8 | true | 10.26 |\n\n\n#### 1.3 model_max_length测试，最优为512\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16 | flops |\n| --- | --- | --- | --- | --- | --- |\n| 16 | 2 | 128 | 8 | true | 8.95 |\n| 16 | 2 | 256 | 8 | true | 9.56 |\n| **16** | **2** | **512** | **8** | **true** | **10.33** |\n| 16 | 2 | 1024 | 8 | true | OOM |\n\n\n#### 1.4 gradient_accumulation_steps测试，最优为16\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 16 | 2 | 512 | 1 | true | 9.07 |\n| 16 | 2 | 512 | 2 | true | 9.24 |\n| 16 | 2 | 512 | 4 | true | 9.89 |\n| 16 | 2 | 512 | 8 | true | 10.33 |\n| **16** | **2** | **512** | **16** | **true** | **11.63** |\n\n\n#### 1.5 开启Fp16测试\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| **16** | **2** | **512** | **16** | **true** | **67.13** |\n\n\n#### 1.6 关闭gradient_checkpointing\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | gradient_checkpointing | tflops |\n| --- | --- | --- | --- | --- | --- | --- |\n| **16** | **2** | **512** | **16** | **true** | **true** | **67.13** |\n| 16 | 2 | 512 | 16 | true | false | OOM |\n\n\n| 参数配置 | per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16/fp16 | gradient_checkpointing | tflops |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| 默认 | 2 | 1 | 512 | 8 | bf16 | **true** | 8.32 |\n| 调优后 | **<font style=\"color:#DF2A3F;\">16</font>** | **<font style=\"color:#DF2A3F;\">2</font>** | **<font style=\"color:#DF2A3F;\">512</font>** | **<font style=\"color:#DF2A3F;\">16</font>** | **<font style=\"color:#DF2A3F;\">fp16</font>** | **<font style=\"color:#DF2A3F;\">true</font>** | **<font style=\"color:#DF2A3F;\">67.13</font>** |\n\n\n基于2机8卡的V100，Qwen-7B模型的lora微调训练中（采用deepspeed的zero2的内存优化并行方式），Tflops的值最高为<font style=\"color:#DF2A3F;\">67.13</font>**（8.32）\n\n最佳参数配置（per_device_train_batch_size：16，per_device_eval_batch_size：2，model_max_length：512，gradient_accumulation_steps：16，Fp16精度，gradient_checkpointing：True）\n\n### 2. qlora\n\nqlora使用4比特量化模型以及paged attention等技术实现更小的显存开销\n\n#### 2.1 per_device_train_batch_size测试，最优为32\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 2 | 1 | 512 | 8 | true | 31.71 |\n| 4 | 1 | 512 | 8 | true | 42.91 |\n| 8 | 1 | 512 | 8 | true | 51.14 |\n| 16 | 1 | 512 | 8 | true | 55.75 |\n| **32** | **1** | **512** | **8** | **true** | **58.44** |\n| 64 | 1 | 512 | 8 | true | OOM |\n\n\n#### 2.2 per_device_eval_batch_size测试，最优为4\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 32 | 1 | 512 | 8 | true | 58.44 |\n| 32 | 2 | 512 | 8 | true | 58.84 |\n| **32** | **4** | **512** | **8** | **true** | **59.05** |\n| 32 | 8 | 512 | 8 | true | 58.42 |\n\n\n#### 2.3 model_max_length测试，最优为512\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 32 | 4 | 128 | 8 | true | 48.68 |\n| 32 | 4 | 256 | 8 | true | 53.02 |\n| **32** | **4** | **512** | **8** | **true** | **59.05** |\n| 32 | 4 | 1024 | 8 | true | OOM |\n\n\n#### 2.4 gradient_accumulation_steps测试，最优为\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 32 | 4 | 512 | 1 | true | 52.66 |\n| 32 | 4 | 512 | 2 | true | 52.18 |\n| 32 | 4 | 512 | 4 | true | 54.99 |\n| 32 | 4 | 512 | 8 | true | 59.05 |\n| 32 | 4 | 512 | 16 | true | 64.26 |\n| 32 | 4 | 512 | 32 | true | 80.37 |\n| **32** | **4** | **512** | **64** | **true** | **103.89** |\n\n\n#### 2.5 开启BF16测试\n\n| per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | bf16/fp16 | tflops |\n| --- | --- | --- | --- | --- | --- |\n| 32 | 4 | 512 | 64 | bf16 | 14.23 |\n| **32** | **4** | **512** | **64** | **fp16** | **103.89** |\n\n\n#### 2.6 关闭gradient_checkpointing\n\n| gradient_checkpointing | per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- | --- |\n| **<font style=\"color:#000000;\">True</font>** | **<font style=\"color:#000000;\">32</font>** | **<font style=\"color:#000000;\">4</font>** | **<font style=\"color:#000000;\">512</font>** | **<font style=\"color:#000000;\">64</font>** | **<font style=\"color:#000000;\">true</font>** | **<font style=\"color:#000000;\">103.89</font>** |\n| False | 32 | 4 | 512 | 64 | true | OOM |\n\n\nDeepSpeed ZeRO 3 对节点间通信速率的要求远大于 ZeRO 2，在多机微调的情况下会大幅降低训练速度。因此，我们不建议在多机微调的情况下使用 DeepSpeed ZeRO 3 配置。\n\n| 参数配置 | per_device_train_batch_size | per_device_eval_batch_size | model_max_length | gradient_accumulation_steps | fp16 | tflops |\n| --- | --- | --- | --- | --- | --- | --- |\n| 默认 | 2 | 1 | 512 | 8 | true | 31.71 |\n| 调优后 | **<font style=\"color:#DF2A3F;\">32</font>** | **<font style=\"color:#DF2A3F;\">4</font>** | **<font style=\"color:#DF2A3F;\">512</font>** | **<font style=\"color:#DF2A3F;\">64</font>** | **<font style=\"color:#DF2A3F;\">true</font>** | **<font style=\"color:#DF2A3F;\">103.89</font>** |\n\n\n基于2机8卡的V100，Qwen-7B模型的qlora微调训练中（采用deepspeed的zero2的内存优化并行方式），Tflops的值最高为<font style=\"color:#DF2A3F;\">103.89 </font>\n\n默认参数配置\n\n最佳参数配置（gradient_checkpointing：True，per_device_train_batch_size：32，per_device_eval_batch_size：4，model_max_length：512，gradient_accumulation_steps：64，Fp16精度）\n\nqlora在拉起训练时，需要对模型权重进行INT-4量化，会损失掉权重小数部分精度，量化后的权重由于使用更少的位数来表示，存储上比原始权重高效，在拉起模型训练过程过中tflops有明显提升。\n\n","tags":["V100","模型训练调优","Nvidia","Qwen","Deepspeed"],"categories":["模型训练调优","NVIDIA"]},{"title":"Git1_git命令","url":"/2025/11/04/Git/Git1_git 命令/","content":"**<font style=\"background-color:#FBDE28;\">git commmit -v和git push origin develop之后，我怎么撤销本次commit提交的代码，将origin develop的仓库返回值提交之前的状态？</font>**\n\n在Git中，如果你想要撤销最近的一次提交并将仓库状态回退到提交之前，你可以使用`git reset`命令。这里有几种方法可以撤销提交：\n\n1. **使用**`git reset --soft`：\n    - 这个命令会将HEAD指针移动到指定的提交，但是不会改变工作目录和索引的状态。这意味着你的更改仍然在暂存区（staged），你可以重新提交。\n    - 命令：`git reset --soft HEAD~1`（将HEAD指针回退到上一个提交）\n2. **使用**`git reset --mixed`（默认选项）：\n    - 这个命令会将HEAD指针移动到指定的提交，并且将索引（staged changes）重置到HEAD的状态，但是不会改变工作目录。\n    - 命令：`git reset --mixed HEAD~1`（将HEAD和索引回退到上一个提交，但工作目录不变）\n3. **使用**`git reset --hard`：\n    - 这个命令会将HEAD指针、索引和工作目录都重置到指定的提交状态。这意味着所有未提交的更改都会丢失。\n    - 命令：`git reset --hard HEAD~1`（将HEAD、索引和工作目录都回退到上一个提交）\n\n如果你已经将代码推送到了远程仓库（比如`origin develop`），并且想要撤销这次推送，你可以使用`git push`命令配合`--force`选项来强制推送本地的更改到远程仓库。但是请注意，这会覆盖远程仓库的历史，所以只有在你确定远程仓库没有其他人的提交或者你不介意丢失这些提交时才应该使用。\n\n命令如下：\n\n```bash\ngit push origin develop --force\n```\n\n或者，如果你使用的是Git 2.0或更高版本，可以使用更安全的`--force-with-lease`选项，它会在远程分支没有新的提交时才允许强制推送。\n\n命令如下：\n\n```bash\ngit push origin develop --force-with-lease\n```\n\n**警告**：使用`--force`选项会覆盖远程仓库的历史，这是一个破坏性的操作。在团队协作的环境中，这可能会导致其他协作者的工作丢失，所以请谨慎使用。\n\n","tags":["开发","git"],"categories":["开发","git"]},{"title":"Git2_gerrit仓库代码提交流程","url":"/2025/11/04/Git/Git2_gerrit仓库代码提交流程/","content":"1. 生成个人密钥\n```bash\nssh-keygen -C wangxiangbo_JTAGI@cmss.chinamobile.com\n```\n2. 修改config文件，增加gerrit仓库配置，增加刚生成的个人密钥`id_ed25519`\n```bash\nhost gerrit\nport 29418\nhostname gerrit.cmss.com\nuser wangxiangbo_JTAGI\nIdentityFile ~/.ssh/id_ed25519\n```\n3. 在gerrit中setting设置中，增加ssh的公钥，即`id_ed25519.pub`\n4. 设置邮箱，邮箱名`wangxiangbo@cmss.chinamobile.com`为不带JTAGI后缀的邮箱。\n5. 用ssh clone代码仓库\n```bash\ngit clone \"ssh://wangxiangbo_JTAGI@gerrit.cmss.com:29418/AGI/CM_OPTIMUS\" && scp -p -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg \"CM_OPTIMUS/.git/hooks/\"\n```\n6. 查看分支\n```bash\ngit branch -a\n```\n7. 查看远端分支\n```bash\ngit branch -r\n```\n8. 通过远端origin/develop仓库，创建一个本地develop开发分支\n```bash\ngit checkout -b develop origin/develop\n```\n9. 修改代码后，add之后并commit提交\n```bash\ngit commit -v\n```\n10. 推送远端origin/develop仓库\n```bash\ngit push origin HEAD:refs/for/develop\n```\n11. 在推送过程中会出现缺失 `Change-Id `的错误\n```bash\ngitdir=$(git rev-parse --git-dir); scp -p -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg ${gitdir}/hooks/\n```\n执行完毕后如果出现subsystem request failed on channel 0，则将-p修改为-O\n```bash\ngitdir=$(git rev-parse --git-dir); scp -O -P 29418 wangxiangbo_JTAGI@gerrit.cmss.com:hooks/commit-msg ${gitdir}/hooks/\n```\n将本次commit提交的末尾加上Change-Id \n```bash\ngit commit --amend --no-edit\n```\n再次push\n```bash\ngit push origin HEAD:refs/for/develop\n```\n12. 提交完成之后，打开gerrit，找到develop分支的gitweb\n13. 选择review\n14. 找到刚刚提交的代码\n15. 点击add reviewer，评审人要最少要两个以上\n16. 第一次之后提交代码步骤\n```bash\ngit pull origin\ngit add xxx\ngit commit -v \ngit push origin HEAD:refs/for/develop\n```\n\n","tags":["开发","gerrit"],"categories":["开发","git"]},{"title":"Git3_gitlab仓库代码提交流程","url":"/2025/11/04/Git/Git3_gitlab仓库代码提交流程/","content":"1. gitlab上fork主仓库，生成个人的远端仓库origin/develop\n2. 修改git提交用户配置为九天账号\n```bash\ngit config --global --list\ngit config --global user.name wangxiangbo_JTAGI\ngit config --global user.email wangxiangbo_JTAGI@cmss.chinamobile.com\n```\n3. 拉取个人的远端仓库\n```bash\ngit clone http://gitlab.cmss.com/wangxiangbo/CM_OPTIMUS.git \n```\n4. 查看分支\n```bash\ngit branch -a\n```\n5. 查看远程仓库分支\n```bash\ngit branch -r\n```\n6. 通过个人的远端origin/develop仓库，创建一个本地develop开发分支\n```bash\ngit checkout -b develop origin/develop\n```\n7. <font style=\"color:rgba(0, 0, 0, 0.85);\">列出所有的远程仓库以及对应的 URL</font>\n```bash\ngit remote -v\n```\n8. <font style=\"color:rgba(0, 0, 0, 0.85);\">将远程仓库地址添加到本地Git仓库的远程仓库列表中，本地的upstream/develop仓库会和gitlab远程仓库关联起来</font>\n```bash\ngit remote add upstream http://gitlab.cmss.com/AGI/CM_OPTIMUS.git\n```\n8. upstream/develop远端仓库拉取最新的代码\n```bash\ngit fetch upstream\n```\n9. 将upstream/develop远端仓库的最新代码合并到本地的develop分支中\n```bash\ngit merge upstream/develop\n```\n10. 将本地develop最新的代码推到个人的远端仓库origin/develop中\n```bash\ngit push origin develop\n```\n11. <font style=\"background-color:#FBDE28;\">提交develop到origin/develop代码之前，保证自己的本地个人的远端仓库origin/develop和本地gitlab的远端仓库upstream/develop保持一致再提交</font>，防止后续origin/develop合并打upstream/develop上出现冲突。\n```bash\n#本地代码修改后拉取最新代码\n#1.拉取源代码\ngit fetch upstream \n#2.暂存本地修改的代码\ngit stash\n#3.合并源代码到当前的develop仓库\ngit merge upstream/develop\n#4.将本地修改的推送到远程的origin分支，使origin/develop和upstream/develop保持一致\ngit push origin develop\n#5.将暂存的代码弹出来\ngit stash pop\n```\n12. souretree上add需要提交的代码\n13. 提交add之后的代码\n```bash\ngit commit -v\n```\n```bash\n#修改下方的提交信息，第一行为提交描述信息\ndeepseek-7B预训练\n\nCode Source From: Self Code\nDescription: deepseek-7B预训练\nJira: #CMOPTIMUS-1089\n市场项目编号（名称）：CM_OPTIMUS\n```\n14. <font style=\"color:rgb(55, 53, 47);\">将提交完之后的代码合并到origin/develop上</font>\n```bash\ngit push origin develop\n```\n15. <font style=\"color:rgb(55, 53, 47);\">gitlab中找的到自己仓库下CM_OPTIMUS</font>\n16. <font style=\"color:rgb(55, 53, 47);\">找到Merge requests，然后点击开始一个合并</font>\n17. <font style=\"color:rgb(55, 53, 47);\">然后选择自己的分支为develop，还有旁边的分支develop，然后点击最下方的merge即可。该操作是将自己个人远端的orgin/develop仓库提交修改后的代码合并到gitlab远端的主仓库中（upstream/develop已和其关联，并通过git fetch保持最新）</font>\n选中自己刚刚提交到origin/develop上的代码，合并到AGI/CM_OPTIMUS的develop分支上\n在Write里添加合并人信息\n\n\n\n","tags":["开发","gitlab"],"categories":["开发","git"]},{"title":"Linux1_kimi问题","url":"/2025/11/04/Linux/Linux1_kimi问题/","content":"1. 权重转换代码换行符\nKimi: 出现这些错误通常是因为脚本文件在不同的操作系统环境下被编辑或创建，导致文件的换行符不一致。Unix/Linux系统使用LF（Line Feed，`\\n`）作为换行符，而Windows系统使用CR+LF（Carriage Return + Line Feed，`\\r\\n`）作为换行符。\n错误信息中的 `$'\\r': command not found` 表示脚本尝试执行一个以 `\\r` 开头的命令，这通常是因为脚本文件中的换行符是 `\\r\\n` 而不是 `\\n`。\n此外，`syntax error near unexpected token 'elif'` 错误表明脚本解析器在遇到 `elif` 时遇到了语法错误，这可能是因为 `elif` 之前的行（可能是第59行）以 `\\r\\n` 结尾，导致 `elif` 行实际上是从一个新的行开始，而不是在预期的位置。\n要解决这个问题，你可以使用 `dos2unix` 工具将脚本文件从Windows格式转换为Unix/Linux格式。如果你没有 `dos2unix` 工具，可以使用 `sed` 命令来移除 `\\r` 字符：\n```bash\nsed -i 's/\\r$//' hf2mcore_convertor.sh\n```\n这条命令会查找文件中的所有行，并移除每行末尾的 `\\r` 字符。\n如果你有 `dos2unix` 工具，可以使用以下命令：\n```bash\ndos2unix hf2mcore_convertor.sh\n```\n这将把文件从Windows格式转换为Unix/Linux格式。\n在运行这些命令之后，再次执行脚本应该就不会出现这些错误了。\n2. 修改文件遇到Read-only file system\nKimi: 当你遇到 \"Read-only file system\" 错误时，通常意味着你的文件系统出于某种原因被挂载为只读模式。这可能是由于文件系统损坏、磁盘硬件问题、或者挂载时指定了错误的参数。以下是一些解决步骤：\n1. **检查文件系统挂载状态**：  \n使用 `mount` 命令查看所有挂载的文件系统及其状态。你可以使用如下命令：\n```bash\nmount | grep /path/to/mount/point\n```\n查看挂载点是否为只读（ro）。\n2. **重新挂载文件系统为读写模式**：  \n如果文件系统被挂载为只读，你可以尝试重新挂载它为读写模式。使用以下命令：\n```bash\nsudo mount -o remount,rw /path/to/mount/point\n```\n请将 `/path/to/mount/point` 替换为实际的挂载点路径。\n3. **检查和修复文件系统**：  \n如果重新挂载不起作用，可能是文件系统损坏。你可以使用 `fsck` 命令来检查和修复文件系统。例如，对于 ext4 文件系统，可以使用：\n```bash\nsudo fsck -f /dev/sda1\n```\n请确保替换 `/dev/sda1` 为你的实际分区。\n\n","tags":["运维","Linux"],"categories":["Linux"]},{"title":"运维1_docker命令","url":"/2025/11/04/服务器运维/运维1_docker命令/","content":"1. 启动一个容器\n```bash\nsudo docker run -it --name qwen --gpus all nvidia/cuda:12.2.2-cudnn8-devel-ubuntu22.04\n```\n启动一个容器后，又通过exit关闭。发现docker ps没有了，该如何重新启动这个qwen容器？\n```bash\ndocker ps -a\n\ndocker start qwen\n\ndocker exec -it 容器id bash\n```\n\n2. 拉取一个新镜像后，通过这个镜像创建一个容器。\n```bash\ndocker run -it --name qwen --gpus all 镜像名:镜像tags bash\n```\n\n3. 将此时qwen容器打成镜像\n```bash\ndocker commit -a \"wangxiangbo\" -m \"qwen 7B\" 02649afd9710 qwen-7b:v1.0\n```\n\n\n\n\n","tags":["运维","docker"],"categories":["运维","docker"]},{"title":"Linux2_Linux命令","url":"/2025/11/04/Linux/Linux2_linux命令/","content":"1. 当执行训练任务时，手动关闭训练，此时gup资源仍然占用，需要手动kill掉进程\n查询正在运行的进程并gerp finetune_lora_single_gpu.sh\n`ps aux | grep finetune_lora_single_gpu.sh`\n`ps aux | grep finetune_lora_ds.sh`\n`kill -9 372813` 根据上一步查出来的进程号，用kill -9强制删除\n`nvidia-smi `查看是否gpu显存已经清空\n2. 查看某个文件的前20行\n`head -n 20 train_data_law.json`\n3. 查看当前文件夹的大小\n`du -sh .`\n4. 查看当前文件夹挂载的是哪一个盘\n`df -h .`\n从大到小查看当前文件夹的下的文件大小\n`du -sh * | sort -rh`\n4. 查看当前文件下所有文件的大小\n`ls -lh`\n5. <font style=\"color:#000000;\">把一个名为Yuan2的文件夹的所有内容（包括该文件夹名），cp到/mnt/users/wangxiangbo/nemo/model目录下，使最后的目录为 /mnt/users/wangxiangbo/nemo/model/Yuan2</font>\n`cp -a Yuan2 /mnt/users/wangxiangbo/nemo/model/`\n6. 通过pid查看某个进程的详细信息\n`<font style=\"color:rgb(56, 58, 66);background-color:rgb(250, 250, 250);\">ps -fp 167891</font>`\n7. 查看当前目录下所有文件的磁盘占用情况\n`du -ah | sort -hr | head -n 20`\n8.  使用以下命令将`megatron-core`文件夹压缩成一个名为`megatron-core.zip`的zip文件\n \t` zip -r megatron-core.zip megatron-core  `\n9.  使用以下命令将`megatron-core.zip`解压缩  \n` unzip megatron-core.zip  `\n11. hg上面下载模型\n```bash\npip install -U huggingface_hub\nhuggingface-cli download bigscience/bloom-560m --local-dir bloom-560m\nhuggingface-cli download Qwen/Qwen2-7B-Instruct --local-dir Qwen2-7B-Instruct\nhuggingface-cli download Qwen/Qwen2.5-7B-Instruct --local-dir Qwen2.5-7B-Instruct\nhuggingface-cli download BAAI/IndustryCorpus_computer --repo-type dataset --local-dir IndustryCorpus_computer\nhuggingface-cli download BAAI/IndustryCorpus2_current_affairs_government_administration --repo-type dataset --local-dir government_administration\nhuggingface-cli download ShengbinYue/DISC-Law-SFT --repo-type dataset --local-dir DISC-Law-SFT\nhuggingface-cli download TigerResearch/sft_zh\n```\n12. 归档压缩文件\n```bash\n#-c：创建一个新的压缩文件。\n#-z：通过 gzip 压缩文件。\n#-v：显示详细的压缩过程。\n#-f：指定压缩后的文件名，这里是 colossalai.tar.gz\ntar -czvf colossalai.tar.gz colossalai/  \n\n#-x：表示解压。\n#-z：表示解压 .gz 格式的文件。\n#-v：显示解压过程。\n#-f：指定解压的文件\ntar -xzvf file.tar.gz\n```\n\n\n\n","tags":["运维","Linux"],"categories":["Linux"]},{"title":"运维2_k8s命令","url":"/2025/11/04/服务器运维/运维2_k8s命令/","content":"1. 获取特定命名空间的详细信息\n```bash\nkubectl get ns namespace-name -o wide\n```\n2. kubectl通过ns获取所有的pods的详细信息\n```bash\nkubectl get pods -n <ns_name> -o wide\n```\n3. 删除启动失败的pod，可以直接删除启动时的yaml，否则pod会自动重启\n```bash\nkubectl delete -f qwentest.yaml\n```\n4. 查看所有的node的lable标签信息\n```bash\nkubectl get node --show-labels\n\nkubectl get node xxx --show-labels\n\nkubectl get node --show-labels | grep model\n```\n5. 打标签\n```bash\nkubectl label nodes ecs-jhjs-1234-003 key=vaule\n```\n6. 删除某节点的lable标签\n```bash\nkubectl label nodes cce100-64-29-79.cce-stack.com model-\n```\n6. 创建一个命名空间namespace\n```bash\nkubectl create namespace xxx\n```\n7. 查看所有节点的标签\n```bash\nkubectl get nodes --show-labels\nkubectl get nodes --show-labels | grep model=llama2-70b\n```\n","tags":["运维","k8s"],"categories":["运维","k8s"]},{"title":"测试 Hexo 标签分类2","url":"/2025/10/31/页面测试/test copy/","content":"这是文章正文。","tags":["测试2","hexo"],"categories":["测试"]}]